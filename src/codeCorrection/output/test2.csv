Page;Username;Repo;Commit;Bug;Code
1;home-assistant;core;d3f01f7ea92e3a3866db4bc4308374fe131630f3;Reduce memory pressure from history_stats with large data sets (#73289);"@@ -23,6 +23,14 @@ class HistoryStatsState:
     period: tuple[datetime.datetime, datetime.datetime]
 
 
+@dataclass
+class HistoryState:
+    """"""A minimal state to avoid holding on to State objects.""""""
+
+    state: str
+    last_changed: float
+
+
 class HistoryStats:
     """"""Manage history stats.""""""
 
@@ -40,7 +48,7 @@ def __init__(
         self.entity_id = entity_id
         self._period = (MIN_TIME_UTC, MIN_TIME_UTC)
         self._state: HistoryStatsState = HistoryStatsState(None, None, self._period)
-        self._history_current_period: list[State] = []
+        self._history_current_period: list[HistoryState] = []
         self._previous_run_before_start = False
         self._entity_states = set(entity_states)
         self._duration = duration
@@ -103,20 +111,18 @@ async def async_update(self, event: Event | None) -> HistoryStatsState:
                     <= floored_timestamp(new_state.last_changed)
                     <= current_period_end_timestamp
                 ):
-                    self._history_current_period.append(new_state)
+                    self._history_current_period.append(
+                        HistoryState(
+                            new_state.state, new_state.last_changed.timestamp()
+                        )
+                    )
                     new_data = True
             if not new_data and current_period_end_timestamp < now_timestamp:
                 # If period has not changed and current time after the period end...
                 # Don't compute anything as the value cannot have changed
                 return self._state
         else:
-            self._history_current_period = await get_instance(
-                self.hass
-            ).async_add_executor_job(
-                self._update_from_database,
-                current_period_start,
-                current_period_end,
-            )
+            await self._async_history_from_db(current_period_start, current_period_end)
             self._previous_run_before_start = False
 
         hours_matched, match_count = self._async_compute_hours_and_changes(
@@ -127,7 +133,24 @@ async def async_update(self, event: Event | None) -> HistoryStatsState:
         self._state = HistoryStatsState(hours_matched, match_count, self._period)
         return self._state
 
-    def _update_from_database(
+    async def _async_history_from_db(
+        self,
+        current_period_start: datetime.datetime,
+        current_period_end: datetime.datetime,
+    ) -> None:
+        """"""Update history data for the current period from the database.""""""
+        instance = get_instance(self.hass)
+        states = await instance.async_add_executor_job(
+            self._state_changes_during_period,
+            current_period_start,
+            current_period_end,
+        )
+        self._history_current_period = [
+            HistoryState(state.state, state.last_changed.timestamp())
+            for state in states
+        ]
+
+    def _state_changes_during_period(
         self, start: datetime.datetime, end: datetime.datetime
     ) -> list[State]:
         return history.state_changes_during_period(
@@ -155,9 +178,9 @@ def _async_compute_hours_and_changes(
         match_count = 1 if previous_state_matches else 0
 
         # Make calculations
-        for item in self._history_current_period:
-            current_state_matches = item.state in self._entity_states
-            state_change_timestamp = item.last_changed.timestamp()
+        for history_state in self._history_current_period:
+            current_state_matches = history_state.state in self._entity_states
+            state_change_timestamp = history_state.last_changed
 
             if previous_state_matches:
                 elapsed += state_change_timestamp - last_state_change_timestamp"
1;CorentinJ;Real-Time-Voice-Cloning;ded7b37234e229d9bde0a9a506f7c65605803731;"Low memory inference fix (#536)

* For low_mem, use spawned workers instead of forked workers (resolves #36)
Used implementation from @lilydjwg: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/36#issuecomment-529380190

* Different method of passing the seed for low_mem inference
Resolves #491, #529, #535";"@@ -2,6 +2,7 @@
 from synthesizer.hparams import hparams
 from multiprocess.pool import Pool  # You're free to use either one
 #from multiprocessing import Pool   # 
+from multiprocess.context import SpawnContext
 from synthesizer import audio
 from pathlib import Path
 from typing import Union, List
@@ -97,16 +98,16 @@ def synthesize_spectrograms(self, texts: List[str],
             # Low memory inference mode: load the model upon every request. The model has to be 
             # loaded in a separate process to be able to release GPU memory (a simple workaround 
             # to tensorflow's intricacies)
-            specs, alignments = Pool(1).starmap(Synthesizer._one_shot_synthesize_spectrograms, 
-                                                [(self.checkpoint_fpath, embeddings, texts)])[0]
+            specs, alignments = Pool(1, context=SpawnContext()).starmap(Synthesizer._one_shot_synthesize_spectrograms,
+                                                [(self.checkpoint_fpath, embeddings, texts, self._seed)])[0]
     
         return (specs, alignments) if return_alignments else specs
 
     @staticmethod
-    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts):
+    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts, seed):
         # Load the model and forward the inputs
         tf.compat.v1.reset_default_graph()
-        model = Tacotron2(checkpoint_fpath, hparams, seed=self._seed)
+        model = Tacotron2(checkpoint_fpath, hparams, seed=seed)
         specs, alignments = model.my_synthesize(embeddings, texts)
         
         # Detach the outputs (not doing so will cause the process to hang)"
1;ultralytics;yolov5;6e4661773e08aee5e9673f68aacc1817b8a6fca9;"AutoBatch checks against failed solutions (#8159)

* AutoBatch checks against failed solutions

@kalenmike this is a simple improvement to AutoBatch to verify that returned solutions have not already failed, i.e. return batch-size 8 when 8 already produced CUDA out of memory.

This is a halfway fix until I can implement a 'final solution' that will actively verify the solved-for batch size rather than passively assume it works.

* Update autobatch.py

* Update autobatch.py";"@@ -8,7 +8,7 @@
 import numpy as np
 import torch
 
-from utils.general import LOGGER, colorstr
+from utils.general import LOGGER, colorstr, emojis
 from utils.torch_utils import profile
 
 
@@ -26,32 +26,41 @@ def autobatch(model, imgsz=640, fraction=0.9, batch_size=16):
     #     model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)
     #     print(autobatch(model))
 
+    # Check device
     prefix = colorstr('AutoBatch: ')
     LOGGER.info(f'{prefix}Computing optimal batch size for --imgsz {imgsz}')
     device = next(model.parameters()).device  # get model device
     if device.type == 'cpu':
         LOGGER.info(f'{prefix}CUDA not detected, using default CPU batch-size {batch_size}')
         return batch_size
 
+    # Inspect CUDA memory
     gb = 1 << 30  # bytes to GiB (1024 ** 3)
     d = str(device).upper()  # 'CUDA:0'
     properties = torch.cuda.get_device_properties(device)  # device properties
-    t = properties.total_memory / gb  # (GiB)
-    r = torch.cuda.memory_reserved(device) / gb  # (GiB)
-    a = torch.cuda.memory_allocated(device) / gb  # (GiB)
-    f = t - (r + a)  # free inside reserved
+    t = properties.total_memory / gb  # GiB total
+    r = torch.cuda.memory_reserved(device) / gb  # GiB reserved
+    a = torch.cuda.memory_allocated(device) / gb  # GiB allocated
+    f = t - (r + a)  # GiB free
     LOGGER.info(f'{prefix}{d} ({properties.name}) {t:.2f}G total, {r:.2f}G reserved, {a:.2f}G allocated, {f:.2f}G free')
 
+    # Profile batch sizes
     batch_sizes = [1, 2, 4, 8, 16]
     try:
         img = [torch.zeros(b, 3, imgsz, imgsz) for b in batch_sizes]
-        y = profile(img, model, n=3, device=device)
+        results = profile(img, model, n=3, device=device)
     except Exception as e:
         LOGGER.warning(f'{prefix}{e}')
 
-    y = [x[2] for x in y if x]  # memory [2]
-    batch_sizes = batch_sizes[:len(y)]
-    p = np.polyfit(batch_sizes, y, deg=1)  # first degree polynomial fit
+    # Fit a solution
+    y = [x[2] for x in results if x]  # memory [2]
+    p = np.polyfit(batch_sizes[:len(y)], y, deg=1)  # first degree polynomial fit
     b = int((f * fraction - p[1]) / p[0])  # y intercept (optimal batch size)
-    LOGGER.info(f'{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%)')
+    if None in results:  # some sizes failed
+        i = results.index(None)  # first fail index
+        if b >= batch_sizes[i]:  # y intercept above failure point
+            b = batch_sizes[max(i - 1, 0)]  # select prior safe point
+
+    fraction = np.polyval(p, b) / t  # actual fraction predicted
+    LOGGER.info(emojis(f'{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%) ✅'))
     return b"
3;facebookresearch;fairseq;eb2d7862c29990e5be35ee227a6952ae21d621a1;"fix ema memory leak (#3384)

Summary:
fixes memory leak in ema module by making sure the update happens in no_grad regime

X-link: https://github.com/fairinternal/fairseq-py/pull/3384

Reviewed By: arbabu123

Differential Revision: D36352890

Pulled By: alexeib

fbshipit-source-id: 0f3575ac356a13483e00ed431375b2c798621a3a";"@@ -98,7 +98,7 @@ def _step_internal(self, new_model):
         ema_params = (
             self.fp32_params if self.config.ema_fp32 else self.model.state_dict()
         )
-        for key, param in new_model.state_dict().items():
+        for key, param in new_model.named_parameters():
             if isinstance(param, dict):
                 continue
             try:
@@ -107,6 +107,7 @@ def _step_internal(self, new_model):
                 ema_param = (
                     param.float().clone() if param.ndim == 1 else copy.deepcopy(param)
                 )
+                ema_params[key] = ema_param
 
             if param.shape != ema_param.shape:
                 raise ValueError(
@@ -118,15 +119,21 @@ def _step_internal(self, new_model):
                 # Do not decay a model.version pytorch param
                 continue
 
-            if key in self.skip_keys:
-                ema_param = param.to(dtype=ema_param.dtype).clone()
-                ema_params[key].copy_(ema_param)
+            if key in self.skip_keys or not param.requires_grad:
+                ema_params[key].copy_(param.to(dtype=ema_param.dtype).data)
+                ema_param = ema_params[key]
             else:
                 ema_param.mul_(decay)
-                ema_param.add_(param.to(dtype=ema_param.dtype), alpha=1 - decay)
+                ema_param.add_(param.data.to(dtype=ema_param.dtype), alpha=1 - decay)
+
             ema_state_dict[key] = ema_param
+
+        for key, param in new_model.named_buffers():
+            ema_state_dict[key] = param
+
         self.restore(ema_state_dict, build_fp32_params=False)
 
+    @torch.no_grad()
     def step(self, new_model):
         self._step_internal(new_model)
 "
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -279,6 +279,17 @@ def truncate_emb(key):
                     logger.info(""Overwriting "" + prefix + ""classification_heads."" + k)
                     state_dict[prefix + ""classification_heads."" + k] = v
 
+    def set_beam_size(self, beam):
+        """"""Set beam size for efficient beamable enc-dec attention.""""""
+        beamable = False
+        for layer in self.decoder.layers:
+            if layer.encoder_attn is not None:
+                if hasattr(layer.encoder_attn, ""set_beam_size""):
+                    layer.encoder_attn.set_beam_size(beam)
+                    beamable = True
+        if beamable:
+            self.encoder.reorder_encoder_out = self.encoder._reorder_encoder_out
+
 
 class BARTClassificationHead(nn.Module):
     """"""Head for sentence-level classification tasks."""""""
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -287,9 +287,6 @@ def extract_features_scriptable(
         padding_mask: Optional[Tensor] = None
         if encoder_out is not None and len(encoder_out[""encoder_out""]) > 0:
             enc = encoder_out[""encoder_out""][0]
-            assert (
-                enc.size()[1] == bs
-            ), f""Expected enc.shape == (t, {bs}, c) got {enc.shape}""
         if encoder_out is not None and len(encoder_out[""encoder_padding_mask""]) > 0:
             padding_mask = encoder_out[""encoder_padding_mask""][0]
 "
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -313,6 +313,11 @@ def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
             ""src_lengths"": src_lengths,  # B x 1
         }
 
+    @torch.jit.export
+    def _reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
+        """"""Dummy re-order function for beamable enc-dec attention""""""
+        return encoder_out
+
     def max_positions(self):
         """"""Maximum input length supported by the encoder.""""""
         if self.embed_positions is None:"
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -84,7 +84,7 @@ def __init__(
             self.bias_k = self.bias_v = None
 
         self.add_zero_attn = add_zero_attn
-
+        self.beam_size = 1
         self.reset_parameters()
 
         self.onnx_trace = False
@@ -286,9 +286,8 @@ def forward(
         if key is not None:
             src_len, key_bsz, _ = key.size()
             if not torch.jit.is_scripting():
-                assert key_bsz == bsz
                 assert value is not None
-                assert src_len, bsz == value.shape[:2]
+                assert src_len, key_bsz == value.shape[:2]
 
         if (
             not self.onnx_trace
@@ -351,6 +350,11 @@ def forward(
                 assert value is None
                 k = v = None
             else:
+                if self.beam_size > 1 and bsz == key.size(1):
+                    # key is [T, bsz*beam_size, C], reduce to [T, bsz, C]
+                    key = key.view(key.size(0), -1, self.beam_size, key.size(2))[:, :, 0, :]
+                    if key_padding_mask is not None:
+                        key_padding_mask = key_padding_mask.view(-1, self.beam_size, key_padding_mask.size(1))[:, 0, :]
                 k = self.k_proj(key)
                 v = self.v_proj(key)
 
@@ -383,16 +387,18 @@ def forward(
             .view(tgt_len, bsz * self.num_heads, self.head_dim)
             .transpose(0, 1)
         )
+        kv_bsz = bsz  # need default value for scripting
         if k is not None:
+            kv_bsz = k.size(1)
             k = (
                 k.contiguous()
-                .view(-1, bsz * self.num_heads, self.head_dim)
+                .view(-1, kv_bsz * self.num_heads, self.head_dim)
                 .transpose(0, 1)
             )
         if v is not None:
             v = (
                 v.contiguous()
-                .view(-1, bsz * self.num_heads, self.head_dim)
+                .view(-1, kv_bsz * self.num_heads, self.head_dim)
                 .transpose(0, 1)
             )
 
@@ -401,7 +407,8 @@ def forward(
             if ""prev_key"" in saved_state:
                 _prev_key = saved_state[""prev_key""]
                 assert _prev_key is not None
-                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)
+                kv_bsz = _prev_key.size(0)
+                prev_key = _prev_key.view(kv_bsz * self.num_heads, -1, self.head_dim)
                 if static_kv:
                     k = prev_key
                 else:
@@ -411,7 +418,8 @@ def forward(
             if ""prev_value"" in saved_state:
                 _prev_value = saved_state[""prev_value""]
                 assert _prev_value is not None
-                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)
+                assert kv_bsz == _prev_value.size(0)
+                prev_value = _prev_value.view(kv_bsz * self.num_heads, -1, self.head_dim)
                 if static_kv:
                     v = prev_value
                 else:
@@ -424,13 +432,13 @@ def forward(
             key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(
                 key_padding_mask=key_padding_mask,
                 prev_key_padding_mask=prev_key_padding_mask,
-                batch_size=bsz,
+                batch_size=kv_bsz,
                 src_len=k.size(1),
                 static_kv=static_kv,
             )
 
-            saved_state[""prev_key""] = k.view(bsz, self.num_heads, -1, self.head_dim)
-            saved_state[""prev_value""] = v.view(bsz, self.num_heads, -1, self.head_dim)
+            saved_state[""prev_key""] = k.view(kv_bsz, self.num_heads, -1, self.head_dim)
+            saved_state[""prev_value""] = v.view(kv_bsz, self.num_heads, -1, self.head_dim)
             saved_state[""prev_key_padding_mask""] = key_padding_mask
             # In this branch incremental_state is never None
             assert incremental_state is not None
@@ -444,7 +452,7 @@ def forward(
             key_padding_mask = None
 
         if key_padding_mask is not None:
-            assert key_padding_mask.size(0) == bsz
+            assert key_padding_mask.size(0) == kv_bsz
             assert key_padding_mask.size(1) == src_len
 
         if self.add_zero_attn:
@@ -467,7 +475,15 @@ def forward(
                     dim=1,
                 )
 
-        attn_weights = torch.bmm(q, k.transpose(1, 2))
+        if self.encoder_decoder_attention and bsz != kv_bsz:
+            attn_weights = torch.einsum(
+                ""bxhtd,bhsd->bxhts"",
+                q.view((kv_bsz, -1, self.num_heads) + q.size()[1:]),
+                k.view((kv_bsz, self.num_heads) + k.size()[1:]),
+            )
+            attn_weights = attn_weights.reshape((-1,) + attn_weights.size()[-2:])
+        else:
+            attn_weights = torch.bmm(q, k.transpose(1, 2))
         attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
 
         assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
@@ -482,8 +498,12 @@ def forward(
             # don't attend to padding symbols
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
             if not is_tpu:
+                attn_weights = attn_weights.view(kv_bsz, -1, self.num_heads, tgt_len, src_len)
                 attn_weights = attn_weights.masked_fill(
-                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),
+                    key_padding_mask.unsqueeze(1)
+                    .unsqueeze(2)
+                    .unsqueeze(3)
+                    .to(torch.bool),
                     float(""-inf""),
                 )
             else:
@@ -502,7 +522,15 @@ def forward(
         attn_probs = self.dropout_module(attn_weights)
 
         assert v is not None
-        attn = torch.bmm(attn_probs, v)
+        if self.encoder_decoder_attention and bsz != kv_bsz:
+            attn = torch.einsum(
+                ""bxhts,bhsd->bxhtd"",
+                attn_probs.view((kv_bsz, -1, self.num_heads,) + attn_probs.size()[1:]),
+                v.view((kv_bsz, self.num_heads,) + v.size()[1:]),
+            )
+            attn = attn.reshape((-1,) + attn.size()[-2:])
+        else:
+            attn = torch.bmm(attn_probs, v)
         assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
         if self.onnx_trace and attn.size(1) == 1:
             # when ONNX tracing a single decoder step (sequence length == 1)
@@ -578,14 +606,22 @@ def reorder_incremental_state(
             for k in input_buffer.keys():
                 input_buffer_k = input_buffer[k]
                 if input_buffer_k is not None:
-                    if self.encoder_decoder_attention and input_buffer_k.size(
-                        0
-                    ) == new_order.size(0):
-                        break
-                    input_buffer[k] = input_buffer_k.index_select(0, new_order)
+                    if self.encoder_decoder_attention:
+                        if input_buffer_k.size(0) * self.beam_size == new_order.size(0):
+                            return incremental_state
+                        elif self.beam_size > 1:
+                            input_buffer[k] = input_buffer_k.index_select(0, new_order.reshape(-1, self.beam_size)[:, 0] // self.beam_size)
+                        else:
+                            input_buffer[k] = input_buffer_k.index_select(0, new_order)
+                    else:
+                        input_buffer[k] = input_buffer_k.index_select(0, new_order)
             incremental_state = self._set_input_buffer(incremental_state, input_buffer)
         return incremental_state
 
+    def set_beam_size(self, beam_size):
+        """"""Used for effiecient beamable enc-dec attention""""""
+        self.beam_size = beam_size
+
     def _get_input_buffer(
         self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]
     ) -> Dict[str, Optional[Tensor]]:"
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -80,6 +80,7 @@ def __init__(
         self.beam_size = beam_size
         # the max beam size is the dictionary size - 1, since we never select pad
         self.beam_size = min(beam_size, self.vocab_size - 1)
+        self.model.set_decoder_beam_size(self.beam_size)
         self.max_len_a = max_len_a
         self.max_len_b = max_len_b
         self.min_len = min_len
@@ -768,6 +769,13 @@ def max_decoder_positions(self):
             + [sys.maxsize]
         )
 
+    def set_decoder_beam_size(self, beam_size):
+        """"""Set beam size for efficient beamable enc-dec attention.""""""
+        if beam_size > 1:
+            for model in self.models:
+                if hasattr(model, 'set_beam_size'):
+                    model.set_beam_size(beam_size)
+
     @torch.jit.export
     def forward_encoder(self, net_input: Dict[str, Tensor]):
         if not self.has_encoder():"
4;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"@@ -0,0 +1 @@
+Fix a crash with SIGBUS when the file system fills up while ``memray run`` is writing a capture file."
4;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"@@ -136,19 +136,15 @@ FileSink::grow(size_t needed)
     new_size = (new_size / 4096 + 1) * 4096;
     assert(new_size > d_fileSize);  // check for overflow
 
-    // Seek to 1 byte before the new size
-    off_t offset = lseek(d_fd, new_size - 1, SEEK_SET);
-    if (offset == -1) {
-        return false;
-    }
-
-    // Then write 1 byte.
-    ssize_t rc;
+    off_t delta = new_size - d_fileSize;
+    int rc;
     do {
-        rc = write(d_fd, ""\0"", 1);
-    } while (rc < 0 && errno == EINTR);
+        // posix_fallocate returns an error number instead of setting errno
+        rc = posix_fallocate(d_fd, d_fileSize, delta);
+    } while (rc == EINTR);
 
-    if (rc < 0) {
+    if (rc != 0) {
+        errno = rc;
         return false;
     }
 "
5;pjialin;py12306;390fcecdd24b9786519ff3eb99c40488b754769d;"Merge pull request #317 from out0fmemory/master

增加时间判断逻辑";"@@ -117,8 +117,8 @@ QUERY_JOBS = [
         # 'job_name':  'bj -> sz',  # 任务名称，不填默认会以车站名命名，不可重复
         'account_key': 0,  # 将会使用指定账号下单
         'left_dates': [  # 出发日期 :Array
-            ""2019-01-25"",
-            ""2019-01-26"",
+            ""2020-01-25"",
+            ""2020-01-26"",
         ],
         'stations': {  # 车站 支持多个车站同时查询  :Dict or :List
             'left': '北京',"
5;pjialin;py12306;390fcecdd24b9786519ff3eb99c40488b754769d;"Merge pull request #317 from out0fmemory/master

增加时间判断逻辑";"@@ -1,5 +1,6 @@
 import sys
 from datetime import timedelta
+from datetime import datetime
 
 from py12306.app import app_available_check
 from py12306.cluster.cluster import Cluster
@@ -66,6 +67,8 @@ class Job:
     INDEX_LEFT_TIME = 8
     INDEX_ARRIVE_TIME = 9
 
+    max_buy_time = 32
+
     def __init__(self, info, query):
         self.cluster = Cluster()
         self.query = query
@@ -136,11 +139,29 @@ def start(self):
                 QueryLog.add_log('\n').flush(sep='\t\t', publish=False)
             if Const.IS_TEST: return
 
+    def judge_date_legal(self, date):
+        date_now = datetime.datetime.now()
+        date_query = datetime.datetime.strptime(str(date), ""%Y-%m-%d"")
+        diff = (date_query - date_now).days
+        if date_now.day == date_query.day:
+            diff = 0
+        if diff < 0:
+            msg = '乘车日期错误，比当前时间还早！！'
+            QueryLog.add_quick_log(msg).flush(publish=False)
+            raise RuntimeError(msg)
+        elif diff > self.max_buy_time:
+            msg = '乘车日期错误，超出一个月预售期！！'
+            QueryLog.add_quick_log(msg).flush(publish=False)
+            raise RuntimeError(msg)
+        else:
+            pass
+
     def query_by_date(self, date):
         """"""
         通过日期进行查询
         :return:
         """"""
+        self.judge_date_legal(date)
         from py12306.helpers.cdn import Cdn
         QueryLog.add_log(('\n' if not is_main_thread() else '') + QueryLog.MESSAGE_QUERY_START_BY_DATE.format(date,
                                                                                                               self.left_station,"
5;pjialin;py12306;998b387875d5620e734285e8c11250e03c661a8a;"Merge pull request #288 from out0fmemory/master

修改人员的注释，存在不注意情况下有多余的人员信息";"@@ -132,7 +132,7 @@ QUERY_JOBS = [
         # }],
         'members': [  # 乘客姓名，会根据当前账号自动识别乘客类型 购买儿童票 设置两个相同的姓名即可，程序会自动识别 如  ['张三', '张三']
             ""张三"",
-            ""*王五"", #在姓名前加*表示学生购买成人票
+            #""*王五"", #在姓名前加*表示学生购买成人票
             # 7,  # 支持通过序号确定唯一乘客，序号查看可通过  python main.py -t 登录成功之后在 runtime/user/ 下找到对应的 用户名_passengers.json 文件，找到对应的 code 填入
         ],
         'allow_less_member': 0,  # 是否允许余票不足时提交部分乘客"
5;alexjc;neural-enhance;904071d9936fe6914f636ef81bf74ce82e0b25d0;Fix memory error in Docker by changing default tile size. Closes #74.;"@@ -37,8 +37,8 @@
 add_arg = parser.add_argument
 add_arg('files',                nargs='*', default=[])
 add_arg('--zoom',               default=2, type=int,                help='Resolution increase factor for inference.')
-add_arg('--rendering-tile',     default=128, type=int,              help='Size of tiles used for rendering images.')
-add_arg('--rendering-overlap',  default=32, type=int,               help='Number of pixels padding around each tile.')
+add_arg('--rendering-tile',     default=80, type=int,               help='Size of tiles used for rendering images.')
+add_arg('--rendering-overlap',  default=24, type=int,               help='Number of pixels padding around each tile.')
 add_arg('--rendering-histogram',default=False, action='store_true', help='Match color histogram of output to input.')
 add_arg('--type',               default='photo', type=str,          help='Name of the neural network to load/save.')
 add_arg('--model',              default='default', type=str,        help='Specific trained version of the model.')"
5;frappe;erpnext;eb53a9727d2e465f74597d987f3f82b9f0530d7c;"perf: commit GL reposting periodically

If you have a huge list of docs to repost then maintaining transaction
throughtout entire GL reposting is not only unnecessary but also creates
performance issues. Periodically commiting the changes prevents lost
progress and reduces memory usage.";"@@ -1145,7 +1145,7 @@ def _delete_gl_entries(voucher_type, voucher_no):
 	precision = get_field_precision(frappe.get_meta(""GL Entry"").get_field(""debit"")) or 2
 
 	gle = get_voucherwise_gl_entries(stock_vouchers, posting_date)
-	for voucher_type, voucher_no in stock_vouchers:
+	for idx, (voucher_type, voucher_no) in enumerate(stock_vouchers):
 		existing_gle = gle.get((voucher_type, voucher_no), [])
 		voucher_obj = frappe.get_doc(voucher_type, voucher_no)
 		# Some transactions post credit as negative debit, this is handled while posting GLE
@@ -1160,6 +1160,11 @@ def _delete_gl_entries(voucher_type, voucher_no):
 		else:
 			_delete_gl_entries(voucher_type, voucher_no)
 
+		if idx % 20 == 0:
+			# Commit every 20 documents to avoid losing progress
+			# and reducing memory usage
+			frappe.db.commit()
+
 
 def sort_stock_vouchers_by_posting_date(
 	stock_vouchers: List[Tuple[str, str]]"
6;pjialin;py12306;390fcecdd24b9786519ff3eb99c40488b754769d;"Merge pull request #317 from out0fmemory/master

增加时间判断逻辑";"@@ -117,8 +117,8 @@ QUERY_JOBS = [
         # 'job_name':  'bj -> sz',  # 任务名称，不填默认会以车站名命名，不可重复
         'account_key': 0,  # 将会使用指定账号下单
         'left_dates': [  # 出发日期 :Array
-            ""2019-01-25"",
-            ""2019-01-26"",
+            ""2020-01-25"",
+            ""2020-01-26"",
         ],
         'stations': {  # 车站 支持多个车站同时查询  :Dict or :List
             'left': '北京',"
6;pjialin;py12306;390fcecdd24b9786519ff3eb99c40488b754769d;"Merge pull request #317 from out0fmemory/master

增加时间判断逻辑";"@@ -1,5 +1,6 @@
 import sys
 from datetime import timedelta
+from datetime import datetime
 
 from py12306.app import app_available_check
 from py12306.cluster.cluster import Cluster
@@ -66,6 +67,8 @@ class Job:
     INDEX_LEFT_TIME = 8
     INDEX_ARRIVE_TIME = 9
 
+    max_buy_time = 32
+
     def __init__(self, info, query):
         self.cluster = Cluster()
         self.query = query
@@ -136,11 +139,29 @@ def start(self):
                 QueryLog.add_log('\n').flush(sep='\t\t', publish=False)
             if Const.IS_TEST: return
 
+    def judge_date_legal(self, date):
+        date_now = datetime.datetime.now()
+        date_query = datetime.datetime.strptime(str(date), ""%Y-%m-%d"")
+        diff = (date_query - date_now).days
+        if date_now.day == date_query.day:
+            diff = 0
+        if diff < 0:
+            msg = '乘车日期错误，比当前时间还早！！'
+            QueryLog.add_quick_log(msg).flush(publish=False)
+            raise RuntimeError(msg)
+        elif diff > self.max_buy_time:
+            msg = '乘车日期错误，超出一个月预售期！！'
+            QueryLog.add_quick_log(msg).flush(publish=False)
+            raise RuntimeError(msg)
+        else:
+            pass
+
     def query_by_date(self, date):
         """"""
         通过日期进行查询
         :return:
         """"""
+        self.judge_date_legal(date)
         from py12306.helpers.cdn import Cdn
         QueryLog.add_log(('\n' if not is_main_thread() else '') + QueryLog.MESSAGE_QUERY_START_BY_DATE.format(date,
                                                                                                               self.left_station,"
6;pjialin;py12306;998b387875d5620e734285e8c11250e03c661a8a;"Merge pull request #288 from out0fmemory/master

修改人员的注释，存在不注意情况下有多余的人员信息";"@@ -132,7 +132,7 @@ QUERY_JOBS = [
         # }],
         'members': [  # 乘客姓名，会根据当前账号自动识别乘客类型 购买儿童票 设置两个相同的姓名即可，程序会自动识别 如  ['张三', '张三']
             ""张三"",
-            ""*王五"", #在姓名前加*表示学生购买成人票
+            #""*王五"", #在姓名前加*表示学生购买成人票
             # 7,  # 支持通过序号确定唯一乘客，序号查看可通过  python main.py -t 登录成功之后在 runtime/user/ 下找到对应的 用户名_passengers.json 文件，找到对应的 code 填入
         ],
         'allow_less_member': 0,  # 是否允许余票不足时提交部分乘客"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -0,0 +1 @@
+Reduce the amount of state we pull from the DB."
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -245,6 +245,8 @@ def __init__(self, hs: ""HomeServer""):
         self.store = hs.get_datastores().main
         self.state = hs.get_state_handler()
 
+        self._storage_controllers = hs.get_storage_controllers()
+
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
 
@@ -602,7 +604,9 @@ async def send_read_receipt(self, receipt: ReadReceipt) -> None:
         room_id = receipt.room_id
 
         # Work out which remote servers should be poked and poke them.
-        domains_set = await self.state.get_current_hosts_in_room(room_id)
+        domains_set = await self._storage_controllers.state.get_current_hosts_in_room(
+            room_id
+        )
         domains = [
             d
             for d in domains_set"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -59,6 +59,7 @@ class FollowerTypingHandler:
 
     def __init__(self, hs: ""HomeServer""):
         self.store = hs.get_datastores().main
+        self._storage_controllers = hs.get_storage_controllers()
         self.server_name = hs.config.server.server_name
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
@@ -131,15 +132,17 @@ async def _push_remote(self, member: RoomMember, typing: bool) -> None:
             return
 
         try:
-            users = await self.store.get_users_in_room(member.room_id)
             self._member_last_federation_poke[member] = self.clock.time_msec()
 
             now = self.clock.time_msec()
             self.wheel_timer.insert(
                 now=now, obj=member, then=now + FEDERATION_PING_INTERVAL
             )
 
-            for domain in {get_domain_from_id(u) for u in users}:
+            hosts = await self._storage_controllers.state.get_current_hosts_in_room(
+                member.room_id
+            )
+            for domain in hosts:
                 if domain != self.server_name:
                     logger.debug(""sending typing update to %s"", domain)
                     self.federation.build_and_send_edu("
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -172,10 +172,6 @@ async def get_current_users_in_room(
         entry = await self.resolve_state_groups_for_events(room_id, latest_event_ids)
         return await self.store.get_joined_users_from_state(room_id, entry)
 
-    async def get_current_hosts_in_room(self, room_id: str) -> FrozenSet[str]:
-        event_ids = await self.store.get_latest_event_ids_in_room(room_id)
-        return await self.get_hosts_in_room_at_events(room_id, event_ids)
-
     async def get_hosts_in_room_at_events(
         self, room_id: str, event_ids: Collection[str]
     ) -> FrozenSet[str]:"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -71,6 +71,7 @@ def _invalidate_state_caches(
             self._attempt_to_invalidate_cache(""is_host_joined"", (room_id, host))
         if members_changed:
             self._attempt_to_invalidate_cache(""get_users_in_room"", (room_id,))
+            self._attempt_to_invalidate_cache(""get_current_hosts_in_room"", (room_id,))
             self._attempt_to_invalidate_cache(
                 ""get_users_in_room_with_profiles"", (room_id,)
             )"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -23,6 +23,7 @@
     List,
     Mapping,
     Optional,
+    Set,
     Tuple,
 )
 
@@ -482,3 +483,10 @@ async def get_current_state_event(
             room_id, StateFilter.from_types((key,))
         )
         return state_map.get(key)
+
+    async def get_current_hosts_in_room(self, room_id: str) -> Set[str]:
+        """"""Get current hosts in room based on current state.""""""
+
+        await self._partial_state_room_tracker.await_full_state(room_id)
+
+        return await self.stores.main.get_current_hosts_in_room(room_id)"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -893,6 +893,43 @@ async def _check_host_room_membership(
 
         return True
 
+    @cached(iterable=True, max_entries=10000)
+    async def get_current_hosts_in_room(self, room_id: str) -> Set[str]:
+        """"""Get current hosts in room based on current state.""""""
+
+        # First we check if we already have `get_users_in_room` in the cache, as
+        # we can just calculate result from that
+        users = self.get_users_in_room.cache.get_immediate(
+            (room_id,), None, update_metrics=False
+        )
+        if users is not None:
+            return {get_domain_from_id(u) for u in users}
+
+        if isinstance(self.database_engine, Sqlite3Engine):
+            # If we're using SQLite then let's just always use
+            # `get_users_in_room` rather than funky SQL.
+            users = await self.get_users_in_room(room_id)
+            return {get_domain_from_id(u) for u in users}
+
+        # For PostgreSQL we can use a regex to pull out the domains from the
+        # joined users in `current_state_events` via regex.
+
+        def get_current_hosts_in_room_txn(txn: LoggingTransaction) -> Set[str]:
+            sql = """"""
+                SELECT DISTINCT substring(state_key FROM '@[^:]*:(.*)$')
+                FROM current_state_events
+                WHERE
+                    type = 'm.room.member'
+                    AND membership = 'join'
+                    AND room_id = ?
+            """"""
+            txn.execute(sql, (room_id,))
+            return {d for d, in txn}
+
+        return await self.db_pool.runInteraction(
+            ""get_current_hosts_in_room"", get_current_hosts_in_room_txn
+        )
+
     async def get_joined_hosts(
         self, room_id: str, state_entry: ""_StateCacheEntry""
     ) -> FrozenSet[str]:"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -30,16 +30,16 @@
 
 class FederationSenderReceiptsTestCases(HomeserverTestCase):
     def make_homeserver(self, reactor, clock):
-        mock_state_handler = Mock(spec=[""get_current_hosts_in_room""])
-        # Ensure a new Awaitable is created for each call.
-        mock_state_handler.get_current_hosts_in_room.return_value = make_awaitable(
-            [""test"", ""host2""]
-        )
-        return self.setup_test_homeserver(
-            state_handler=mock_state_handler,
+        hs = self.setup_test_homeserver(
             federation_transport_client=Mock(spec=[""send_transaction""]),
         )
 
+        hs.get_storage_controllers().state.get_current_hosts_in_room = Mock(
+            return_value=make_awaitable({""test"", ""host2""})
+        )
+
+        return hs
+
     @override_config({""send_federation"": True})
     def test_send_receipts(self):
         mock_send_transaction = ("
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -129,10 +129,12 @@ async def check_host_in_room(room_id: str, server_name: str) -> bool:
 
         hs.get_event_auth_handler().check_host_in_room = check_host_in_room
 
-        def get_joined_hosts_for_room(room_id: str):
+        async def get_current_hosts_in_room(room_id: str):
             return {member.domain for member in self.room_members}
 
-        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room
+        hs.get_storage_controllers().state.get_current_hosts_in_room = (
+            get_current_hosts_in_room
+        )
 
         async def get_users_in_room(room_id: str):
             return {str(u) for u in self.room_members}"
8;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"@@ -0,0 +1 @@
+Fix a crash with SIGBUS when the file system fills up while ``memray run`` is writing a capture file."
8;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"@@ -136,19 +136,15 @@ FileSink::grow(size_t needed)
     new_size = (new_size / 4096 + 1) * 4096;
     assert(new_size > d_fileSize);  // check for overflow
 
-    // Seek to 1 byte before the new size
-    off_t offset = lseek(d_fd, new_size - 1, SEEK_SET);
-    if (offset == -1) {
-        return false;
-    }
-
-    // Then write 1 byte.
-    ssize_t rc;
+    off_t delta = new_size - d_fileSize;
+    int rc;
     do {
-        rc = write(d_fd, ""\0"", 1);
-    } while (rc < 0 && errno == EINTR);
+        // posix_fallocate returns an error number instead of setting errno
+        rc = posix_fallocate(d_fd, d_fileSize, delta);
+    } while (rc == EINTR);
 
-    if (rc < 0) {
+    if (rc != 0) {
+        errno = rc;
         return false;
     }
 "
8;Cadene;pretrained-models.pytorch;b63db90071564844c857739d0806b2b70e29537a;Add pnasnetalarge and nasnetalarge to tests (improve memory with set_grad_enabled);"@@ -5,6 +5,9 @@
 import pretrainedmodels as pm
 import pretrainedmodels.utils as utils
 
+# torch 1.0.x
+set_grad_enabled = getattr(torch.autograd, 'set_grad_enabled', None)
+
 pm_args = []
 for model_name in pm.model_names:
     for pretrained in pm.pretrained_settings[model_name]:
@@ -19,20 +22,14 @@ def equal(x,y):
 
 @pytest.mark.parametrize('model_name, pretrained', pm_args)
 def test_pm_imagenet(model_name, pretrained):
+    if set_grad_enabled: set_grad_enabled(False)
+
     print('test_pm_imagenet(""{}"")'.format(model_name))
     net = pm.__dict__[model_name](
         num_classes=1000,
         pretrained=pretrained)
     net.eval()
 
-    if 'nasnetalarge' == model_name:
-        # nasnetalarge too big for travis
-        return
-
-    if 'pnasnet5large' == model_name:
-        # pnasnet5large too big for travis
-        return
-
     tensor = utils.TransformImage(net)(img)
     tensor = tensor.unsqueeze(0)
     x = Variable(tensor, requires_grad=False)
@@ -58,3 +55,5 @@ def test_pm_imagenet(model_name, pretrained):
 
     out_logits_3 = net.logits(out_feats)
     assert out_logits_3.shape == torch.Size([1,10])
+
+    if set_grad_enabled: set_grad_enabled(True)"
9;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"@@ -0,0 +1 @@
+Fix a crash with SIGBUS when the file system fills up while ``memray run`` is writing a capture file."
9;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"@@ -136,19 +136,15 @@ FileSink::grow(size_t needed)
     new_size = (new_size / 4096 + 1) * 4096;
     assert(new_size > d_fileSize);  // check for overflow
 
-    // Seek to 1 byte before the new size
-    off_t offset = lseek(d_fd, new_size - 1, SEEK_SET);
-    if (offset == -1) {
-        return false;
-    }
-
-    // Then write 1 byte.
-    ssize_t rc;
+    off_t delta = new_size - d_fileSize;
+    int rc;
     do {
-        rc = write(d_fd, ""\0"", 1);
-    } while (rc < 0 && errno == EINTR);
+        // posix_fallocate returns an error number instead of setting errno
+        rc = posix_fallocate(d_fd, d_fileSize, delta);
+    } while (rc == EINTR);
 
-    if (rc < 0) {
+    if (rc != 0) {
+        errno = rc;
         return false;
     }
 "
9;giampaolo;psutil;f1f299527634a425cb34b621d6201fa9172d3529;"[Linux] Speedup `Process.full_memory_info()` (#2108)

`Process.memory_full_info()` (reporting proecss USS/PSS/Swap memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps`` which makes it 5 times faster.

Without patch:
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
500 loops, best of 5: 518 usec per loop
```

With patch (5 times faster):
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
2000 loops, best of 5: 111 usec per loop
```

----

`make test-memleaks` suite, who heavily rely on `Process.memory_full_info()`, also received a nice speedup:

Before patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.646s

OK (skipped=9)
SUCCESS
```

After patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.195s

OK (skipped=9)
SUCCESS
```";"@@ -12,6 +12,9 @@ XXXX-XX-XX
   ``/proc`` pseudo files line by line. This should help having more consistent
   results.
 - 2057_, [OpenBSD]: add support for `cpu_freq()`_.
+- 2107_ [Linux]: `Process.memory_full_info()`_ (reporting process USS/PSS/Swap
+  memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps``,
+  which makes it 5 times faster.
 
 **Bug fixes**
 "
9;giampaolo;psutil;f1f299527634a425cb34b621d6201fa9172d3529;"[Linux] Speedup `Process.full_memory_info()` (#2108)

`Process.memory_full_info()` (reporting proecss USS/PSS/Swap memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps`` which makes it 5 times faster.

Without patch:
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
500 loops, best of 5: 518 usec per loop
```

With patch (5 times faster):
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
2000 loops, best of 5: 111 usec per loop
```

----

`make test-memleaks` suite, who heavily rely on `Process.memory_full_info()`, also received a nice speedup:

Before patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.646s

OK (skipped=9)
SUCCESS
```

After patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.195s

OK (skipped=9)
SUCCESS
```";"@@ -77,7 +77,8 @@
 
 
 POWER_SUPPLY_PATH = ""/sys/class/power_supply""
-HAS_SMAPS = os.path.exists('/proc/%s/smaps' % os.getpid())
+HAS_PROC_SMAPS = os.path.exists('/proc/%s/smaps' % os.getpid())
+HAS_PROC_SMAPS_ROLLUP = os.path.exists('/proc/%s/smaps_rollup' % os.getpid())
 HAS_PROC_IO_PRIORITY = hasattr(cext, ""proc_ioprio_get"")
 HAS_CPU_AFFINITY = hasattr(cext, ""proc_cpu_affinity_get"")
 
@@ -1875,18 +1876,42 @@ def memory_info(self):
                 [int(x) * PAGESIZE for x in f.readline().split()[:7]]
         return pmem(rss, vms, shared, text, lib, data, dirty)
 
-    # /proc/pid/smaps does not exist on kernels < 2.6.14 or if
-    # CONFIG_MMU kernel configuration option is not enabled.
-    if HAS_SMAPS:
+    if HAS_PROC_SMAPS_ROLLUP or HAS_PROC_SMAPS:
 
         @wrap_exceptions
-        def memory_full_info(
+        def _parse_smaps_rollup(self):
+            # /proc/pid/smaps_rollup was added to Linux in 2017. Faster
+            # than /proc/pid/smaps. It reports higher PSS than */smaps
+            # (from 1k up to 200k higher; tested against all processes).
+            uss = pss = swap = 0
+            try:
+                with open_binary(""{}/{}/smaps_rollup"".format(
+                        self._procfs_path, self.pid)) as f:
+                    for line in f:
+                        if line.startswith(b""Private_""):
+                            # Private_Clean, Private_Dirty, Private_Hugetlb
+                            uss += int(line.split()[1]) * 1024
+                        elif line.startswith(b""Pss:""):
+                            pss = int(line.split()[1]) * 1024
+                        elif line.startswith(b""Swap:""):
+                            swap = int(line.split()[1]) * 1024
+            except ProcessLookupError:  # happens on readline()
+                if not pid_exists(self.pid):
+                    raise NoSuchProcess(self.pid, self._name)
+                else:
+                    raise ZombieProcess(self.pid, self._name, self._ppid)
+            return (uss, pss, swap)
+
+        @wrap_exceptions
+        def _parse_smaps(
                 self,
                 # Gets Private_Clean, Private_Dirty, Private_Hugetlb.
                 _private_re=re.compile(br""\nPrivate.*:\s+(\d+)""),
                 _pss_re=re.compile(br""\nPss\:\s+(\d+)""),
                 _swap_re=re.compile(br""\nSwap\:\s+(\d+)"")):
-            basic_mem = self.memory_info()
+            # /proc/pid/smaps does not exist on kernels < 2.6.14 or if
+            # CONFIG_MMU kernel configuration option is not enabled.
+
             # Note: using 3 regexes is faster than reading the file
             # line by line.
             # XXX: on Python 3 the 2 regexes are 30% slower than on
@@ -1905,12 +1930,20 @@ def memory_full_info(
             uss = sum(map(int, _private_re.findall(smaps_data))) * 1024
             pss = sum(map(int, _pss_re.findall(smaps_data))) * 1024
             swap = sum(map(int, _swap_re.findall(smaps_data))) * 1024
+            return (uss, pss, swap)
+
+        def memory_full_info(self):
+            if HAS_PROC_SMAPS_ROLLUP:  # faster
+                uss, pss, swap = self._parse_smaps_rollup()
+            else:
+                uss, pss, swap = self._parse_smaps()
+            basic_mem = self.memory_info()
             return pfullmem(*basic_mem + (uss, pss, swap))
 
     else:
         memory_full_info = memory_info
 
-    if HAS_SMAPS:
+    if HAS_PROC_SMAPS:
 
         @wrap_exceptions
         def memory_maps(self):"
9;giampaolo;psutil;f1f299527634a425cb34b621d6201fa9172d3529;"[Linux] Speedup `Process.full_memory_info()` (#2108)

`Process.memory_full_info()` (reporting proecss USS/PSS/Swap memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps`` which makes it 5 times faster.

Without patch:
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
500 loops, best of 5: 518 usec per loop
```

With patch (5 times faster):
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
2000 loops, best of 5: 111 usec per loop
```

----

`make test-memleaks` suite, who heavily rely on `Process.memory_full_info()`, also received a nice speedup:

Before patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.646s

OK (skipped=9)
SUCCESS
```

After patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.195s

OK (skipped=9)
SUCCESS
```";"@@ -1775,28 +1775,19 @@ def open_mock(name, *args, **kwargs):
 class TestProcess(PsutilTestCase):
 
     @retry_on_failure()
-    def test_memory_full_info(self):
-        testfn = self.get_testfn()
-        src = textwrap.dedent(""""""
-            import time
-            with open(""%s"", ""w"") as f:
-                time.sleep(10)
-            """""" % testfn)
-        sproc = self.pyrun(src)
-        call_until(lambda: os.listdir('.'), ""'%s' not in ret"" % testfn)
-        p = psutil.Process(sproc.pid)
-        time.sleep(.1)
-        mem = p.memory_full_info()
-        maps = p.memory_maps(grouped=False)
+    def test_parse_smaps_vs_memory_maps(self):
+        sproc = self.spawn_testproc()
+        uss, pss, swap = psutil._pslinux.Process(sproc.pid)._parse_smaps()
+        maps = psutil.Process(sproc.pid).memory_maps(grouped=False)
         self.assertAlmostEqual(
-            mem.uss, sum([x.private_dirty + x.private_clean for x in maps]),
+            uss, sum([x.private_dirty + x.private_clean for x in maps]),
             delta=4096)
         self.assertAlmostEqual(
-            mem.pss, sum([x.pss for x in maps]), delta=4096)
+            pss, sum([x.pss for x in maps]), delta=4096)
         self.assertAlmostEqual(
-            mem.swap, sum([x.swap for x in maps]), delta=4096)
+            swap, sum([x.swap for x in maps]), delta=4096)
 
-    def test_memory_full_info_mocked(self):
+    def test_parse_smaps_mocked(self):
         # See: https://github.com/giampaolo/psutil/issues/1222
         with mock_open_content(
             ""/proc/%s/smaps"" % os.getpid(),
@@ -1823,12 +1814,12 @@ def test_memory_full_info_mocked(self):
                 Locked:                19 kB
                 VmFlags: rd ex
                 """""").encode()) as m:
-            p = psutil.Process()
-            mem = p.memory_full_info()
+            p = psutil._pslinux.Process(os.getpid())
+            uss, pss, swap = p._parse_smaps()
             assert m.called
-            self.assertEqual(mem.uss, (6 + 7 + 14) * 1024)
-            self.assertEqual(mem.pss, 3 * 1024)
-            self.assertEqual(mem.swap, 15 * 1024)
+            self.assertEqual(uss, (6 + 7 + 14) * 1024)
+            self.assertEqual(pss, 3 * 1024)
+            self.assertEqual(swap, 15 * 1024)
 
     # On PYPY file descriptors are not closed fast enough.
     @unittest.skipIf(PYPY, ""unreliable on PYPY"")"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -1441,6 +1441,11 @@ constexpr const char* pipeline_exec_scope = ""pipeline_exec_scope"";
  */
 constexpr const char* device_scope = ""device_scope"";
 
+/*!
+ * \brief Mark that the attached statement runs asynchronously.
+ */
+constexpr const char* async_scope = ""async_scope"";
+
 /*!
  * \brief Mark that the shape of TensorCore fragment
  */"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -644,6 +644,12 @@ TVM_DLL Pass AnnotateEntryFunc();
  */
 TVM_DLL Pass Filter(runtime::TypedPackedFunc<bool(PrimFunc)> fcond);
 
+/*!
+ * \brief Pass to rewrite global to shared memory copy on CUDA with asyncronous copy.
+ * \return The pass.
+ */
+TVM_DLL Pass InjectPTXAsyncCopy();
+
 }  // namespace transform
 }  // namespace tir
 }  // namespace tvm"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -1599,6 +1599,13 @@ def terminate_self():
     sys.exit(-1)
 
 
+def is_ampere_or_newer():
+    """"""Check if the target environment has an NVIDIA Ampere GPU or newer.""""""
+    arch = tvm.contrib.nvcc.get_target_compute_version()
+    major, _ = tvm.contrib.nvcc.parse_compute_version(arch)
+    return major >= 8
+
+
 def main():
     test_file = inspect.getsourcefile(sys._getframe(1))
     sys.exit(pytest.main([test_file] + sys.argv[1:]))"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -825,3 +825,14 @@ def Filter(fcond: Callable):
         The result pass
     """"""
     return _ffi_api.Filter(fcond)  # type: ignore
+
+
+def InjectPTXAsyncCopy():
+    """"""Rewrite global to shared memory copy on CUDA with asyncronous copy.
+
+    Returns
+    -------
+    fpass : tvm.transform.Pass
+        The result pass
+    """"""
+    return _ffi_api.InjectPTXAsyncCopy()  # type: ignore"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -50,6 +50,7 @@ TVM_REGISTER_PASS_CONFIG_OPTION(""tir.disable_storage_rewrite"", Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(""tir.is_entry_func"", Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(""tir.add_lower_pass"", Array<Array<ObjectRef>>);
 TVM_REGISTER_PASS_CONFIG_OPTION(""tir.debug_keep_trivial_loop"", Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(""tir.use_ptx_async_copy"", Bool);
 
 using runtime::PackedFunc;
 using runtime::TVMArgs;
@@ -559,6 +560,13 @@ transform::Sequential MixedModulePassManager(IRModule mixed_mod, Target target)
   mixed_pass_list.push_back(tir::transform::InferFragment());
   mixed_pass_list.push_back(tir::transform::LowerThreadAllreduce());
 
+  bool use_ptx_async_copy =
+      pass_ctx->GetConfig<Bool>(""tir.use_ptx_async_copy"", Bool(false)).value();
+
+  if (use_ptx_async_copy) {
+    mixed_pass_list.push_back(tir::transform::InjectPTXAsyncCopy());
+  }
+
   bool unpacked_api = mixed_mod->GetAttr<relay::Executor>(tvm::attr::kExecutor)
                           .value_or(relay::Executor::Create(""graph"", {}))
                           ->GetAttr<Bool>(""unpacked-api"")"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -651,7 +651,7 @@ std::string PrintCpAsyncAssembly(const std::string& shared_ptr,
       : ""l""((void *)({smem_addr}))
     );
     __asm__ __volatile__(
-      ""cp.async.cg.shared.global [%0], [%1], %2;""
+      ""cp.async.{cg_or_ca}.shared.global [%0], [%1], %2;""
        :: ""r""(addr), ""l""((void*)({global_ptr})), ""n""({bytes})
     );
   }
@@ -660,6 +660,7 @@ std::string PrintCpAsyncAssembly(const std::string& shared_ptr,
   replacer.register_rule(""{smem_addr}"", shared_ptr + "" + "" + shared_elem_offset);
   replacer.register_rule(""{global_ptr}"", global_ptr + "" + "" + global_elem_offset);
   replacer.register_rule(""{bytes}"", bytes);
+  replacer.register_rule(""{cg_or_ca}"", bytes == ""16"" ? ""cg"" : ""ca"");
   asm_code = replacer.rewrite(asm_code);
   return asm_code;
 }"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+/*!
+ * \brief Replace copy from global to shared with async copy
+ * \file inject_ptx_async_copy.cc
+ */
+#include <tvm/tir/analysis.h>
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/expr.h>
+#include <tvm/tir/stmt_functor.h>
+#include <tvm/tir/transform.h>
+
+#include ""../ir/buffer_common.h""
+#include ""storage_access.h""
+#include ""tvm/tir/stmt.h""
+
+namespace tvm {
+namespace tir {
+
+class PTXAsyncCopyInjector : public StmtMutator {
+ public:
+  Stmt VisitStmt_(const AttrStmtNode* attr) {
+    if (attr->attr_key == tir::attr::async_scope) {
+      ICHECK(in_async == false) << ""Nested async scopes not supported"";
+      in_async = true;
+      auto body = this->VisitStmt(attr->body);
+      in_async = false;
+      return body;
+    }
+    return StmtMutator::VisitStmt_(attr);
+  }
+
+  Stmt VisitStmt_(const BufferStoreNode* store) {
+    if (in_async && (store->buffer.scope() == ""shared"" || store->buffer.scope() == ""shared.dyn"")) {
+      if (auto* load = store->value.as<BufferLoadNode>()) {
+        if (load->buffer.scope() == ""global"") {
+          ICHECK(load->indices.size() == 1 && store->indices.size() == 1);
+          ICHECK(load->indices[0]->dtype.lanes() == store->indices[0]->dtype.lanes());
+
+          const int indices_lanes = load->indices[0]->dtype.lanes();
+          const int bytes = indices_lanes * load->buffer->dtype.bytes();
+
+          if (bytes == 4 || bytes == 8 || bytes == 16) {
+            auto dst_elem_type = GetPointerType(store->buffer->data->type_annotation);
+            auto src_elem_type = GetPointerType(load->buffer->data->type_annotation);
+            ICHECK(dst_elem_type.first && src_elem_type.first)
+                << ""Both store and load buffer should have a pointer type annotation."";
+
+            int index_factor = 1;
+            if (dst_elem_type != src_elem_type) {
+              // The only case where src and dst have different dtypes is when the dst shared memory
+              // is a byte buffer generated by merging dynamic shared memory.
+              ICHECK(store->buffer.scope() == ""shared.dyn"");
+              ICHECK(dst_elem_type.second == DataType::UInt(8));
+              // BufferStore/Load have the ""pointer reinterpret"" semantics according to their
+              // ""value"" dtype. Their ""indices"" are supposed to be applied after such pointer cast,
+              // for example: ((*float16)(byte_buffer))[buffer->indices] = fp16_value;
+              // To replace BufferStore/Load with cp.async, we need to multiply the store index by
+              // the byte size of the ""value"" dtype, to get the correct offset into the byte buffer.
+              index_factor = src_elem_type.second.bytes();
+            }
+
+            if (indices_lanes == 1) {
+              auto src_offset = load->indices[0];
+              auto dst_offset = store->indices[0];
+              return Evaluate(
+                  Call(store->buffer->dtype, tvm::tir::builtin::ptx_cp_async(),
+                       {store->buffer->data, tir::Mul(dst_offset, PrimExpr(index_factor)),
+                        load->buffer->data, src_offset, PrimExpr(bytes)}));
+            }
+
+            // Only some vectorized indexing patterns are supported for now.
+            auto src_offset = [=]() -> PrimExpr {
+              if (load->indices[0]->IsInstance<RampNode>()) {
+                return load->indices[0].as<RampNode>()->base;
+              }
+              return PrimExpr();
+            }();
+
+            auto dst_offset = [=]() -> PrimExpr {
+              if (store->indices[0].as<RampNode>()) {
+                return store->indices[0].as<RampNode>()->base;
+              } else if (store->indices[0].as<AddNode>()) {
+                // The case where the dst buffer is a byte buffer generated by merging dynamic
+                // shared memory.
+                // A_shared.dyn[(ramp(...), 1, 8) + x8(17408))] = A_global[ramp(...),1, 8)]
+                auto* add = store->indices[0].as<AddNode>();
+                if (!add->a->IsInstance<RampNode>()) return PrimExpr();
+                if (!add->b->IsInstance<BroadcastNode>()) return PrimExpr();
+                return tir::Add(add->a.as<RampNode>()->base, add->b.as<BroadcastNode>()->value);
+              }
+              return PrimExpr();
+            }();
+
+            if (src_offset.defined() && dst_offset.defined()) {
+              return Evaluate(
+                  Call(store->buffer->dtype, tvm::tir::builtin::ptx_cp_async(),
+                       {store->buffer->data, tir::Mul(dst_offset, PrimExpr(index_factor)),
+                        load->buffer->data, src_offset, PrimExpr(bytes)}));
+            }
+          }
+        }
+      }
+    }
+    return StmtMutator::VisitStmt_(store);
+  }
+
+ private:
+  bool in_async{false};
+};
+
+namespace transform {
+
+Pass InjectPTXAsyncCopy() {
+  auto pass_func = [=](PrimFunc f, IRModule m, PassContext ctx) {
+    auto* n = f.CopyOnWrite();
+    n->body = PTXAsyncCopyInjector()(n->body);
+    return f;
+  };
+  return CreatePrimFuncPass(pass_func, 0, ""tir.InjectPTXAsyncCopy"", {});
+}
+
+TVM_REGISTER_GLOBAL(""tir.transform.InjectPTXAsyncCopy"").set_body_typed(InjectPTXAsyncCopy);
+
+}  // namespace transform
+
+}  // namespace tir
+}  // namespace tvm"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -40,8 +40,8 @@ def ptx_cp_async(A: T.Buffer[(32, 128), ""float16""], B: T.Buffer[(32, 128), ""floa
             )
 
         # TODO(masahi): Remove dtype requirement from TVMScript parser
-        T.evaluate(T.ptx_commit_group(dtype=""float16""))
-        T.evaluate(T.ptx_wait_group(0, dtype=""float16""))
+        T.evaluate(T.ptx_commit_group(dtype=""""))
+        T.evaluate(T.ptx_wait_group(0, dtype=""""))
 
         for i in range(128):
             B[tx, i] = A_shared[tx, i]"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -76,12 +76,6 @@ def maybe_swap(i, j):
     return (a, b, c)
 
 
-def is_ampere_or_newer():
-    arch = tvm.contrib.nvcc.get_target_compute_version()
-    major, _ = tvm.contrib.nvcc.parse_compute_version(arch)
-    return major >= 8
-
-
 def run_test(
     k_inner,
     in_dtype,
@@ -117,7 +111,7 @@ def run_test(
         mma_store_intrin,
     )
 
-    if not is_ampere_or_newer():
+    if not tvm.testing.is_ampere_or_newer():
         return None
 
     f = tvm.build(sch.mod[""main""], target=""cuda"", name=""dense"")"
9;apache;tvm;53d163c96850c8476d479803c59344c6977ef9e8;"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)

* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async

* add missing doc

* black

* missing src

* clang format

* clang format

* check against nested async scope";"@@ -0,0 +1,183 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+import tvm
+from tvm.script import tir as T
+import numpy as np
+import tvm.testing
+
+
+def count_cp_async(stmt):
+    num_alloc = [0]
+
+    def verify(n):
+        if isinstance(n, tvm.tir.Call) and str(n.op) == ""tir.ptx_cp_async"":
+            num_alloc[0] += 1
+
+    tvm.tir.stmt_functor.post_order_visit(stmt, verify)
+    return num_alloc[0]
+
+
+def generate_global_to_shared_vectorized_copy(dtype, vector_size):
+    num_iters = 128 // vector_size
+    vector_size_expr = tvm.runtime.convert(vector_size)
+
+    @T.prim_func
+    def ptx_global_to_shared_copy(
+        A: T.Buffer[(32, 128), dtype], B: T.Buffer[(32, 128), dtype]
+    ) -> None:
+        T.func_attr({""global_symbol"": ""main"", ""tir.noalias"": True})
+        bx = T.env_thread(""blockIdx.x"")
+        tx = T.env_thread(""threadIdx.x"")
+        T.launch_thread(bx, 1)
+        T.launch_thread(tx, 32)
+        with T.block():
+            A_shared = T.alloc_buffer([32, 128], dtype, scope=""shared"")
+            T.reads(A[0:32, 0:128])
+            T.writes(B[0:32, 0:128])
+
+            T.attr(""default"", ""async_scope"", 1)
+            for i in T.serial(num_iters):
+                for j in T.vectorized(vector_size):
+                    A_shared[tx, i * vector_size_expr + j] = A[tx, i * vector_size_expr + j]
+
+            T.evaluate(T.ptx_commit_group(dtype=""""))
+            T.evaluate(T.ptx_wait_group(0, dtype=""""))
+
+            for i in range(128):
+                B[tx, i] = A_shared[tx, i]
+
+    return ptx_global_to_shared_copy
+
+
+@T.prim_func
+def ptx_global_to_shared_copy_fp32x1(
+    A: T.Buffer[(32, 128), ""float32""], B: T.Buffer[(32, 128), ""float32""]
+) -> None:
+    T.func_attr({""global_symbol"": ""main"", ""tir.noalias"": True})
+    bx = T.env_thread(""blockIdx.x"")
+    tx = T.env_thread(""threadIdx.x"")
+    T.launch_thread(bx, 1)
+    T.launch_thread(tx, 32)
+    with T.block():
+        A_shared = T.alloc_buffer([32, 128], ""float32"", scope=""shared"")
+        T.reads(A[0:32, 0:128])
+        T.writes(B[0:32, 0:128])
+
+        T.attr(""default"", ""async_scope"", 1)
+        for i in T.serial(128):
+            A_shared[tx, i] = A[tx, i]
+
+        T.evaluate(T.ptx_commit_group(dtype=""""))
+        T.evaluate(T.ptx_wait_group(0, dtype=""""))
+
+        for i in range(128):
+            B[tx, i] = A_shared[tx, i]
+
+
+@T.prim_func
+def ptx_global_to_shared_dyn_copy_fp16x8(
+    A: T.Buffer[(32, 128), ""float16""],
+    B: T.Buffer[(32, 128), ""float16""],
+    C: T.Buffer[(32, 128), ""float16""],
+) -> None:
+    T.func_attr({""global_symbol"": ""main"", ""tir.noalias"": True})
+    bx = T.env_thread(""blockIdx.x"")
+    tx = T.env_thread(""threadIdx.x"")
+    T.launch_thread(bx, 1)
+    T.launch_thread(tx, 32)
+    with T.block():
+        A_shared = T.alloc_buffer([32, 128], ""float16"", scope=""shared.dyn"")
+        B_shared = T.alloc_buffer([32, 128], ""float16"", scope=""shared.dyn"")
+        T.reads(A[0:32, 0:128], B[0:32, 0:128])
+        T.writes(C[0:32, 0:128])
+
+        T.attr(""default"", ""async_scope"", 1)
+        for i in T.serial(16):
+            for j in T.vectorized(8):
+                A_shared[tx, i * 8 + j] = A[tx, i * 8 + j]
+                B_shared[tx, i * 8 + j] = B[tx, i * 8 + j]
+
+        T.evaluate(T.ptx_commit_group(dtype=""""))
+        T.evaluate(T.ptx_wait_group(0, dtype=""""))
+
+        for i in range(128):
+            C[tx, i] = A_shared[tx, i] + B_shared[tx, i]
+
+
+@tvm.testing.requires_cuda
+def test_inject_async_copy():
+    for dtype, vec_size in [(""float16"", 8), (""float16"", 4), (""float32"", 4), (""float32"", 1)]:
+        if vec_size == 1:
+            f = ptx_global_to_shared_copy_fp32x1
+        else:
+            f = generate_global_to_shared_vectorized_copy(dtype, vec_size)
+
+        mod = tvm.IRModule.from_expr(f)
+        mod = tvm.tir.transform.FlattenBuffer()(mod)
+        if vec_size > 1:
+            mod = tvm.tir.transform.VectorizeLoop()(mod)
+        mod = tvm.tir.transform.InjectPTXAsyncCopy()(mod)
+
+        assert count_cp_async(mod[""main""].body) == 1
+
+        if not tvm.testing.is_ampere_or_newer():
+            continue
+
+        with tvm.transform.PassContext(config={""tir.use_ptx_async_copy"": 1}):
+            mod = tvm.build(tvm.IRModule.from_expr(f), target=""cuda"")
+
+        A_np = np.random.rand(32, 128).astype(dtype)
+        B_np = np.zeros((32, 128)).astype(dtype)
+        dev = tvm.cuda(0)
+        A_nd = tvm.nd.array(A_np, device=dev)
+        B_nd = tvm.nd.array(B_np, device=dev)
+        mod(A_nd, B_nd)
+        tvm.testing.assert_allclose(B_nd.numpy(), A_np)
+
+
+@tvm.testing.requires_cuda
+def test_inject_async_copy_shared_dyn():
+    f = ptx_global_to_shared_dyn_copy_fp16x8
+
+    mod = tvm.IRModule.from_expr(f)
+    mod = tvm.tir.transform.FlattenBuffer()(mod)
+    mod = tvm.tir.transform.VectorizeLoop()(mod)
+    mod = tvm.tir.transform.MergeDynamicSharedMemoryAllocations()(mod)
+    mod = tvm.tir.transform.InjectPTXAsyncCopy()(mod)
+
+    assert count_cp_async(mod[""main""].body) == 2
+
+    if not tvm.testing.is_ampere_or_newer():
+        return
+
+    with tvm.transform.PassContext(config={""tir.use_ptx_async_copy"": 1}):
+        mod = tvm.build(tvm.IRModule.from_expr(f), target=""cuda"")
+
+    A_np = np.random.rand(32, 128).astype(""float16"")
+    B_np = np.random.rand(32, 128).astype(""float16"")
+    C_np = np.zeros((32, 128)).astype(""float16"")
+    dev = tvm.cuda(0)
+    A_nd = tvm.nd.array(A_np, device=dev)
+    B_nd = tvm.nd.array(B_np, device=dev)
+    C_nd = tvm.nd.array(C_np, device=dev)
+    mod(A_nd, B_nd, C_nd)
+    tvm.testing.assert_allclose(C_nd.numpy(), A_np + B_np)
+
+
+if __name__ == ""__main__"":
+    test_inject_async_copy()
+    test_inject_async_copy_shared_dyn()"
9;apache;tvm;0b46efa33e045ab4d892e40c1f5084dd6651c8b6;"[BUG] Disable second PlanDevices pass (#11662)

Though started with the best of intentions, the second
PlanDevices pass to account for memory scope's introduced
by lowering is buggy and not ready for prime time. It
has caused an ICHECK fail since for some reason the new
constraints are not flowing into device_copies.";"@@ -244,6 +244,7 @@ class VMFunctionCompiler : DeviceAwareExprFunctor<void(const Expr& n)> {
         host_virtual_device_(std::move(host_virtual_device)) {}
 
   VMFunction Compile(const GlobalVar& var, const Function& func) {
+    VLOG(1) << ""Compiling:"" << std::endl << PrettyPrint(func);
     std::vector<Index> param_device_indexes;
     if (IsClosure(func)) {
       // After lifting we'll have functions of the form:
@@ -1102,9 +1103,8 @@ IRModule VMCompiler::OptimizeModuleImpl(IRModule mod) {
   // let-bound functions.
   pass_seqs.push_back(DeadCodeElimination(/*inline_once=*/false));
 
-  // Now that we have PrimFuncs, flow and solve VirtualDevice constraints again to account for
-  // any memory scopes which lowering has settled on.
-  pass_seqs.push_back(transform::PlanDevices(config_));
+  // At this point it's possible to run PlanDevices again to pick up any additional constraints
+  // introduced during lowering. However we'll not do this until more testing has been done.
 
   // Inline the functions that are lifted to the module scope. We perform this
   // pass after all other optimization passes but before the memory allocation"
10;milesial;Pytorch-UNet;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"@@ -36,7 +36,7 @@ sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
-sudo docker run --rm --gpus all -it milesial/unet
+sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
@@ -60,7 +60,7 @@ A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
-docker run -it --rm --gpus all milesial/unet
+docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet
 ```
 
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -4,7 +4,7 @@ channels:
   - defaults
 dependencies:
   # required
-  - pip
+  - pip<22.0
   - audioread==2.1.5
   - numpy==1.17.0
   - scipy==1.2.0"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -6,7 +6,8 @@ jobs:
     steps:
       - uses: actions/checkout@v2
       - uses: actions/setup-python@v2
-      - run: pip install bandit codespell flake8
+      - run: pip install bandit codespell flake8 velin
       - run: bandit --recursive --skip B101,B110 .
       - run: codespell --ignore-words-list=""ba,trough,ue"" --skip=""*demo.ipynb""
       - run: flake8 librosa --count --select=E9,F63,F7,F82 --show-source --statistics
+      - run: python -m velin --check librosa"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -77,7 +77,10 @@ Contributors
 * N. Dorukhan Sergin <https://github.com/dorukhansergin>
 * Paul Biberstein <https://github.com/P-bibs>
 * Myungchul Keum <https://github.com/dofuuz>
-
+* Daniel Faronbi <https://github.com/dafaronbi>
+* Iran Roman <https://github.com/iranroman>
+* philstem <https://github.com/philstem>
+* Alex Malins <https://github.com/alexmalins>
 
 Some feature extraction code was based on <https://github.com/ronw/frontend> by Ron Weiss.
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -2,6 +2,91 @@
 Changelog
 *********
 
+v0.9
+====
+
+v0.9.0
+------
+
+2022-02-??
+
+The main feature of this release is (nearly) full support for arbitrary multi-channel processing, along with several speed and stability enhancements.
+A detailed list of changes is provided below.
+
+New Features
+    - `#1130`_ Nearly full support for multi-channel processing. *Brian McFee, Daniel Faronbi, Iran Roman*
+    - `#1331`_ Option to disable unicode characters in display functions. *Brian McFee*
+    - `#1441`_ Significantly expanded the library of example audio clips. *Brian McFee*
+
+API changes
+    - `#1114`_ Most functions now require keyword arguments. *Brian McFee*
+    - `#1382`_ The default padding mode for most functions (including STFT) is now zero-padding. *Brian McFee*
+    - `#1418`_ `librosa.load` and `librosa.stream` can now operate directly on open `soundfile` objects. *Brian McFee*
+    - `#1414`_ `librosa.display.specshow` now uses centered coordinate grids. *Brian McFee*
+    - `#1398`_ `librosa.iirt` now exposes API control over resampling modes. *Brian McFee*
+    - `#1416`_ Removed deprecated functions `librosa.display.waveplot` and `librosa.util.example_audio_file`. *Brian McFee*
+
+Bug fixes
+    - `#1387`_ Fixed errors in support of odd frame lengths in various functions. *Brian McFee*
+    - `#1273`_ Minor corrections to constants in perceptual weighting functions. *Brian McFee*
+    - `#1350`_ Removed uses of deprecated numpy numerical types. *Brian McFee*
+    - `#1361`_ Maximum frequency is now correctly inferred as Nyquist in onset strength calculation. *Brian McFee*
+    - `#1362`_ `librosa.effects.deemphasis` no longer modifies the input signal in-place. *Brian McFee*
+    - `#1375`_ `librosa.util.frame` now correctly works for arbitrary memory layouts and numbers of axes. *Brian McFee*
+    - `#1425`_ Fixed an off-by-one error in `librosa.yin` and `librosa.pyin`. *@philstem, Brian McFee*
+    - `#1430`_ Removed unnecessary `__all__` specifications to better support static analysis. *Fabian Keller*
+    - `#1407`_ Corrected a normalization error in inverse CQT. *Brian McFee, Vincent Lostanlen*
+
+Documentation
+    - `#1328`_ Retired the `examples/` folder and expanded the `Advanced Examples` gallery. *Brian McFee*
+    - `#1427`_ Fixed docstring for `librosa.reassigned_spectrogram`. *Fabian Keller*
+
+Other changes
+    - `#418`_ `librosa.cqt` now supports arbitrary hop lengths. *Brian McFee*
+    - `#1405`_ Improvements and generalizations to constant-Q/variable-Q basis construction. *Brian McFee, Vincent Lostanlen*
+    - `#1324`_ Added a run-time check for minimally supported matplotlib versions. *Brian McFee*
+    - `#1325`_ Enhanced continuous integration testing for oldest and newest environments. *Brian McFee*
+    - `#1358`_ Harmonic interpolation now preemptively detects repeated values that produce unstable estimates. *Brian McFee*
+    - `#1432`_ Specify stack levels in warnings to provide more helpful warning messages. *Brian McFee*
+    - `#1404`_, `#1406`_ Improved packaging configurations. *Alex Malins*
+    - `#1384`_ Fixed package configuration error for documentation builds. *Adam Weiss*
+
+Deprecations
+    - `#1389`_ The `mono` parameter of `librosa.util.valid_audio` is deprecated and the default is now set to `False`. *Brian McFee*
+    - `#1405`_ CQT filter-bank constructors `librosa.filters.constant_q` are now deprecated in favor of `librosa.filters.wavelet`. *Brian McFee, Vincent Lostanlen*
+
+
+.. _#418: https://github.com/librosa/librosa/issues/418
+.. _#1114: https://github.com/librosa/librosa/issues/1114
+.. _#1130: https://github.com/librosa/librosa/issues/1130
+.. _#1273: https://github.com/librosa/librosa/issues/1273
+.. _#1324: https://github.com/librosa/librosa/issues/1324
+.. _#1325: https://github.com/librosa/librosa/issues/1325
+.. _#1328: https://github.com/librosa/librosa/issues/1328
+.. _#1331: https://github.com/librosa/librosa/issues/1331
+.. _#1350: https://github.com/librosa/librosa/issues/1350
+.. _#1358: https://github.com/librosa/librosa/issues/1358
+.. _#1361: https://github.com/librosa/librosa/issues/1361
+.. _#1362: https://github.com/librosa/librosa/issues/1362
+.. _#1375: https://github.com/librosa/librosa/issues/1375
+.. _#1382: https://github.com/librosa/librosa/issues/1382
+.. _#1384: https://github.com/librosa/librosa/issues/1384
+.. _#1387: https://github.com/librosa/librosa/issues/1387
+.. _#1389: https://github.com/librosa/librosa/issues/1389
+.. _#1398: https://github.com/librosa/librosa/issues/1398
+.. _#1404: https://github.com/librosa/librosa/issues/1404
+.. _#1405: https://github.com/librosa/librosa/issues/1405
+.. _#1406: https://github.com/librosa/librosa/issues/1406
+.. _#1407: https://github.com/librosa/librosa/issues/1407
+.. _#1414: https://github.com/librosa/librosa/issues/1414
+.. _#1416: https://github.com/librosa/librosa/issues/1416
+.. _#1418: https://github.com/librosa/librosa/issues/1418
+.. _#1425: https://github.com/librosa/librosa/issues/1425
+.. _#1427: https://github.com/librosa/librosa/issues/1427
+.. _#1430: https://github.com/librosa/librosa/issues/1430
+.. _#1432: https://github.com/librosa/librosa/issues/1432
+.. _#1441: https://github.com/librosa/librosa/issues/1441
+
 v0.8
 ====
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -390,7 +390,7 @@ def reset_mpl(gallery_conf, fname):
 autodoc_member_order = ""bysource""
 
 smv_branch_whitelist = r""^(main)$""  # build main branch, and anything relating to documentation
-smv_tag_whitelist = r""^((0\.7\.2)|(0\.[89]\.\d+))$""  # use this for final builds
+smv_tag_whitelist = r""^((0\.7\.2)|(0\.[89]\.\d+)|(0\.9\.0rc0))$""  # use this for final builds
 smv_released_pattern = r'.*tags.*'
 smv_remote_whitelist = None
 smv_greatest_tag = True"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -51,59 +51,43 @@ def beat_track(
            Journal of New Music Research 36.1 (2007): 51-60.
            http://labrosa.ee.columbia.edu/projects/beattrack/
 
-
     Parameters
     ----------
-
     y : np.ndarray [shape=(n,)] or None
         audio time series
-
     sr : number > 0 [scalar]
         sampling rate of ``y``
-
     onset_envelope : np.ndarray [shape=(n,)] or None
         (optional) pre-computed onset strength envelope.
-
     hop_length : int > 0 [scalar]
         number of audio samples between successive ``onset_envelope`` values
-
-    start_bpm  : float > 0 [scalar]
+    start_bpm : float > 0 [scalar]
         initial guess for the tempo estimator (in beats per minute)
-
-    tightness  : float [scalar]
+    tightness : float [scalar]
         tightness of beat distribution around tempo
-
-    trim       : bool [scalar]
+    trim : bool [scalar]
         trim leading/trailing beats with weak onsets
-
-    bpm        : float [scalar]
+    bpm : float [scalar]
         (optional) If provided, use ``bpm`` as the tempo instead of
         estimating it from ``onsets``.
-
-    prior      : scipy.stats.rv_continuous [optional]
+    prior : scipy.stats.rv_continuous [optional]
         An optional prior distribution over tempo.
         If provided, ``start_bpm`` will be ignored.
-
     units : {'frames', 'samples', 'time'}
         The units to encode detected beat events in.
         By default, 'frames' are used.
 
-
     Returns
     -------
-
     tempo : float [scalar, non-negative]
         estimated global tempo (in beats per minute)
-
     beats : np.ndarray [shape=(m,)]
         estimated beat event locations in the specified units
         (default is frame indices)
-
     .. note::
         If no onset strength could be detected, beat_tracker estimates 0 BPM
         and returns an empty list.
 
-
     Raises
     ------
     ParameterError
@@ -114,7 +98,6 @@ def beat_track(
     --------
     librosa.onset.onset_strength
 
-
     Examples
     --------
     Track beats using time series input
@@ -125,14 +108,12 @@ def beat_track(
     >>> tempo
     135.99917763157896
 
-
     Print the frames corresponding to beats
 
     >>> beats
     array([  3,  21,  40,  59,  78,  96, 116, 135, 154, 173, 192, 211,
            230, 249, 268, 287, 306, 325, 344, 363])
 
-
     Or print them as timestamps
 
     >>> librosa.frames_to_time(beats, sr=sr)
@@ -152,7 +133,6 @@ def beat_track(
     array([  3,  21,  40,  59,  78,  96, 116, 135, 154, 173, 192, 211,
            230, 249, 268, 287, 306, 325, 344, 363])
 
-
     Plot the beat events against the onset strength envelope
 
     >>> import matplotlib.pyplot as plt
@@ -230,32 +210,23 @@ def tempo(
     ----------
     y : np.ndarray [shape=(..., n)] or None
         audio time series. Multi-channel is supported.
-
     sr : number > 0 [scalar]
         sampling rate of the time series
-
-    onset_envelope    : np.ndarray [shape=(..., n)]
+    onset_envelope : np.ndarray [shape=(..., n)]
         pre-computed onset strength envelope
-
     hop_length : int > 0 [scalar]
         hop length of the time series
-
     start_bpm : float [scalar]
         initial guess of the BPM
-
     std_bpm : float > 0 [scalar]
         standard deviation of tempo distribution
-
     ac_size : float > 0 [scalar]
         length (in seconds) of the auto-correlation window
-
     max_tempo : float > 0 [scalar, optional]
         If provided, only estimate tempo below this threshold
-
     aggregate : callable [optional]
         Aggregation function for estimating global tempo.
         If `None`, then tempo is estimated independently for each frame.
-
     prior : scipy.stats.rv_continuous [optional]
         A prior distribution over tempo (in beats per minute).
         By default, a pseudo-log-normal prior is used.
@@ -408,7 +379,6 @@ def plp(
     since `plp` does not require the entire signal to make predictions, it may be
     preferable when beat-tracking long recordings in a streaming setting.
 
-
     .. [#] Grosche, P., & Muller, M. (2011).
         ""Extracting predominant local pulse information from music recordings.""
         IEEE Transactions on Audio, Speech, and Language Processing, 19(6), 1688-1701.
@@ -495,7 +465,6 @@ def plp(
     >>> ax[2].set(title='Log-normal tempo prior, mean=120', xlim=[5, 20])
     >>> ax[2].legend()
 
-
     PLP local maxima can be used as estimates of beat positions.
 
     >>> tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env)
@@ -584,16 +553,12 @@ def __beat_tracker(onset_envelope, bpm, fft_res, tightness, trim):
     ----------
     onset_envelope : np.ndarray [shape=(n,)]
         onset strength envelope
-
     bpm : float [scalar]
         tempo estimate
-
-    fft_res  : float [scalar]
+    fft_res : float [scalar]
         resolution of the fft (sr / hop_length)
-
-    tightness: float [scalar]
+    tightness : float [scalar]
         how closely do we adhere to bpm?
-
     trim : bool [scalar]
         trim leading/trailing beats with weak onsets?
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -76,7 +76,7 @@ def load(
         On the contrary, if the codec is not supported by `soundfile`
         (for example, MP3), then `path` must be a file path (string or `pathlib.Path`).
 
-    sr   : number > 0 [scalar]
+    sr : number > 0 [scalar]
         target sampling rate
 
         'None' uses the native sampling rate
@@ -106,16 +106,13 @@ def load(
 
            See :ref:`ioformats` for alternate loading methods.
 
-
     Returns
     -------
-    y    : np.ndarray [shape=(n,) or (..., n)]
+    y : np.ndarray [shape=(n,) or (..., n)]
         audio time series. Multi-channel is supported.
-
-    sr   : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-
     Examples
     --------
     >>> # Load an ogg vorbis file
@@ -171,8 +168,7 @@ def load(
     except RuntimeError as exc:
         # If soundfile failed, try audioread instead
         if isinstance(path, (str, pathlib.PurePath)):
-            warnings.warn(""PySoundFile failed. Trying audioread instead."",
-                          stacklevel=2)
+            warnings.warn(""PySoundFile failed. Trying audioread instead."", stacklevel=2)
             y, sr_native = __audioread_load(path, offset, duration, dtype)
         else:
             raise (exc)
@@ -535,7 +531,7 @@ def resample(
         Scale the resampled signal so that ``y`` and ``y_hat`` have approximately
         equal total energy.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         If ``fix==True``, additional keyword arguments to pass to
         `librosa.util.fix_length`.
 
@@ -668,13 +664,13 @@ def get_duration(
         up to the frame resolution. If high precision is required,
         it is better to use the audio time series directly.
 
-    n_fft       : int > 0 [scalar]
+    n_fft : int > 0 [scalar]
         FFT window size for ``S``
 
-    hop_length  : int > 0 [ scalar]
+    hop_length : int > 0 [ scalar]
         number of audio samples between columns of ``S``
 
-    center  : boolean
+    center : boolean
         - If ``True``, ``S[:, t]`` is centered at ``y[t * hop_length]``
         - If ``False``, then ``S[:, t]`` begins at ``y[t * hop_length]``
 
@@ -775,11 +771,9 @@ def autocorrelate(y, *, max_size=None, axis=-1):
     ----------
     y : np.ndarray
         array to autocorrelate
-
-    max_size  : int > 0 or None
+    max_size : int > 0 or None
         maximum correlation lag.
         If unspecified, defaults to ``y.shape[axis]`` (unbounded)
-
     axis : int
         The axis along which to autocorrelate.
         By default, the last axis (-1) is taken.
@@ -860,10 +854,8 @@ def lpc(y, *, order, axis=-1):
     ----------
     y : np.ndarray [shape=(..., n)]
         Time series to fit. Multi-channel is supported..
-
     order : int > 0
         Order of the linear filter
-
     axis : int
         Axis along which to compute the coefficients
 
@@ -881,7 +873,7 @@ def lpc(y, *, order, axis=-1):
     FloatingPointError
         - If ``y`` is ill-conditioned
 
-    See also
+    See Also
     --------
     scipy.signal.lfilter
 
@@ -1034,7 +1026,6 @@ def zero_crossings(
     If ``y`` is multi-dimensional, then zero-crossings are computed along
     the specified ``axis``.
 
-
     Parameters
     ----------
     y : np.ndarray
@@ -1174,44 +1165,34 @@ def clicks(
     ----------
     times : np.ndarray or None
         times to place clicks, in seconds
-
     frames : np.ndarray or None
         frame indices to place clicks
-
     sr : number > 0
         desired sampling rate of the output signal
-
     hop_length : int > 0
         if positions are specified by ``frames``, the number of samples between frames.
-
     click_freq : float > 0
         frequency (in Hz) of the default click signal.  Default is 1KHz.
-
     click_duration : float > 0
         duration (in seconds) of the default click signal.  Default is 100ms.
-
     click : np.ndarray or None
         (optional) click signal sample to use instead of the default click.
         Multi-channel is supported.
-
     length : int > 0
         desired number of samples in the output signal
 
-
     Returns
     -------
     click_signal : np.ndarray
         Synthesized click signal.
         This will be monophonic by default, or match the number of channels to a provided ``click`` signal.
 
-
     Raises
     ------
     ParameterError
         - If neither ``times`` nor ``frames`` are provided.
         - If any of ``click_freq``, ``click_duration``, or ``length`` are out of range.
 
-
     Examples
     --------
     >>> # Sonify detected beat events
@@ -1308,37 +1289,30 @@ def tone(frequency, *, sr=22050, length=None, duration=None, phi=None):
     ----------
     frequency : float > 0
         frequency
-
     sr : number > 0
         desired sampling rate of the output signal
-
     length : int > 0
         desired number of samples in the output signal.
         When both ``duration`` and ``length`` are defined,
         ``length`` takes priority.
-
     duration : float > 0
         desired duration in seconds.
         When both ``duration`` and ``length`` are defined,
         ``length`` takes priority.
-
     phi : float or None
         phase offset, in radians. If unspecified, defaults to ``-np.pi * 0.5``.
 
-
     Returns
     -------
     tone_signal : np.ndarray [shape=(length,), dtype=float64]
         Synthesized pure sine tone signal
 
-
     Raises
     ------
     ParameterError
         - If ``frequency`` is not provided.
         - If neither ``length`` nor ``duration`` are provided.
 
-
     Examples
     --------
     Generate a pure sine tone A4
@@ -1409,25 +1383,21 @@ def chirp(*, fmin, fmax, sr=22050, length=None, duration=None, linear=False, phi
         phase offset, in radians.
         If unspecified, defaults to ``-np.pi * 0.5``.
 
-
     Returns
     -------
     chirp_signal : np.ndarray [shape=(length,), dtype=float64]
         Synthesized chirp signal
 
-
     Raises
     ------
     ParameterError
         - If either ``fmin`` or ``fmax`` are not provided.
         - If neither ``length`` nor ``duration`` are provided.
 
-
     See Also
     --------
     scipy.signal.chirp
 
-
     Examples
     --------
     Generate a exponential chirp from A2 to A8
@@ -1490,7 +1460,6 @@ def mu_compress(x, *, mu=255, quantize=True):
 
         sign(x) * ln(1 + mu * abs(x)) /  ln(1 + mu)
 
-
     Parameters
     ----------
     x : np.ndarray with values in [-1, +1]
@@ -1590,11 +1559,9 @@ def mu_expand(x, *, mu=255.0, quantize=True):
     x : np.ndarray
         The compressed signal.
         If ``quantize=True``, values must be in the range [-1, +1].
-
     mu : positive number
         The compression parameter.  Values of the form ``2**n - 1``
         (e.g., 15, 31, 63, etc.) are most common.
-
     quantize : boolean
         If ``True``, the input is assumed to be quantized to
         ``1 + mu`` distinct integer values."
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -249,6 +249,10 @@ def hybrid_cqt(
     filter_scale : float > 0
         Filter filter_scale factor. Larger values use longer windows.
 
+    norm : {inf, -inf, 0, float > 0}
+        Type of norm to use for basis function normalization.
+        See `librosa.util.normalize`.
+
     sparsity : float in [0, 1)
         Sparsify the CQT basis by discarding up to ``sparsity``
         fraction of the energy in each basis.
@@ -259,6 +263,13 @@ def hybrid_cqt(
         Window specification for the basis filters.
         See `filters.get_window` for details.
 
+    scale : bool
+        If ``True``, scale the CQT response by square-root the length of
+        each channel's filter.  This is analogous to ``norm='ortho'`` in FFT.
+
+        If ``False``, do not scale the CQT. This is analogous to
+        ``norm=None`` in FFT.
+
     pad_mode : string
         Padding mode for centered frame analysis.
 
@@ -422,6 +433,10 @@ def pseudo_cqt(
     filter_scale : float > 0
         Filter filter_scale factor. Larger values use longer windows.
 
+    norm : {inf, -inf, 0, float > 0}
+        Type of norm to use for basis function normalization.
+        See `librosa.util.normalize`.
+
     sparsity : float in [0, 1)
         Sparsify the CQT basis by discarding up to ``sparsity``
         fraction of the energy in each basis.
@@ -432,6 +447,13 @@ def pseudo_cqt(
         Window specification for the basis filters.
         See `filters.get_window` for details.
 
+    scale : bool
+        If ``True``, scale the CQT response by square-root the length of
+        each channel's filter.  This is analogous to ``norm='ortho'`` in FFT.
+
+        If ``False``, do not scale the CQT. This is analogous to
+        ``norm=None`` in FFT.
+
     pad_mode : string
         Padding mode for centered frame analysis.
 
@@ -533,18 +555,23 @@ def icqt(
     Given a constant-Q transform representation ``C`` of an audio signal ``y``,
     this function produces an approximation ``y_hat``.
 
-
     Parameters
     ----------
     C : np.ndarray, [shape=(..., n_bins, n_frames)]
         Constant-Q representation as produced by `cqt`
 
+    sr : number > 0 [scalar]
+        sampling rate of the signal
+
     hop_length : int > 0 [scalar]
         number of samples between successive frames
 
     fmin : float > 0 [scalar]
         Minimum frequency. Defaults to `C1 ~= 32.70 Hz`
 
+    bins_per_octave : int > 0 [scalar]
+        Number of bins per octave
+
     tuning : float [scalar]
         Tuning offset in fractions of a bin.
 
@@ -711,7 +738,12 @@ def icqt(
         y_oct = istft(D_oct, window=""ones"", hop_length=my_hop, dtype=dtype)
 
         y_oct = audio.resample(
-            y_oct, orig_sr=1, target_sr=sr // my_sr, res_type=res_type, scale=False, fix=False
+            y_oct,
+            orig_sr=1,
+            target_sr=sr // my_sr,
+            res_type=res_type,
+            scale=False,
+            fix=False,
         )
 
         if y is None:
@@ -1004,7 +1036,9 @@ def vqt(
         if my_hop % 2 == 0:
             my_hop //= 2
             my_sr /= 2.0
-            my_y = audio.resample(my_y, orig_sr=2, target_sr=1, res_type=res_type, scale=True)
+            my_y = audio.resample(
+                my_y, orig_sr=2, target_sr=1, res_type=res_type, scale=True
+            )
 
     V = __trim_stack(vqt_resp, n_bins, dtype)
 
@@ -1163,7 +1197,9 @@ def __early_downsample(
             )
 
         new_sr = sr / float(downsample_factor)
-        y = audio.resample(y, orig_sr=sr, target_sr=new_sr, res_type=res_type, scale=True)
+        y = audio.resample(
+            y, orig_sr=sr, target_sr=new_sr, res_type=res_type, scale=True
+        )
 
         # If we're not going to length-scale after CQT, we
         # need to compensate for the downsampling factor here
@@ -1328,13 +1364,11 @@ def griffinlim_cqt(
 
         If ``None``, defaults to the current `np.random` object.
 
-
     Returns
     -------
     y : np.ndarray [shape=(..., n)]
         time-domain signal reconstructed from ``C``
 
-
     See Also
     --------
     cqt
@@ -1382,7 +1416,7 @@ def griffinlim_cqt(
         warnings.warn(
             ""Griffin-Lim with momentum={} > 1 can be unstable. ""
             ""Proceed with caution!"".format(momentum),
-            stacklevel=2
+            stacklevel=2,
         )
     elif momentum < 0:
         raise ParameterError("
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -57,12 +57,10 @@ def frames_to_samples(frames, *, hop_length=512, n_fft=None):
 
     Parameters
     ----------
-    frames     : number or np.ndarray [shape=(n,)]
+    frames : number or np.ndarray [shape=(n,)]
         frame index or vector of frame indices
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
@@ -150,15 +148,12 @@ def frames_to_time(frames, *, sr=22050, hop_length=512, n_fft=None):
 
     Parameters
     ----------
-    frames     : np.ndarray [shape=(n,)]
+    frames : np.ndarray [shape=(n,)]
         frame index or vector of frame indices
-
-    sr         : number > 0 [scalar]
+    sr : number > 0 [scalar]
         audio sampling rate
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
@@ -243,7 +238,6 @@ def time_to_samples(times, *, sr=22050):
     ----------
     times : number or np.ndarray
         Time value or array of time values (in seconds)
-
     sr : number > 0
         Sampling rate
 
@@ -275,7 +269,6 @@ def samples_to_time(samples, *, sr=22050):
     ----------
     samples : np.ndarray
         Sample index or array of sample indices
-
     sr : number > 0
         Sampling rate
 
@@ -313,7 +306,6 @@ def blocks_to_frames(blocks, *, block_length):
     ----------
     blocks : np.ndarray
         Block index or array of block indices
-
     block_length : int > 0
         The number of frames per block
 
@@ -350,10 +342,8 @@ def blocks_to_samples(blocks, *, block_length, hop_length):
     ----------
     blocks : np.ndarray
         Block index or array of block indices
-
     block_length : int > 0
         The number of frames per block
-
     hop_length : int > 0
         The number of samples to advance between frames
 
@@ -395,13 +385,10 @@ def blocks_to_time(blocks, *, block_length, hop_length, sr):
     ----------
     blocks : np.ndarray
         Block index or array of block indices
-
     block_length : int > 0
         The number of frames per block
-
     hop_length : int > 0
         The number of samples to advance between frames
-
     sr : int > 0
         The sampling rate (samples per second)
 
@@ -457,8 +444,7 @@ def note_to_hz(note, **kwargs):
     ----------
     note : str or iterable of str
         One or more note names to convert
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters to `note_to_midi`
 
     Returns
@@ -488,7 +474,6 @@ def note_to_midi(note, *, round_midi=True):
     ----------
     note : str or iterable of str
         One or more note names.
-
     round_midi : bool
         - If ``True``, allow for fractional midi notes
         - Otherwise, round cent deviations to the nearest note
@@ -619,23 +604,22 @@ def midi_to_note(midi, *, octave=True, cents=False, key=""C:maj"", unicode=True):
     >>> librosa.midi_to_note(range(12, 24), key='F:min')
     ['C0', 'D♭0', 'D0', 'E♭0', 'E0', 'F0', 'G♭0', 'G0', 'A♭0', 'A0', 'B♭0', 'B0']
 
-
     Parameters
     ----------
     midi : int or iterable of int
         Midi numbers to convert.
 
-    octave: bool
+    octave : bool
         If True, include the octave number
 
-    cents: bool
+    cents : bool
         If true, cent markers will be appended for fractional notes.
         Eg, ``midi_to_note(69.3, cents=True) == 'A4+03'``
 
     key : str
         A key signature to use when resolving enharmonic equivalences.
 
-    unicode: bool
+    unicode : bool
         If ``True`` (default), accidentals will use Unicode notation: ♭ or ♯
 
         If ``False``, accidentals will use ASCII-compatible notation: b or #
@@ -697,12 +681,12 @@ def midi_to_hz(notes):
 
     Parameters
     ----------
-    notes       : int or np.ndarray [shape=(n,), dtype=int]
+    notes : int or np.ndarray [shape=(n,), dtype=int]
         midi number(s) of the note(s)
 
     Returns
     -------
-    frequency   : number or np.ndarray [shape=(n,), dtype=float]
+    frequency : number or np.ndarray [shape=(n,), dtype=float]
         frequency (frequencies) of ``notes`` in Hz
 
     See Also
@@ -726,12 +710,12 @@ def hz_to_midi(frequencies):
 
     Parameters
     ----------
-    frequencies   : float or np.ndarray [shape=(n,), dtype=float]
+    frequencies : float or np.ndarray [shape=(n,), dtype=float]
         frequencies to convert
 
     Returns
     -------
-    note_nums     : number or np.ndarray [shape=(n,), dtype=float]
+    note_nums : number or np.ndarray [shape=(n,), dtype=float]
         MIDI notes to ``frequencies``
 
     See Also
@@ -751,25 +735,21 @@ def hz_to_note(frequencies, **kwargs):
     ----------
     frequencies : float or iterable of float
         Input frequencies, specified in Hz
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments passed through to `midi_to_note`
 
-
     Returns
     -------
     notes : list of str
         ``notes[i]`` is the closest note name to ``frequency[i]``
         (or ``frequency`` if the input is scalar)
 
-
     See Also
     --------
     hz_to_midi
     midi_to_note
     note_to_hz
 
-
     Examples
     --------
     Get a single note name for a frequency
@@ -804,14 +784,14 @@ def hz_to_mel(frequencies, *, htk=False):
 
     Parameters
     ----------
-    frequencies   : number or np.ndarray [shape=(n,)] , float
+    frequencies : number or np.ndarray [shape=(n,)] , float
         scalar or array of frequencies
-    htk           : bool
+    htk : bool
         use HTK formula instead of Slaney
 
     Returns
     -------
-    mels        : number or np.ndarray [shape=(n,)]
+    mels : number or np.ndarray [shape=(n,)]
         input frequencies in Mels
 
     See Also
@@ -860,14 +840,14 @@ def mel_to_hz(mels, *, htk=False):
 
     Parameters
     ----------
-    mels          : np.ndarray [shape=(n,)], float
+    mels : np.ndarray [shape=(n,)], float
         mel bins to convert
-    htk           : bool
+    htk : bool
         use HTK formula instead of Slaney
 
     Returns
     -------
-    frequencies   : np.ndarray [shape=(n,)]
+    frequencies : np.ndarray [shape=(n,)]
         input mels in Hz
 
     See Also
@@ -913,18 +893,16 @@ def hz_to_octs(frequencies, *, tuning=0.0, bins_per_octave=12):
 
     Parameters
     ----------
-    frequencies   : number >0 or np.ndarray [shape=(n,)] or float
+    frequencies : number >0 or np.ndarray [shape=(n,)] or float
         scalar or vector of frequencies
-
-    tuning        : float
+    tuning : float
         Tuning deviation from A440 in (fractional) bins per octave.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    octaves       : number or np.ndarray [shape=(n,)]
+    octaves : number or np.ndarray [shape=(n,)]
         octave number for each frequency
 
     See Also
@@ -951,18 +929,16 @@ def octs_to_hz(octs, *, tuning=0.0, bins_per_octave=12):
 
     Parameters
     ----------
-    octaves       : np.ndarray [shape=(n,)] or float
+    octs : np.ndarray [shape=(n,)] or float
         octave number for each frequency
-
     tuning : float
         Tuning deviation from A440 in (fractional) bins per octave.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    frequencies   : number or np.ndarray [shape=(n,)]
+    frequencies : number or np.ndarray [shape=(n,)]
         scalar or vector of frequencies
 
     See Also
@@ -1006,15 +982,14 @@ def A4_to_tuning(A4, *, bins_per_octave=12):
 
     Parameters
     ----------
-    A4: float or np.ndarray [shape=(n,), dtype=float]
+    A4 : float or np.ndarray [shape=(n,), dtype=float]
         Reference frequency(s) corresponding to A4.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    tuning   : float or np.ndarray [shape=(n,), dtype=float]
+    tuning : float or np.ndarray [shape=(n,), dtype=float]
         Tuning deviation from A440 in (fractional) bins per octave.
 
     See Also
@@ -1059,13 +1034,12 @@ def tuning_to_A4(tuning, *, bins_per_octave=12):
     ----------
     tuning : float or np.ndarray [shape=(n,), dtype=float]
         Tuning deviation from A440 in fractional bins per octave.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    A4  : float or np.ndarray [shape=(n,), dtype=float]
+    A4 : float or np.ndarray [shape=(n,), dtype=float]
         Reference frequency corresponding to A4.
 
     See Also
@@ -1082,17 +1056,14 @@ def fft_frequencies(*, sr=22050, n_fft=2048):
     ----------
     sr : number > 0 [scalar]
         Audio sampling rate
-
     n_fft : int > 0 [scalar]
         FFT window size
 
-
     Returns
     -------
     freqs : np.ndarray [shape=(1 + n_fft/2,)]
         Frequencies ``(0, sr/n_fft, 2*sr/n_fft, ..., sr/2)``
 
-
     Examples
     --------
     >>> librosa.fft_frequencies(sr=22050, n_fft=16)
@@ -1118,15 +1089,12 @@ def cqt_frequencies(n_bins, *, fmin, bins_per_octave=12, tuning=0.0):
 
     Parameters
     ----------
-    n_bins  : int > 0 [scalar]
+    n_bins : int > 0 [scalar]
         Number of constant-Q bins
-
-    fmin    : float > 0 [scalar]
+    fmin : float > 0 [scalar]
         Minimum frequency
-
     bins_per_octave : int > 0 [scalar]
         Number of bins per octave
-
     tuning : float
         Deviation from A440 tuning in fractional bins
 
@@ -1174,27 +1142,22 @@ def mel_frequencies(n_mels=128, *, fmin=0.0, fmax=11025.0, htk=False):
         Moore, G., Odell, J., Ollason, D., Povey, D., Valtchev, V., & Woodland, P.
         The HTK book, version 3.4. Cambridge University, March 2009.
 
-
     See Also
     --------
     hz_to_mel
     mel_to_hz
     librosa.feature.melspectrogram
     librosa.feature.mfcc
 
-
     Parameters
     ----------
-    n_mels    : int > 0 [scalar]
+    n_mels : int > 0 [scalar]
         Number of mel bins.
-
-    fmin      : float >= 0 [scalar]
+    fmin : float >= 0 [scalar]
         Minimum frequency (Hz).
-
-    fmax      : float >= 0 [scalar]
+    fmax : float >= 0 [scalar]
         Maximum frequency (Hz).
-
-    htk       : bool
+    htk : bool
         If True, use HTK formula to convert Hz to mel.
         Otherwise (False), use Slaney's Auditory Toolbox.
 
@@ -1237,10 +1200,8 @@ def tempo_frequencies(n_bins, *, hop_length=512, sr=22050):
     ----------
     n_bins : int > 0
         The number of lag bins
-
     hop_length : int > 0
         The number of samples between each bin
-
     sr : number > 0
         The audio sampling rate
 
@@ -1276,10 +1237,8 @@ def fourier_tempo_frequencies(*, sr=22050, win_length=384, hop_length=512):
     ----------
     sr : number > 0
         The audio sampling rate
-
     win_length : int > 0
         The number of frames per analysis window
-
     hop_length : int > 0
         The number of samples between each bin
 
@@ -1310,7 +1269,6 @@ def A_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1329,10 +1287,8 @@ def A_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the A-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1366,7 +1322,6 @@ def B_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1385,10 +1340,8 @@ def B_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the B-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1421,7 +1374,6 @@ def C_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1440,10 +1392,8 @@ def C_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     B_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the C-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1474,7 +1424,6 @@ def D_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1493,10 +1442,8 @@ def D_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     B_weighting
     C_weighting
 
-
     Examples
     --------
-
     Get the D-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1540,20 +1487,17 @@ def Z_weighting(frequencies, *, min_db=None):  # pylint: disable=invalid-name
 }
 
 
-def frequency_weighting(frequencies, *, kind=""A"", **kw):
+def frequency_weighting(frequencies, *, kind=""A"", **kwargs):
     """"""Compute the weighting of a set of frequencies.
 
     Parameters
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     kind : str in
         The weighting kind. e.g. `'A'`, `'B'`, `'C'`, `'D'`, `'Z'`
-
-    min_db : float [scalar] or None
-        Clip weights below this threshold.
-        If `None`, no clipping is performed.
+    **kwargs
+        Additional keyword arguments to A_weighting, B_weighting, etc.
 
     Returns
     -------
@@ -1569,10 +1513,8 @@ def frequency_weighting(frequencies, *, kind=""A"", **kw):
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the A-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1585,21 +1527,19 @@ def frequency_weighting(frequencies, *, kind=""A"", **kw):
     """"""
     if isinstance(kind, str):
         kind = kind.upper()
-    return WEIGHTING_FUNCTIONS[kind](frequencies, **kw)
+    return WEIGHTING_FUNCTIONS[kind](frequencies, **kwargs)
 
 
-def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kw):
+def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kwargs):
     """"""Compute multiple weightings of a set of frequencies.
 
     Parameters
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     kinds : list or tuple or str
         An iterable of weighting kinds. e.g. `('Z', 'B')`, `'ZAD'`, `'C'`
-
-    **kw : keywords to pass to the weighting function.
+    **kwargs : keywords to pass to the weighting function.
 
     Returns
     -------
@@ -1616,10 +1556,8 @@ def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kw):
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the A, B, C, D, and Z weightings for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1634,7 +1572,7 @@ def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kw):
     >>> ax.legend()
     """"""
     return np.stack(
-        [frequency_weighting(frequencies, kind=k, **kw) for k in kinds], axis=0
+        [frequency_weighting(frequencies, kind=k, **kwargs) for k in kinds], axis=0
     )
 
 
@@ -1646,18 +1584,14 @@ def times_like(X, *, sr=22050, hop_length=512, n_fft=None, axis=-1):
     X : np.ndarray or scalar
         - If ndarray, X is a feature matrix, e.g. STFT, chromagram, or mel spectrogram.
         - If scalar, X represents the number of frames.
-
     sr : number > 0 [scalar]
         audio sampling rate
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
         to counteract windowing effects when using a non-centered STFT.
-
     axis : int [scalar]
         The axis representing the time axis of X.
         By default, the last axis (-1) is taken.
@@ -1669,7 +1603,8 @@ def times_like(X, *, sr=22050, hop_length=512, n_fft=None, axis=-1):
 
     See Also
     --------
-    samples_like : Return an array of sample indices to match the time axis from a feature matrix.
+    samples_like :
+        Return an array of sample indices to match the time axis from a feature matrix.
 
     Examples
     --------
@@ -1701,15 +1636,12 @@ def samples_like(X, *, hop_length=512, n_fft=None, axis=-1):
     X : np.ndarray or scalar
         - If ndarray, X is a feature matrix, e.g. STFT, chromagram, or mel spectrogram.
         - If scalar, X represents the number of frames.
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
         to counteract windowing effects when using a non-centered STFT.
-
     axis : int [scalar]
         The axis representing the time axis of ``X``.
         By default, the last axis (-1) is taken.
@@ -1721,7 +1653,8 @@ def samples_like(X, *, hop_length=512, n_fft=None, axis=-1):
 
     See Also
     --------
-    times_like : Return an array of time values to match the time axis from a feature matrix.
+    times_like :
+        Return an array of time values to match the time axis from a feature matrix.
 
     Examples
     --------"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -141,23 +141,18 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
     ----------
     x : np.ndarray
         The input energy
-
     freqs : np.ndarray, shape=(X.shape[axis])
         The frequency values corresponding to X's elements along the
         chosen axis.
-
     harmonics : list-like, non-negative
         Harmonics to compute as ``harmonics[i] * freqs``.
         The first harmonic (1) corresponds to ``freqs``.
         Values less than one (e.g., 1/2) correspond to sub-harmonics.
-
     kind : str
         Interpolation type.  See `scipy.interpolate.interp1d`.
-
     fill_value : float
         The value to fill when extrapolating beyond the observed
         frequency range.
-
     axis : int
         The axis along which to compute harmonics
 
@@ -173,7 +168,6 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
     --------
     scipy.interpolate.interp1d
 
-
     Examples
     --------
     Estimate the harmonics of a time-averaged tempogram
@@ -227,7 +221,7 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
         if not is_unique(freqs, axis=0):
             warnings.warn(
                 ""Frequencies are not unique. This may produce incorrect harmonic interpolations."",
-                stacklevel=2
+                stacklevel=2,
             )
 
         f_interp = scipy.interpolate.interp1d(
@@ -250,7 +244,7 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
         if not np.all(is_unique(freqs, axis=axis)):
             warnings.warn(
                 ""Frequencies are not unique. This may produce incorrect harmonic interpolations."",
-                stacklevel=2
+                stacklevel=2,
             )
 
         # If we have time-varying frequencies, then it must match exactly the shape of the input"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -483,7 +483,7 @@ def key_to_notes(key, *, unicode=True):
 
         Examples: ``C:maj, Db:min, A♭:min``.
 
-    unicode: bool
+    unicode : bool
         If ``True`` (default), use Unicode symbols (♯𝄪♭𝄫)for accidentals.
 
         If ``False``, Unicode symbols will be mapped to low-order ASCII representations::"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -31,26 +31,20 @@ def estimate_tuning(
 
     Parameters
     ----------
-    y: np.ndarray [shape=(..., n)] or None
+    y : np.ndarray [shape=(..., n)] or None
         audio signal. Multi-channel is supported..
-
     sr : number > 0 [scalar]
         audio sampling rate of ``y``
-
-    S: np.ndarray [shape=(..., d, t)] or None
+    S : np.ndarray [shape=(..., d, t)] or None
         magnitude or power spectrogram
-
     n_fft : int > 0 [scalar] or None
         number of FFT bins to use, if ``y`` is provided.
-
     resolution : float in `(0, 1)`
         Resolution of the tuning as a fraction of a bin.
         0.01 corresponds to measurements in cents.
-
     bins_per_octave : int > 0 [scalar]
         How many frequency bins per octave
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments passed to `piptrack`
 
     Returns
@@ -63,8 +57,7 @@ def estimate_tuning(
 
     See Also
     --------
-    piptrack
-        Pitch tracking by parabolic interpolation
+    piptrack : Pitch tracking by parabolic interpolation
 
     Examples
     --------
@@ -118,11 +111,9 @@ def pitch_tuning(frequencies, *, resolution=0.01, bins_per_octave=12):
     frequencies : array-like, float
         A collection of frequencies detected in the signal.
         See `piptrack`
-
     resolution : float in `(0, 1)`
         Resolution of the tuning as a fraction of a bin.
         0.01 corresponds to cents.
-
     bins_per_octave : int > 0 [scalar]
         How many frequency bins per octave
 
@@ -133,8 +124,7 @@ def pitch_tuning(frequencies, *, resolution=0.01, bins_per_octave=12):
 
     See Also
     --------
-    estimate_tuning
-        Estimating tuning from time-series or spectrogram input
+    estimate_tuning : Estimating tuning from time-series or spectrogram input
 
     Examples
     --------
@@ -160,7 +150,9 @@ def pitch_tuning(frequencies, *, resolution=0.01, bins_per_octave=12):
     frequencies = frequencies[frequencies > 0]
 
     if not np.any(frequencies):
-        warnings.warn(""Trying to estimate tuning from empty frequency set."", stacklevel=2)
+        warnings.warn(
+            ""Trying to estimate tuning from empty frequency set."", stacklevel=2
+        )
         return 0.0
 
     # Compute the residual relative to the number of bins
@@ -204,13 +196,13 @@ def piptrack(
 
     Parameters
     ----------
-    y: np.ndarray [shape=(..., n)] or None
+    y : np.ndarray [shape=(..., n)] or None
         audio signal. Multi-channel is supported..
 
     sr : number > 0 [scalar]
         audio sampling rate of ``y``
 
-    S: np.ndarray [shape=(..., d, t)] or None
+    S : np.ndarray [shape=(..., d, t)] or None
         magnitude or power spectrogram
 
     n_fft : int > 0 [scalar] or None
@@ -387,16 +379,12 @@ def _cumulative_mean_normalized_difference(
     ----------
     y_frames : np.ndarray [shape=(frame_length, n_frames)]
         framed audio time series.
-
     frame_length : int > 0 [scalar]
-         length of the frames in samples.
-
+        length of the frames in samples.
     win_length : int > 0 [scalar]
         length of the window for calculating autocorrelation in samples.
-
     min_period : int > 0 [scalar]
         minimum period.
-
     max_period : int > 0 [scalar]
         maximum period.
 
@@ -492,43 +480,34 @@ def yin(
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported..
-
-    fmin: number > 0 [scalar]
+    fmin : number > 0 [scalar]
         minimum frequency in Hertz.
         The recommended minimum is ``librosa.note_to_hz('C2')`` (~65 Hz)
         though lower values may be feasible.
-
-    fmax: number > 0 [scalar]
+    fmax : number > 0 [scalar]
         maximum frequency in Hertz.
         The recommended maximum is ``librosa.note_to_hz('C7')`` (~2093 Hz)
         though higher values may be feasible.
-
     sr : number > 0 [scalar]
         sampling rate of ``y`` in Hertz.
-
     frame_length : int > 0 [scalar]
-         length of the frames in samples.
-         By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
-         a sampling rate of 22050 Hz.
-
+        length of the frames in samples.
+        By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
+        a sampling rate of 22050 Hz.
     win_length : None or int > 0 [scalar]
         length of the window for calculating autocorrelation in samples.
         If ``None``, defaults to ``frame_length // 2``
-
     hop_length : None or int > 0 [scalar]
-         number of audio samples between adjacent YIN predictions.
-         If ``None``, defaults to ``frame_length // 4``.
-
-    trough_threshold: number > 0 [scalar]
+        number of audio samples between adjacent YIN predictions.
+        If ``None``, defaults to ``frame_length // 4``.
+    trough_threshold : number > 0 [scalar]
         absolute threshold for peak estimation.
-
     center : boolean
         If ``True``, the signal `y` is padded so that frame
         ``D[:, t]`` is centered at `y[t * hop_length]`.
         If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.
         Defaults to ``True``,  which simplifies the alignment of ``D`` onto a
         time grid by means of ``librosa.core.frames_to_samples``.
-
     pad_mode : string or function
         If ``center=True``, this argument is passed to ``np.pad`` for padding
         the edges of the signal ``y``. By default (``pad_mode=""constant""``),
@@ -545,7 +524,7 @@ def yin(
 
     See Also
     --------
-    librosa.pyin
+    librosa.pyin :
         Fundamental frequency (F0) estimation using probabilistic YIN (pYIN).
 
     Examples
@@ -670,72 +649,55 @@ def pyin(
         ""YIN, a fundamental frequency estimator for speech and music.""
         The Journal of the Acoustical Society of America 111.4 (2002): 1917-1930.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    fmin: number > 0 [scalar]
+    fmin : number > 0 [scalar]
         minimum frequency in Hertz.
         The recommended minimum is ``librosa.note_to_hz('C2')`` (~65 Hz)
         though lower values may be feasible.
-
-    fmax: number > 0 [scalar]
+    fmax : number > 0 [scalar]
         maximum frequency in Hertz.
         The recommended maximum is ``librosa.note_to_hz('C7')`` (~2093 Hz)
         though higher values may be feasible.
-
     sr : number > 0 [scalar]
         sampling rate of ``y`` in Hertz.
-
     frame_length : int > 0 [scalar]
-         length of the frames in samples.
-         By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
-         a sampling rate of 22050 Hz.
-
+        length of the frames in samples.
+        By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
+        a sampling rate of 22050 Hz.
     win_length : None or int > 0 [scalar]
         length of the window for calculating autocorrelation in samples.
         If ``None``, defaults to ``frame_length // 2``
-
     hop_length : None or int > 0 [scalar]
         number of audio samples between adjacent pYIN predictions.
         If ``None``, defaults to ``frame_length // 4``.
-
     n_thresholds : int > 0 [scalar]
         number of thresholds for peak estimation.
-
     beta_parameters : tuple
         shape parameters for the beta distribution prior over thresholds.
-
-    boltzmann_parameter: number > 0 [scalar]
+    boltzmann_parameter : number > 0 [scalar]
         shape parameter for the Boltzmann distribution prior over troughs.
         Larger values will assign more mass to smaller periods.
-
     resolution : float in `(0, 1)`
         Resolution of the pitch bins.
         0.01 corresponds to cents.
-
     max_transition_rate : float > 0
         maximum pitch transition rate in octaves per second.
-
     switch_prob : float in ``(0, 1)``
         probability of switching from voiced to unvoiced or vice versa.
-
     no_trough_prob : float in ``(0, 1)``
         maximum probability to add to global minimum if no trough is below threshold.
-
     fill_na : None, float, or ``np.nan``
         default value for unvoiced frames of ``f0``.
         If ``None``, the unvoiced frames will contain a best guess value.
-
     center : boolean
         If ``True``, the signal ``y`` is padded so that frame
         ``D[:, t]`` is centered at ``y[t * hop_length]``.
         If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.
         Defaults to ``True``,  which simplifies the alignment of ``D`` onto a
         time grid by means of ``librosa.core.frames_to_samples``.
-
     pad_mode : string or function
         If ``center=True``, this argument is passed to ``np.pad`` for padding
         the edges of the signal ``y``. By default (``pad_mode=""constant""``),
@@ -747,18 +709,15 @@ def pyin(
     -------
     f0: np.ndarray [shape=(..., n_frames)]
         time series of fundamental frequencies in Hertz.
-
     voiced_flag: np.ndarray [shape=(..., n_frames)]
         time series containing boolean flags indicating whether a frame is voiced or not.
-
     voiced_prob: np.ndarray [shape=(..., n_frames)]
         time series containing the probability that a frame is voiced.
-
     .. note:: If multi-channel input is provided, f0 and voicing are estimated separately for each channel.
 
     See Also
     --------
-    librosa.yin
+    librosa.yin :
         Fundamental frequency (F0) estimation using the YIN algorithm.
 
     Examples
@@ -771,7 +730,6 @@ def pyin(
     ...                                              fmax=librosa.note_to_hz('C7'))
     >>> times = librosa.times_like(f0)
 
-
     Overlay F0 over a spectrogram
 
     >>> import matplotlib.pyplot as plt"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -67,7 +67,6 @@ def stft(
     The integers ``t`` and ``f`` can be converted to physical units by means
     of the utility functions `frames_to_sample` and `fft_frequencies`.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)], real-valued
@@ -143,29 +142,23 @@ def stft(
 
         .. see also:: `numpy.pad`
 
-
     Returns
     -------
     D : np.ndarray [shape=(..., 1 + n_fft/2, n_frames), dtype=dtype]
         Complex-valued matrix of short-term Fourier transform
         coefficients.
 
-
     See Also
     --------
     istft : Inverse STFT
-
     reassigned_spectrogram : Time-frequency reassigned spectrogram
 
-
     Notes
     -----
     This function caches at level 20.
 
-
     Examples
     --------
-
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
     >>> S = np.abs(librosa.stft(y))
     >>> S
@@ -180,12 +173,10 @@ def stft(
 
     >>> S_left = librosa.stft(y, center=False)
 
-
     Use a shorter hop length
 
     >>> D_short = librosa.stft(y, hop_length=64)
 
-
     Display a spectrogram
 
     >>> import matplotlib.pyplot as plt
@@ -223,7 +214,7 @@ def stft(
                 ""n_fft={} is too small for input signal of length={}"".format(
                     n_fft, y.shape[-1]
                 ),
-                stacklevel=2
+                stacklevel=2,
             )
 
         padding = [(0, 0) for _ in range(y.ndim)]
@@ -552,12 +543,11 @@ def __reassign_frequencies(
     freqs : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=real]
         Instantaneous frequencies:
         ``freqs[f, t]`` is the frequency for bin ``f``, frame ``t``.
-
     S : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=complex]
         Short-time Fourier transform
 
     Warns
-    --------
+    -----
     RuntimeWarning
         Frequencies with zero support will produce a divide-by-zero warning and
         will be returned as `np.nan`.
@@ -716,12 +706,11 @@ def __reassign_times(
     times : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=real]
         Reassigned times:
         ``times[f, t]`` is the time for bin ``f``, frame ``t``.
-
     S : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=complex]
         Short-time Fourier transform
 
     Warns
-    --------
+    -----
     RuntimeWarning
         Time estimates with zero support will produce a divide-by-zero warning
         and will be returned as `np.nan`.
@@ -964,7 +953,7 @@ def reassigned_spectrogram(
             ``mags[..., f, t]`` is the magnitude for bin ``f``, frame ``t``.
 
     Warns
-    --------
+    -----
     RuntimeWarning
         Frequency or time estimates with zero support will produce a
         divide-by-zero warning, and will be returned as `np.nan` unless
@@ -1123,26 +1112,21 @@ def magphase(D, *, power=1):
     """"""Separate a complex-valued spectrogram D into its magnitude (S)
     and phase (P) components, so that ``D = S * P``.
 
-
     Parameters
     ----------
     D : np.ndarray [shape=(..., d, t), dtype=complex]
         complex-valued spectrogram
-
     power : float > 0
         Exponent for the magnitude spectrogram,
         e.g., 1 for energy, 2 for power, etc.
 
-
     Returns
     -------
     D_mag : np.ndarray [shape=(..., d, t), dtype=real]
         magnitude of ``D``, raised to ``power``
-
     D_phase : np.ndarray [shape=(..., d, t), dtype=complex]
         ``exp(1.j * phi)`` where ``phi`` is the phase of ``D``
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -1202,7 +1186,6 @@ def phase_vocoder(D, *, rate, hop_length=None, n_fft=None):
 
     .. [#] https://breakfastquay.com/rubberband/
 
-
     Examples
     --------
     >>> # Play at double speed
@@ -1222,7 +1205,7 @@ def phase_vocoder(D, *, rate, hop_length=None, n_fft=None):
     D : np.ndarray [shape=(..., d, t), dtype=complex]
         STFT matrix
 
-    rate :  float > 0 [scalar]
+    rate : float > 0 [scalar]
         Speed-up factor: ``rate > 1`` is faster, ``rate < 1`` is slower.
 
     hop_length : int > 0 [scalar] or None
@@ -1332,44 +1315,34 @@ def iirt(
            ""Information Retrieval for Music and Motion.""
            Springer Verlag. 2007.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
     sr : number > 0 [scalar]
         sampling rate of ``y``
-
     win_length : int > 0, <= n_fft
         Window length.
-
     hop_length : int > 0 [scalar]
         Hop length, number samples between subsequent frames.
         If not supplied, defaults to ``win_length // 4``.
-
     center : boolean
         - If ``True``, the signal ``y`` is padded so that frame
           ``D[..., :, t]`` is centered at ``y[t * hop_length]``.
         - If ``False``, then `D[..., :, t]`` begins at ``y[t * hop_length]``
-
     tuning : float [scalar]
         Tuning deviation from A440 in fractions of a bin.
-
     pad_mode : string
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, this function uses zero padding.
-
     flayout : string
         - If `sos` (default), a series of second-order filters is used for filtering with `scipy.signal.sosfiltfilt`.
           Minimizes numerical precision errors for high-order filters, but is slower.
         - If `ba`, the standard difference equation is used for filtering with `scipy.signal.filtfilt`.
           Can be unstable for high-order filters.
-
     res_type : string
         The resampling mode.  See `librosa.resample` for details.
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments for `librosa.filters.semitone_filterbank`
         (e.g., could be used to provide another set of ``center_freqs`` and ``sample_rates``).
 
@@ -1540,7 +1513,6 @@ def power_to_db(S, *, ref=1.0, amin=1e-10, top_db=80.0):
     -----
     This function caches at level 30.
 
-
     Examples
     --------
     Get a power spectrogram from a waveform ``y``
@@ -1572,7 +1544,6 @@ def power_to_db(S, *, ref=1.0, amin=1e-10, top_db=80.0):
            [16.578, 16.578, ..., 16.578, 16.578],
            [16.578, 16.578, ..., 16.578, 16.578]], dtype=float32)
 
-
     And plot the results
 
     >>> import matplotlib.pyplot as plt
@@ -1633,7 +1604,6 @@ def db_to_power(S_db, *, ref=1.0):
     ----------
     S_db : np.ndarray
         dB-scaled spectrogram
-
     ref : number > 0
         Reference power: output will be scaled by this value
 
@@ -1674,7 +1644,6 @@ def amplitude_to_db(S, *, ref=1.0, amin=1e-5, top_db=80.0):
         threshold the output at ``top_db`` below the peak:
         ``max(20 * log10(S)) - top_db``
 
-
     Returns
     -------
     S_db : np.ndarray
@@ -1696,7 +1665,7 @@ def amplitude_to_db(S, *, ref=1.0, amin=1e-5, top_db=80.0):
             ""amplitude_to_db was called on complex input so phase ""
             ""information will be discarded. To suppress this warning, ""
             ""call amplitude_to_db(np.abs(S)) instead."",
-            stacklevel=2
+            stacklevel=2,
         )
 
     magnitude = np.abs(S)
@@ -1724,8 +1693,7 @@ def db_to_amplitude(S_db, *, ref=1.0):
     ----------
     S_db : np.ndarray
         dB-scaled spectrogram
-
-    ref: number > 0
+    ref : number > 0
         Optional reference power.
 
     Returns
@@ -1750,15 +1718,12 @@ def perceptual_weighting(S, frequencies, *, kind=""A"", **kwargs):
     ----------
     S : np.ndarray [shape=(..., d, t)]
         Power spectrogram
-
     frequencies : np.ndarray [shape=(d,)]
         Center frequency for each row of` `S``
-
     kind : str
         The frequency weighting curve to use.
         e.g. `'A'`, `'B'`, `'C'`, `'D'`, `None or 'Z'`
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional keyword arguments to `power_to_db`.
 
     Returns
@@ -1774,7 +1739,6 @@ def perceptual_weighting(S, frequencies, *, kind=""A"", **kwargs):
     -----
     This function caches at level 30.
 
-
     Examples
     --------
     Re-weight a CQT power spectrum, using peak power as reference
@@ -1827,7 +1791,6 @@ def fmt(y, *, t_min=0.5, n_fmt=None, kind=""cubic"", beta=0.5, over_sample=1, axis
     to scaling of the domain (e.g., time stretching or compression).  This is analogous
     to the magnitude of the Fourier transform being invariant to shifts in the input domain.
 
-
     .. [#] De Sena, Antonio, and Davide Rocchesso.
         ""A fast Mellin and scale transform.""
         EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.
@@ -1885,7 +1848,6 @@ def fmt(y, *, t_min=0.5, n_fmt=None, kind=""cubic"", beta=0.5, over_sample=1, axis
     -----
     This function caches at level 30.
 
-
     Examples
     --------
     >>> # Generate a signal and time-stretch it (with energy normalization)
@@ -2079,7 +2041,6 @@ def pcen(
        Kelling, S., and Bello, J. P. Per-Channel Energy Normalization: Why and How.
        IEEE Signal Processing Letters, 26(1), 39-43.
 
-
     Parameters
     ----------
     S : np.ndarray (non-negative)
@@ -2143,12 +2104,10 @@ def pcen(
 
         If ``False`` (default) only the PCEN values ``P`` are returned.
 
-
     Returns
     -------
     P : np.ndarray, non-negative [shape=(n, m)]
         The per-channel energy normalized version of ``S``.
-
     zf : np.ndarray (optional)
         The final filter delay values.  Only returned if ``return_zf=True``.
 
@@ -2159,7 +2118,6 @@ def pcen(
 
     Examples
     --------
-
     Compare PCEN to log amplitude (dB) scaling on Mel spectra
 
     >>> import matplotlib.pyplot as plt
@@ -2229,7 +2187,7 @@ def pcen(
             ""pcen was called on complex input so phase ""
             ""information will be discarded. To suppress this warning, ""
             ""call pcen(np.abs(D)) instead."",
-            stacklevel=2
+            stacklevel=2,
         )
         S = np.abs(S)
 
@@ -2380,7 +2338,6 @@ def griffinlim(
 
         If `None`, defaults to the current `np.random` object.
 
-
     Returns
     -------
     y : np.ndarray [shape=(..., n)]
@@ -2430,7 +2387,7 @@ def griffinlim(
         warnings.warn(
             ""Griffin-Lim with momentum={} > 1 can be unstable. ""
             ""Proceed with caution!"".format(momentum),
-            stacklevel=2
+            stacklevel=2,
         )
     elif momentum < 0:
         raise ParameterError(
@@ -2518,7 +2475,6 @@ def _spectrogram(
     This is primarily used in feature extraction functions that can operate on
     either audio time-series or spectrogram input.
 
-
     Parameters
     ----------
     y : None or np.ndarray
@@ -2561,13 +2517,11 @@ def _spectrogram(
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, STFT uses zero padding.
 
-
     Returns
     -------
     S_out : np.ndarray [dtype=np.float]
         - If ``S`` is provided as input, then ``S_out == S``
         - Else, ``S_out = |stft(y, ...)|**power``
-
     n_fft : int > 0
         - If ``S`` is provided, then ``n_fft`` is inferred from ``S``
         - Else, copied from input"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -38,7 +38,6 @@ def decompose(
     By default, this is done with with non-negative matrix factorization (NMF),
     but any `sklearn.decomposition`-type object will work.
 
-
     Parameters
     ----------
     S : np.ndarray [shape=(..., n_features, n_samples), dtype=float]
@@ -92,32 +91,27 @@ def decompose(
         If `False`, components are assumed to be pre-computed and stored
         in ``transformer``, and are not changed.
 
-    kwargs : Additional keyword arguments to the default transformer
+    **kwargs : Additional keyword arguments to the default transformer
         `sklearn.decomposition.NMF`
 
-
     Returns
     -------
     components: np.ndarray [shape=(..., n_features, n_components)]
         matrix of components (basis elements).
-
     activations: np.ndarray [shape=(n_components, n_samples)]
         transformed matrix/activation matrix
 
-
     Raises
     ------
     ParameterError
         if ``fit`` is False and no ``transformer`` object is provided.
 
         if the input array is multi-channel and ``sort=True`` is specified.
 
-
     See Also
     --------
     sklearn.decomposition : SciKit-Learn matrix decomposition modules
 
-
     Examples
     --------
     Decompose a magnitude spectrogram into 16 components with NMF
@@ -131,7 +125,6 @@ def decompose(
     >>> comps, acts = librosa.decompose.decompose(S, n_components=16,
     ...                                           sort=True)
 
-
     Or with sparse dictionary learning
 
     >>> import sklearn.decomposition
@@ -254,7 +247,6 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
         Components can be recovered by multiplying ``S * mask_H``
         or ``S * mask_P``.
 
-
     margin : float or tuple (margin_harmonic, margin_percussive)
         margin size(s) for the masks (as described in [2]_)
 
@@ -267,11 +259,9 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
     -------
     harmonic : np.ndarray [shape=(..., d, n)]
         harmonic component (or mask)
-
     percussive : np.ndarray [shape=(..., d, n)]
         percussive component (or mask)
 
-
     See Also
     --------
     librosa.util.softmask
@@ -306,7 +296,6 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
     >>> ax[2].set(title='Percussive power spectrogram')
     >>> fig.colorbar(img, ax=ax, format='%+2.0f dB')
 
-
     Or with a narrower horizontal filter
 
     >>> H, P = librosa.decompose.hpss(D, kernel_size=(13, 31))
@@ -337,7 +326,6 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
     >>> y_perc = librosa.istft(P)
     >>> y_resi = librosa.istft(R)
 
-
     Get a more isolated percussive component by widening its margin
 
     >>> H, P = librosa.decompose.hpss(D, margin=(1.0,5.0))
@@ -441,11 +429,10 @@ def nn_filter(S, *, rec=None, aggregate=None, axis=-1, **kwargs):
         For all other aggregation functions, all neighbors
         are treated equally.
 
-
     axis : int
         The axis along which to filter (by default, columns)
 
-    kwargs
+    **kwargs
         Additional keyword arguments provided to
         `librosa.segment.recurrence_matrix` if ``rec`` is not provided
 
@@ -459,21 +446,18 @@ def nn_filter(S, *, rec=None, aggregate=None, axis=-1, **kwargs):
     ParameterError
         if ``rec`` is provided and its shape is incompatible with ``S``.
 
-    See also
+    See Also
     --------
     decompose
     hpss
     librosa.segment.recurrence_matrix
 
-
     Notes
     -----
     This function caches at level 30.
 
-
     Examples
     --------
-
     De-noise a chromagram by non-local median filtering.
     By default this would use euclidean distance to select neighbors,
     but this can be overridden directly by setting the ``metric`` parameter.
@@ -549,14 +533,11 @@ def __nn_filter_helper(R_data, R_indices, R_ptr, S, aggregate):
     ----------
     R_data, R_indices, R_ptr : np.ndarrays
         The ``data``, ``indices``, and ``indptr`` of a scipy.sparse matrix
-
     S : np.ndarray
         The observation data to filter
-
     aggregate : callable
         The aggregation operator
 
-
     Returns
     -------
     S_out : np.ndarray like S"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -566,17 +566,13 @@ def cmap(
     ----------
     data : np.ndarray
         Input data
-
     robust : bool
         If True, discard the top and bottom 2% of data when calculating
         range.
-
     cmap_seq : str
         The sequential colormap name
-
     cmap_bool : str
         The boolean colormap name
-
     cmap_div : str
         The diverging colormap name
 
@@ -731,7 +727,6 @@ def specshow(
             using `feature.fourier_tempogram`.
 
     x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]
-
         Optional positioning coordinates of the input data.
         These can be use to explicitly set the location of each
         element ``data[i, j]``, e.g., for displaying beat-synchronous
@@ -801,7 +796,7 @@ def specshow(
     ax : matplotlib.axes.Axes or None
         Axes to plot on instead of the default `plt.gca()`.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments passed through to `matplotlib.pyplot.pcolormesh`.
 
         By default, the following options are set:
@@ -815,14 +810,11 @@ def specshow(
     colormesh : `matplotlib.collections.QuadMesh`
         The color mesh object produced by `matplotlib.pyplot.pcolormesh`
 
-
     See Also
     --------
     cmap : Automatic colormap detection
-
     matplotlib.pyplot.pcolormesh
 
-
     Examples
     --------
     Visualize an STFT power spectrum using default parameters
@@ -836,7 +828,6 @@ def specshow(
     >>> ax[0].set(title='Linear-frequency power spectrogram')
     >>> ax[0].label_outer()
 
-
     Or on a logarithmic scale, and using a larger hop
 
     >>> hop_length = 1024
@@ -852,7 +843,7 @@ def specshow(
     if np.issubdtype(data.dtype, np.complexfloating):
         warnings.warn(
             ""Trying to display complex-valued input. "" ""Showing magnitude instead."",
-            stacklevel=2
+            stacklevel=2,
         )
         data = np.abs(data)
 
@@ -1326,7 +1317,6 @@ def waveshow(
         If you want to visualize both channels at the sample level, it is recommended to
         plot each signal independently.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(n,) or (2,n)]
@@ -1362,7 +1352,6 @@ def waveshow(
 
         - `None`, 'none', or 'off': ticks and tick markers are hidden.
 
-
     ax : matplotlib.axes.Axes or None
         Axes to plot on instead of the default `plt.gca()`.
 
@@ -1386,7 +1375,7 @@ def waveshow(
         The label string applied to this plot.
         Note that the label
 
-    kwargs
+    **kwargs
         Additional keyword arguments to `matplotlib.pyplot.fill_between` and
         `matplotlib.pyplot.step`.
 
@@ -1398,14 +1387,13 @@ def waveshow(
     librosa.display.AdaptiveWaveplot
         An object of type `librosa.display.AdaptiveWaveplot`
 
-    See also
+    See Also
     --------
     AdaptiveWaveplot
     matplotlib.pyplot.step
     matplotlib.pyplot.fill_between
     matplotlib.markers
 
-
     Examples
     --------
     Plot a monophonic waveform with an envelope view"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -60,21 +60,17 @@ def hpss(y, **kwargs):
     This function automates the STFT->HPSS->ISTFT pipeline, and ensures that
     the output waveforms have equal length to the input waveform ``y``.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.hpss` for details.
 
-
     Returns
     -------
     y_harmonic : np.ndarray [shape=(..., n)]
         audio time series of the harmonic elements
-
     y_percussive : np.ndarray [shape=(..., n)]
         audio time series of the percussive elements
 
@@ -84,7 +80,6 @@ def hpss(y, **kwargs):
     percussive : Extract only the percussive component
     librosa.decompose.hpss : HPSS on spectrograms
 
-
     Examples
     --------
     >>> # Extract harmonic and percussive components
@@ -116,8 +111,7 @@ def harmonic(y, **kwargs):
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.hpss` for details.
 
     Returns
@@ -161,8 +155,7 @@ def percussive(y, **kwargs):
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.hpss` for details.
 
     Returns
@@ -202,17 +195,14 @@ def percussive(y, **kwargs):
 def time_stretch(y, *, rate, **kwargs):
     """"""Time-stretch an audio series by a fixed rate.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
     rate : float > 0 [scalar]
         Stretch factor.  If ``rate > 1``, then the signal is sped up.
         If ``rate < 1``, then the signal is slowed down.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.stft` for details.
 
     Returns
@@ -222,9 +212,12 @@ def time_stretch(y, *, rate, **kwargs):
 
     See Also
     --------
-    pitch_shift : pitch shifting
-    librosa.phase_vocoder : spectrogram phase vocoder
-    pyrubberband.pyrb.time_stretch : high-quality time stretching using RubberBand
+    pitch_shift :
+        pitch shifting
+    librosa.phase_vocoder :
+        spectrogram phase vocoder
+    pyrubberband.pyrb.time_stretch :
+        high-quality time stretching using RubberBand
 
     Examples
     --------
@@ -288,20 +281,22 @@ def pitch_shift(
 
         See `librosa.resample` for more information.
 
-    kwargs: additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.stft` for details.
 
     Returns
     -------
     y_shift : np.ndarray [shape=(..., n)]
         The pitch-shifted audio time-series
 
-
     See Also
     --------
-    time_stretch : time stretching
-    librosa.phase_vocoder : spectrogram phase vocoder
-    pyrubberband.pyrb.pitch_shift : high-quality pitch shifting using RubberBand
+    time_stretch :
+        time stretching
+    librosa.phase_vocoder :
+        spectrogram phase vocoder
+    pyrubberband.pyrb.pitch_shift :
+        high-quality pitch shifting using RubberBand
 
     Examples
     --------
@@ -327,7 +322,10 @@ def pitch_shift(
 
     # Stretch in time, then resample
     y_shift = core.resample(
-        time_stretch(y, rate=rate, **kwargs), orig_sr=float(sr) / rate, target_sr=sr, res_type=res_type
+        time_stretch(y, rate=rate, **kwargs),
+        orig_sr=float(sr) / rate,
+        target_sr=sr,
+        res_type=res_type,
     )
 
     # Crop to the same dimension as the input
@@ -337,53 +335,44 @@ def pitch_shift(
 def remix(y, intervals, *, align_zeros=True):
     """"""Remix an audio signal by re-ordering time intervals.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., t)]
         Audio time series. Multi-channel is supported.
-
     intervals : iterable of tuples (start, end)
         An iterable (list-like or generator) where the ``i``th item
         ``intervals[i]`` indicates the start and end (in samples)
         of a slice of ``y``.
-
     align_zeros : boolean
         If ``True``, interval boundaries are mapped to the closest
         zero-crossing in ``y``.  If ``y`` is stereo, zero-crossings
         are computed after converting to mono.
 
-
     Returns
     -------
     y_remix : np.ndarray [shape=(..., d)]
         ``y`` remixed in the order specified by ``intervals``
 
-
     Examples
     --------
     Load in the example track and reverse the beats
 
     >>> y, sr = librosa.load(librosa.ex('choice'))
 
-
     Compute beats
 
     >>> _, beat_frames = librosa.beat.beat_track(y=y, sr=sr,
     ...                                          hop_length=512)
 
-
     Convert from frames to sample indices
 
     >>> beat_samples = librosa.frames_to_samples(beat_frames)
 
-
     Generate intervals from consecutive events
 
     >>> intervals = librosa.util.frame(beat_samples, frame_length=2,
     ...                                hop_length=1).T
 
-
     Reverse the beat intervals
 
     >>> y_out = librosa.effects.remix(y, intervals[::-1])
@@ -465,35 +454,28 @@ def trim(
     ----------
     y : np.ndarray, shape=(..., n)
         Audio signal. Multi-channel is supported.
-
     top_db : number > 0
         The threshold (in decibels) below reference to consider as
         silence
-
     ref : number or callable
         The reference power.  By default, it uses `np.max` and compares
         to the peak power in the signal.
-
     frame_length : int > 0
         The number of samples per analysis frame
-
     hop_length : int > 0
         The number of samples between analysis frames
-
     aggregate : callable [default: np.max]
         Function to aggregate across channels (if y.ndim > 1)
 
     Returns
     -------
     y_trimmed : np.ndarray, shape=(..., m)
         The trimmed signal
-
     index : np.ndarray, shape=(2,)
         the interval of ``y`` corresponding to the non-silent region:
         ``y_trimmed = y[index[0]:index[1]]`` (for mono) or
         ``y_trimmed = y[:, index[0]:index[1]]`` (for stereo).
 
-
     Examples
     --------
     >>> # Load some audio
@@ -544,22 +526,17 @@ def split(
     ----------
     y : np.ndarray, shape=(..., n)
         An audio signal. Multi-channel is supported.
-
     top_db : number > 0
         The threshold (in decibels) below reference to consider as
         silence
-
     ref : number or callable
         The reference power.  By default, it uses `np.max` and compares
         to the peak power in the signal.
-
     frame_length : int > 0
         The number of samples per analysis frame
-
     hop_length : int > 0
         The number of samples between analysis frames
-
-    aggregate callable [default: np.max]
+    aggregate : callable [default: np.max]
         Function to aggregate across channels (if y.ndim > 1)
 
     Returns
@@ -610,7 +587,6 @@ def preemphasis(y, *, coef=0.97, zi=None, return_zf=False):
 
         y[n] -> y[n] - coef * y[n-1]
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
@@ -643,7 +619,6 @@ def preemphasis(y, *, coef=0.97, zi=None, return_zf=False):
     -------
     y_out : np.ndarray
         pre-emphasized signal
-
     zf : number
         if ``return_zf=True``, the final filter state is also returned
 
@@ -731,12 +706,10 @@ def deemphasis(y, *, coef=0.97, zi=None, return_zf=False):
         If ``True``, return the final filter state.
         If ``False``, only return the pre-emphasized signal.
 
-
     Returns
     -------
     y_out : np.ndarray
         de-emphasized signal
-
     zf : number
         if ``return_zf=True``, the final filter state is also returned
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -24,35 +24,28 @@ def mel_to_stft(M, *, sr=22050, n_fft=2048, power=2.0, **kwargs):
     ----------
     M : np.ndarray [shape=(..., n_mels, n), non-negative]
         The spectrogram as produced by `feature.melspectrogram`
-
     sr : number > 0 [scalar]
         sampling rate of the underlying signal
-
     n_fft : int > 0 [scalar]
         number of FFT components in the resulting STFT
-
     power : float > 0 [scalar]
         Exponent for the magnitude melspectrogram
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Mel filter bank parameters.
         See `librosa.filters.mel` for details
 
-
     Returns
     -------
     S : np.ndarray [shape=(..., n_fft, t), non-negative]
         An approximate linear magnitude spectrogram
 
-
     See Also
     --------
     librosa.feature.melspectrogram
     librosa.stft
     librosa.filters.mel
     librosa.util.nnls
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -118,47 +111,34 @@ def mel_to_audio(
     ----------
     M : np.ndarray [shape=(..., n_mels, n), non-negative]
         The spectrogram as produced by `feature.melspectrogram`
-
     sr : number > 0 [scalar]
         sampling rate of the underlying signal
-
     n_fft : int > 0 [scalar]
         number of FFT components in the resulting STFT
-
     hop_length : None or int > 0
         The hop length of the STFT.  If not provided, it will default to ``n_fft // 4``
-
     win_length : None or int > 0
         The window length of the STFT.  By default, it will equal ``n_fft``
-
     window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]
         A window specification as supported by `stft` or `istft`
-
     center : boolean
         If `True`, the STFT is assumed to use centered frames.
         If `False`, the STFT is assumed to use left-aligned frames.
-
     pad_mode : string
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, STFT uses zero padding.
-
     power : float > 0 [scalar]
         Exponent for the magnitude melspectrogram
-
     n_iter : int > 0
         The number of iterations for Griffin-Lim
-
     length : None or int > 0
         If provided, the output ``y`` is zero-padded or clipped to exactly ``length``
         samples.
-
     dtype : np.dtype
         Real numeric type for the time-domain signal.  Default is 32-bit float.
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Mel filter bank parameters
 
-
     Returns
     -------
     y : np.ndarray [shape(..., n,)]
@@ -197,7 +177,6 @@ def mfcc_to_mel(mfcc, *, n_mels=128, dct_type=2, norm=""ortho"", ref=1.0, lifter=0
         1. The inverse DCT is applied to the MFCCs
         2. `librosa.db_to_power` is applied to map the dB-scaled result to a power spectrogram
 
-
     Parameters
     ----------
     mfcc : np.ndarray [shape=(..., n_mfcc, n)]
@@ -230,7 +209,7 @@ def mfcc_to_mel(mfcc, *, n_mels=128, dct_type=2, norm=""ortho"", ref=1.0, lifter=0
         An approximate Mel power spectrum recovered from ``mfcc``
 
     Warns
-    --------
+    -----
     UserWarning
         due to critical values in lifter array that invokes underflow.
 
@@ -274,7 +253,6 @@ def mfcc_to_audio(
         1. Convert mfcc to Mel power spectrum (`mfcc_to_mel`)
         2. Convert Mel power spectrum to time-domain audio (`mel_to_audio`)
 
-
     Parameters
     ----------
     mfcc : np.ndarray [shape=(..., n_mfcc, n)]
@@ -301,7 +279,7 @@ def mfcc_to_audio(
 
             M[n, :] <- M[n, :] / (1 + sin(pi * (n + 1) / lifter)) * lifter / 2
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Parameters to pass through to `mel_to_audio`
 
     Returns"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -87,7 +87,6 @@ def tempogram(
     librosa.util.normalize
     librosa.stft
 
-
     Examples
     --------
     >>> # Compute local onset autocorrelation
@@ -194,26 +193,20 @@ def fourier_tempogram(
     ----------
     y : np.ndarray [shape=(..., n)] or None
         Audio time series.  Multi-channel is supported.
-
     sr : number > 0 [scalar]
         sampling rate of ``y``
-
     onset_envelope : np.ndarray [shape=(..., n)] or None
         Optional pre-computed onset strength envelope as provided by
         ``librosa.onset.onset_strength``.
         Multi-channel is supported.
-
     hop_length : int > 0
         number of audio samples between successive onset measurements
-
     win_length : int > 0
         length of the onset window (in frames/onset measurements)
         The default settings (384) corresponds to ``384 * hop_length / sr ~= 8.9s``.
-
     center : bool
         If `True`, onset windows are centered.
         If `False`, windows are left-aligned.
-
     window : string, function, number, tuple, or np.ndarray [shape=(win_length,)]
         A window specification as in `stft`.
 
@@ -236,7 +229,6 @@ def fourier_tempogram(
     librosa.util.normalize
     librosa.stft
 
-
     Examples
     --------
     >>> # Compute local onset autocorrelation"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -116,19 +116,15 @@ def spectral_centroid(
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, STFT uses zero padding.
 
-
     Returns
     -------
     centroid : np.ndarray [shape=(..., 1, t)]
         centroid frequencies
 
     See Also
     --------
-    librosa.stft
-        Short-time Fourier Transform
-
-    librosa.reassigned_spectrogram
-        Time-frequency reassigned spectrogram
+    librosa.stft : Short-time Fourier Transform
+    librosa.reassigned_spectrogram : Time-frequency reassigned spectrogram
 
     Examples
     --------
@@ -282,13 +278,11 @@ def spectral_bandwidth(
     p : float > 0
         Power to raise deviation from spectral centroid.
 
-
     Returns
     -------
     bandwidth : np.ndarray [shape=(..., 1, t)]
         frequency bandwidth for each frame
 
-
     Examples
     --------
     From time-series input
@@ -471,14 +465,12 @@ def spectral_contrast(
         If `False`, return the logarithmic difference:
         ``log(peaks) - log(valleys)``.
 
-
     Returns
     -------
     contrast : np.ndarray [shape=(..., n_bands + 1, t)]
         each row of spectral contrast values corresponds to a given
         octave-based frequency
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -653,7 +645,6 @@ def spectral_rolloff(
     rolloff : np.ndarray [shape=(..., 1, t)]
         roll-off frequency for each frame
 
-
     Examples
     --------
     From time-series input
@@ -814,7 +805,6 @@ def spectral_flatness(
         spectral flatness for each frame.
         The returned value is in [0, 1] and often converted to dB scale.
 
-
     Examples
     --------
     From time-series input
@@ -885,7 +875,6 @@ def rms(
     representation of energy over time because its frames can be windowed,
     thus prefer using ``S`` if it's already available.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)] or None
@@ -916,7 +905,6 @@ def rms(
     rms : np.ndarray [shape=(..., 1, t)]
         RMS value for each frame
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -1159,7 +1147,7 @@ def zero_crossing_rate(y, *, frame_length=2048, hop_length=512, center=True, **k
         This is similar to the padding in `librosa.stft`,
         but uses edge-value copies instead of zero-padding.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         See `librosa.zero_crossings`
 
         .. note:: By default, the ``pad`` parameter is set to `False`, which
@@ -1173,8 +1161,7 @@ def zero_crossing_rate(y, *, frame_length=2048, hop_length=512, center=True, **k
 
     See Also
     --------
-    librosa.zero_crossings
-        Compute zero-crossings in a time-series
+    librosa.zero_crossings : Compute zero-crossings in a time-series
 
     Examples
     --------
@@ -1281,7 +1268,7 @@ def chroma_stft(
     n_chroma : int > 0 [scalar]
         Number of chroma bins to produce (12 by default).
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to parameterize chroma filters.
         See `librosa.filters.chroma` for details.
 
@@ -1292,11 +1279,8 @@ def chroma_stft(
 
     See Also
     --------
-    librosa.filters.chroma
-        Chroma filter bank construction
-
-    librosa.util.normalize
-        Vector normalization
+    librosa.filters.chroma : Chroma filter bank construction
+    librosa.util.normalize : Vector normalization
 
     Examples
     --------
@@ -1430,7 +1414,6 @@ def chroma_cqt(
 
         If `None`, it will match ``n_chroma``.
 
-
     cqt_mode : ['full', 'hybrid']
         Constant-Q transform mode
 
@@ -1450,7 +1433,6 @@ def chroma_cqt(
     --------
     Compare a long-window STFT chromagram to the CQT chromagram
 
-
     >>> y, sr = librosa.load(librosa.ex('nutcracker'), duration=15)
     >>> chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr,
     ...                                           n_chroma=12, n_fft=4096)
@@ -1601,20 +1583,14 @@ def chroma_cens(
 
     See Also
     --------
-    chroma_cqt
-        Compute a chromagram from a constant-Q transform.
-
-    chroma_stft
-        Compute a chromagram from an STFT spectrogram or waveform.
-
-    librosa.filters.get_window
-        Compute a window function.
+    chroma_cqt : Compute a chromagram from a constant-Q transform.
+    chroma_stft : Compute a chromagram from an STFT spectrogram or waveform.
+    librosa.filters.get_window : Compute a window function.
 
     Examples
     --------
     Compare standard cqt chroma to CENS.
 
-
     >>> y, sr = librosa.load(librosa.ex('nutcracker'), duration=15)
     >>> chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
     >>> chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)
@@ -1707,7 +1683,7 @@ def tonnetz(*, y=None, sr=22050, chroma=None, **kwargs):
 
         If `None`, a cqt chromagram is performed.
 
-    kwargs
+    **kwargs
         Additional keyword arguments to `chroma_cqt`, if ``chroma`` is not
         pre-computed.
 
@@ -1726,11 +1702,8 @@ def tonnetz(*, y=None, sr=22050, chroma=None, **kwargs):
 
     See Also
     --------
-    chroma_cqt
-        Compute a chromagram from a constant-Q transform.
-
-    chroma_stft
-        Compute a chromagram from an STFT spectrogram or waveform.
+    chroma_cqt : Compute a chromagram from a constant-Q transform.
+    chroma_stft : Compute a chromagram from an STFT spectrogram or waveform.
 
     Examples
     --------
@@ -1812,7 +1785,7 @@ def mfcc(
     S : np.ndarray [shape=(..., d, t)] or None
         log-power Mel spectrogram
 
-    n_mfcc: int > 0 [scalar]
+    n_mfcc : int > 0 [scalar]
         number of MFCCs to return
 
     dct_type : {1, 2, 3}
@@ -1833,7 +1806,7 @@ def mfcc(
         Setting ``lifter >= 2 * n_mfcc`` emphasizes the higher-order coefficients.
         As ``lifter`` increases, the coefficient weighting becomes approximately linear.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to `melspectrogram`, if operating
         on time series input
 
@@ -2004,7 +1977,7 @@ def melspectrogram(
         Exponent for the magnitude melspectrogram.
         e.g., 1 for energy, 2 for power, etc.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Mel filter bank parameters.
 
         See `librosa.filters.mel` for details.
@@ -2016,12 +1989,8 @@ def melspectrogram(
 
     See Also
     --------
-    librosa.filters.mel
-        Mel filter bank construction
-
-    librosa.stft
-        Short-time Fourier Transform
-
+    librosa.filters.mel : Mel filter bank construction
+    librosa.stft : Short-time Fourier Transform
 
     Examples
     --------"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -21,32 +21,32 @@ def delta(data, *, width=9, order=1, axis=-1, mode=""interp"", **kwargs):
 
     Parameters
     ----------
-    data      : np.ndarray
+    data : np.ndarray
         the input data matrix (eg, spectrogram)
 
-    width     : int, positive, odd [scalar]
+    width : int, positive, odd [scalar]
         Number of frames over which to compute the delta features.
         Cannot exceed the length of ``data`` along the specified axis.
 
         If ``mode='interp'``, then ``width`` must be at least ``data.shape[axis]``.
 
-    order     : int > 0 [scalar]
+    order : int > 0 [scalar]
         the order of the difference operator.
         1 for first derivative, 2 for second, etc.
 
-    axis      : int [scalar]
+    axis : int [scalar]
         the axis along which to compute deltas.
         Default is -1 (columns).
 
     mode : str, {'interp', 'nearest', 'mirror', 'constant', 'wrap'}
         Padding mode for estimating differences at the boundaries.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         See `scipy.signal.savgol_filter`
 
     Returns
     -------
-    delta_data   : np.ndarray [shape=(..., t)]
+    delta_data : np.ndarray [shape=(..., t)]
         delta matrix of ``data`` at specified order
 
     Notes
@@ -133,7 +133,6 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
     overridden by supplying additional keyword arguments which are passed
     to `np.pad()`.
 
-
     Parameters
     ----------
     data : np.ndarray [shape=(..., d, t)]
@@ -150,8 +149,8 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
 
         Negative values embed from the future (subsequent columns).
 
-    kwargs : additional keyword arguments
-      Additional arguments to pass to `numpy.pad`
+    **kwargs : additional keyword arguments
+        Additional arguments to pass to `numpy.pad`
 
     Returns
     -------
@@ -163,7 +162,6 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
     -----
     This function caches at level 40.
 
-
     Examples
     --------
     Keep two steps (current and previous)
@@ -214,8 +212,6 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
     >>> ax.text(1.0, 1/6, ""Lag=0"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
     >>> ax.text(1.0, 3/6, ""Lag=1"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
     >>> ax.text(1.0, 5/6, ""Lag=2"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
-    >>> ax.axline((0, 1/3), (1, 1/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
-    >>> ax.axline((0, 2/3), (1, 2/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
     >>> ax.set(title='Time-lagged chroma', ylabel="""")
     """"""
 
@@ -270,11 +266,8 @@ def __stack(history, data, n_steps, delay):
     Parameters
     ----------
     history : output array (2-dimensional)
-
     data : pre-padded input array (2-dimensional)
-
     n_steps : int > 0, the number of steps to stack
-
     delay : int != 0, the amount of delay between steps
 
     Returns"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -140,23 +140,23 @@ def mel(
 
     Parameters
     ----------
-    sr        : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of the incoming signal
 
-    n_fft     : int > 0 [scalar]
+    n_fft : int > 0 [scalar]
         number of FFT components
 
-    n_mels    : int > 0 [scalar]
+    n_mels : int > 0 [scalar]
         number of Mel bands to generate
 
-    fmin      : float >= 0 [scalar]
+    fmin : float >= 0 [scalar]
         lowest frequency (in Hz)
 
-    fmax      : float >= 0 [scalar]
+    fmax : float >= 0 [scalar]
         highest frequency (in Hz).
         If `None`, use ``fmax = sr / 2.0``
 
-    htk       : bool [scalar]
+    htk : bool [scalar]
         use HTK formula instead of Slaney
 
     norm : {None, 'slaney', or number} [scalar]
@@ -175,10 +175,10 @@ def mel(
 
     Returns
     -------
-    M         : np.ndarray [shape=(n_mels, 1 + n_fft/2)]
+    M : np.ndarray [shape=(n_mels, 1 + n_fft/2)]
         Mel transform matrix
 
-    See also
+    See Also
     --------
     librosa.util.normalize
 
@@ -196,7 +196,6 @@ def mel(
            [ 0.   ,  0.   , ...,  0.   ,  0.   ],
            [ 0.   ,  0.   , ...,  0.   ,  0.   ]])
 
-
     Clip the maximum frequency to 8KHz
 
     >>> librosa.filters.mel(sr=22050, n_fft=2048, fmax=8000)
@@ -206,7 +205,6 @@ def mel(
            [ 0.  ,  0.  , ...,  0.  ,  0.  ],
            [ 0.  ,  0.  , ...,  0.  ,  0.  ]])
 
-
     >>> import matplotlib.pyplot as plt
     >>> fig, ax = plt.subplots()
     >>> img = librosa.display.specshow(melfb, x_axis='linear', ax=ax)
@@ -277,24 +275,23 @@ def chroma(
     This creates a linear transformation matrix to project
     FFT bins onto chroma bins (i.e. pitch classes).
 
-
     Parameters
     ----------
-    sr        : number > 0 [scalar]
+    sr : number > 0 [scalar]
         audio sampling rate
 
-    n_fft     : int > 0 [scalar]
+    n_fft : int > 0 [scalar]
         number of FFT bins
 
-    n_chroma  : int > 0 [scalar]
+    n_chroma : int > 0 [scalar]
         number of chroma bins
 
     tuning : float
         Tuning deviation from A440 in fractions of a chroma bin.
 
-    ctroct    : float > 0 [scalar]
+    ctroct : float > 0 [scalar]
 
-    octwidth  : float > 0 or None [scalar]
+    octwidth : float > 0 or None [scalar]
         ``ctroct`` and ``octwidth`` specify a dominance window:
         a Gaussian weighting centered on ``ctroct`` (in octs, A0 = 27.5Hz)
         and with a gaussian half-width of ``octwidth``.
@@ -346,7 +343,6 @@ def chroma(
            [  1.162e-05,   2.372e-04, ...,   6.417e-38,   9.923e-38],
            [  1.180e-05,   2.260e-04, ...,   4.697e-50,   7.772e-50]])
 
-
     Equally weight all octaves
 
     >>> librosa.filters.chroma(sr=22050, n_fft=4096, octwidth=None)
@@ -461,6 +457,9 @@ def constant_q(
     Frequencies are spaced geometrically, increasing by a factor of
     ``(2**(1./bins_per_octave))`` at each successive band.
 
+    .. warning:: This function is deprecated as of v0.9 and will be removed in 1.0.
+        See `librosa.filters.wavelet`.
+
     Parameters
     ----------
     sr : number > 0 [scalar]
@@ -500,14 +499,13 @@ def constant_q(
         The data type of the output basis.
         By default, uses 64-bit (single precision) complex floating point.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to `np.pad()` when ``pad==True``.
 
     Returns
     -------
     filters : np.ndarray, ``len(filters) == n_bins``
         ``filters[i]`` is ``i``\ th time-domain CQT basis filter
-
     lengths : np.ndarray, ``len(lengths) == n_bins``
         The (fractional) length of each filter
 
@@ -523,7 +521,6 @@ def constant_q(
     librosa.vqt
     librosa.util.normalize
 
-
     Examples
     --------
     Use a shorter window for each filter
@@ -605,25 +602,26 @@ def constant_q_lengths(
 ):
     r""""""Return length of each filter in a constant-Q basis.
 
+    .. warning:: This function is deprecated as of v0.9 and will be removed in 1.0.
+        See `librosa.filters.wavelet_lengths`.
+
     Parameters
     ----------
     sr : number > 0 [scalar]
         Audio sampling rate
-
     fmin : float > 0 [scalar]
         Minimum frequency bin.
-
     n_bins : int > 0 [scalar]
         Number of frequencies.  Defaults to 7 octaves (84 bins).
-
     bins_per_octave : int > 0 [scalar]
         Number of bins per octave
-
     window : str or callable
         Window function to use on filters
-
     filter_scale : float > 0 [scalar]
         Resolution of filter windows. Larger values use longer windows.
+    gamma : number >= 0
+        Bandwidth offset for variable-Q transforms.
+        ``gamma=0`` produces a constant-Q filterbank.
 
     Returns
     -------
@@ -738,7 +736,6 @@ def wavelet_lengths(
     -------
     lengths : np.ndarray
         The length of each filter.
-
     f_cutoff : float
         The lowest frequency at which all filters' main lobes have decayed by
         at least 3dB.
@@ -828,7 +825,6 @@ def wavelet(
 
     Parameters
     ----------
-
     freqs : np.ndarray (positive)
         Center frequencies of the filters (in Hz).
         Must be in ascending order.
@@ -867,14 +863,13 @@ def wavelet(
 
         If two or more frequencies are provided, this parameter is ignored.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to `np.pad()` when ``pad==True``.
 
     Returns
     -------
     filters : np.ndarray, ``len(filters) == n_bins``
         each ``filters[i]`` is a (complex) time-domain filter
-
     lengths : np.ndarray, ``len(lengths) == n_bins``
         The (fractional) length of each filter in samples
 
@@ -889,7 +884,6 @@ def wavelet(
     librosa.vqt
     librosa.util.normalize
 
-
     Examples
     --------
     Create a constant-Q basis
@@ -973,35 +967,27 @@ def cq_to_chroma(
     """"""Construct a linear transformation matrix to map Constant-Q bins
     onto chroma bins (i.e., pitch classes).
 
-
     Parameters
     ----------
     n_input : int > 0 [scalar]
         Number of input components (CQT bins)
-
     bins_per_octave : int > 0 [scalar]
         How many bins per octave in the CQT
-
     n_chroma : int > 0 [scalar]
         Number of output bins (per octave) in the chroma
-
     fmin : None or float > 0
         Center frequency of the first constant-Q channel.
         Default: 'C1' ~= 32.7 Hz
-
     window : None or np.ndarray
         If provided, the cq_to_chroma filter bank will be
         convolved with ``window``.
-
     base_c : bool
         If True, the first chroma bin will start at 'C'
         If False, the first chroma bin will start at 'A'
-
     dtype : np.dtype
         The data type of the output basis.
         By default, uses 32-bit (single-precision) floating point.
 
-
     Returns
     -------
     cq_to_chroma : np.ndarray [shape=(n_chroma, n_input)]
@@ -1097,15 +1083,13 @@ def cq_to_chroma(
 def window_bandwidth(window, n=1000):
     """"""Get the equivalent noise bandwidth of a window function.
 
-
     Parameters
     ----------
     window : callable or string
         A window function, or the name of a window function.
         Examples:
         - scipy.signal.hann
         - 'boxcar'
-
     n : int > 0
         The number of coefficients to use in estimating the
         window bandwidth
@@ -1222,7 +1206,6 @@ def _multirate_fb(
 
      This implementation uses `scipy.signal.iirdesign` to design the filters.
 
-
     Parameters
     ----------
     center_freqs : np.ndarray [shape=(n,), dtype=float]
@@ -1260,12 +1243,10 @@ def _multirate_fb(
 
         - If `zpk`, returns zeros, poles, and system gains of the transfer functions.
 
-
     Returns
     -------
     filterbank : list [shape=(n,), dtype=float]
         Each list entry comprises the filter coefficients for a single filter.
-
     sample_rates : np.ndarray [shape=(n,), dtype=float]
         Samplerate for each filter.
 
@@ -1340,7 +1321,6 @@ def mr_frequencies(tuning):
            ""Information Retrieval for Music and Motion.""
            Springer Verlag. 2007.
 
-
     Parameters
     ----------
     tuning : float [scalar]
@@ -1352,15 +1332,13 @@ def mr_frequencies(tuning):
     center_freqs : np.ndarray [shape=(n,), dtype=float]
         Center frequencies of the filter kernels.
         Also defines the number of filters in the filterbank.
-
     sample_rates : np.ndarray [shape=(n,), dtype=float]
         Sample rate for each filter, used for multirate filterbank.
 
     Notes
     -----
     This function caches at level 10.
 
-
     See Also
     --------
     librosa.filters.semitone_filterbank
@@ -1409,34 +1387,28 @@ def semitone_filterbank(
            ""Information Retrieval for Music and Motion.""
            Springer Verlag. 2007.
 
-
     Parameters
     ----------
     center_freqs : np.ndarray [shape=(n,), dtype=float]
         Center frequencies of the filter kernels.
         Also defines the number of filters in the filterbank.
-
     tuning : float [scalar]
         Tuning deviation from A440 as a fraction of a semitone (1/12 of an octave
         in equal temperament).
-
     sample_rates : np.ndarray [shape=(n,), dtype=float]
         Sample rates of each filter in the multirate filterbank.
-
     flayout : string
         - If `ba`, the standard difference equation is used for filtering with `scipy.signal.filtfilt`.
           Can be unstable for high-order filters.
         - If `sos`, a series of second-order filters is used for filtering with `scipy.signal.sosfiltfilt`.
           Minimizes numerical precision errors for high-order filters, but is slower.
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments to the private function `_multirate_fb()`.
 
     Returns
     -------
     filterbank : list [shape=(n,), dtype=float]
         Each list entry contains the filter coefficients for a single filter.
-
     fb_sample_rates : np.ndarray [shape=(n,), dtype=float]
         Sample rate for each filter.
 
@@ -1501,21 +1473,19 @@ def window_sumsquare(
     ----------
     window : string, tuple, number, callable, or list-like
         Window specification, as in `get_window`
-
     n_frames : int > 0
         The number of analysis frames
-
     hop_length : int > 0
         The number of samples to advance between frames
-
     win_length : [optional]
         The length of the window function.  By default, this matches ``n_fft``.
-
     n_fft : int > 0
         The length of each analysis frame.
-
     dtype : np.dtype
         The data type of the output
+    norm : {np.inf, -np.inf, 0, float > 0, None}
+        Normalization mode used in window construction.
+        Note that this does not affect the squaring operation.
 
     Returns
     -------
@@ -1591,13 +1561,11 @@ def diagonal_filter(window, n, *, slope=1.0, angle=None, zero_mean=False):
         This should be enabled if you want to enhance paths and suppress
         blocks.
 
-
     Returns
     -------
     kernel : np.ndarray, shape=[(m, m)]
         The 2-dimensional filter kernel
 
-
     Notes
     -----
     This function caches at level 10."
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -44,16 +44,15 @@ def onset_detect(
 
     .. [#] https://github.com/CPJKU/onset_db
 
-
     Parameters
     ----------
-    y          : np.ndarray [shape=(n,)]
+    y : np.ndarray [shape=(n,)]
         audio time series, must be monophonic
 
-    sr         : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-    onset_envelope     : np.ndarray [shape=(m,)]
+    onset_envelope : np.ndarray [shape=(m,)]
         (optional) pre-computed onset strength envelope
 
     hop_length : int > 0 [scalar]
@@ -80,15 +79,13 @@ def onset_detect(
 
         Otherwise, the onset envelope is left unnormalized.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters for peak picking.
 
         See `librosa.util.peak_pick` for details.
 
-
     Returns
     -------
-
     onsets : np.ndarray [shape=(n_onsets,)]
         estimated positions of detected onsets, in whichever units
         are specified.  By default, frame indices.
@@ -97,7 +94,6 @@ def onset_detect(
             If no onset strength could be detected, onset_detect returns
             an empty list.
 
-
     Raises
     ------
     ParameterError
@@ -111,7 +107,6 @@ def onset_detect(
     onset_backtrack : backtracking onset events
     librosa.util.peak_pick : pick peaks from a time series
 
-
     Examples
     --------
     Get onset times from a signal
@@ -127,7 +122,6 @@ def onset_detect(
     >>> times = librosa.times_like(o_env, sr=sr)
     >>> onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)
 
-
     >>> import matplotlib.pyplot as plt
     >>> D = np.abs(librosa.stft(y))
     >>> fig, ax = plt.subplots(nrows=2, sharex=True)
@@ -224,16 +218,16 @@ def onset_strength(
 
     Parameters
     ----------
-    y        : np.ndarray [shape=(..., n)]
+    y : np.ndarray [shape=(..., n)]
         audio time-series. Multi-channel is supported.
 
-    sr       : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-    S        : np.ndarray [shape=(..., d, m)]
+    S : np.ndarray [shape=(..., d, m)]
         pre-computed (log-power) spectrogram
 
-    lag      : int > 0
+    lag : int > 0
         time lag for computing differences
 
     max_size : int > 0
@@ -263,31 +257,27 @@ def onset_strength(
 
         Default: `np.mean`
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters to ``feature()``, if ``S`` is not provided.
 
-
     Returns
     -------
-    onset_envelope   : np.ndarray [shape=(..., m,)]
+    onset_envelope : np.ndarray [shape=(..., m,)]
         vector containing the onset strength envelope.
         If the input contains multiple channels, then onset envelope is computed for each channel.
 
-
     Raises
     ------
     ParameterError
         if neither ``(y, sr)`` nor ``S`` are provided
 
         or if ``lag`` or ``max_size`` are not positive integers
 
-
     See Also
     --------
     onset_detect
     onset_strength_multi
 
-
     Examples
     --------
     First, load some audio and plot the spectrogram
@@ -308,7 +298,6 @@ def onset_strength(
     >>> ax[1].plot(times, 2 + onset_env / onset_env.max(), alpha=0.8,
     ...            label='Mean (mel)')
 
-
     Median aggregation, and custom mel options
 
     >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,
@@ -317,7 +306,6 @@ def onset_strength(
     >>> ax[1].plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,
     ...            label='Median (custom mel)')
 
-
     Constant-Q spectrogram instead of Mel
 
     >>> C = np.abs(librosa.cqt(y=y, sr=sr))
@@ -370,7 +358,6 @@ def onset_backtrack(events, energy):
     ----------
     events : np.ndarray, dtype=int
         List of onset event frame indices, as computed by `onset_detect`
-
     energy : np.ndarray, shape=(m,)
         An energy function
 
@@ -451,16 +438,15 @@ def onset_strength_multi(
 
         mean_{f in channels[i]} max(0, S[f, t+1] - S[f, t])
 
-
     Parameters
     ----------
-    y        : np.ndarray [shape=(..., n,)]
+    y : np.ndarray [shape=(..., n,)]
         audio time-series. Multi-channel is supported.
 
-    sr       : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-    S        : np.ndarray [shape=(..., d, m)]
+    S : np.ndarray [shape=(..., d, m)]
         pre-computed (log-power) spectrogram
 
     n_fft : int > 0 [scalar]
@@ -469,7 +455,7 @@ def onset_strength_multi(
     hop_length : int > 0 [scalar]
         hop length for use in ``feature()`` if ``S`` is not provided.
 
-    lag      : int > 0
+    lag : int > 0
         time lag for computing differences
 
     max_size : int > 0
@@ -507,22 +493,19 @@ def onset_strength_multi(
         Array of channel boundaries or slice objects.
         If `None`, then a single channel is generated to span all bands.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters to ``feature()``, if ``S`` is not provided.
 
-
     Returns
     -------
-    onset_envelope   : np.ndarray [shape=(..., n_channels, m)]
+    onset_envelope : np.ndarray [shape=(..., n_channels, m)]
         array containing the onset strength envelope for each specified channel
 
-
     Raises
     ------
     ParameterError
         if neither ``(y, sr)`` nor ``S`` are provided
 
-
     See Also
     --------
     onset_strength"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -70,7 +70,6 @@ def cross_similarity(
     The output is a matrix ``xsim``, where ``xsim[i, j]`` is non-zero
     if ``data_ref[..., i]`` is a k-nearest neighbor of ``data[..., j]``.
 
-
     Parameters
     ----------
     data : np.ndarray [shape=(..., d, n)]
@@ -302,7 +301,6 @@ def recurrence_matrix(
 
     The general term *recurrence matrix* can refer to any of the three forms above.
 
-
     Parameters
     ----------
     data : np.ndarray [shape=(..., d, n)]
@@ -648,7 +646,6 @@ def lag_to_recurrence(lag, *, axis=-1):
     ----------
     lag : np.ndarray or scipy.sparse.spmatrix
         A lag matrix, as produced by ``recurrence_to_lag``
-
     axis : int
         The axis corresponding to the time dimension.
         The alternate axis will be interpreted in lag coordinates.
@@ -735,25 +732,20 @@ def timelag_filter(function, pad=True, index=0):
     ----------
     function : callable
         The filtering function to wrap, e.g., `scipy.ndimage.median_filter`
-
     pad : bool
         Whether to zero-pad the structure feature matrix
-
     index : int >= 0
         If ``function`` accepts input data as a positional argument, it should be
         indexed by ``index``
 
-
     Returns
     -------
     wrapped_function : callable
         A new filter function which applies in time-lag space rather than
         time-time space.
 
-
     Examples
     --------
-
     Apply a 31-bin median filter to the diagonal of a recurrence matrix.
     With default, parameters, this corresponds to a time window of about
     0.72 seconds.
@@ -820,16 +812,13 @@ def subsegment(data, frames, *, n_segments=4, axis=-1):
     ----------
     data : np.ndarray
         Data matrix to use in clustering
-
     frames : np.ndarray [shape=(n_boundaries,)], dtype=int, non-negative]
         Array of beat or segment boundaries, as provided by
         `librosa.beat.beat_track`,
         `librosa.onset.onset_detect`,
         or `agglomerative`.
-
     n_segments : int > 0
         Maximum number of frames to sub-divide each interval.
-
     axis : int
         Axis along which to apply the segmentation.
         By default, the last index (-1) is taken.
@@ -902,16 +891,13 @@ def agglomerative(data, k, *, clusterer=None, axis=-1):
 
     Parameters
     ----------
-    data     : np.ndarray
+    data : np.ndarray
         data to cluster
-
-    k        : int > 0 [scalar]
+    k : int > 0 [scalar]
         number of segments to produce
-
     clusterer : sklearn.cluster.AgglomerativeClustering, optional
         An optional AgglomerativeClustering object.
         If `None`, a constrained Ward object is instantiated.
-
     axis : int
         axis along which to cluster.
         By default, the last axis (-1) is chosen.
@@ -1067,10 +1053,9 @@ def path_enhance(
         If True, the smoothed similarity matrix will be thresholded at 0, and will not contain
         negative entries.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments to pass to `scipy.ndimage.convolve`
 
-
     Returns
     -------
     R_smooth : np.ndarray, shape=R.shape
@@ -1081,7 +1066,6 @@ def path_enhance(
     librosa.filters.diagonal_filter
     recurrence_matrix
 
-
     Examples
     --------
     Use a 51-frame diagonal smoothing filter to enhance paths in a recurrence matrix"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -128,12 +128,10 @@ def dtw(
         accumulated cost matrix.
         D[N, M] is the total alignment cost.
         When doing subsequence DTW, D[N,:] indicates a matching function.
-
     wp : np.ndarray [shape=(N, 2)]
         Warping path with index pairs.
         Each row of the array contains an index pair (n, m).
         Only returned when ``backtrack`` is True.
-
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
@@ -363,26 +361,19 @@ def __dtw_calc_accu_cost(
     ----------
     C : np.ndarray [shape=(N, M)]
         pre-computed cost matrix
-
     D : np.ndarray [shape=(N, M)]
         accumulated cost matrix
-
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
-
     step_sizes_sigma : np.ndarray [shape=[n, 2]]
         Specifies allowed step sizes as used by the dtw.
-
     weights_add : np.ndarray [shape=[n, ]]
         Additive weights to penalize certain step sizes.
-
     weights_mul : np.ndarray [shape=[n, ]]
         Multiplicative weights to penalize certain step sizes.
-
     max_0 : int
         maximum number of steps in step_sizes_sigma in dim 0.
-
     max_1 : int
         maximum number of steps in step_sizes_sigma in dim 1.
 
@@ -392,7 +383,6 @@ def __dtw_calc_accu_cost(
         accumulated cost matrix.
         D[N, M] is the total alignment cost.
         When doing subsequence DTW, D[N,:] indicates a matching function.
-
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
@@ -433,19 +423,15 @@ def __dtw_backtracking(steps, step_sizes_sigma, subseq, start=None):  # pragma:
     step to backtrack the index pairs for an optimal
     warping path.
 
-
     Parameters
     ----------
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
-
     step_sizes_sigma : np.ndarray [shape=[n, 2]]
         Specifies allowed step sizes as used by the dtw.
-
     subseq : bool
         Enable subsequence DTW, e.g., for retrieval tasks.
-
     start : int
         Start column index for backtraing (only allowed for ``subseq=True``)
 
@@ -499,19 +485,15 @@ def dtw_backtracking(steps, *, step_sizes_sigma=None, subseq=False, start=None):
     Uses the saved step sizes from the cost accumulation
     step to backtrack the index pairs for a warping path.
 
-
     Parameters
     ----------
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
-
     step_sizes_sigma : np.ndarray [shape=[n, 2]]
         Specifies allowed step sizes as used by the dtw.
-
     subseq : bool
         Enable subsequence DTW, e.g., for retrieval tasks.
-
     start : int
         Start column index for backtraing (only allowed for ``subseq=True``)
 
@@ -583,7 +565,6 @@ def rqa(sim, *, gap_onset=1, gap_extend=1, knight_moves=True, backtrack=True):
     Note that setting ``gap_onset`` and ``gap_extend`` to `np.inf` recovers the second
     method, and disabling knight moves recovers the first.
 
-
     .. [#] Serrà, Joan, Xavier Serra, and Ralph G. Andrzejak.
         ""Cross recurrence quantification for cover song identification.""
         New Journal of Physics 11, no. 9 (2009): 093017.
@@ -592,7 +573,6 @@ def rqa(sim, *, gap_onset=1, gap_extend=1, knight_moves=True, backtrack=True):
         ""Recurrence plots of dynamical systems.""
         World Scientific Series on Nonlinear Science Series A 16 (1995): 441-446.
 
-
     Parameters
     ----------
     sim : np.ndarray [shape=(N, M), non-negative]
@@ -623,7 +603,6 @@ def rqa(sim, *, gap_onset=1, gap_extend=1, knight_moves=True, backtrack=True):
     score : np.ndarray [shape=(N, M)]
         The alignment score matrix.  ``score[n, m]`` is the cumulative value of
         the best alignment sequence ending in frames ``n`` and ``m``.
-
     path : np.ndarray [shape=(k, 2)] (optional)
         If ``backtrack=True``, ``path`` contains a list of pairs of aligned frames
         in the best alignment sequence.
@@ -907,11 +886,9 @@ def _viterbi(log_prob, log_trans, log_p_init):  # pragma: no cover
     log_prob : np.ndarray [shape=(T, m)]
         ``log_prob[t, s]`` is the conditional log-likelihood
         ``log P[X = X(t) | State(t) = s]``
-
     log_trans : np.ndarray [shape=(m, m)]
         The log transition matrix
         ``log_trans[i, j] = log P[State(t+1) = j | State(t) = i]``
-
     log_p_init : np.ndarray [shape=(m,)]
         log of the initial state distribution
 
@@ -979,27 +956,22 @@ def viterbi(prob, transition, *, p_init=None, return_logp=False):
     prob : np.ndarray [shape=(..., n_states, n_steps), non-negative]
         ``prob[..., s, t]`` is the probability of observation at time ``t``
         being generated by state ``s``.
-
     transition : np.ndarray [shape=(n_states, n_states), non-negative]
         ``transition[i, j]`` is the probability of a transition from i->j.
         Each row must sum to 1.
-
     p_init : np.ndarray [shape=(n_states,)]
         Optional: initial state distribution.
         If not provided, a uniform distribution is assumed.
-
     return_logp : bool
         If ``True``, return the log-likelihood of the state sequence.
 
     Returns
     -------
     Either ``states`` or ``(states, logp)``:
-
     states : np.ndarray [shape=(..., n_steps,)]
         The most likely state sequence.
         If ``prob`` contains multiple channels of input, then each channel is
         decoded independently.
-
     logp : scalar [float] or np.ndarray
         If ``return_logp=True``, the log probability of ``states`` given
         the observations.
@@ -1008,7 +980,6 @@ def viterbi(prob, transition, *, p_init=None, return_logp=False):
     --------
     viterbi_discriminative : Viterbi decoding from state likelihoods
 
-
     Examples
     --------
     Example from https://en.wikipedia.org/wiki/Viterbi_algorithm#Example
@@ -1126,40 +1097,36 @@ def viterbi_discriminative(
         ``prob[s, t]`` is the probability of state ``s`` conditional on
         the observation at time ``t``.
         Must be non-negative and sum to 1 along each column.
-
     transition : np.ndarray [shape=(n_states, n_states), non-negative]
         ``transition[i, j]`` is the probability of a transition from i->j.
         Each row must sum to 1.
-
     p_state : np.ndarray [shape=(n_states,)]
         Optional: marginal probability distribution over states,
         must be non-negative and sum to 1.
         If not provided, a uniform distribution is assumed.
-
     p_init : np.ndarray [shape=(n_states,)]
         Optional: initial state distribution.
         If not provided, it is assumed to be uniform.
-
     return_logp : bool
         If ``True``, return the log-likelihood of the state sequence.
 
     Returns
     -------
     Either ``states`` or ``(states, logp)``:
-
     states : np.ndarray [shape=(..., n_steps,)]
         The most likely state sequence.
         If ``prob`` contains multiple input channels,
         then each channel is decoded independently.
-
     logp : scalar [float] or np.ndarray
         If ``return_logp=True``, the log probability of ``states`` given
         the observations.
 
     See Also
     --------
-    viterbi : Viterbi decoding from observation likelihoods
-    viterbi_binary: Viterbi decoding for multi-label, conditional state likelihoods
+    viterbi :
+        Viterbi decoding from observation likelihoods
+    viterbi_binary :
+        Viterbi decoding for multi-label, conditional state likelihoods
 
     Examples
     --------
@@ -1363,18 +1330,18 @@ def viterbi_binary(prob, transition, *, p_state=None, p_init=None, return_logp=F
     Returns
     -------
     Either ``states`` or ``(states, logp)``:
-
     states : np.ndarray [shape=(..., n_states, n_steps)]
         The most likely state sequence.
-
     logp : np.ndarray [shape=(..., n_states,)]
         If ``return_logp=True``, the log probability of each state activation
         sequence ``states``
 
     See Also
     --------
-    viterbi : Viterbi decoding from observation likelihoods
-    viterbi_discriminative : Viterbi decoding for discriminative (mutually exclusive) state predictions
+    viterbi :
+        Viterbi decoding from observation likelihoods
+    viterbi_discriminative :
+        Viterbi decoding for discriminative (mutually exclusive) state predictions
 
     Examples
     --------
@@ -1485,7 +1452,6 @@ def transition_uniform(n_states):
 
     Examples
     --------
-
     >>> librosa.sequence.transition_uniform(3)
     array([[0.333, 0.333, 0.333],
            [0.333, 0.333, 0.333],
@@ -1667,7 +1633,6 @@ def transition_local(n_states, width, *, window=""triangle"", wrap=False):
             so and effectively have ``width-2`` non-zero values.  You may have to expand
             ``width`` to get the desired behavior.
 
-
     wrap : bool
         If ``True``, then state locality ``|i - j|`` is computed modulo ``n_states``.
         If ``False`` (default), then locality is absolute.
@@ -1683,7 +1648,6 @@ def transition_local(n_states, width, *, window=""triangle"", wrap=False):
 
     Examples
     --------
-
     Triangular distributions with and without wrapping
 
     >>> librosa.sequence.transition_local(5, 3, window='triangle', wrap=False)
@@ -1729,7 +1693,9 @@ def transition_local(n_states, width, *, window=""triangle"", wrap=False):
 
     # Fill in the widths.  This is inefficient, but simple
     for i, width_i in enumerate(width):
-        trans_row = pad_center(get_window(window, width_i, fftbins=False), size=n_states)
+        trans_row = pad_center(
+            get_window(window, width_i, fftbins=False), size=n_states
+        )
         trans_row = np.roll(trans_row, n_states // 2 + i + 1)
 
         if not wrap:"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -44,14 +44,11 @@ def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):
     ----------
     A : np.ndarray [shape=(m, d)]
         The basis matrix
-
     B : np.ndarray [shape=(m, N)]
         The regression targets
-
     x_init : np.ndarray [shape=(d, N)]
         An initial guess
-
-    kwargs
+    **kwargs
         Additional keyword arguments to `scipy.optimize.fmin_l_bfgs_b`
 
     Returns
@@ -93,11 +90,9 @@ def nnls(A, B, **kwargs):
     ----------
     A : np.ndarray [shape=(m, n)]
         The basis matrix
-
     B : np.ndarray [shape=(..., m, N)]
         The target array.  Additional leading dimensions are supported.
-
-    kwargs
+    **kwargs
         Additional keyword arguments to `scipy.optimize.fmin_l_bfgs_b`
 
     Returns"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -26,7 +26,7 @@ def __wrapper(func, *args, **kwargs):
                 moved_from, func.__module__, func.__name__, version, version_removed
             ),
             category=DeprecationWarning,
-            stacklevel=3  # Would be 2, but the decorator adds a level
+            stacklevel=3,  # Would be 2, but the decorator adds a level
         )
         return func(*args, **kwargs)
 
@@ -47,7 +47,7 @@ def __wrapper(func, *args, **kwargs):
                 func.__module__, func.__name__, version, version_removed
             ),
             category=DeprecationWarning,
-            stacklevel=3  # Would be 2, but the decorator adds a level
+            stacklevel=3,  # Would be 2, but the decorator adds a level
         )
         return func(*args, **kwargs)
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -23,14 +23,11 @@ def rename_kw(
     old_name : str
     old_value
         The name and value of the old argument
-
     new_name : str
     new_value
         The name and value of the new argument
-
     version_deprecated : str
         The version at which the old name became deprecated
-
     version_removed : str
         The version at which the old name will be removed
 "
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -55,12 +55,10 @@ def example(key, *, hq=False):
     >>> os.environ['LIBROSA_DATA_DIR'] = '/path/to/store/data'
     >>> import librosa
 
-
     Parameters
     ----------
     key : str
         The identifier for the track to load
-
     hq : bool
         If ``True``, return the high-quality version of the recording.
         If ``False``, return the 22KHz mono version of the recording."
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -144,10 +144,8 @@ def match_intervals(intervals_from, intervals_to, strict=True):
         to ``intervals_from[i, 1]``.
         ``intervals_from[0, 0]`` should be 0, ``intervals_from[-1, 1]``
         should be the track duration.
-
     intervals_to : np.ndarray [shape=(m, 2)]
         Analogous to ``intervals_from``.
-
     strict : bool
         If ``True``, intervals can only match if they intersect.
         If ``False``, disjoint intervals can match.
@@ -239,12 +237,10 @@ def match_events(events_from, events_to, left=True, right=True):
     Parameters
     ----------
     events_from : ndarray [shape=(n,)]
-      Array of events (eg, times, sample or frame indices) to match from.
-
+        Array of events (eg, times, sample or frame indices) to match from.
     events_to : ndarray [shape=(m,)]
-      Array of events (eg, times, sample or frame indices) to
-      match against.
-
+        Array of events (eg, times, sample or frame indices) to
+        match against.
     left : bool
     right : bool
         If ``False``, then matched events cannot be to the left (or right)"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -79,26 +79,20 @@ def frame(x, *, frame_length, hop_length, axis=-1, writeable=False, subok=False)
     adding a new ""frame axis"" either before the framing axis (if ``axis < 0``)
     or after the framing axis (if ``axis >= 0``).
 
-
     Parameters
     ----------
     x : np.ndarray
         Array to frame
-
     frame_length : int > 0 [scalar]
         Length of the frame
-
     hop_length : int > 0 [scalar]
         Number of steps to advance between frames
-
     axis : int
         The axis along which to frame.
-
     writeable : bool
         If ``True``, then the framed view of ``x`` is read-only.
         If ``False``, then the framed view is read-write.  Note that writing to the framed view
         will also write to the input array ``x`` in this case.
-
     subok : bool
         If True, sub-classes will be passed-through, otherwise the returned array will be
         forced to be a base-class array (default).
@@ -231,13 +225,13 @@ def valid_audio(y, *, mono=Deprecated()):
     Parameters
     ----------
     y : np.ndarray
-      The input data to validate
+        The input data to validate
 
     mono : bool
-      Whether or not to require monophonic audio
+        Whether or not to require monophonic audio
 
-      .. warning:: The ``mono`` parameter is deprecated in version 0.9 and will be
-        removed in 0.10.
+        .. warning:: The ``mono`` parameter is deprecated in version 0.9 and will be
+          removed in 0.10.
 
     Returns
     -------
@@ -266,7 +260,7 @@ def valid_audio(y, *, mono=Deprecated()):
     >>> librosa.util.valid_audio(y_stereo, mono=False)
     True
 
-    See also
+    See Also
     --------
     numpy.float32
     """"""
@@ -308,7 +302,6 @@ def valid_int(x, *, cast=None):
     ----------
     x : number
         A scalar value to be cast to int
-
     cast : function [optional]
         A function to modify ``x`` before casting.
         Default: `np.floor`
@@ -395,15 +388,12 @@ def pad_center(data, *, size, axis=-1, **kwargs):
     ----------
     data : np.ndarray
         Vector to be padded and centered
-
     size : int >= len(data) [scalar]
         Length to pad ``data``
-
     axis : int
         Axis along which to pad and center the data
-
-    kwargs : additional keyword arguments
-      arguments passed to `np.pad`
+    **kwargs : additional keyword arguments
+        arguments passed to `np.pad`
 
     Returns
     -------
@@ -445,10 +435,8 @@ def expand_to(x, *, ndim, axes):
     ----------
     x : np.ndarray
         The input array
-
     ndim : int
         The number of dimensions to expand to.  Must be at least ``x.ndim``
-
     axes : int or slice
         The target axis or axes to preserve from x.
         All other axes will have length 1.
@@ -532,15 +520,12 @@ def fix_length(data, *, size, axis=-1, **kwargs):
     Parameters
     ----------
     data : np.ndarray
-      array to be length-adjusted
-
+        array to be length-adjusted
     size : int >= 0 [scalar]
-      desired length of the array
-
+        desired length of the array
     axis : int, <= data.ndim
-      axis along which to fix length
-
-    kwargs : additional keyword arguments
+        axis along which to fix length
+    **kwargs : additional keyword arguments
         Parameters to ``np.pad``
 
     Returns
@@ -604,18 +589,14 @@ def fix_frames(frames, *, x_min=0, x_max=None, pad=True):
     array([  0, 200, 233, 266, 299, 332, 365, 398, 431, 464, 497,
            500])
 
-
     Parameters
     ----------
     frames : np.ndarray [shape=(n_frames,)]
         List of non-negative frame indices
-
     x_min : int >= 0 or None
         Minimum allowed frame index
-
     x_max : int >= 0 or None
         Maximum allowed frame index
-
     pad : boolean
         If ``True``, then ``frames`` is expanded to span the full range
         ``[x_min, x_max]``
@@ -723,7 +704,6 @@ def axis_sort(S, *, axis=-1, index=False, value=None):
     -------
     S_sort : np.ndarray [shape=(d, n)]
         ``S`` with the columns or rows permuted in sorting order
-
     idx : np.ndarray (optional) [shape=(d,) or (n,)]
         If ``index == True``, the sorting index used to permute ``S``.
         Length of ``idx`` corresponds to the selected ``axis``.
@@ -774,7 +754,6 @@ def normalize(S, *, norm=np.inf, axis=0, threshold=None, fill=None):
     `scipy.linalg.norm` in two ways: multi-dimensional arrays
     are supported, but matrix-norms are not.
 
-
     Parameters
     ----------
     S : np.ndarray
@@ -1012,15 +991,14 @@ def localmax(x, *, axis=0):
 
     Parameters
     ----------
-    x     : np.ndarray [shape=(d1,d2,...)]
-      input vector or array
-
+    x : np.ndarray [shape=(d1,d2,...)]
+        input vector or array
     axis : int
-      axis along which to compute local maximality
+        axis along which to compute local maximality
 
     Returns
     -------
-    m     : np.ndarray [shape=x.shape, dtype=bool]
+    m : np.ndarray [shape=x.shape, dtype=bool]
         indicator array of local maximality along ``axis``
 
     See Also
@@ -1074,15 +1052,14 @@ def localmin(x, *, axis=0):
 
     Parameters
     ----------
-    x     : np.ndarray [shape=(d1,d2,...)]
-      input vector or array
-
+    x : np.ndarray [shape=(d1,d2,...)]
+        input vector or array
     axis : int
-      axis along which to compute local minimality
+        axis along which to compute local minimality
 
     Returns
     -------
-    m     : np.ndarray [shape=x.shape, dtype=bool]
+    m : np.ndarray [shape=x.shape, dtype=bool]
         indicator array of local minimality along ``axis``
 
     See Also
@@ -1124,33 +1101,26 @@ def peak_pick(x, *, pre_max, post_max, pre_avg, post_avg, delta, wait):
 
     .. [#] https://github.com/CPJKU/onset_detection/blob/master/onset_program.py
 
-
     Parameters
     ----------
-    x         : np.ndarray [shape=(n,)]
+    x : np.ndarray [shape=(n,)]
         input signal to peak picks from
-
-    pre_max   : int >= 0 [scalar]
+    pre_max : int >= 0 [scalar]
         number of samples before ``n`` over which max is computed
-
-    post_max  : int >= 1 [scalar]
+    post_max : int >= 1 [scalar]
         number of samples after ``n`` over which max is computed
-
-    pre_avg   : int >= 0 [scalar]
+    pre_avg : int >= 0 [scalar]
         number of samples before ``n`` over which mean is computed
-
-    post_avg  : int >= 1 [scalar]
+    post_avg : int >= 1 [scalar]
         number of samples after ``n`` over which mean is computed
-
-    delta     : float >= 0 [scalar]
+    delta : float >= 0 [scalar]
         threshold offset for mean
-
-    wait      : int >= 0 [scalar]
+    wait : int >= 0 [scalar]
         number of samples to wait after picking a peak
 
     Returns
     -------
-    peaks     : np.ndarray [shape=(n_peaks,), dtype=int]
+    peaks : np.ndarray [shape=(n_peaks,), dtype=int]
         indices of peaks in ``x``
 
     Raises
@@ -1275,10 +1245,8 @@ def sparsify_rows(x, *, quantile=0.01, dtype=None):
     ----------
     x : np.ndarray [ndim <= 2]
         The input matrix to sparsify.
-
     quantile : float in [0, 1.0)
         Percentage of magnitude to discard in each row of ``x``
-
     dtype : np.dtype, optional
         The dtype of the output array.
         If not provided, then ``x.dtype`` will be used.
@@ -1377,10 +1345,8 @@ def buf_to_float(x, *, n_bytes=2, dtype=np.float32):
     ----------
     x : np.ndarray [dtype=int]
         The integer-valued data buffer
-
     n_bytes : int [1, 2, 4]
         The number of bytes per sample in ``x``
-
     dtype : numeric type
         The target output type (default: 32-bit float)
 
@@ -1407,14 +1373,11 @@ def index_to_slice(idx, *, idx_min=None, idx_max=None, step=None, pad=True):
     ----------
     idx : list-like
         Array of index boundaries
-
     idx_min, idx_max : None or int
         Minimum and maximum allowed indices
-
     step : None or int
         Step size for each slice.  If `None`, then the default
         step of 1 is used.
-
     pad : boolean
         If `True`, pad ``idx`` to span the range ``idx_min:idx_max``.
 
@@ -1468,20 +1431,15 @@ def sync(data, idx, *, aggregate=None, pad=True, axis=-1):
 
     Parameters
     ----------
-    data      : np.ndarray
+    data : np.ndarray
         multi-dimensional array of features
-
     idx : iterable of ints or slices
         Either an ordered array of boundary indices, or
         an iterable collection of slice objects.
-
-
     aggregate : function
         aggregation function (default: `np.mean`)
-
     pad : boolean
         If `True`, ``idx`` is padded to span the full range ``[0, data.shape[axis]]``
-
     axis : int
         The axis along which to aggregate data
 
@@ -1528,7 +1486,6 @@ def sync(data, idx, *, aggregate=None, pad=True, axis=-1):
     >>> sub_beats = librosa.util.fix_frames(sub_beats)
     >>> C_med_sub = librosa.util.sync(C, sub_beats, aggregate=np.median)
 
-
     Plot the results
 
     >>> import matplotlib.pyplot as plt
@@ -1590,7 +1547,6 @@ def softmask(X, X_ref, *, power=1, split_zeros=False):
 
         ``M = X**power / (X**power + X_ref**power)``
 
-
     Parameters
     ----------
     X : np.ndarray
@@ -1606,14 +1562,12 @@ def softmask(X, X_ref, *, power=1, split_zeros=False):
         If infinite, returns a hard (binary) mask equivalent to ``X > X_ref``.
         Note: for hard masks, ties are always broken in favor of ``X_ref`` (``mask=0``).
 
-
     split_zeros : bool
         If `True`, entries where ``X`` and ``X_ref`` are both small (close to 0)
         will receive mask values of 0.5.
 
         Otherwise, the mask is set to 0 for these entries.
 
-
     Returns
     -------
     mask : np.ndarray, shape=X.shape
@@ -1630,7 +1584,6 @@ def softmask(X, X_ref, *, power=1, split_zeros=False):
 
     Examples
     --------
-
     >>> X = 2 * np.ones((3, 3))
     >>> X_ref = np.vander(np.arange(3.0))
     >>> X
@@ -1731,7 +1684,6 @@ def tiny(x):
 
     Examples
     --------
-
     For a standard double-precision floating point number:
 
     >>> librosa.util.tiny(1.0)
@@ -1853,11 +1805,9 @@ def cyclic_gradient(data, *, edge_order=1, axis=-1):
     data : np.ndarray
         The function values observed at uniformly spaced positions on
         a periodic domain
-
-    edge_order: {1, 2}
+    edge_order : {1, 2}
         The order of the difference approximation used for estimating
         the gradient
-
     axis : int
         The axis along which gradients are calculated.
 
@@ -1963,15 +1913,12 @@ def shear(X, *, factor=1, axis=-1):
     to a horizontal.  Shearing with ``factor=1`` converts a horizontal to
     a diagonal.
 
-
     Parameters
     ----------
     X : np.ndarray [ndim=2] or scipy.sparse matrix
         The array to be sheared
-
     factor : integer
         The shear factor: ``X[:, n] -> np.roll(X[:, n], factor * n)``
-
     axis : integer
         The axis along which to shear
 
@@ -2020,12 +1967,10 @@ def stack(arrays, *, axis=0):
     ----------
     arrays : list
         one or more `np.ndarray`
-
     axis : integer
         The target axis along which to stack.  ``axis=0`` creates a new first axis,
         and ``axis=-1`` creates a new last axis.
 
-
     Returns
     -------
     arr_stack : np.ndarray [shape=(len(arrays), array_shape) or shape=(array_shape, len(arrays))]
@@ -2038,7 +1983,6 @@ def stack(arrays, *, axis=0):
     Raises
     ------
     ParameterError
-
         - If ``arrays`` do not all have the same shape
         - If no ``arrays`` are given
 
@@ -2122,13 +2066,11 @@ def dtype_r2c(d, *, default=np.complex64):
     A `float32` (single-precision) type maps to `complex64`,
     while a `float64` (double-precision) maps to `complex128`.
 
-
     Parameters
     ----------
     d : np.dtype
         The real-valued dtype to convert to complex.
         If ``d`` is a complex type already, it will be returned.
-
     default : np.dtype, optional
         The default complex target type, if ``d`` does not match a
         known dtype
@@ -2180,13 +2122,11 @@ def dtype_c2r(d, *, default=np.float32):
     A `complex64` (single-precision) type maps to `float32`,
     while a `complex128` (double-precision) maps to `float64`.
 
-
     Parameters
     ----------
     d : np.dtype
         The complex-valued dtype to convert to real.
         If ``d`` is a real (float) type already, it will be returned.
-
     default : np.dtype, optional
         The default real target type, if ``d`` does not match a
         known dtype
@@ -2250,7 +2190,6 @@ def count_unique(data, *, axis=-1):
     ----------
     data : np.ndarray
         The input array
-
     axis : int
         The target axis to count
 
@@ -2266,7 +2205,6 @@ def count_unique(data, *, axis=-1):
 
     Examples
     --------
-
     >>> x = np.vander(np.arange(5))
     >>> x
     array([[  0,   0,   0,   0,   1],
@@ -2304,7 +2242,6 @@ def is_unique(data, *, axis=-1):
     ----------
     data : np.ndarray
         The input array
-
     axis : int
         The target axis
 
@@ -2321,7 +2258,6 @@ def is_unique(data, *, axis=-1):
 
     Examples
     --------
-
     >>> x = np.vander(np.arange(5))
     >>> x
     array([[  0,   0,   0,   0,   1],"
10;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -5,8 +5,8 @@
 import sys
 import importlib
 
-short_version = ""0.8""
-version = ""0.8.1""
+short_version = ""0.9""
+version = ""0.9.0rc0""
 
 
 def __get_mod_version(modname):"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -18,7 +18,7 @@ dependencies:
   - packaging==20.0
 
   # optional, but required for testing
-  - matplotlib==3.0.1
+  - matplotlib==3.3.0
   - pytest-mpl
   - pytest-cov
   - pytest"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -27,9 +27,9 @@ deposited under `build/html`.  To deploy, we sync the compiled site to the
 
 Because the historical docs include example code that is executed to generate
 figures, the environment for building historical docs can be brittle.
-Presently, the oldest compiled doc is for release 0.6.3.
+Presently, the oldest compiled doc is for release 0.7.2.
 The historical docs work with the following dependency versions:
 
     - numba=0.48 : decorators submodule move in 0.49 gives warnings, and 0.50 breaks old librosa
     - numpy=1.17 : strict dtype requirements in linspace parameters break some of our old examples from 1.18 on
-    - matplotlib=3.2 : log axes API changes cause warnings in 3.3 onward
+    - matplotlib=3.3,<3.5 : log axes API changes cause warnings in 3.3 onward"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -374,7 +374,7 @@
 # beats contains the frame indices of each detected beat
 # for synchronization and visualization, we'll need to expand this
 # to cover the limits of the data.  This can be done as follows:
-beats = librosa.util.fix_frames(beats, x_min=0, x_max=chroma.shape[1])
+beats = librosa.util.fix_frames(beats, x_min=0)
 
 # Now beat-synchronize the chroma features
 chroma_sync = librosa.util.sync(chroma, beats, aggregate=np.median)"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -60,8 +60,7 @@
 # For plotting purposes, we'll need the timing of the beats
 # we fix_frames to include non-beat frames 0 and C.shape[1] (final frame)
 beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats,
-                                                            x_min=0,
-                                                            x_max=C.shape[1]),
+                                                            x_min=0),
                                     sr=sr)
 
 fig, ax = plt.subplots()
@@ -189,9 +188,12 @@
                          y_coords=beat_times, ax=ax[0])
 ax[0].set(title='Structure components')
 
-img = librosa.display.specshow(np.atleast_2d(seg_ids).T, cmap=colors,
-                         y_axis='time', y_coords=beat_times, ax=ax[2])
-ax[2].set(title='Estimated segments')
+img = librosa.display.specshow(np.atleast_2d(seg_ids).T, cmap=colors, 
+                         y_axis='time',
+                         x_coords=[0, 1], y_coords=list(beat_times) + [beat_times[-1]], 
+                         ax=ax[2])
+ax[2].set(title='Estimated labels')
+
 ax[2].label_outer()
 fig.colorbar(img, ax=[ax[2]], ticks=range(k))
 "
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -870,7 +870,7 @@ def specshow(
             tempograms are calculated in the Frequency domain
             using `feature.fourier_tempogram`.
 
-    x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]+1]
+    x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]
 
         Optional positioning coordinates of the input data.
         These can be use to explicitly set the location of each
@@ -947,7 +947,7 @@ def specshow(
         By default, the following options are set:
 
             - ``rasterized=True``
-            - ``shading='flat'``
+            - ``shading='auto'``
             - ``edgecolors='None'``
 
     Returns
@@ -998,7 +998,7 @@ def specshow(
     kwargs.setdefault(""cmap"", cmap(data))
     kwargs.setdefault(""rasterized"", True)
     kwargs.setdefault(""edgecolors"", ""None"")
-    kwargs.setdefault(""shading"", ""flat"")
+    kwargs.setdefault(""shading"", ""auto"")
 
     all_params = dict(
         kwargs=kwargs,
@@ -1021,10 +1021,8 @@ def specshow(
 
     axes = __check_axes(ax)
     out = axes.pcolormesh(x_coords, y_coords, data, **kwargs)
-    __set_current_image(ax, out)
 
-    axes.set_xlim(x_coords.min(), x_coords.max())
-    axes.set_ylim(y_coords.min(), y_coords.max())
+    __set_current_image(ax, out)
 
     # Set up axis scaling
     __scale_axes(axes, x_axis, ""x"")
@@ -1037,6 +1035,7 @@ def specshow(
     # If the plot is a self-similarity/covariance etc. plot, square it
     if __same_axes(x_axis, y_axis, axes.get_xlim(), axes.get_ylim()) and auto_aspect:
         axes.set_aspect(""equal"")
+
     return out
 
 
@@ -1057,9 +1056,9 @@ def __mesh_coords(ax_type, coords, n, **kwargs):
     """"""Compute axis coordinates""""""
 
     if coords is not None:
-        if len(coords) < n:
+        if len(coords) not in (n, n+1):
             raise ParameterError(
-                ""Coordinate shape mismatch: "" ""{}<{}"".format(len(coords), n)
+                f""Coordinate shape mismatch: {len(coords)}!={n} or {n}+1""
             )
         return coords
 
@@ -1171,14 +1170,14 @@ def __decorate_axis(axis, ax_type, key=""C:maj"", Sa=None, mela=None, thaat=None,
 
     if ax_type == ""tonnetz"":
         axis.set_major_formatter(TonnetzFormatter())
-        axis.set_major_locator(FixedLocator(0.5 + np.arange(6)))
+        axis.set_major_locator(FixedLocator(np.arange(6)))
         axis.set_label_text(""Tonnetz"")
 
     elif ax_type == ""chroma"":
         axis.set_major_formatter(ChromaFormatter(key=key, unicode=unicode))
         degrees = core.key_to_degrees(key)
         axis.set_major_locator(
-            FixedLocator(0.5 + np.add.outer(12 * np.arange(10), degrees).ravel())
+            FixedLocator(np.add.outer(12 * np.arange(10), degrees).ravel())
         )
         axis.set_label_text(""Pitch class"")
 
@@ -1194,7 +1193,7 @@ def __decorate_axis(axis, ax_type, key=""C:maj"", Sa=None, mela=None, thaat=None,
         # Rotate degrees relative to Sa
         degrees = np.mod(degrees + Sa, 12)
         axis.set_major_locator(
-            FixedLocator(0.5 + np.add.outer(12 * np.arange(10), degrees).ravel())
+            FixedLocator(np.add.outer(12 * np.arange(10), degrees).ravel())
         )
         axis.set_label_text(""Svara"")
 
@@ -1206,7 +1205,7 @@ def __decorate_axis(axis, ax_type, key=""C:maj"", Sa=None, mela=None, thaat=None,
         # Rotate degrees relative to Sa
         degrees = np.mod(degrees + Sa, 12)
         axis.set_major_locator(
-            FixedLocator(0.5 + np.add.outer(12 * np.arange(10), degrees).ravel())
+            FixedLocator(np.add.outer(12 * np.arange(10), degrees).ravel())
         )
         axis.set_label_text(""Svara"")
 
@@ -1335,9 +1334,6 @@ def __coord_fft_hz(n, sr=22050, n_fft=None, **_kwargs):
     # The following code centers the FFT bins at their frequencies
     # and clips to the non-negative frequency range [0, nyquist]
     basis = core.fft_frequencies(sr=sr, n_fft=n_fft)
-    fmax = basis[-1]
-    basis -= 0.5 * (basis[1] - basis[0])
-    basis = np.append(np.maximum(0, basis), [fmax])
     return basis
 
 
@@ -1350,8 +1346,6 @@ def __coord_mel_hz(n, fmin=0, fmax=None, sr=22050, htk=False, **_kwargs):
         fmax = 0.5 * sr
 
     basis = core.mel_frequencies(n, fmin=fmin, fmax=fmax, htk=htk)
-    basis[1:] -= 0.5 * np.diff(basis)
-    basis = np.append(np.maximum(0, basis), [fmax])
     return basis
 
 
@@ -1365,8 +1359,8 @@ def __coord_cqt_hz(n, fmin=None, bins_per_octave=12, sr=22050, **_kwargs):
 
     # we drop by half a bin so that CQT bins are centered vertically
     freqs = core.cqt_frequencies(
-        n + 1,
-        fmin=fmin / 2.0 ** (0.5 / bins_per_octave),
+        n,
+        fmin=fmin,
         bins_per_octave=bins_per_octave,
     )
 
@@ -1381,14 +1375,13 @@ def __coord_cqt_hz(n, fmin=None, bins_per_octave=12, sr=22050, **_kwargs):
 
 def __coord_chroma(n, bins_per_octave=12, **_kwargs):
     """"""Get chroma bin numbers""""""
-    return np.linspace(0, (12.0 * n) / bins_per_octave, num=n + 1, endpoint=True)
+    return np.linspace(0, (12.0 * n) / bins_per_octave, num=n, endpoint=False)
 
 
 def __coord_tempo(n, sr=22050, hop_length=512, **_kwargs):
     """"""Tempo coordinates""""""
-    basis = core.tempo_frequencies(n + 2, sr=sr, hop_length=hop_length)[1:]
-    edges = np.arange(1, n + 2)
-    return basis * (edges + 0.5) / edges
+    basis = core.tempo_frequencies(n + 1, sr=sr, hop_length=hop_length)[1:]
+    return basis
 
 
 def __coord_fourier_tempo(n, sr=22050, hop_length=512, win_length=None, **_kwargs):
@@ -1400,20 +1393,17 @@ def __coord_fourier_tempo(n, sr=22050, hop_length=512, win_length=None, **_kwarg
     basis = core.fourier_tempo_frequencies(
         sr=sr, hop_length=hop_length, win_length=win_length
     )
-    fmax = basis[-1]
-    basis -= 0.5 * (basis[1] - basis[0])
-    basis = np.append(np.maximum(0, basis), [fmax])
     return basis
 
 
 def __coord_n(n, **_kwargs):
     """"""Get bare positions""""""
-    return np.arange(n + 1)
+    return np.arange(n)
 
 
 def __coord_time(n, sr=22050, hop_length=512, **_kwargs):
     """"""Get time coordinates from frames""""""
-    return core.frames_to_time(np.arange(n + 1), sr=sr, hop_length=hop_length)
+    return core.frames_to_time(np.arange(n), sr=sr, hop_length=hop_length)
 
 
 def __same_axes(x_axis, y_axis, xlim, ylim):"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -321,7 +321,8 @@ def spectral_bandwidth(
     >>> librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
     ...                          y_axis='log', x_axis='time', ax=ax[1])
     >>> ax[1].set(title='log Power spectrogram')
-    >>> ax[1].fill_between(times, centroid[0] - spec_bw[0], centroid[0] + spec_bw[0],
+    >>> ax[1].fill_between(times, np.maximum(0, centroid[0] - spec_bw[0]),
+    ...                 np.minimum(centroid[0] + spec_bw[0], sr/2),
     ...                 alpha=0.5, label='Centroid +- bandwidth')
     >>> ax[1].plot(times, centroid[0], label='Spectral centroid', color='w')
     >>> ax[1].legend(loc='lower right')"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -197,10 +197,10 @@ def stack_memory(data, n_steps=2, delay=1, **kwargs):
 
     Stack time-lagged beat-synchronous chroma edge padding
 
-    >>> y, sr = librosa.load(librosa.ex('choice'))
+    >>> y, sr = librosa.load(librosa.ex('choice'), duration=10)
     >>> chroma = librosa.feature.chroma_stft(y=y, sr=sr)
     >>> tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)
-    >>> beats = librosa.util.fix_frames(beats, x_min=0, x_max=chroma.shape[1])
+    >>> beats = librosa.util.fix_frames(beats, x_min=0)
     >>> chroma_sync = librosa.util.sync(chroma, beats)
     >>> chroma_lag = librosa.feature.stack_memory(chroma_sync, n_steps=3,
     ...                                           mode='edge')
@@ -212,9 +212,12 @@ def stack_memory(data, n_steps=2, delay=1, **kwargs):
     >>> beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=512)
     >>> librosa.display.specshow(chroma_lag, y_axis='chroma', x_axis='time',
     ...                          x_coords=beat_times, ax=ax)
-    >>> ax.set(yticks=[0, 12, 24], yticklabels=['Lag=0', 'Lag=1', 'Lag=2'],
-    ...           title='Time-lagged chroma')
-    >>> ax.hlines([0, 12, 24], beat_times.min(), beat_times.max(), color='w')
+    >>> ax.text(1.0, 1/6, ""Lag=0"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
+    >>> ax.text(1.0, 3/6, ""Lag=1"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
+    >>> ax.text(1.0, 5/6, ""Lag=2"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
+    >>> ax.axline((0, 1/3), (1, 1/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
+    >>> ax.axline((0, 2/3), (1, 2/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
+    >>> ax.set(title='Time-lagged chroma', ylabel="""")
     """"""
 
     if n_steps < 1:"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -940,10 +940,14 @@ def agglomerative(data, k, clusterer=None, axis=-1):
     Plot the segmentation over the chromagram
 
     >>> import matplotlib.pyplot as plt
+    >>> import matplotlib.transforms as mpt
     >>> fig, ax = plt.subplots()
+    >>> trans = mpt.blended_transform_factory(
+    ...             ax.transData, ax.transAxes)
     >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=ax)
-    >>> ax.vlines(bound_times, 0, chroma.shape[0], color='linen', linestyle='--',
-    ...           linewidth=2, alpha=0.9, label='Segment boundaries')
+    >>> ax.vlines(bound_times, 0, 1, color='linen', linestyle='--',
+    ...           linewidth=2, alpha=0.9, label='Segment boundaries',
+    ...           transform=trans)
     >>> ax.legend()
     >>> ax.set(title='Power spectrogram')
     """""""
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -1208,11 +1208,11 @@ def viterbi_discriminative(
     >>> fig, ax = plt.subplots()
     >>> librosa.display.specshow(probs, x_axis='time', cmap='gray', ax=ax)
     >>> times = librosa.times_like(chords_vit)
-    >>> ax.scatter(times, chords_ind + 0.75, color='lime', alpha=0.5, marker='+',
+    >>> ax.scatter(times, chords_ind + 0.25, color='lime', alpha=0.5, marker='+',
     ...            s=15, label='Independent')
-    >>> ax.scatter(times, chords_vit + 0.25, color='deeppink', alpha=0.5, marker='o',
+    >>> ax.scatter(times, chords_vit - 0.25, color='deeppink', alpha=0.5, marker='o',
     ...            s=15, label='Viterbi')
-    >>> ax.set(yticks=0.5 + np.unique(chords_vit),
+    >>> ax.set(yticks=np.unique(chords_vit),
     ...        yticklabels=[labels[i] for i in np.unique(chords_vit)])
     >>> ax.legend()
     """""""
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -65,15 +65,15 @@ docs =
     sphinx != 1.3.1
     sphinx_rtd_theme==0.5.*
     numba < 0.50
-    matplotlib >= 2.0.0, < 3.3
+    matplotlib >= 3.3.0
     sphinx-multiversion >= 0.2.3
     sphinx-gallery >= 0.7
     mir_eval >= 0.5
     ipython >= 7.0
     sphinxcontrib-svg2pdfconverter
     presets
 tests =
-    matplotlib >= 3.0.1
+    matplotlib >= 3.3.0
     pytest-mpl
     pytest-cov
     pytest"
10;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -18,20 +18,14 @@
 
 matplotlib = pytest.importorskip(""matplotlib"", minversion=""3.4"")
 
-matplotlib.use(""Agg"")
-matplotlib.rcParams.update(matplotlib.rcParamsDefault)
-
-import matplotlib.style
-
-matplotlib.style.use(""seaborn-ticks"")
+STYLE = ""default""
 
 import matplotlib.pyplot as plt
 
 import librosa
 import librosa.display
 import numpy as np
 
-
 # Workaround for old freetype builds with our image fixtures
 FT_VERSION = version.parse(matplotlib.ft2font.__freetype_version__)
 OLD_FT = not (FT_VERSION >= version.parse(""2.10""))
@@ -92,7 +86,7 @@ def tempo(rhythm):
 
 @pytest.fixture
 def beats(rhythm, C):
-    return librosa.util.fix_frames(rhythm[1], x_max=C.shape[1])
+    return librosa.util.fix_frames(rhythm[1])
 
 
 @pytest.fixture
@@ -118,7 +112,7 @@ def test_unknown_time_unit(y):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""complex""], extensions=[""png""], tolerance=6
+    baseline_images=[""complex""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_complex_input(S):
@@ -127,7 +121,7 @@ def test_complex_input(S):
     return plt.gcf()
 
 
-@pytest.mark.mpl_image_compare(baseline_images=[""abs""], extensions=[""png""], tolerance=6)
+@pytest.mark.mpl_image_compare(baseline_images=[""abs""], extensions=[""png""], tolerance=6, style=STYLE)
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_abs_input(S_abs):
     plt.figure()
@@ -136,7 +130,7 @@ def test_abs_input(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""cqt_note""], extensions=[""png""], tolerance=6
+    baseline_images=[""cqt_note""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_cqt_note(C):
@@ -146,7 +140,7 @@ def test_cqt_note(C):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""fft_note""], extensions=[""png""], tolerance=6
+    baseline_images=[""fft_note""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_fft_note(S_abs):
@@ -156,7 +150,7 @@ def test_fft_note(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""cqt_hz""], extensions=[""png""], tolerance=6
+    baseline_images=[""cqt_hz""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_cqt_hz(C):
@@ -166,7 +160,7 @@ def test_cqt_hz(C):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""tempo""], extensions=[""png""], tolerance=6
+    baseline_images=[""tempo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_tempo(y, sr):
@@ -178,7 +172,7 @@ def test_tempo(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""fourier_tempo""], extensions=[""png""], tolerance=6
+    baseline_images=[""fourier_tempo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_fourier_tempo(y, sr):
@@ -190,7 +184,7 @@ def test_fourier_tempo(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""tonnetz""], extensions=[""png""], tolerance=6
+    baseline_images=[""tonnetz""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_tonnetz(C):
@@ -202,7 +196,7 @@ def test_tonnetz(C):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""chroma""], extensions=[""png""], tolerance=6
+    baseline_images=[""chroma""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_chroma(S_abs, sr):
@@ -222,7 +216,7 @@ def test_chroma(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""chroma_svara""], extensions=[""png""], tolerance=6
+    baseline_images=[""chroma_svara""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_chroma_svara(C, sr):
@@ -251,7 +245,7 @@ def test_chroma_svara(C, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""double_chroma""], extensions=[""png""], tolerance=6
+    baseline_images=[""double_chroma""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_double_chroma(S_abs, sr):
@@ -264,7 +258,7 @@ def test_double_chroma(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_mel""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_mel""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_x_mel(S_abs):
@@ -276,7 +270,7 @@ def test_x_mel(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""y_mel""], extensions=[""png""], tolerance=6
+    baseline_images=[""y_mel""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_y_mel(S_abs):
@@ -288,7 +282,7 @@ def test_y_mel(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""y_mel_bounded""], extensions=[""png""], tolerance=6
+    baseline_images=[""y_mel_bounded""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_y_mel_bounded(S_abs):
@@ -301,7 +295,7 @@ def test_y_mel_bounded(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_linear""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_linear""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_linear(S_abs, S_signed, S_bin):
@@ -318,7 +312,7 @@ def test_xaxis_none_yaxis_linear(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""specshow_ext_axes""], extensions=[""png""], tolerance=6
+    baseline_images=[""specshow_ext_axes""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_specshow_ext_axes(S_abs):
@@ -333,7 +327,7 @@ def test_specshow_ext_axes(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_log""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_log""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_log(S_abs, S_signed, S_bin):
@@ -351,7 +345,7 @@ def test_xaxis_none_yaxis_log(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_linear_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_linear_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_linear_yaxis_none(S_abs, S_signed, S_bin):
@@ -369,7 +363,7 @@ def test_xaxis_linear_yaxis_none(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_log_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_log_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_log_yaxis_none(S_abs, S_signed, S_bin):
@@ -388,7 +382,7 @@ def test_xaxis_log_yaxis_none(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_time_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_time_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_time_yaxis_none(S_abs):
@@ -399,7 +393,7 @@ def test_xaxis_time_yaxis_none(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_time""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_time""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_time(S_abs):
@@ -410,7 +404,7 @@ def test_xaxis_none_yaxis_time(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_frames_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_frames_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_frames_yaxis_none(S_abs):
@@ -421,7 +415,7 @@ def test_xaxis_frames_yaxis_none(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_frames""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_frames""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_frames(S_abs):
@@ -432,7 +426,7 @@ def test_xaxis_none_yaxis_frames(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_lag_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_lag_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_lag_yaxis_none(S_abs):
@@ -443,7 +437,7 @@ def test_xaxis_lag_yaxis_none(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_lag""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_lag""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_time_yaxis_lag(S_abs):
@@ -454,7 +448,7 @@ def test_xaxis_time_yaxis_lag(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""time_scales_auto""], extensions=[""png""], tolerance=6
+    baseline_images=[""time_scales_auto""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_time_scales_auto(S_abs, sr):
@@ -483,7 +477,7 @@ def test_time_scales_auto(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""time_unit""], extensions=[""png""], tolerance=6
+    baseline_images=[""time_unit""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_time_unit(S_abs, sr):
@@ -508,7 +502,7 @@ def test_time_unit(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""time_unit_lag""], extensions=[""png""], tolerance=6
+    baseline_images=[""time_unit_lag""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_time_unit_lag(S_abs, sr):
@@ -531,7 +525,7 @@ def test_time_unit_lag(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveplot_mono""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveplot_mono""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveplot_mono(y, sr):
@@ -549,7 +543,7 @@ def test_waveplot_mono(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_mono""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_mono""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_mono(y, sr):
@@ -560,7 +554,7 @@ def test_waveshow_mono(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_mono_zoom""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_mono_zoom""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_mono_zoom(y, sr):
@@ -573,7 +567,7 @@ def test_waveshow_mono_zoom(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_mono_zoom_out""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_mono_zoom_out""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_mono_zoom_out(y, sr):
@@ -588,7 +582,7 @@ def test_waveshow_mono_zoom_out(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveplot_ext_axes""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveplot_ext_axes""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveplot_ext_axes(y):
@@ -603,7 +597,7 @@ def test_waveplot_ext_axes(y):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_ext_axes""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_ext_axes""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_ext_axes(y):
@@ -618,7 +612,7 @@ def test_waveshow_ext_axes(y):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveplot_stereo""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveplot_stereo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveplot_stereo(y, sr):
@@ -631,7 +625,7 @@ def test_waveplot_stereo(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_stereo""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_stereo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_stereo(y, sr):
@@ -716,7 +710,7 @@ def test_cmap_robust(data):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""coords""], extensions=[""png""], tolerance=6
+    baseline_images=[""coords""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_coords(Csync, beat_t):
@@ -734,7 +728,7 @@ def test_bad_coords(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""sharex_specshow_ms""], extensions=[""png""], tolerance=6
+    baseline_images=[""sharex_specshow_ms""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_sharex_specshow_ms(S_abs, y, sr):
@@ -752,7 +746,7 @@ def test_sharex_specshow_ms(S_abs, y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""sharex_waveplot_ms""], extensions=[""png""], tolerance=6
+    baseline_images=[""sharex_waveplot_ms""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_sharex_waveplot_ms(y, sr, S_abs):
@@ -788,7 +782,7 @@ def test_axis_bound_warning(format_str):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""cqt_svara""], extensions=[""png""], tolerance=6
+    baseline_images=[""cqt_svara""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_display_cqt_svara(C, sr):
@@ -815,7 +809,7 @@ def test_display_cqt_svara(C, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""fft_svara""], extensions=[""png""], tolerance=6
+    baseline_images=[""fft_svara""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_display_fft_svara(S_abs, sr):
@@ -841,7 +835,7 @@ def test_display_fft_svara(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-        baseline_images=[""nfft_odd""], extensions=[""png""], tolerance=6
+        baseline_images=[""nfft_odd""], extensions=[""png""], tolerance=6, style=STYLE
 )
 def test_display_fft_odd():
 
@@ -867,7 +861,7 @@ def test_display_fft_odd():
 
 
 @pytest.mark.mpl_image_compare(
-        baseline_images=[""nfft_odd_ftempo""], extensions=[""png""], tolerance=6
+        baseline_images=[""nfft_odd_ftempo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 def test_display_fourier_tempo_odd():
 
@@ -937,7 +931,7 @@ def test_auto_aspect():
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""specshow_unicode_true""], extensions=[""png""], tolerance=6
+    baseline_images=[""specshow_unicode_true""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_specshow_unicode_true(C, sr):
@@ -968,7 +962,7 @@ def test_specshow_unicode_true(C, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""specshow_unicode_false""], extensions=[""png""], tolerance=6
+    baseline_images=[""specshow_unicode_false""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_specshow_unicode_false(C, sr):"
