Label;Page;Username;Repo;Commit;Bug;Code
KO;1;lucidrains;perceiver-ar-pytorch;be3765300f5aae03b779edf0e256b7a74bda5fc8;allow for processing heads in chunks in the initial cross attention layer, to save on peak memory;"def __init__(
         dim,
         dim_head = 64,
         heads = 8,
         dropout = 0.
     ):
         super().__init__()
         self.scale = dim_head ** -0.5
         self.heads = heads
         inner_dim = heads * dim_head
 
         self.norm = nn.LayerNorm(dim)
@@ -138,25 +141,43 @@ def forward(self, x, context, context_mask = None, rotary_pos_emb = None):
             q = apply_rotary_pos_emb(rotary_pos_emb, q)
             k = apply_rotary_pos_emb(rotary_pos_emb, k)
 
-        sim = einsum('b h i d, b h j d -> b h i j', q, k)
-
-        i, j = sim.shape[-2:]
 
-        mask_value = -torch.finfo(sim.dtype).max
 
         if exists(context_mask):
             mask_len = context_mask.shape[-1]
             context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)
             context_mask = rearrange(context_mask, 'b j -> b 1 1 j')
-            sim = sim.masked_fill(~context_mask, mask_value)
 
         causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)
-        sim = sim.masked_fill(causal_mask, mask_value)
 
-        attn = sim.softmax(dim = -1)
-        attn = self.dropout(attn)
 
-        out = einsum('b h i j, b h j d -> b h i d', attn, v)
 
         out = rearrange(out, 'b h n d -> b n (h d)')
 
@@ -175,7 +196,8 @@ def __init__(
         heads = 8,
         dropout = 0.,
         ff_mult = 4,
-        perceive_depth = 1
     ):
         super().__init__()
         assert max_seq_len > cross_attn_seq_len, 'max_seq_len must be greater than cross_attn_seq_len, the length of the sequence for which to cross attend to ""perceiver"" style'
@@ -191,7 +213,7 @@ def __init__(
 
         for _ in range(perceive_depth):
             self.perceive_layers.append(nn.ModuleList([
-                CausalPrefixAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = dropout),
                 FeedForward(dim, mult = ff_mult, dropout = dropout)
             ]))
 "
OK;1;lucidrains;perceiver-ar-pytorch;be3765300f5aae03b779edf0e256b7a74bda5fc8;allow for processing heads in chunks in the initial cross attention layer, to save on peak memory;"def __init__(
         dim,
         dim_head = 64,
         heads = 8,
+        max_heads_process = 2,
         dropout = 0.
     ):
         super().__init__()
         self.scale = dim_head ** -0.5
         self.heads = heads
+        self.max_heads_process = max_heads_process
+
         inner_dim = heads * dim_head
 
         self.norm = nn.LayerNorm(dim)
@@ -138,25 +141,43 @@ def forward(self, x, context, context_mask = None, rotary_pos_emb = None):
             q = apply_rotary_pos_emb(rotary_pos_emb, q)
             k = apply_rotary_pos_emb(rotary_pos_emb, k)
 
+        # take care of masking
 
+        i, j = q.shape[-2], k.shape[-2]
+        mask_value = -torch.finfo(q.dtype).max
 
         if exists(context_mask):
             mask_len = context_mask.shape[-1]
             context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)
             context_mask = rearrange(context_mask, 'b j -> b 1 1 j')
 
         causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)
 
+        # process in chunks of heads
 
+        out = []
+
+        max_heads = self.max_heads_process
+
+        for q_chunk, k_chunk, v_chunk in zip(q.split(max_heads, dim = 1), k.split(max_heads, dim = 1), v.split(max_heads, dim = 1)):
+            sim = einsum('b h i d, b h j d -> b h i j', q_chunk, k_chunk)
+
+            if exists(context_mask):
+                sim = sim.masked_fill(~context_mask, mask_value)
+
+            sim = sim.masked_fill(causal_mask, mask_value)
+
+            attn = sim.softmax(dim = -1)
+            attn = self.dropout(attn)
+
+            out_chunk = einsum('b h i j, b h j d -> b h i d', attn, v_chunk)
+            out.append(out_chunk)
+
+        # concat all the heads together
+
+        out = torch.cat(out, dim = 1)
+
+        # merge heads and then combine with linear
 
         out = rearrange(out, 'b h n d -> b n (h d)')
 
@@ -175,7 +196,8 @@ def __init__(
         heads = 8,
         dropout = 0.,
         ff_mult = 4,
+        perceive_depth = 1,
+        perceive_max_heads_process = 2 # processes the heads in the perceiver layer in chunks to lower peak memory, in the case the prefix is really long
     ):
         super().__init__()
         assert max_seq_len > cross_attn_seq_len, 'max_seq_len must be greater than cross_attn_seq_len, the length of the sequence for which to cross attend to ""perceiver"" style'
@@ -191,7 +213,7 @@ def __init__(
 
         for _ in range(perceive_depth):
             self.perceive_layers.append(nn.ModuleList([
+                CausalPrefixAttention(dim = dim, dim_head = dim_head, heads = heads, max_heads_process = perceive_max_heads_process, dropout = dropout),
                 FeedForward(dim, mult = ff_mult, dropout = dropout)
             ]))
 "
KO;9;tigert1998;opcounter;22d1dcd3aef379f4ac69e7b47602e05e9b87cd83;Fix incorrect memory calculation for Conv2DForwardHook;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
-            mem = 4 * (b * module.in_channels * h * w + np.prod(module.kernel_size) +
                        b * module.out_channels * out_h * out_w)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem"
OK;9;tigert1998;opcounter;22d1dcd3aef379f4ac69e7b47602e05e9b87cd83;Fix incorrect memory calculation for Conv2DForwardHook;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
+            mem = 4 * (b * module.in_channels * h * w + np.prod(module.weight.shape) +
                        b * module.out_channels * out_h * out_w)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem"
KO;9;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;" from opcounter.hooks import *
 
 if __name__ == ""__main__"":
-    model = resnet18(pretrained=True)
     input_tuple = (torch.randn(1, 3, 224, 224),)
     dst = counter(model, input_tuple, [
         Conv2DForwardHook(),"
OK;9;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;" from opcounter.hooks import *
 
 if __name__ == ""__main__"":
+    model = resnet50(pretrained=True)
     input_tuple = (torch.randn(1, 3, 224, 224),)
     dst = counter(model, input_tuple, [
         Conv2DForwardHook(),"
KO;9;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
-            mem = b * module.in_channels * h * w + np.prod(module.kernel_size) + \
-                b * module.out_channels * out_h * out_w
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
 
@@ -35,7 +35,8 @@ def func(module, input_tensors, output_tensors):
             in_features, out_features = module.in_features, module.out_features
             b, _ = input_tensors[0].shape
             muladds = b * in_features * out_features
-            mem = b * in_features + b * out_features + in_features * out_features
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
         return func"
OK;9;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
+            mem = 4 * (b * module.in_channels * h * w + np.prod(module.kernel_size) +
+                       b * module.out_channels * out_h * out_w)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
 
@@ -35,7 +35,8 @@ def func(module, input_tensors, output_tensors):
             in_features, out_features = module.in_features, module.out_features
             b, _ = input_tensors[0].shape
             muladds = b * in_features * out_features
+            mem = 4 * (b * in_features + b * out_features +
+                       in_features * out_features)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
         return func"
KO;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"benchmark.py
 index.py
 store_keys.txt
 b-tree.py"
OK;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"benchmark.py
 index.py
 store_keys.txt
 b-tree.py
+storemmap.txt
+exper.py"
KO;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def commands_cli():
         else:
             command_split = command.split()
             command_len = len(command_split)
             if command_len < 2:
                 print(""Few arguments given"", file=sys.stderr)
             elif command_len > 3:
                 print(""Too many arguments"", file=sys.stderr)
-
-            if command_split[0] not in DATABASE_COMMANDS:
-                print(""Command not supported"", file=sys.stderr)
             else:
-                # socket is re-initialised everytime
-                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
-                    try:
-                        sock.connect((HOST, PORT))
-                    except ConnectionRefusedError:
-                        raise ConnectionRefusedError(""Cannot connect to host server, perhaps start db server"")
-                    sock.sendall(bytes(command + ""\n"", ""utf-8""))
-                    response = str(sock.recv(1024), ""utf-8"")
-                    print(response)
 
 
 if __name__ == ""__main__"":"
OK;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def commands_cli():
         else:
             command_split = command.split()
             command_len = len(command_split)
+
             if command_len < 2:
                 print(""Few arguments given"", file=sys.stderr)
             elif command_len > 3:
                 print(""Too many arguments"", file=sys.stderr)
             else:
+
+                if command_split[0] not in DATABASE_COMMANDS:
+                    print(""Command not supported"", file=sys.stderr)
+                else:
+                    # socket is re-initialised everytime
+                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
+                        try:
+                            sock.connect((HOST, PORT))
+                        except ConnectionRefusedError:
+                            raise ConnectionRefusedError(""Cannot connect to host server, perhaps start db server"")
+                        sock.sendall(bytes(command + ""\n"", ""utf-8""))
+                        response = str(sock.recv(1024), ""utf-8"")
+                        print(response)
 
 
 if __name__ == ""__main__"":"
KO;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def __init__(self, use_btrees=False):
 
     def insert(self, key, value):
         with open(self.STORE_KEYS_FILE, mode=""a"") as file:
-            file.write(f""{key} {value} \n"")
-            logger.info(f""{key} set in db"")
     
     def retrieve(self, key):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE, mode=""r"") as file:
-                keys = file.readlines()
-                for line in keys:
                     if line.startswith(key):
                         value = re.split(r'(\n|\s)', line)[2]
                         return value
 
     def update(self, key, new_value):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE) as file:
-                keys = file.readlines()
-                for line in keys:
                     if line.startswith(key):
                         # line.replace
                         pass
 "
OK;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def __init__(self, use_btrees=False):
 
     def insert(self, key, value):
         with open(self.STORE_KEYS_FILE, mode=""a"") as file:
+            try:
+                file.write(f""{key} {value} \n"")
+                logger.info(f""{key} set in db"")
+                return True
+            except Exception:
+                logger.error(f""[ATTEMPT-FAIL] {key} could not be set in db"")
+                return False
     
     def retrieve(self, key):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE, mode=""r"") as file:
+                data = file.readlines()
+                for line in data:
                     if line.startswith(key):
                         value = re.split(r'(\n|\s)', line)[2]
                         return value
 
     def update(self, key, new_value):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE) as file:
+                data = file.readlines()
+                for line in data:
                     if line.startswith(key):
                         # line.replace
                         pass
 
+    def delete(self, key):
+        if not self.use_btrees:
+            with open(self.STORE_KEYS_FILE) as file:
+                data = file.readlines()
+                for line in data:
+                    if line.startswith(key):
+                        pass
+"
KO;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;" from keys import Key
 import os
 
 class TCPServerHandler(socketserver.StreamRequestHandler):
 
     def handle(self):
         db = Key()
         self.data = self.rfile.readline().strip().decode()
         command_split = self.data.split()
 
         if len(command_split) == 3:
@@ -18,28 +34,43 @@ def handle(self):
         elif len(command_split) == 2:
             db_command, key = command_split
             command_len = 2
-
-        if hasattr(db, f""{db_command}""):
             command = getattr(db, f""{db_command}"")
             if command_len == 3:
-                command(key, value)
-                self.wfile.write(""0"".encode())
             elif command_len == 2:
-                get_key = command(key)
-                self.wfile.write(f""{get_key}"".encode())
 
 
 if __name__ == ""__main__"":
     HOST, PORT = ""localhost"", 4000
 
     with socketserver.TCPServer((HOST, PORT), TCPServerHandler) as server:
         try:
             process_id = os.getpid()
             logger.info(f""Starting process id {process_id}"")
-            logger.info(f""Key value db server starting, address={HOST}:{PORT}"")
             server.serve_forever()
 
-        except BaseException:
             logger.info(f""Closing process id {process_id}"")
             logger.info(f""Key value db server shutting down"")
             sys.exit()"
OK;16;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;" from keys import Key
 import os
 
+# experiment with tracemalloc
+import tracemalloc
+tracemalloc.start()
+
+
+def memory_trace(limit=10):
+    print(""-"" *70)
+    print(""-"" * 70)
+    print(""-------------------------MEMORY USAGE----------------------------"")
+    snapshot = tracemalloc.take_snapshot()
+    memory_stats = snapshot.statistics(""lineno"")
+    for stat in memory_stats[:limit]:
+        print(stat)
+   
+
 class TCPServerHandler(socketserver.StreamRequestHandler):
 
     def handle(self):
         db = Key()
         self.data = self.rfile.readline().strip().decode()
+        logger.info(f""Message sent by key db cli client with address {self.client_address[0]}:{self.client_address[1]}"")
         command_split = self.data.split()
 
         if len(command_split) == 3:
@@ -18,28 +34,43 @@ def handle(self):
         elif len(command_split) == 2:
             db_command, key = command_split
             command_len = 2
+        
+        # callable to make sure no key db attributes which are not methods are called by getattr
+        if hasattr(db, f""{db_command}"") and callable(getattr(db, f""{db_command}"")):
             command = getattr(db, f""{db_command}"")
             if command_len == 3:
+                if command(key, value):
+                    self.wfile.write(""0"".encode())
             elif command_len == 2:
+                get_value = command(key)
+                self.wfile.write(f""{get_value}"".encode())
 
 
 if __name__ == ""__main__"":
     HOST, PORT = ""localhost"", 4000
+    SERVER_IP_ADDR = ""127.0.0.1"" if HOST == ""localhost"" else HOST   
+    try:
+        DEBUG = sys.argv[1] if sys.argv[1] == ""debug"" else False
+    except IndexError:
+        DEBUG = False
 
     with socketserver.TCPServer((HOST, PORT), TCPServerHandler) as server:
         try:
             process_id = os.getpid()
+            if DEBUG:
+                logger.info(""DEBUG=ON"")
+            else:
+                logger.info(""DEBUG=OFF"")
             logger.info(f""Starting process id {process_id}"")
+            logger.info(f""Key value db server starting"")
+            logger.info(f""Server listening on address {SERVER_IP_ADDR}:{PORT}"")
             server.serve_forever()
 
+        except KeyboardInterrupt:
             logger.info(f""Closing process id {process_id}"")
             logger.info(f""Key value db server shutting down"")
+            if DEBUG:
+                memory_trace()
             sys.exit()
+
+  "
KO;20;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
-sudo docker run --rm --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
@@ -60,7 +60,7 @@ A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
-docker run -it --rm --gpus all milesial/unet
 ```
 
 "
OK;20;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
+sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
@@ -60,7 +60,7 @@ A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
+docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet
 ```
 
 "
KO;21;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
-sudo docker run --rm --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
@@ -60,7 +60,7 @@ A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
-docker run -it --rm --gpus all milesial/unet
 ```
 
 "
OK;21;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
+sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
@@ -60,7 +60,7 @@ A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
+docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet
 ```
 
 "
KO;45;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def standardize_inputs(self, x: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
-        self.MeanIn = Mean
-        self.RangeIn = Range
 
         #if hasattr(self,""MeanIn""):
         #    self.MeanIn = Mean
@@ -304,8 +304,8 @@ def standardize_outputs(self, input: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
-        self.MeanOut = Mean
-        self.RangeOut = Range
 
         #if hasattr(self,""MeanOut""):
         #    self.MeanOut = Mean.to(self.MeanOut.device)"
OK;45;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def standardize_inputs(self, x: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
+        self.MeanIn = Mean.to(self.device_)
+        self.RangeIn = Range.to(self.device_)
 
         #if hasattr(self,""MeanIn""):
         #    self.MeanIn = Mean
@@ -304,8 +304,8 @@ def standardize_outputs(self, input: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
+        self.MeanOut = Mean.to(self.device_)
+        self.RangeOut = Range.to(self.device_)
 
         #if hasattr(self,""MeanOut""):
         #    self.MeanOut = Mean.to(self.MeanOut.device)"
KO;45;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def fit(
             dataset = create_time_lagged_dataset(X,t,lag_time)
             train_loader = FastTensorDataLoader(dataset.tensors, batch_size=batch_size, shuffle=False) 
 
-        # standardize inputs (unravel dataset and copy to device) #TODO check memory usage on GPU
-        x_train = torch.cat([batch[0] for batch in train_loader]).to(self.device_)
         if standardize_inputs:
             self.standardize_inputs(x_train)
 "
OK;45;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def fit(
             dataset = create_time_lagged_dataset(X,t,lag_time)
             train_loader = FastTensorDataLoader(dataset.tensors, batch_size=batch_size, shuffle=False) 
 
+        # standardize inputs (unravel dataset to compute average)
+        x_train = torch.cat([batch[0] for batch in train_loader])
         if standardize_inputs:
             self.standardize_inputs(x_train)
 "
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/io.cod""
 
 5 malloc"
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/io.cod""
 
 5 malloc"
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/io.cod""
 
 5 malloc"
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/io.cod""
 
 5 malloc"
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/io.cod""
 include ""std/math.cod""
 "
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/io.cod""
 include ""std/math.cod""
 "
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
KO;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/cstr.cod"""
OK;1;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;"+include ""std/memory.cod""
 include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/cstr.cod"""
KO;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def generate_rt_calls(program, indent_count=1):
             result += f""{indent}stack_push(argc);\n""
         elif part[""type""] == ""argv"":
             result += f""{indent}stack_push((uint64_t)argv);\n""
-        elif part[""type""] == ""malloc"":
-            result += f""{indent}stack_malloc();\n""
-        elif part[""type""] == ""free"":
-            result += f""{indent}stack_free();\n""
-        elif part[""type""] == ""realloc"":
-            result += f""{indent}stack_realloc();\n""
-        elif part[""type""] == ""write8"":
-            result += f""{indent}stack_write8();\n""
-        elif part[""type""] == ""read8"":
-            result += f""{indent}stack_read8();\n""
-        elif part[""type""] == ""read64"":
-            result += f""{indent}stack_read64();\n""
 
         elif part[""type""] == ""parseInt"":
             result += f""{indent}stack_parse_int64();\n"""
OK;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def generate_rt_calls(program, indent_count=1):
             result += f""{indent}stack_push(argc);\n""
         elif part[""type""] == ""argv"":
             result += f""{indent}stack_push((uint64_t)argv);\n""
 
         elif part[""type""] == ""parseInt"":
             result += f""{indent}stack_parse_int64();\n"""
KO;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def parse_from_words(words, root=False):
     builtin_words = [
         ""argc"",
         ""argv"",
-
-        ""malloc"",
-        ""free"",
-        ""realloc"",
-        ""write8"",
-        ""read8"",
-        ""read64"",
         
         ""parseInt""
     ]"
OK;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def parse_from_words(words, root=False):
     builtin_words = [
         ""argc"",
         ""argv"",
         
         ""parseInt""
     ]"
KO;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;\ No newline at end of file
OK;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"+subroutine malloc {
+    raw ""stack_malloc();""
+}
+
+subroutine free {
+    raw ""stack_free();""
+}
+
+subroutine realloc {
+    raw ""stack_realloc();""
+}
+
+subroutine write8 {
+    raw ""stack_write8();""
+}
+
+subroutine read8 {
+    raw ""stack_read8();""
+}
+
+subroutine read64 {
+    raw ""stack_read64();""
+}
\ No newline at end of file"
KO;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;\ No newline at end of file
OK;1;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"+subroutine malloc {
+    raw ""stack_malloc();""
+}
+
+subroutine free {
+    raw ""stack_free();""
+}
+
+subroutine realloc {
+    raw ""stack_realloc();""
+}
+
+subroutine write8 {
+    raw ""stack_write8();""
+}
+
+subroutine read8 {
+    raw ""stack_read8();""
+}
+
+subroutine read64 {
+    raw ""stack_read64();""
+}
\ No newline at end of file"
KO;10;Lincoln-LM;py-gdb-nx;4be70bb77e14d9b8c0443485c25fa5cd5c5880c9;Create methods for reading and writing memory;" """"""Wrapper around pygdbmi.GdbController for easier switch connection""""""
 
 from typing import Optional,List
 import os.path
 import pygdbmi.gdbcontroller
@@ -88,7 +89,8 @@ def attach(
         self,
         process_name: str = ""Application"",
     ):
-        """"""Attach to process of name process_name
 
         Args:
             process_name (str, optional): Name of switch process to attach to.
@@ -120,11 +122,197 @@ def get_bases(
                 self.main_base, self.main_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[2:4])
 
     def add_breakpoint(
         self,
         bkpt: Breakpoint,
     ):
-        """"""Activate breakpoint
 
         Args:
             bkpt (Breakpoint): Breakpoint object to activate
@@ -178,7 +366,8 @@ def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
     ) -> List[dict]:
-        """"""Wait until response from gdb of type target_type
 
         Args:
             target_type (Optional[str], optional): mi3 type to wait for. Defaults to ""console"".
@@ -200,7 +389,8 @@ def log_response(
         response: List[dict],
         detailed: Optional[bool] = False,
     ):
-        """"""Log a mi3 response List[dict]
 
         Args:
             response (List[dict]): mi3 response to log
@@ -233,7 +423,8 @@ def filter_response(
     def extract_payloads(
         response: List[dict],
     ) -> List[str]:
-        """"""Extract only the payloads of a mi3 response
 
         Args:
             response (List[dict]): mi3 response to extract from"
OK;10;Lincoln-LM;py-gdb-nx;4be70bb77e14d9b8c0443485c25fa5cd5c5880c9;Create methods for reading and writing memory;" """"""Wrapper around pygdbmi.GdbController for easier switch connection""""""
 
+import struct
 from typing import Optional,List
 import os.path
 import pygdbmi.gdbcontroller
@@ -88,7 +89,8 @@ def attach(
         self,
         process_name: str = ""Application"",
     ):
+        """"""
+        Attach to process of name process_name
 
         Args:
             process_name (str, optional): Name of switch process to attach to.
@@ -120,11 +122,197 @@ def get_bases(
                 self.main_base, self.main_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[2:4])
 
+    def read_instruction(
+        self,
+        address: int,
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ) -> str:
+        """"""
+        Read instruction from address
+
+        Args:
+            address (int): Address to read from
+            offset_main (bool, optional): Whether or not to offset address by
+            self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address by
+            self.heap_base. Defaults to False
+
+        Returns:
+            str: Instruction information
+        """"""
+        if offset_main:
+            address += self.main_base
+        elif offset_heap:
+            address += self.heap_base
+        self.write(f""x/1iw {address}"", read_response = False)
+        return self.filter_response(
+            self.get_gdb_response(),
+            ""console""
+            )[0]['payload'].split("":"")[1].replace(""\\t"",""\t"").replace(""\\n"","""")
+
+    def read_int(
+        self,
+        address: int,
+        size: str = ""g"",
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ) -> int:
+        """"""
+        Read memory at address
+
+        Args:
+            address (int): Address to read from
+            size (str, optional): GDB size of int to read. Defaults to ""g""
+            offset_main (bool, optional): Whether or not to offset address by
+            self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address by
+            self.heap_base. Defaults to False
+
+        Returns:
+            int: Integer read from address
+        """"""
+        if offset_main:
+            address += self.main_base
+        elif offset_heap:
+            address += self.heap_base
+        self.write(f""x/1x{size} {address}"", read_response = False)
+        return int(self.filter_response(
+            self.get_gdb_response(),
+            ""console""
+            )[0]['payload'].split(""0x"")[-1][:-2].replace("":"",""""),16)
+
+    def read_bytes(
+        self,
+        address: int,
+        size: str = ""g"",
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ) -> bytes:
+        """"""
+        Read memory at address and convert it to bytes
+
+        Args:
+            address (int): Address to read from
+            size (str, optional): GDB size of int to read. Defaults to ""g""
+            offset_main (bool, optional): Whether or not to offset address by
+            self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address by
+            self.heap_base. Defaults to False
+
+        Returns:
+            bytes: Bytes read from address
+        """"""
+        if size == ""b"":
+            struct_size = ""B""
+        elif size == ""h"":
+            struct_size = ""H""
+        elif size == ""w"":
+            struct_size = ""I""
+        elif size == ""g"":
+            struct_size = ""Q""
+        return struct.pack(struct_size, self.read_int(address, size, offset_main, offset_heap))
+
+    def read_float(
+        self,
+        address: int,
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ) -> float:
+        """"""
+        Read float at address
+
+        Args:
+            address (int): Address to read from
+            offset_main (bool, optional): Whether or not to offset address by
+            self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address by
+            self.heap_base. Defaults to False
+        """"""
+        return struct.unpack(""f"", self.read_bytes(address, ""w"", offset_main, offset_heap))
+
+    def write_int(
+        self,
+        address: int,
+        value: int,
+        size: str = ""g"",
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ):
+        """"""
+        Write integer of size to address
+
+        Args:
+            address (int): Address to write to
+            value (int): Value to write to memory
+            size (str, optional): GDB size of int to write. Defaults to ""g""
+            offset_main (bool, optional): Whether or not to offset address by
+            self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address by
+            self.heap_base. Defaults to False
+        """"""
+        if size == ""b"":
+            size = ""unsigned char""
+        elif size == ""h"":
+            size = ""unsigned short""
+        elif size == ""w"":
+            size = ""unsigned word""
+        elif size == ""g"":
+            size = ""unsigned long""
+        if offset_main:
+            address += self.main_base
+        elif offset_heap:
+            address += self.heap_base
+        self.write(f""set {{{size}}}{address} = {value}"")
+
+    def write_bytes(
+        self,
+        address: int,
+        value: bytes,
+        size: str = ""g"",
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ):
+        """"""
+        Write bytes of size to address
+
+        Args:
+            address (int): Address to write to
+            value (bytes): Value to write to memory
+            size (str, optional): GDB size of bytes to write. Defaults to ""g""
+            offset_main (bool, optional): Whether or not to offset address by
+            self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address by
+            self.heap_base. Defaults to False
+        """"""
+        self.write_int(address, struct.unpack(""I"", value), size, offset_main, offset_heap)
+
+    def write_float(
+        self,
+        address: int,
+        value: float,
+        offset_main: Optional[bool] = False,
+        offset_heap: Optional[bool] = False,
+    ):
+        """"""
+        Write float to address
+
+        Args:
+            address (int): Address to write to
+            value (float): Value to write to memory
+            offset_main (bool, optional): Whether or not to offset address
+            by self.main_base. Defaults to False
+            offset_heap (bool, optional): Whether or not to offset address
+            by self.heap_base. Defaults to False
+        """"""
+        self.write_bytes(address, struct.pack(""f"", value), ""w"", offset_main, offset_heap)
+
     def add_breakpoint(
         self,
         bkpt: Breakpoint,
     ):
+        """"""
+        Activate breakpoint
 
         Args:
             bkpt (Breakpoint): Breakpoint object to activate
@@ -178,7 +366,8 @@ def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
     ) -> List[dict]:
+        """"""
+        Wait until response from gdb of type target_type
 
         Args:
             target_type (Optional[str], optional): mi3 type to wait for. Defaults to ""console"".
@@ -200,7 +389,8 @@ def log_response(
         response: List[dict],
         detailed: Optional[bool] = False,
     ):
+        """"""
+        Log a mi3 response List[dict]
 
         Args:
             response (List[dict]): mi3 response to log
@@ -233,7 +423,8 @@ def filter_response(
     def extract_payloads(
         response: List[dict],
     ) -> List[str]:
+        """"""
+        Extract only the payloads of a mi3 response
 
         Args:
             response (List[dict]): mi3 response to extract from"
KO;10;Lincoln-LM;py-gdb-nx;8fa4ecdd069e327eeee3aa01a81aab9b0ba07c01;Attach to process and read bases of memory regions;"def __init__(
         self.ip_address = ip_address
         self.active_breakpoints = {}
         self.main_base: int = None
         self.heap_base: int = None
         self.clear_responses()
         self.connect()
 
     def clear_responses(
         self,
@@ -59,6 +66,42 @@ def connect(
         self.write(f""target extended-remote {self.ip_address}:22225"", read_response=False)
         self.log_response(self.wait_for_response())
 
     def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
@@ -94,7 +137,7 @@ def log_response(
         if detailed:
             print(response)
         else:
-            for line in self.extract_payloads(response):
                 print(line)
 
     @staticmethod"
OK;10;Lincoln-LM;py-gdb-nx;8fa4ecdd069e327eeee3aa01a81aab9b0ba07c01;Attach to process and read bases of memory regions;"def __init__(
         self.ip_address = ip_address
         self.active_breakpoints = {}
         self.main_base: int = None
+        self.main_max: int = None
         self.heap_base: int = None
+        self.heap_max: int = None
+        self.stack_base: int = None
+        self.stack_max: int = None
         self.clear_responses()
         self.connect()
+        self.attach()
+        self.get_bases()
+        self.write(""set step-mode on"")
 
     def clear_responses(
         self,
@@ -59,6 +66,42 @@ def connect(
         self.write(f""target extended-remote {self.ip_address}:22225"", read_response=False)
         self.log_response(self.wait_for_response())
 
+    def attach(
+        self,
+        process_name: str = ""Application"",
+    ):
+        """"""Attach to process of name process_name
+
+        Args:
+            process_name (str, optional): Name of switch process to attach to.
+            Defaults to ""Application"".
+        """"""
+        self.write(""info os processes"", read_response = False)
+        processes = self.wait_for_response()
+        for line in reversed(processes): # sort by latest process started
+            if line['type'] == ""console"" and process_name in line[""payload""]:
+                process_id = int(line[""payload""].split("" "",1)[0])
+                self.log_response(self.write(f""attach {process_id}""))
+                break
+
+    def get_bases(
+        self,
+    ):
+        """"""
+        Read the base addresses of sections of the switch's memory
+        """"""
+        self.write(""monitor get base"", read_response = False)
+        for line in self.filter_response(self.wait_for_response(""target""), ""target""):
+            if ""Heap"" in line['payload']:
+                self.heap_base, self.heap_max = \
+                    (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[4:6])
+            elif ""Stack"" in line['payload']:
+                self.stack_base, self.stack_max = \
+                    (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[3:5])
+            elif "".nss"" in line['payload']:
+                self.main_base, self.main_max = \
+                    (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[2:4])   
+
     def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
@@ -94,7 +137,7 @@ def log_response(
         if detailed:
             print(response)
         else:
+            for line in self.extract_payloads(self.filter_response(response)):
                 print(line)
 
     @staticmethod"
KO;22;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
-
-
-            
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
@@ -48,17 +46,14 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
-        self.__totalTime = 0
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
        
-    def totalTime(self):
-        for job in self.__job:
-            self.__totalTime += job.jobTime()
-        return self.__totalTime
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -99,8 +94,12 @@ def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
-                        print(f'Job {str(job.jobStream())}: {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms') 
                         break
                     
         
         while len(tempList) -1 >= 2:
@@ -113,7 +112,6 @@ def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
-                        print(f'Job {str(job.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms')
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
@@ -128,18 +126,23 @@ def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
-                                    print(f'Job {str(job2.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job2.jobTime())} ms')
                                     break
                 else:
                     continue
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
-        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -159,20 +162,13 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
-        self.__totalTime = 0
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
-        self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
-        self.__totalWQ = {}
-       
-    def totalTime(self):
-        for job in self.__job:
-            self.__totalTime += job.jobTime()
-        return self.__totalTime
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -182,7 +178,6 @@ def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
-            self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -214,7 +209,7 @@ def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
-            print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
@@ -252,7 +247,7 @@ def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
-                print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
@@ -261,8 +256,8 @@ def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
-        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(self.__jobCount/self.__timer)} jobs per unit of time')
-        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -272,7 +267,6 @@ def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
-        print(self.__totalWT)
 
 
 # Worst Fit Memory Allocation
@@ -283,25 +277,25 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
-        self.__totalTime = len(self.__job)
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
-        self.__totalWQ = {}
-    
-    
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
-            self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -332,13 +326,12 @@ def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
-
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
-            print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
-            self.sumIF(value,key)        
-        
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
@@ -373,7 +366,7 @@ def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
-                print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
@@ -382,8 +375,8 @@ def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
-        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time')
-        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -395,12 +388,11 @@ def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
-        print(self.__totalTime)
-        print(len(self.__job))
 
 
 
 def main():
     memoryBlockList = []
     jobList = []
     
@@ -420,19 +412,16 @@ def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
-    while True:
-        print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
-        key = input("""")
-        if key == '1':
-            wf = WorstFit(memoryBlockList, jobList).worstFit()
-        elif key == '2':
-            bf = BestFit(memoryBlockList, jobList).bestFit()
-        elif key == '3':
-            ff = FirstFit(memoryBlockList, jobList).firstFit()
-        else:
-            print(""Invalid Key. Please try again.\n"")
-    
-    
 
-main()
 "
OK;22;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
+     
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
@@ -48,17 +46,14 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
+        self.__totalTime = 140  # total time from the given jobs based on the MP3
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
+        self.__totalWT = 0
        
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -99,8 +94,12 @@ def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
                         break
+        
+        for key,value in self.__allocation.items():
+            self.__totalWT += value.jobTime()
+            print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
+            
                     
         
         while len(tempList) -1 >= 2:
@@ -113,7 +112,6 @@ def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
@@ -128,18 +126,23 @@ def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
                                     break
                 else:
                     continue
+            for key,value in self.__allocation.items():
+                if value != None:
+                    self.__totalWT += value.jobTime()
+                    print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
+                    
+                    
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
+        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.__totalWT/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -159,20 +162,13 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
+        self.__totalTime = 140  # constant based on the MP3 given jobs
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -182,7 +178,6 @@ def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -214,7 +209,7 @@ def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
+            print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
@@ -252,7 +247,7 @@ def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
+                print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
@@ -261,8 +256,8 @@ def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
+        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
+        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT)/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -272,7 +267,6 @@ def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
 
 
 # Worst Fit Memory Allocation
@@ -283,25 +277,25 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
+        self.__totalTime = 140  # constant given in the description of MP
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
+        self.__temp = job
+
+
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
+
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -332,13 +326,12 @@ def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
+            self.sumIF(value,key) 
+            print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
+                   
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
@@ -373,7 +366,7 @@ def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
+                print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
@@ -382,8 +375,8 @@ def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
+        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
+        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT + self.__totalTime) /self.__jobCount,2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -395,12 +388,11 @@ def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
 
 
 
 def main():
+
     memoryBlockList = []
     jobList = []
     
@@ -420,19 +412,16 @@ def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
 
+    print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
+    key = input("""")
+    if key == '1':
+        wf = WorstFit(memoryBlockList, jobList).worstFit()
+    elif key == '2':
+        bf = BestFit(memoryBlockList, jobList).bestFit()
+    elif key == '3':
+        ff = FirstFit(memoryBlockList, jobList).firstFit()
+    else:
+        print(""Invalid Key. Please try again.\n"")
 
+main()"
KO;27;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
-
-
-            
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
@@ -48,17 +46,14 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
-        self.__totalTime = 0
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
        
-    def totalTime(self):
-        for job in self.__job:
-            self.__totalTime += job.jobTime()
-        return self.__totalTime
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -99,8 +94,12 @@ def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
-                        print(f'Job {str(job.jobStream())}: {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms') 
                         break
                     
         
         while len(tempList) -1 >= 2:
@@ -113,7 +112,6 @@ def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
-                        print(f'Job {str(job.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms')
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
@@ -128,18 +126,23 @@ def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
-                                    print(f'Job {str(job2.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job2.jobTime())} ms')
                                     break
                 else:
                     continue
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
-        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -159,20 +162,13 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
-        self.__totalTime = 0
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
-        self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
-        self.__totalWQ = {}
-       
-    def totalTime(self):
-        for job in self.__job:
-            self.__totalTime += job.jobTime()
-        return self.__totalTime
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -182,7 +178,6 @@ def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
-            self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -214,7 +209,7 @@ def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
-            print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
@@ -252,7 +247,7 @@ def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
-                print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
@@ -261,8 +256,8 @@ def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
-        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(self.__jobCount/self.__timer)} jobs per unit of time')
-        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -272,7 +267,6 @@ def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
-        print(self.__totalWT)
 
 
 # Worst Fit Memory Allocation
@@ -283,25 +277,25 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
-        self.__totalTime = len(self.__job)
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
-        self.__totalWQ = {}
-    
-    
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
-            self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -332,13 +326,12 @@ def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
-
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
-            print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
-            self.sumIF(value,key)        
-        
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
@@ -373,7 +366,7 @@ def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
-                print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
@@ -382,8 +375,8 @@ def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
-        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time')
-        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -395,12 +388,11 @@ def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
-        print(self.__totalTime)
-        print(len(self.__job))
 
 
 
 def main():
     memoryBlockList = []
     jobList = []
     
@@ -420,19 +412,16 @@ def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
-    while True:
-        print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
-        key = input("""")
-        if key == '1':
-            wf = WorstFit(memoryBlockList, jobList).worstFit()
-        elif key == '2':
-            bf = BestFit(memoryBlockList, jobList).bestFit()
-        elif key == '3':
-            ff = FirstFit(memoryBlockList, jobList).firstFit()
-        else:
-            print(""Invalid Key. Please try again.\n"")
-    
-    
 
-main()
 "
OK;27;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
+     
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
@@ -48,17 +46,14 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
+        self.__totalTime = 140  # total time from the given jobs based on the MP3
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
+        self.__totalWT = 0
        
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -99,8 +94,12 @@ def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
                         break
+        
+        for key,value in self.__allocation.items():
+            self.__totalWT += value.jobTime()
+            print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
+            
                     
         
         while len(tempList) -1 >= 2:
@@ -113,7 +112,6 @@ def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
@@ -128,18 +126,23 @@ def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
                                     break
                 else:
                     continue
+            for key,value in self.__allocation.items():
+                if value != None:
+                    self.__totalWT += value.jobTime()
+                    print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
+                    
+                    
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
+        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.__totalWT/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -159,20 +162,13 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
+        self.__totalTime = 140  # constant based on the MP3 given jobs
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
@@ -182,7 +178,6 @@ def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -214,7 +209,7 @@ def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
+            print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
@@ -252,7 +247,7 @@ def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
+                print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
@@ -261,8 +256,8 @@ def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
+        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
+        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT)/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -272,7 +267,6 @@ def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
 
 
 # Worst Fit Memory Allocation
@@ -283,25 +277,25 @@ def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
+        self.__totalTime = 140  # constant given in the description of MP
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
+        self.__temp = job
+
+
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
+
             
         else:
             temp = (memory.memorySize() - job.jobSize())
@@ -332,13 +326,12 @@ def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
+            self.sumIF(value,key) 
+            print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
+                   
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
@@ -373,7 +366,7 @@ def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
+                print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
@@ -382,8 +375,8 @@ def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
+        print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
+        print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT + self.__totalTime) /self.__jobCount,2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
@@ -395,12 +388,11 @@ def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
 
 
 
 def main():
+
     memoryBlockList = []
     jobList = []
     
@@ -420,19 +412,16 @@ def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
 
+    print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
+    key = input("""")
+    if key == '1':
+        wf = WorstFit(memoryBlockList, jobList).worstFit()
+    elif key == '2':
+        bf = BestFit(memoryBlockList, jobList).bestFit()
+    elif key == '3':
+        ff = FirstFit(memoryBlockList, jobList).firstFit()
+    else:
+        print(""Invalid Key. Please try again.\n"")
 
+main()"
KO;43;vanegascata;GAN;dec28e3dd38ec52c02c8fb3e4e8c4baf216555cf;fixed bug with pandas sampling frac -  decrease memory usage. Fixes for issue #14;"def preprocess_data(self, train_df, target, test_df, ) -> Tuple[pd.DataFrame, pd
     def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.DataFrame]:
         self._validate_data(train_df, target, test_df)
         train_df[self.TEMP_TARGET] = target
-        generated_df = train_df.sample(frac=(1 + self.pregeneration_frac * self.get_generated_shape(train_df)),
-                                       replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         gc.collect()
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
@@ -135,6 +134,7 @@ def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         gc.collect()
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)"
OK;43;vanegascata;GAN;dec28e3dd38ec52c02c8fb3e4e8c4baf216555cf;fixed bug with pandas sampling frac -  decrease memory usage. Fixes for issue #14;"def preprocess_data(self, train_df, target, test_df, ) -> Tuple[pd.DataFrame, pd
     def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.DataFrame]:
         self._validate_data(train_df, target, test_df)
         train_df[self.TEMP_TARGET] = target
+        generated_df = train_df.sample(frac=(1 + self.pregeneration_frac), replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         gc.collect()
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
@@ -135,6 +134,7 @@ def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
+        del ad_model
         gc.collect()
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)"
KO;43;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;" todo write description
 """"""
 
 import logging
 import warnings
 from typing import Tuple
@@ -99,6 +100,7 @@ def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
         generated_df = train_df.sample(frac=(1 + self.pregeneration_frac * self.get_generated_shape(train_df)),
                                        replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
 
     def postprocess_data(self, train_df, target, test_df, ):
@@ -119,6 +121,7 @@ def postprocess_data(self, train_df, target, test_df, ):
             for cat_col in self.cat_cols:
                 filtered_df = train_df[train_df[cat_col].isin(test_df[cat_col].unique())]
                 train_df = filtered_df
         return train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True), train_df[self.TEMP_TARGET].reset_index(
             drop=True)
 
@@ -132,6 +135,7 @@ def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)
 
@@ -162,14 +166,15 @@ def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
             ].astype(data_dtype[i])
 
         train_df = pd.concat([train_df, generated_df, ]).reset_index(drop=True)
         return train_df.drop(self.TEMP_TARGET, axis=1), train_df[self.TEMP_TARGET]
 
 
 def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
     _logger = logging.getLogger(__name__)
     _logger.info(""Starting generating data:"")
     _logger.info(creator.generate_data_pipe(in_train, in_target, in_test))
-    _logger.info(""Finished generatation\n"")
 
 
 if __name__ == ""__main__"":"
OK;43;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;" todo write description
 """"""
 
+import gc
 import logging
 import warnings
 from typing import Tuple
@@ -99,6 +100,7 @@ def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
         generated_df = train_df.sample(frac=(1 + self.pregeneration_frac * self.get_generated_shape(train_df)),
                                        replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
+        gc.collect()
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
 
     def postprocess_data(self, train_df, target, test_df, ):
@@ -119,6 +121,7 @@ def postprocess_data(self, train_df, target, test_df, ):
             for cat_col in self.cat_cols:
                 filtered_df = train_df[train_df[cat_col].isin(test_df[cat_col].unique())]
                 train_df = filtered_df
+        gc.collect()
         return train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True), train_df[self.TEMP_TARGET].reset_index(
             drop=True)
 
@@ -132,6 +135,7 @@ def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
+        gc.collect()
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)
 
@@ -162,14 +166,15 @@ def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
             ].astype(data_dtype[i])
 
         train_df = pd.concat([train_df, generated_df, ]).reset_index(drop=True)
+        gc.collect()
         return train_df.drop(self.TEMP_TARGET, axis=1), train_df[self.TEMP_TARGET]
 
 
 def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
     _logger = logging.getLogger(__name__)
     _logger.info(""Starting generating data:"")
     _logger.info(creator.generate_data_pipe(in_train, in_target, in_test))
+    _logger.info(""Finished generation\n"")
 
 
 if __name__ == ""__main__"":"
KO;43;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;"def test_postprocess_data(self):
         new_train, new_target = self.sampler.postprocess_data(gen_train, gen_target, test_df)
         self.assertEqual(new_train.shape[0], new_target.shape[0])
         self.assertGreaterEqual(new_train.iloc[:, 0].min(), test_df.iloc[:, 0].min())
 
     def test_adversarial_filtering(self):
         new_train, new_target, test_df = self.sampler.preprocess_data(self.train.copy(),"
OK;43;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;"def test_postprocess_data(self):
         new_train, new_target = self.sampler.postprocess_data(gen_train, gen_target, test_df)
         self.assertEqual(new_train.shape[0], new_target.shape[0])
         self.assertGreaterEqual(new_train.iloc[:, 0].min(), test_df.iloc[:, 0].min())
+        self.assertGreaterEqual(test_df.iloc[:, 0].max(), new_train.iloc[:, 0].max())
 
     def test_adversarial_filtering(self):
         new_train, new_target, test_df = self.sampler.preprocess_data(self.train.copy(),"
KO;1;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";"build-backend = ""setuptools.build_meta""
 
 [project]
 name = ""cardano-nft-vending-machine""
-version = ""0.3.0-beta2""
 
 description = ""Library to perform NFT mints automatically on the Cardano blockchain""
 readme = ""README.md"""
OK;1;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";"build-backend = ""setuptools.build_meta""
 
 [project]
 name = ""cardano-nft-vending-machine""
+version = ""0.3.0-beta3""
 
 description = ""Library to perform NFT mints automatically on the Cardano blockchain""
 readme = ""README.md"""
KO;1;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";" Repreentation of the Blockfrost web API used in retrieving metadata about txn i/o on the chain.
 """"""
 class BlockfrostApi(object):
     def __init__(self, project, mainnet=False):
         self.project = project
         self.mainnet = mainnet
@@ -39,21 +42,26 @@ def get_input_address(self, txn_hash):
         return utxo_inputs.pop()
 
     def get_utxos(self, address, exclusions):
-        try:
-            utxo_data = self.__call_get_api(f""addresses/{address}/utxos"")
-        except requests.exceptions.HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                return []
-            raise e
         available_utxos = set()
-        #print('EXCLUSIONS\t', [f'{utxo.hash}#{utxo.ix}' for utxo in exclusions])
-        for raw_utxo in utxo_data:
-            balances = [Utxo.Balance(int(balance['quantity']), balance['unit']) for balance in raw_utxo['amount']]
-            utxo = Utxo(raw_utxo['tx_hash'], raw_utxo['output_index'], balances)
-            if utxo in exclusions:
-                print(f'Skipping {utxo.hash}#{utxo.ix}')
-                continue
-            available_utxos.add(utxo)
         return available_utxos
 
     def get_protocol_parameters(self):"
OK;1;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";" Repreentation of the Blockfrost web API used in retrieving metadata about txn i/o on the chain.
 """"""
 class BlockfrostApi(object):
+
+    _UTXO_LIST_LIMIT = 100
+
     def __init__(self, project, mainnet=False):
         self.project = project
         self.mainnet = mainnet
@@ -39,21 +42,26 @@ def get_input_address(self, txn_hash):
         return utxo_inputs.pop()
 
     def get_utxos(self, address, exclusions):
         available_utxos = set()
+        current_page = 0
+        while True:
+            current_page += 1
+            try:
+                utxo_data = self.__call_get_api(f""addresses/{address}/utxos?count={BlockfrostApi._UTXO_LIST_LIMIT}&page={current_page}"")
+            except requests.exceptions.HTTPError as e:
+                if e.response.status_code == HTTPStatus.NOT_FOUND:
+                    return []
+                raise e
+            #print('EXCLUSIONS\t', [f'{utxo.hash}#{utxo.ix}' for utxo in exclusions])
+            for raw_utxo in utxo_data:
+                balances = [Utxo.Balance(int(balance['quantity']), balance['unit']) for balance in raw_utxo['amount']]
+                utxo = Utxo(raw_utxo['tx_hash'], raw_utxo['output_index'], balances)
+                if utxo in exclusions:
+                    print(f'Skipping {utxo.hash}#{utxo.ix}')
+                    continue
+                available_utxos.add(utxo)
+            if len(utxo_data) < BlockfrostApi._UTXO_LIST_LIMIT:
+                break
         return available_utxos
 
     def get_protocol_parameters(self):"
KO;1;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
@@ -93,7 +95,7 @@ def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
-                result = self.sess.run(tf.nn.softmax(out_bid_logit), feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
OK;1;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
+        self.output_softmax = tf.nn.softmax(self.graph.get_tensor_by_name('out_bid_logit:0'))
+        self.graph.finalize()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
@@ -93,7 +95,7 @@ def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
+                result = self.sess.run(self.output_softmax, feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
KO;1;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";" import numpy as np
 import tensorflow as tf
 
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
@@ -18,6 +20,7 @@ def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.model = self.init_model()
 
     def close(self):
@@ -47,7 +50,7 @@ def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
-        return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1], 32))))#[:,-1,:]
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
@@ -56,7 +59,7 @@ def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
-        return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32))))#[:,-1,:]
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
OK;1;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";" import numpy as np
 import tensorflow as tf
 
+from scipy.special import softmax
+
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
@@ -18,6 +20,7 @@ def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
+        self.graph.finalize()
         self.model = self.init_model()
 
     def close(self):
@@ -47,7 +50,7 @@ def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
+        return softmax(card_logit.reshape((x.shape[0], x.shape[1], 32)), axis=2)
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
@@ -56,7 +59,7 @@ def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
+        return softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32)), axis=2)
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
KO;1;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
@@ -93,7 +95,7 @@ def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
-                result = self.sess.run(tf.nn.softmax(out_bid_logit), feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
OK;1;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
+        self.output_softmax = tf.nn.softmax(self.graph.get_tensor_by_name('out_bid_logit:0'))
+        self.graph.finalize()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
@@ -93,7 +95,7 @@ def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
+                result = self.sess.run(self.output_softmax, feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
KO;1;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;" import numpy as np
 import tensorflow as tf
 
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
@@ -18,6 +20,7 @@ def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.model = self.init_model()
 
     def close(self):
@@ -47,7 +50,7 @@ def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
-        return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1], 32))))#[:,-1,:]
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
@@ -56,7 +59,7 @@ def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
-        return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32))))#[:,-1,:]
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
OK;1;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;" import numpy as np
 import tensorflow as tf
 
+from scipy.special import softmax
+
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
@@ -18,6 +20,7 @@ def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
+        self.graph.finalize()
         self.model = self.init_model()
 
     def close(self):
@@ -47,7 +50,7 @@ def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
+        return softmax(card_logit.reshape((x.shape[0], x.shape[1], 32)), axis=2)
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
@@ -56,7 +59,7 @@ def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
+        return softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32)), axis=2)
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
KO;3;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
@@ -344,30 +373,68 @@ def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
-        cur_deltas = all_centers - old_centers  # (B, N, K, dim)
         dim = self.dim
-        assert cur_deltas.shape == (B, N, K, dim)
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
-        # cur_sumsq: (B, N, K), is the sum-squared error if we were to
-        # make the n'th choice without making any of the other N-1 choices, i.e.
-        # if we were to leave the other choices at the value we had at input.
-        # Specifically, it is always supposed to equal the value of
-        #  ((x_err + cur_deltas)**2).sum(dim=-1)
-        # .. but we keep it around separately because it enables an optimization.
-        modified_err = x_err + cur_deltas # (B, N, K, dim)
-
-        # cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
-        # We really want batched vector-vector product her, which torch does not
-        # explicitly support, so we use a matrix multiplication with 1x1 output.
-        cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
-                                 modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
-
-        assert cur_sumsq.shape == (B, N, K)
-        # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
-        gather_deltas = None # will be a lambda, see below.
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
@@ -400,16 +467,25 @@ def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
-                # cur_indexes is (B, N, new_K, dim), but sorted from worst to best.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
-                if cur_deltas is not None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
                     cur_deltas = gather_deltas(this_indexes)
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
OK;3;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
+        # At all points in the algorithm we maintain cur_sumsq and (conceptually)
+        # cur_deltas (however in some parts cur_deltas is not instantiated, see
+        # gather_deltas).
+        #
+        # cur_indexes: (B, N, K, L), initially (B, num_codebooks, codebook_size, 1),
+        #   gives the codebook indexes corresponding to the k'th value of the n'th
+        #   choice.  Initially this is just an arange expression but from the 1st
+        #   iter of the algorithm it changes to something nontrivial.
+        #
+        # cur_sumsq: (B, N, K), is the sum-squared error of x versus its predicted value
+        # from the codebooks, if we were to
+        # make the n'th choice with value k without making any of the other N-1 choices, i.e.
+        # if we were to leave the other choices at the value we had at input.
+        # Specifically, it is always supposed to equal the value of
+        #  ((x_err + cur_deltas)**2).sum(dim=-1)
+        # .. but we keep it around separately because it enables an optimization.
+        #
+        # cur_deltas: (B, N, K, dim), is the change in x_err (with x_err =
+        # x_approx - x and x_approx being a sum of codebook indexes) if we were
+        # to make the n'th choice with value k without making any of the other
+        # N-1 choices.
+        # At the current point, i.e. at the start of the algorithm,
+        # cur_deltas[b][n][k] says ""what would be the change in x_err if we
+        # were to replace the current choice of the n'th codebook entry-- i.e.
+        # the choice reflected in `indexes`-- with value k?  [In general,
+        # cur_deltas[b][n][k] refers not directly to a codebook indexes, but
+        # to an indexes into `cur_indexes` which corresponds to the sequence/combination
+        # of codebook indexes that are stored in cur_indexes[b][n][k].
+
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
@@ -344,30 +373,68 @@ def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
+        # Conceptually we could do:
+        # cur_deltas = all_centers - old_centers  # (B, N, K, dim)
+        # ... however actually we won't be instantiating cur_deltas at this stage of the
+        # algorithm.
         dim = self.dim
+
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
+        if True:
+            # compute cur_sumsq using an efficient approach
+            x_err_sumsq = (x_err ** 2).sum(dim=-1) # (B, 1, 1)
+
+            x_remaining = x_err - old_centers  # (B, num_codebooks, 1, dim): the x_err after subtracting
+            # each of the codebooks; if we add back to this any given
+            # codebook vector (from all_centers), we'll get the error
+            # if we were to
+            # choose that codebook entry instead of the one actually chosen.
+
+            x_remaining_sumsq = (x_remaining ** 2).sum(dim=-1) # (B, num_codebooks, 1)
+            # all_centers_sumsq is the sumsq of all the centers..
+            all_centers_sumsq = (all_centers ** 2).sum(dim=-1) # (1, num_codebooks, codebook_size)
+
+            cross_sum = torch.matmul(all_centers, # (1, num_codebooks, codebook_size, dim)
+                                     x_remaining.permute(2, 1, 3, 0)  # (1, num_codebooks, dim, B)
+            ) # (1, num_codebooks, codebook_size, B)
+            cross_sum = cross_sum.squeeze(0).permute(2, 0, 1) # (B, num_codebooks, codebook_size)
+            # (B, num_codebooks, codebook_size); interpret as (B, N, K)
+            cur_sumsq = x_remaining_sumsq + all_centers_sumsq + 2 * cross_sum
+            assert cur_sumsq.shape == (B, N, K)
+
+            # gather_deltas (which will be re-defined below) is a lambda from
+            # `this_indexes`, a LongTensor of shape (B, N, new_K, 1) [which
+            # at the current iteration would equal (B, num_codebooks, new_K, 1)]
+            # with elements in
+            # {0..K-1} [i.e. 0..codebook_size-1], to the new ""cur_deltas"".
+            # It is provided as a workaround in
+            # case we did not physically instantiate cur_deltas on this iteration.
+            # In general cur_deltas is supposed to represent ""change in encoded
+            # value"" if we were to make a particular modified index choice, leaving
+            # all other choices as they were on entry.
+            # gather_deltas is supposed to be a lambda from this_indexes to the
+            # something equivalent to following expression (if cur_deltas had actually
+            # existed):
+            #   torch.gather(input=cur_deltas, dim=2, index=this_indexes.expand(B, N, new_K, dim))
+
+            gather_deltas = lambda this_indexes: (
+                torch.gather(input=all_centers.expand(B, N, K, dim), dim=2,
+                             index=this_indexes.expand(B, N, -1, dim)) - old_centers
+            )
+        else:
+            cur_deltas = all_centers - old_centers  # (B, N, K, dim)
+            ## cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
+            ## We really want batched vector-vector product her, which torch does not
+            ## explicitly support, so we use a matrix multiplication with 1x1 output.
+            modified_err = x_err + cur_deltas # (B, N, K, dim)
+            cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
+                                     modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
+            gather_deltas = None
+
+            # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
@@ -400,16 +467,25 @@ def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
+                # cur_indexes is (B, N, new_K, L), but with only the chosen
+                # indexes kept.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
+                if gather_deltas is None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
+                    # gather_deltas should be a lambda from:
+                    # this_indexes: a LongTensor of shape (B, N, new_K, 1) containing elements in {0..K-1}
+                    # to the new ""deltas"" which should be of shape
+                    # (B, N, new_K, dim)
+                    # representing the difference from the baseline ""x_offset"" if we choose this
+                    # index for this codebook or range of codebooks, leaving other choices
+                    # as they were at entry to this function.
                     cur_deltas = gather_deltas(this_indexes)
+                    gather_deltas = None
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
KO;3;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
-    #_test_train_from_file()
     _test_joint_predictor()"
OK;3;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
+    _test_train_from_file()
     _test_joint_predictor()"
KO;3;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
@@ -344,30 +373,68 @@ def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
-        cur_deltas = all_centers - old_centers  # (B, N, K, dim)
         dim = self.dim
-        assert cur_deltas.shape == (B, N, K, dim)
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
-        # cur_sumsq: (B, N, K), is the sum-squared error if we were to
-        # make the n'th choice without making any of the other N-1 choices, i.e.
-        # if we were to leave the other choices at the value we had at input.
-        # Specifically, it is always supposed to equal the value of
-        #  ((x_err + cur_deltas)**2).sum(dim=-1)
-        # .. but we keep it around separately because it enables an optimization.
-        modified_err = x_err + cur_deltas # (B, N, K, dim)
-
-        # cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
-        # We really want batched vector-vector product her, which torch does not
-        # explicitly support, so we use a matrix multiplication with 1x1 output.
-        cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
-                                 modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
-
-        assert cur_sumsq.shape == (B, N, K)
-        # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
-        gather_deltas = None # will be a lambda, see below.
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
@@ -400,16 +467,33 @@ def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
-                # cur_indexes is (B, N, new_K, dim), but sorted from worst to best.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
-                if cur_deltas is not None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
                     cur_deltas = gather_deltas(this_indexes)
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
OK;3;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
+        # At all points in the algorithm we maintain cur_sumsq and (conceptually)
+        # cur_deltas (however in some parts cur_deltas is not instantiated, see
+        # gather_deltas).
+        #
+        # cur_indexes: (B, N, K, L), initially (B, num_codebooks, codebook_size, 1),
+        #   gives the codebook indexes corresponding to the k'th value of the n'th
+        #   choice.  Initially this is just an arange expression but from the 1st
+        #   iter of the algorithm it changes to something nontrivial.
+        #
+        # cur_sumsq: (B, N, K), is the sum-squared error of x versus its predicted value
+        # from the codebooks, if we were to
+        # make the n'th choice with value k without making any of the other N-1 choices, i.e.
+        # if we were to leave the other choices at the value we had at input.
+        # Specifically, it is always supposed to equal the value of
+        #  ((x_err + cur_deltas)**2).sum(dim=-1)
+        # .. but we keep it around separately because it enables an optimization.
+        #
+        # cur_deltas: (B, N, K, dim), is the change in x_err (with x_err =
+        # x_approx - x and x_approx being a sum of codebook indexes) if we were
+        # to make the n'th choice with value k without making any of the other
+        # N-1 choices.
+        # At the current point, i.e. at the start of the algorithm,
+        # cur_deltas[b][n][k] says ""what would be the change in x_err if we
+        # were to replace the current choice of the n'th codebook entry-- i.e.
+        # the choice reflected in `indexes`-- with value k?  [In general,
+        # cur_deltas[b][n][k] refers not directly to a codebook indexes, but
+        # to an indexes into `cur_indexes` which corresponds to the sequence/combination
+        # of codebook indexes that are stored in cur_indexes[b][n][k].
+
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
@@ -344,30 +373,68 @@ def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
+        # Conceptually we could do:
+        # cur_deltas = all_centers - old_centers  # (B, N, K, dim)
+        # ... however actually we won't be instantiating cur_deltas at this stage of the
+        # algorithm.
         dim = self.dim
+
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
+        if True:
+            # compute cur_sumsq using an efficient approach
+            x_err_sumsq = (x_err ** 2).sum(dim=-1) # (B, 1, 1)
+
+            x_remaining = x_err - old_centers  # (B, num_codebooks, 1, dim): the x_err after subtracting
+            # each of the codebooks; if we add back to this any given
+            # codebook vector (from all_centers), we'll get the error
+            # if we were to
+            # choose that codebook entry instead of the one actually chosen.
+
+            x_remaining_sumsq = (x_remaining ** 2).sum(dim=-1) # (B, num_codebooks, 1)
+            # all_centers_sumsq is the sumsq of all the centers..
+            all_centers_sumsq = (all_centers ** 2).sum(dim=-1) # (1, num_codebooks, codebook_size)
+
+            cross_sum = torch.matmul(all_centers, # (1, num_codebooks, codebook_size, dim)
+                                     x_remaining.permute(2, 1, 3, 0)  # (1, num_codebooks, dim, B)
+            ) # (1, num_codebooks, codebook_size, B)
+            cross_sum = cross_sum.squeeze(0).permute(2, 0, 1) # (B, num_codebooks, codebook_size)
+            # (B, num_codebooks, codebook_size); interpret as (B, N, K)
+            cur_sumsq = x_remaining_sumsq + all_centers_sumsq + 2 * cross_sum
+            assert cur_sumsq.shape == (B, N, K)
+
+            # gather_deltas (which will be re-defined below) is a lambda from
+            # `this_indexes`, a LongTensor of shape (B, N, new_K, 1) [which
+            # at the current iteration would equal (B, num_codebooks, new_K, 1)]
+            # with elements in
+            # {0..K-1} [i.e. 0..codebook_size-1], to the new ""cur_deltas"".
+            # It is provided as a workaround in
+            # case we did not physically instantiate cur_deltas on this iteration.
+            # In general cur_deltas is supposed to represent ""change in encoded
+            # value"" if we were to make a particular modified index choice, leaving
+            # all other choices as they were on entry.
+            # gather_deltas is supposed to be a lambda from this_indexes to the
+            # something equivalent to following expression (if cur_deltas had actually
+            # existed):
+            #   torch.gather(input=cur_deltas, dim=2, index=this_indexes.expand(B, N, new_K, dim))
+
+            gather_deltas = lambda this_indexes: (
+                torch.gather(input=all_centers.expand(B, N, K, dim), dim=2,
+                             index=this_indexes.expand(B, N, -1, dim)) - old_centers
+            )
+        else:
+            cur_deltas = all_centers - old_centers  # (B, N, K, dim)
+            ## cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
+            ## We really want batched vector-vector product her, which torch does not
+            ## explicitly support, so we use a matrix multiplication with 1x1 output.
+            modified_err = x_err + cur_deltas # (B, N, K, dim)
+            cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
+                                     modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
+            gather_deltas = None
+
+            # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
@@ -400,16 +467,33 @@ def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
+                # cur_indexes is (B, N, new_K, L), but with only the chosen
+                # indexes kept.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
+                if gather_deltas is None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
+                    # gather_deltas should be a lambda from:
+                    # this_indexes: a LongTensor of shape (B, N, new_K, 1) containing elements in {0..K-1}
+                    # to the new ""deltas"" which should be of shape
+                    # (B, N, new_K, dim)
+                    # representing the difference from the baseline ""x_offset"" if we choose this
+                    # index for this codebook or range of codebooks, leaving other choices
+                    # as they were at entry to this function.
+
+                    #if cur_deltas is not None:
+                    #    cur_deltas_alt = torch.gather(input=cur_deltas, dim=2,
+                    #                                  index=this_indexes.expand(B, N, new_K, dim))
                     cur_deltas = gather_deltas(this_indexes)
+                    #if cur_deltas is not None and cur_deltas.shape == cur_deltas_alt.shape:
+                    #    print(""cur_deltas: "", cur_deltas[:3,:3,:3,:3])
+                    #    print(""cur_deltas_alt: "", cur_deltas_alt[:3,:3,:3,:3])
+                    #    assert torch.allclose(cur_deltas, cur_deltas_alt)
+                    gather_deltas = None
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
KO;3;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
-    #_test_train_from_file()
     _test_joint_predictor()"
OK;3;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
+    _test_train_from_file()
     _test_joint_predictor()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" 
 A Python shared memory toolkit for process picture between different processes.
 
 ### How to use
 
 main process - 
 
 ```python
-# optional
 import cv2
-from shared_memory_toolkit import dump_image_into_shared_memory
 
-image = cv2.imread('test.pic')
-dump_image_into_shared_memory('uuid_content', image)
-```
 
-sub process - 
 
-```python
-from shared_memory_toolkit import load_image_from_shared_memory
 
-# ... some other codes
 
-image = load_image_from_shared_memory('uuid_content')
 ```
 
-imagebytes
-
 #### TODO:
 
-1. unittest for raw_image.
-2. base64_image module.
-3. README and docs."
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" 
 A Python shared memory toolkit for process picture between different processes.
 
+# Upgrade: 
+
 ### How to use
 
 main process - 
 
 ```python
+from shared_memory_toolkit import load_image_from_shared_memory, dump_image_into_shared_memory
+
+# load image
 import cv2
 
+raw_image = cv2.imread('image')
 
+image_shm_name = 'camera_1817'
+dump_image_into_shared_memory(image_shm_name, raw_image)
 
+# in other process
 
+image = load_image_from_shared_memory('camera_1817')
 
+# raw_image == image
 ```
 
 #### TODO:
 
+1. base64_image module."
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+# -*- coding: utf-8 -*-
+""""""setup with setuptools.""""""
+
+from setuptools import setup, find_packages
+
+setup(
+    name='stream_watcher',
+    version='0.1',
+    keywords='Stream',
+    description='A Pythonic way to manage streams in one file.',
+    author='Logic',
+    author_email='logic.irl@outlook.com',
+    url='https://github.com/TheStar-LikeDust/shared_memory_toolkit.git',
+    python_requires='>=3.8',
+    packages=find_packages(exclude=['tests*']),
+    license='Apache License 2.0'
+)"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""
 
 """"""
 
 from .raw_image import dump_image_into_shared_memory, load_image_from_shared_memory
 
-from .sync import initial_sync_in_fork, initial_sync_in_spawn"
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""
 
 """"""
+# system
+from .sync import initial_sync_in_fork, initial_sync_in_spawn
 
+# base
+from .core import get_share_memory
 from .raw_image import dump_image_into_shared_memory, load_image_from_shared_memory
 
+# extra
+# TODO:"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""Dict[str, SharedMemory]: """"""
 
 
-def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
     """"""
 
-    FIX_LENGTH
-
-    Note:
-        
-
-    Note:
-        initial_shared_memory_lock
 
     Args:
-        shared_memory_name (str): 
 
     Returns:
-        Tuple[SharedMemory, Lock]: 
     """"""
     # 
     lock = get_shm_lock(shared_memory_name)
@@ -62,8 +57,11 @@ def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
             # 
             # case: 
             try:
-                shared = SharedMemory(name=shared_memory_name, create=True, size=FIX_LENGTH)
-                assert len(shared.buf) == FIX_LENGTH
             # case: 
             except FileExistsError:
                 shared = SharedMemory(name=shared_memory_name, create=False)"
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""Dict[str, SharedMemory]: """"""
 
 
+def get_share_memory(shared_memory_name: str, memory_size: int = None) -> Tuple[SharedMemory, Lock]:
     """"""
 
+    FIX_LENGTH
 
     Args:
+        shared_memory_name (str): 
+        memory_size (int, optional): . Defaults to None.
 
     Returns:
+        Tuple[SharedMemory, Lock]: 
     """"""
     # 
     lock = get_shm_lock(shared_memory_name)
@@ -62,8 +57,11 @@ def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
             # 
             # case: 
             try:
+                shared = SharedMemory(
+                    name=shared_memory_name,
+                    create=True,
+                    size=memory_size if memory_size else FIX_LENGTH
+                )
             # case: 
             except FileExistsError:
                 shared = SharedMemory(name=shared_memory_name, create=False)"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""numpy
 
-
 :
 
-    1. IMAGE_SHAPE
 
 :
 
-    1. dump_image_into_shared_memory::
-
-        Dump image
-
-    2. load_image_from_shared_memory::
-
-        Load image
-
-Note:
-
-    
 
 
 """"""
 import numpy
 
-from .core import _get_share_memory, FIX_LENGTH
 
-IMAGE_SHAPE = (1080, 1920, 3)
 """"""""""""
 
 
-def dump_image_into_shared_memory(shared_memory_name: str, image: numpy.ndarray) -> memoryview:
     """"""dump
 
     Args:
         shared_memory_name (str): 
         image (numpy.ndarray): numpy
 
     Returns:
-        memoryview: .buf
     """"""
-    shared_memory, lock = _get_share_memory(shared_memory_name)
 
     with lock:
         shared_memory.buf[:FIX_LENGTH] = image.tobytes()
-    return shared_memory.buf
 
 
-def load_image_from_shared_memory(shared_memory_name: str) -> numpy.ndarray:
     """"""
 
     Args:
         shared_memory_name (str): 
 
     Returns:
         numpy.ndarray: numpy
     """"""
-    shared_memory, lock = _get_share_memory(shared_memory_name)
 
     with lock:
-        image = numpy.frombuffer(shared_memory.buf, dtype=numpy.uint8)[:FIX_LENGTH].reshape(IMAGE_SHAPE)
     return image"
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""numpy
 
 :
 
+    1. IMAGE_SHAPE: 
 
 :
 
+    1. dump_image_into_shared_memory: Dump image
+    2. load_image_from_shared_memory: Load image
 
+TODO:
 
+    1. shmnumpy
 """"""
+from typing import Tuple
+from multiprocessing.shared_memory import SharedMemory
+
 import numpy
 
+from .core import get_share_memory, FIX_LENGTH
 
+DEFAULT_IMAGE_SHAPE: Tuple[int, int, int] = (1080, 1920, 3)
 """"""""""""
 
+get_image_size = lambda x: x[0] * x[1] * x[2]
+""""""""""""
+
 
+def dump_image_into_shared_memory(
+        shared_memory_name: str,
+        image: numpy.ndarray,
+        memory_size: int = get_image_size(DEFAULT_IMAGE_SHAPE),
+) -> SharedMemory:
     """"""dump
 
     Args:
         shared_memory_name (str): 
         image (numpy.ndarray): numpy
+        memory_size (int): . Default is 6220800
 
     Returns:
+        SharedMemory: 
     """"""
+    shared_memory, lock = get_share_memory(shared_memory_name, memory_size)
 
     with lock:
         shared_memory.buf[:FIX_LENGTH] = image.tobytes()
+    return shared_memory
 
 
+def load_image_from_shared_memory(
+        shared_memory_name: str,
+        image_shape: Tuple[int, int, int] = DEFAULT_IMAGE_SHAPE,
+) -> numpy.ndarray:
     """"""
 
     Args:
         shared_memory_name (str): 
+        image_shape (Tuple[int, int, int]): . Default is (1080, 1920, 3)
 
     Returns:
         numpy.ndarray: numpy
     """"""
+    shared_memory, lock = get_share_memory(shared_memory_name)
 
     with lock:
+        image = numpy.frombuffer(shared_memory.buf, dtype=numpy.uint8)[:get_image_size(image_shape)].reshape(
+            image_shape)
     return image"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" def get_shm_lock(shm_name) -> Lock:
     """"""""""""
 
     # 
     with _lock:
         # case: 
@@ -57,7 +61,7 @@ def initial_sync_in_fork(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], L
 
 
 def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], List[Lock]]:
-    """"""Spawn
 
     Args:
         lock_number (int, optional): . Defaults to 64.
@@ -75,7 +79,7 @@ def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int],
 
 
 def synchronization_setter(lock: Lock, name_index_mapper: Dict[str, int], lock_list: List[Lock]) -> NoReturn:
-    """"""Spawn
 
     Args:
         lock (Lock): "
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" def get_shm_lock(shm_name) -> Lock:
     """"""""""""
 
+    # 
+    if not _lock_list:
+        return _lock
+
     # 
     with _lock:
         # case: 
@@ -57,7 +61,7 @@ def initial_sync_in_fork(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], L
 
 
 def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], List[Lock]]:
+    """"""Spawn
 
     Args:
         lock_number (int, optional): . Defaults to 64.
@@ -75,7 +79,7 @@ def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int],
 
 
 def synchronization_setter(lock: Lock, name_index_mapper: Dict[str, int], lock_list: List[Lock]) -> NoReturn:
+    """"""Spawn
 
     Args:
         lock (Lock): "
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+import unittest
+
+
+class MyTestCase(unittest.TestCase):
+    def test_something(self):
+        self.assertEqual(True, False)  # add assertion here
+
+
+if __name__ == '__main__':
+    unittest.main()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+# -*- coding: utf-8 -*-
+""""""Unitest testcases.
+
+
+"""""""
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+# -*- coding: utf-8 -*-
+""""""Example Google style docstrings.
+
+Example:
+    Examples can be given using either the ``Example`` or ``Examples``
+    sections. Sections support any reStructuredText formatting, including
+    literal blocks::
+
+        $ python example_google.py
+
+Attributes:
+    module_level_variable1 (int): Module level variables may be documented in
+        either the ``Attributes`` section of the module docstring, or in an
+        inline docstring immediately following the variable.
+
+Todo:
+    * For module TODOs
+    * You have to also use ``sphinx.ext.todo`` extension
+
+.. _Google Python Style Guide:
+   http://google.github.io/styleguide/pyguide.html
+
+"""""""
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+import time
+import unittest
+
+import random
+from multiprocessing import Process, shared_memory
+from concurrent.futures import ProcessPoolExecutor
+
+from shared_memory_toolkit.core import get_share_memory
+
+
+def dump_bytes(shm_name: str, random_bytes: bytes):
+    shm, lock = get_share_memory(shm_name)
+
+    shm.buf[:len(random_bytes)] = random_bytes
+
+
+def load_bytes(shm_name: str) -> bytes:
+    shm, lock = get_share_memory(shm_name)
+
+    return bytes(shm.buf)
+
+
+def dump_bytes_in_process(shm_name: str, random_bytes: bytes):
+    # cannot pickle local function as subprocess
+    p = Process(target=dump_bytes, args=(shm_name, random_bytes))
+    p.daemon = True
+    p.start()
+    p.join()
+
+
+def get_lock_id(shm_name):
+    shm, lock = get_share_memory(shm_name)
+    print(id(lock))
+
+    return id(lock)
+
+
+class CoreTestCase(unittest.TestCase):
+
+    def test_default_property(self):
+        """"""""""""
+        from shared_memory_toolkit import core
+
+        with self.subTest('Default FIX_LENGTH: 6220800'):
+            assert core.FIX_LENGTH == 6220800
+
+        with self.subTest('Default _share_memory_cache_mapper: empty'):
+            assert core._share_memory_cache_mapper == {}
+
+    def test_get_share_memory_same_name(self):
+        """"""_get_share_memory: """"""
+        same_name = 'same_name'
+
+        shm_0, lock_0 = get_share_memory(same_name)
+        shm_1, lock_1 = get_share_memory(same_name)
+
+        assert shm_0.name == shm_1.name
+        assert shm_0.buf == shm_1.buf
+
+    def test_get_shm_in_different_process(self):
+        """"""""""""
+        random_bytes_content = random.randbytes(1920 * 1080 * 3)
+
+        #  
+        shm_in_main, lock_main = get_share_memory('same_name')
+
+        # 
+        dump_bytes_in_process(shm_name='same_name', random_bytes=random_bytes_content)
+
+        assert bytes(shm_in_main.buf) == random_bytes_content
+
+
+if __name__ == '__main__':
+    unittest.main()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+import unittest
+
+from shared_memory_toolkit.sync import get_shm_lock
+
+
+class MyTestCase(unittest.TestCase):
+    def test_get_shm_lock_without_initial(self):
+        """"""""""""
+
+        lock_0 = get_shm_lock('test_memory_0')
+        lock_1 = get_shm_lock('test_memory_1')
+
+        assert lock_0 == lock_1
+
+
+if __name__ == '__main__':
+    unittest.main()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+import time
+import unittest
+
+from multiprocessing import get_start_method, Process
+from concurrent.futures import ProcessPoolExecutor
+from shared_memory_toolkit.sync import initial_sync_in_fork, get_shm_lock
+
+
+def process_state(*args):
+    from shared_memory_toolkit.sync import _lock, _lock_list, _name_index_mapper
+
+    return _lock._id, _name_index_mapper._id, _lock_list._id
+
+
+def process_id(n: str = 'name'):
+    lock = get_shm_lock(n)
+    return lock._id
+
+
+@unittest.skipUnless(condition=get_start_method() == 'fork', reason='fork')
+class SyncForkTestCase(unittest.TestCase):
+
+    def test_initial_sync_in_fork(self):
+        syncs = initial_sync_in_fork()
+
+        with ProcessPoolExecutor(4) as pool:
+            sub_results = [pool.submit(process_state).result() for _ in range(10)]
+
+        with self.subTest('same lock id'):
+            for sub_result in sub_results:
+                self.assertEquals(sub_result[0], syncs[0]._id)
+
+        with self.subTest('same mapper id'):
+            for sub_result in sub_results:
+                self.assertEquals(sub_result[1], syncs[1]._id)
+
+        with self.subTest('same lock_list id'):
+            for sub_result in sub_results:
+                self.assertEquals(sub_result[2], syncs[2]._id)
+
+    def test_get_shm_lock_same_lock(self):
+        """"""""""""
+        initial_sync_in_fork()
+
+        lock_0 = get_shm_lock('uuid')
+        lock_1 = get_shm_lock('uuid')
+
+        assert lock_0._id == lock_1._id
+
+    def test_get_shm_lock_between_process(self):
+        initial_sync_in_fork()
+
+        with ProcessPoolExecutor(4) as pool:
+            futures = [pool.submit(process_id) for _ in range(10)]
+            ids = [_.result() for _ in futures]
+
+        [self.assertEquals(_, ids[0]) for _ in ids]
+
+
+if __name__ == '__main__':
+    unittest.main()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+import time
+import unittest
+from concurrent.futures import ProcessPoolExecutor
+from multiprocessing import get_start_method, Process
+
+from shared_memory_toolkit.sync import initial_sync_in_spawn, get_shm_lock, synchronization_setter
+
+
+def process_state(*args):
+    synchronization_setter(*args)
+
+    from shared_memory_toolkit.sync import _lock, _lock_list, _name_index_mapper
+
+    return _lock._id, _name_index_mapper._id, _lock_list._id
+
+
+def process_id(n: str = 'name'):
+    lock = get_shm_lock(n)
+    return lock._id
+
+
+@unittest.skipUnless(condition=get_start_method() == 'spawn', reason='spawn')
+class SyncSpawnTestCase(unittest.TestCase):
+    def test_initial_sync_in_spawn(self):
+        syncs = initial_sync_in_spawn()
+
+        with ProcessPoolExecutor(4) as pool:
+            sub_results = [pool.submit(process_state, *syncs).result() for _ in range(10)]
+
+        with self.subTest('same lock id'):
+            for sub_result in sub_results:
+                self.assertEquals(sub_result[0], syncs[0]._id)
+
+        with self.subTest('same mapper id'):
+            for sub_result in sub_results:
+                self.assertEquals(sub_result[1], syncs[1]._id)
+
+        with self.subTest('same lock_list id'):
+            for sub_result in sub_results:
+                self.assertEquals(sub_result[2], syncs[2]._id)
+
+    def test_get_shm_lock_same_lock(self):
+        """"""""""""
+        sync = initial_sync_in_spawn()
+        synchronization_setter(*sync)
+
+        lock_0 = get_shm_lock('uuid')
+        lock_1 = get_shm_lock('uuid')
+
+        assert lock_0._id == lock_1._id
+
+    def test_get_shm_lock_between_process(self):
+        _, d, l = initial_sync_in_spawn()
+
+        from shared_memory_toolkit.sync import synchronization_setter
+        with ProcessPoolExecutor(4, initializer=synchronization_setter, initargs=(_, d, l)) as pool:
+            futures = [pool.submit(process_id) for _ in range(10)]
+            ids = [_.result() for _ in futures]
+
+        [self.assertEquals(_, ids[0]) for _ in ids]
+
+
+if __name__ == '__main__':
+    unittest.main()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+# -*- coding: utf-8 -*-
+""""""Example Google style docstrings.
+
+Example:
+    Examples can be given using either the ``Example`` or ``Examples``
+    sections. Sections support any reStructuredText formatting, including
+    literal blocks::
+
+        $ python example_google.py
+
+Attributes:
+    module_level_variable1 (int): Module level variables may be documented in
+        either the ``Attributes`` section of the module docstring, or in an
+        inline docstring immediately following the variable.
+
+Todo:
+    * For module TODOs
+    * You have to also use ``sphinx.ext.todo`` extension
+
+.. _Google Python Style Guide:
+   http://google.github.io/styleguide/pyguide.html
+
+"""""""
KO;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";"+import unittest
+
+
+class MyTestCase(unittest.TestCase):
+    def test_something(self):
+        pass
+
+if __name__ == '__main__':
+    unittest.main()"
KO;4;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";
OK;4;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";"+# -*- coding: utf-8 -*-
+""""""
+
+""""""
+
+from .core import initial_shared_memory_lock
+
+from .raw_image import dump_image_into_shared_memory, load_image_from_shared_memory"
KO;4;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";
OK;4;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";"+# -*- coding: utf-8 -*-
+""""""The core of shared memory toolkit.
+
+
+
+:
+
+    1. FIX_LENGTH::
+
+        : 1920 * 1080 * 3 == 6220800
+
+    2. _share_memory_cache_mapper::
+
+        SharedMemory
+
+    3. _share_memory_lock_mapper  _manager::
+
+        /
+
+
+""""""
+
+from multiprocessing.shared_memory import SharedMemory
+from multiprocessing import Manager, Lock
+from typing import Dict, Tuple, Optional, NoReturn
+
+FIX_LENGTH: int = 6220800
+""""""1920*1080*3""""""
+
+_share_memory_cache_mapper: Dict[str, SharedMemory] = {}
+""""""Dict[str, SharedMemory]: """"""
+
+_share_memory_lock_mapper: Optional[Dict[str, Lock]] = {}
+""""""Dict[str, Lock]: """"""
+
+_manager: Optional[Manager] = None
+""""""Managermemory""""""
+
+
+def initial_shared_memory_lock() -> NoReturn:
+    """"""
+
+    
+
+    
+    """"""
+    global _manager, _share_memory_lock_mapper
+    _manager = Manager()
+    _share_memory_lock_mapper = _manager.dict()
+
+
+def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
+    """"""
+
+    FIX_LENGTH
+
+    Note:
+        
+
+    Note:
+        initial_shared_memory_lock
+
+    Args:
+        shared_memory_name (str): 
+
+    Returns:
+        Tuple[SharedMemory, Lock]: 
+    """"""
+    # 
+    shared = _share_memory_cache_mapper.get(shared_memory_name)
+
+    # case: 
+    if shared is None:
+        # 
+        # case: 
+        try:
+            shared = SharedMemory(name=shared_memory_name, create=True, size=FIX_LENGTH)
+            assert len(shared.buf) == FIX_LENGTH
+        # case: 
+        except FileExistsError:
+            shared = SharedMemory(name=shared_memory_name, create=False)
+
+        _share_memory_cache_mapper[shared_memory_name] = shared
+
+    # case: FIX_LENGTH
+    if len(shared.buf) <= FIX_LENGTH:
+        shared.close()
+        shared.unlink()
+        shared = SharedMemory(name=shared_memory_name, create=True, size=FIX_LENGTH)
+
+        _share_memory_cache_mapper[shared_memory_name] = shared
+
+    # 
+    if shared_memory_name not in _share_memory_lock_mapper:
+        # manager
+        lock = _manager.Lock() if _manager is not None else Lock()
+        _share_memory_lock_mapper[shared_memory_name] = lock
+
+    return _share_memory_cache_mapper[shared_memory_name], _share_memory_lock_mapper[shared_memory_name]"
KO;4;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";
OK;4;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";"+# -*- coding: utf-8 -*-
+""""""numpy
+
+
+:
+
+    1. IMAGE_SHAPE
+
+:
+
+    1. dump_image_into_shared_memory::
+
+        Dump image
+
+    2. load_image_from_shared_memory::
+
+        Load image
+
+Note:
+
+    
+
+
+""""""
+import numpy
+
+from .core import _get_share_memory, FIX_LENGTH
+
+IMAGE_SHAPE = (1080, 1920, 3)
+""""""""""""
+
+
+def dump_image_into_shared_memory(shared_memory_name: str, image: numpy.ndarray) -> memoryview:
+    """"""dump
+
+    Args:
+        shared_memory_name (str): 
+        image (numpy.ndarray): numpy
+
+    Returns:
+        memoryview: .buf
+    """"""
+    shared_memory, lock = _get_share_memory(shared_memory_name)
+
+    with lock:
+        shared_memory.buf[:FIX_LENGTH] = image.tobytes()
+    return shared_memory.buf
+
+
+def load_image_from_shared_memory(shared_memory_name: str) -> numpy.ndarray:
+    """"""
+
+    Args:
+        shared_memory_name (str): 
+
+    Returns:
+        numpy.ndarray: numpy
+    """"""
+    shared_memory, lock = _get_share_memory(shared_memory_name)
+
+    with lock:
+        image = numpy.frombuffer(shared_memory.buf, dtype=numpy.uint8)[:FIX_LENGTH].reshape(IMAGE_SHAPE)
+    return image"
KO;5;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" import os
 import os.path as osp
 
 from PIL import Image
 from pycocotools.coco import COCO
 from pycocotools.cocoeval import COCOeval
@@ -10,6 +11,10 @@
 from utils.utils import get_classes, get_coco_label_map
 from utils.utils_map import Make_json, prep_metrics
 
 if __name__ == '__main__':
     #------------------------------------------------------------------------------------------------------------------#
     #   map_mode"
OK;5;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" import os
 import os.path as osp
 
+import tensorflow as tf
 from PIL import Image
 from pycocotools.coco import COCO
 from pycocotools.cocoeval import COCOeval
@@ -10,6 +11,10 @@
 from utils.utils import get_classes, get_coco_label_map
 from utils.utils_map import Make_json, prep_metrics
 
+gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
+for gpu in gpus:
+    tf.config.experimental.set_memory_growth(gpu, True)
+
 if __name__ == '__main__':
     #------------------------------------------------------------------------------------------------------------------#
     #   map_mode"
KO;5;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" 
 import cv2
 import numpy as np
 from PIL import Image
 
 from mask_rcnn import MASK_RCNN
 
 if __name__ == ""__main__"":
     mask_rcnn = MASK_RCNN()
     #----------------------------------------------------------------------------------------------------------#"
OK;5;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" 
 import cv2
 import numpy as np
+import tensorflow as tf
 from PIL import Image
 
 from mask_rcnn import MASK_RCNN
 
+gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
+for gpu in gpus:
+    tf.config.experimental.set_memory_growth(gpu, True)
+
 if __name__ == ""__main__"":
     mask_rcnn = MASK_RCNN()
     #----------------------------------------------------------------------------------------------------------#"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"jobs:
           - 14.x
   publish_sdk:
     name: Publish SDKs
-    runs-on: ${{ matrix.language == 'nodejs' && 'macos-latest' || 'ubuntu-latest' }}
     needs: publish_binary
     steps:
       - name: Checkout Repo"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"jobs:
           - 14.x
   publish_sdk:
     name: Publish SDKs
+    runs-on: 'ubuntu-latest'
     needs: publish_binary
     steps:
       - name: Checkout Repo"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"-VERSION         := 0.1.11
 
 PACK            := azure-justrun
 PROJECT         := github.com/pulumi/pulumi-${PACK}
@@ -88,8 +88,7 @@ gen_nodejs_sdk::
 build_nodejs_sdk:: gen_nodejs_sdk
 	cd sdk/nodejs/ && \
 		yarn install && \
-		yarn run tsc --version && \
-		yarn run tsc && \
 		cp -R scripts/ bin && \
 		cp ../../README.md ../../LICENSE package.json yarn.lock ./bin/ && \
 		sed -i.bak -e ""s/\$${VERSION}/$(VERSION)/g"" ./bin/package.json && \"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"+VERSION         := 0.1.12
 
 PACK            := azure-justrun
 PROJECT         := github.com/pulumi/pulumi-${PACK}
@@ -88,8 +88,7 @@ gen_nodejs_sdk::
 build_nodejs_sdk:: gen_nodejs_sdk
 	cd sdk/nodejs/ && \
 		yarn install && \
+		NODE_OPTIONS=--max-old-space-size=8192 yarn run tsc --diagnostics \
 		cp -R scripts/ bin && \
 		cp ../../README.md ../../LICENSE package.json yarn.lock ./bin/ && \
 		sed -i.bak -e ""s/\$${VERSION}/$(VERSION)/g"" ./bin/package.json && \"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"-*.pyc
-venv/"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"+*.pyc
+venv/"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"-""""""An Azure RM Python Pulumi program""""""
-
-import pulumi
-import pulumi_azure_justrun 
-
-
-webapp = pulumi_azure_justrun.Webapp(""mywebapp"", file_path=""./www"")
-
-pulumi.export(""url"",webapp.url)"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"+""""""An Azure RM Python Pulumi program""""""
+
+import pulumi
+import pulumi_azure_justrun 
+
+
+webapp = pulumi_azure_justrun.Webapp(""mywebapp"", file_path=""./www"")
+
+pulumi.export(""url"",webapp.url)"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"-pulumi>=3.0.0,<4.0.0
-pulumi-azure-native>=1.0.0,<2.0.0"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"+pulumi>=3.0.0,<4.0.0
+pulumi-azure-native>=1.0.0,<2.0.0"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""name"": ""@pulumi/azure-justrun"",
-  ""version"": ""0.1.11"",
   ""devDependencies"": {
     ""@types/node"": ""^17.0.40"",
     ""@vercel/ncc"": ""^0.28.6"","
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""name"": ""@pulumi/azure-justrun"",
+  ""version"": ""0.1.12"",
   ""devDependencies"": {
     ""@types/node"": ""^17.0.40"",
     ""@vercel/ncc"": ""^0.28.6"","
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""azure-justrun"",
-    ""version"": ""v0.1.11"",
     ""types"": {
         ""azure-justrun:index:PublicAccess"":{
             ""type"": ""string"","
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""azure-justrun"",
+    ""version"": ""v0.1.12"",
     ""types"": {
         ""azure-justrun:index:PublicAccess"":{
             ""type"": ""string"","
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
-  ""version"": ""0.1.11""
 }"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
+  ""version"": ""0.1.12""
 }"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
-  ""version"": ""0.1.11""
 }"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
+  ""version"": ""0.1.12""
 }"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""@pulumi/azure-justrun"",
-    ""version"": ""0.1.11"",
     ""scripts"": {
         ""build"": ""tsc"",
-        ""install"": ""node scripts/install-pulumi-plugin.js resource azure-justrun 0.1.11""
     },
     ""dependencies"": {
         ""@pulumi/azure-native"": ""^1.0.0"",
@@ -14,6 +14,6 @@
     },
     ""pulumi"": {
         ""resource"": true,
-        ""version"": ""0.1.11""
     }
 }"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""@pulumi/azure-justrun"",
+    ""version"": ""0.1.12"",
     ""scripts"": {
         ""build"": ""tsc"",
+        ""install"": ""node scripts/install-pulumi-plugin.js resource azure-justrun 0.1.12""
     },
     ""dependencies"": {
         ""@pulumi/azure-native"": ""^1.0.0"",
@@ -14,6 +14,6 @@
     },
     ""pulumi"": {
         ""resource"": true,
+        ""version"": ""0.1.12""
     }
 }"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"-# Pulumi Component Provider Boilerplate (TypeScript)
 
-This repo is a boilerplate showing how to create a Pulumi component provider written in TypeScript. You can search-replace `xyz` with the name of your desired provider as a starting point for creating a component provider for your component resources.
 
-## Background
-This repository is part of the [guide for authoring and publishing a Pulumi Package](https://www.pulumi.com/docs/guides/pulumi-packages/how-to-author).
-
-Learn about the concepts behind [Pulumi Packages](https://www.pulumi.com/docs/guides/pulumi-packages/#pulumi-packages) and, more specifically, [Pulumi Components](https://www.pulumi.com/docs/intro/concepts/resources/components/)
-
-## Sample xyz Component Provider
-
-An example `StaticPage` [component resource](https://www.pulumi.com/docs/intro/concepts/resources/#components) is available in `provider/cmd/pulumi-resource-xyz/staticPage.ts`. This component creates a static web page hosted in an AWS S3 Bucket. There is nothing special about `StaticPage` -- it is a typical component resource written in TypeScript.
-
-The component provider makes component resources available to other languages. The implementation is in `provider/cmd/pulumi-resource-xyz/provider.ts`. Each component resource in the provider must have an implementation in the `construct` method to create an instance of the requested component resource and return its `URN` and state (outputs). There is an initial implementation that demonstrates an implementation of `construct` for the example `StaticPage` component.
-
-A code generator is available which generates SDKs in TypeScript, Python, Go and .NET which are also checked in to the `sdk` folder. The SDKs are generated from a schema in `schema.json`. This file should be kept aligned with the component resources supported by the component provider implementation.
-
-An example of using the `StaticPage` component in TypeScript is in `examples/simple`.
-
-Note that the provider plugin (`pulumi-resource-xyz`) must be on your `PATH` to be used by Pulumi deployments. In this case, `pulumi-resource-xyz` is a platform-specific binary that includes its Node.js dependency along with the provider code, created using [pkg](https://github.com/vercel/pkg). By default, running `make install` will create the binary specific to your host environment.
-
-After running `make install`, `pulumi-resource-xyz` will be available in the `./bin` directory. You can add this to your path in bash with `export PATH=$PATH:$PWD/bin`.
-
-If creating a provider for distribution to other users, they will need the `pulumi-resource-xyz` directory on their `PATH`. See the Packaging section below for more on distributing the provider to users.
-
-## Prerequisites
-
-- Pulumi CLI
-- Node.js
-- Yarn
-- Go 1.17 (to regenerate the SDKs)
-- Python 3.6+ (to build the Python SDK)
-- .NET Core SDK (to build the .NET SDK)
-
-## Build and Test
-
-```bash
-# Build and install the provider
-make install_provider
-
-# Regenerate SDKs
-make generate
-
-# Ensure the pulumi-provider-xyz script is on PATH
-$ export PATH=$PATH:$PWD/bin
-
-# Test Node.js SDK
-$ make install_nodejs_sdk
-$ cd examples/simple
-$ yarn install
-$ yarn link @pulumi/xyz
-$ pulumi stack init test
-$ pulumi config set aws:region us-east-1
-$ pulumi up
-```
-
-## Naming
-
-The `xyz` provider's plugin must be named `pulumi-resource-xyz` (in the format `pulumi-resource-<provider>`).
-
-While the provider plugin must follow this naming convention, the SDK package naming can be customized. TODO explain.
-
-## Packaging
-
-The provider plugin can be packaged into a tarball and hosted at a custom server URL to make it easier to distribute to users.
-
-Currently, five tarball files are necessary for Linux, macOS, and Windows (`pulumi-resource-xyz-v0.0.1-linux-amd64.tar.gz`, `pulumi-resource-xyz-v0.0.1-linux-arm64.tar.gz` `pulumi-resource-xyz-v0.0.1-darwin-amd64.tar.gz`, `pulumi-resource-xyz-v0.0.1-darwin-arm64.tar.gz`, `pulumi-resource-xyz-v0.0.1-windows-amd64.tar.gz`) each containing the same files: the platform-specific binary `pulumi-resource-xyz`, README and LICENSE. The fill set of binaries can be automatically generated using the command `make dist`.
-
-TODO explain custom server hosting in more detail.
-
-## Configuring CI and releases
-
-1. Follow the instructions laid out in the [deployment templates](./deployment-templates/README-DEPLOYMENT.md).
-
-
-## Example component
-
-Let's look at the example `StaticPage` component resource in more detail.
-
-### Schema
-
-The example `StaticPage` component resource is defined in `schema.json`:
-
-```json
-""resources"": {
-    ""xyz:index:StaticPage"": {
-        ""isComponent"": true,
-        ""inputProperties"": {
-            ""indexContent"": {
-                ""type"": ""string"",
-                ""description"": ""The HTML content for index.html.""
-            }
-        },
-        ""requiredInputs"": [
-            ""indexContent""
-        ],
-        ""properties"": {
-            ""bucket"": {
-                ""$ref"": ""/aws/v3.30.0/schema.json#/resources/aws:s3%2Fbucket:Bucket"",
-                ""description"": ""The bucket resource.""
-            },
-            ""websiteUrl"": {
-                ""type"": ""string"",
-                ""description"": ""The website URL.""
-            }
-        },
-        ""required"": [
-            ""bucket"",
-            ""websiteUrl""
-        ]
-    }
-}
-```
-
-The component resource's type token is `xyz:index:StaticPage` in the format of `<package>:<module>:<type>`. In this case, it's in the `xyz` package and `index` module. This is the same type token passed inside the implementation of `StaticPage` in `provider/cmd/pulumi-resource-xyz/staticPage.ts`, and also the same token referenced in `construct` in `provider/cmd/pulumi-resource-xyz/provider.ts`.
-
-This component has a required `indexContent` input property typed as `string`, and two required output properties: `bucket` and `websiteUrl`. Note that `bucket` is typed as the `aws:s3/bucket:Bucket` resource from the `aws` provider (in the schema the `/` is escaped as `%2F`).
-
-Since this component returns a type from the `aws` provider, each SDK must reference the associated Pulumi `aws` SDK for the language. For the .NET, Node.js, and Python SDKs, dependencies are specified in the `language` section of the schema:
-
-```json
-""language"": {
-    ""csharp"": {
-        ""packageReferences"": {
-            ""Pulumi"": ""2.*"",
-            ""Pulumi.Aws"": ""3.*""
-        }
-    },
-    ""nodejs"": {
-        ""dependencies"": {
-            ""@pulumi/aws"": ""^3.30.0""
-        },
-        ""devDependencies"": {
-            ""typescript"": ""^3.7.0""
-        }
-    },
-    ""python"": {
-        ""requires"": {
-            ""pulumi"": "">=2.21.2,<3.0.0"",
-            ""pulumi-aws"": "">=3.30.0,<4.0.0""
-        }
-    }
-}
-```
-
-For the Go SDK, dependencies are specified in the `sdk/go.mod` file.
-
-### Implementation
-
-The implementation of this component is in `provider/cmd/pulumi-resource-xyz/staticPage.ts` and the structure of the component's inputs and outputs aligns with what is defined in `schema.json`:
-
-```typescript
-export interface StaticPageArgs {
-    indexContent: pulumi.Input<string>;
-}
-
-export class StaticPage extends pulumi.ComponentResource {
-    public readonly bucket: aws.s3.Bucket;
-    public readonly websiteUrl: pulumi.Output<string>;
-
-    constructor(name: string, args: StaticPageArgs, opts?: pulumi.ComponentResourceOptions) {
-        super(""xyz:index:StaticPage"", name, args, opts);
-
-        ...
-    }
-}
-```
-
-The provider makes this component resource available in the `construct` method in `provider/cmd/pulumi-resource-xyz/provider.ts`. When `construct` is called and the `type` argument is `xyz:index:StaticPage`, we create an instance of the `StaticPage` component resource and return its `URN` and outputs as its state.
-
-
-```typescript
-async function constructStaticPage(name: string, inputs: pulumi.Inputs,
-    options: pulumi.ComponentResourceOptions): Promise<provider.ConstructResult> {
-
-    // Create the component resource.
-    const staticPage = new StaticPage(name, inputs as StaticPageArgs, options);
-
-    // Return the component resource's URN and outputs as its state.
-    return {
-        urn: staticPage.urn,
-        state: {
-            bucket: staticPage.bucket,
-            websiteUrl: staticPage.websiteUrl,
-        },
-    };
-}
-```
\ No newline at end of file"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"+# Pulumi Azure JustRun
 
+Azure-JustRun allows you to deploy a static site to Azure in just a few lines of code.
 
+## Contributing
+When contributing to this package, make sure to bump the version in the schema.json as well as the makefile when preparing a new release, and then regenerate the SDKs before pushing. 
+Add tags of the form v0.0.0 AND sdk/v0.0.0
\ No newline at end of file"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
-  ""version"": ""0.1.11""
 }"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
+  ""version"": ""0.1.12""
 }"
KO;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" from subprocess import check_call
 
 
-VERSION = ""0.1.11""
-PLUGIN_VERSION = ""0.1.11""
 
 class InstallPluginCommand(install):
     def run(self):"
OK;8;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" from subprocess import check_call
 
 
+VERSION = ""0.1.12""
+PLUGIN_VERSION = ""0.1.12""
 
 class InstallPluginCommand(install):
     def run(self):"
KO;13;fmathiou;Replay-SLDA;f371be6bf9d9ed294740ba47541be1eec3c93976;Delete memory.py;"-from torch.utils.data import Dataset
-from torchvision import transforms
-import torch
-import random
-
-
-class Memory(Dataset):
-    """"""Memory buffer used for rehearsal.
-
-    Attributes:
-        max_samples (int): Maximum allowed number of samples to be stored.
-        reservoir (list): Contains stored instances in the form [instance, label].
-        seen_samples (int): Number of instances ecnountered so far.
-        transform (Tansform): Transformation operation when replaying data.
-    """"""
-    
-    def __init__(self, max_samples=200):
-        super(Memory, self).__init__()
-        self.max_samples = max_samples
-        self.reservoir =[]
-        self.seen_samples = 0
-        self.transform = transforms.Compose([transforms.Resize(256), 
-                                             transforms.CenterCrop(224),
-                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],
-                                                                  std=[0.229, 0.224, 0.225])])
-
-    def __len__ (self):
-        return len(self.reservoir)
-        
-    def __getitem__(self, index):
-        sample = self.reservoir[index]
-        return sample
-        
-    def reservoir_sampling(self, samples, labels):
-        """"""Perform reservoir sampling.""""""
-        nr_of_smaples = labels.shape[0]
-        for i in range(nr_of_smaples):
-            if(self.seen_samples < self.max_samples):
-                self.seen_samples += 1
-                self.reservoir.append([samples[i], labels[i]])
-            else:
-                self.seen_samples += 1   
-                random_index = random.randrange(self.seen_samples)
-                if(random_index < self.max_samples):
-                    self.reservoir[random_index] = [samples[i], labels[i]]
-
-    def random_replay(self, batch_size):
-        """"""Draw instances from the reservoir uniformly at random.
-
-        Args:
-            batch_size (int): Numbber of instances to sample.
-
-        Returns:
-            tuple: Samples and corresponding labels.
-        """"""
-        if(len(self.reservoir)) >= batch_size:
-            random_indices = random.sample(range(len(self.reservoir)), batch_size)
-            batch = list(map(self.__getitem__, random_indices))
-            samples = list(map(lambda x: x[0], batch))
-            samples = torch.stack(samples)
-            samples = self.transform(samples)
-            labels = list(map(lambda x: x[1], batch)) 
-            labels = torch.stack(labels)
-            
-        else:
-            samples = float('NaN')
-            labels = float('NaN')
-            
-        return samples, labels
\ No newline at end of file"
OK;13;fmathiou;Replay-SLDA;f371be6bf9d9ed294740ba47541be1eec3c93976;Delete memory.py;\ No newline at end of file
KO;13;fmathiou;Replay-SLDA;1d1898771f816f796a6c071f6592f21aa308fb72;Delete memory.py;"-from torch.utils.data import Dataset
-from torchvision import transforms
-import torch
-import random
-
-
-class Memory(Dataset):
-    """"""Memory buffer used for rehearsal.
-
-    Attributes:
-        max_samples (int): Maximum allowed number of samples to be stored.
-        reservoir (list): Contains stored instances in the form [instance, label].
-        seen_samples (int): Number of instances ecnountered so far.
-        transform (Tansform): Transformation operation when replaying data.
-    """"""
-    
-    def __init__(self, max_samples=200):
-        super(Memory, self).__init__()
-        self.max_samples = max_samples
-        self.reservoir =[]
-        self.seen_samples = 0
-        self.transform = transforms.Compose([transforms.Resize(256), 
-                                             transforms.CenterCrop(224),
-                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],
-                                                                  std=[0.229, 0.224, 0.225])])
-
-    def __len__ (self):
-        return len(self.reservoir)
-        
-    def __getitem__(self, index):
-        sample = self.reservoir[index]
-        return sample
-        
-    def reservoir_sampling(self, samples, labels):
-        """"""Perform reservoir sampling.""""""
-        nr_of_smaples = labels.shape[0]
-        for i in range(nr_of_smaples):
-            if(self.seen_samples < self.max_samples):
-                self.seen_samples += 1
-                self.reservoir.append([samples[i], labels[i]])
-            else:
-                self.seen_samples += 1   
-                random_index = random.randrange(self.seen_samples)
-                if(random_index < self.max_samples):
-                    self.reservoir[random_index] = [samples[i], labels[i]]
-
-    def random_replay(self, batch_size):
-        """"""Draw instances from the reservoir uniformly at random.
-
-        Args:
-            batch_size (int): Numbber of instances to sample.
-
-        Returns:
-            tuple: Samples and corresponding labels.
-        """"""
-        if(len(self.reservoir)) >= batch_size:
-            random_indices = random.sample(range(len(self.reservoir)), batch_size)
-            batch = list(map(self.__getitem__, random_indices))
-            samples = list(map(lambda x: x[0], batch))
-            samples = torch.stack(samples)
-            samples = self.transform(samples)
-            labels = list(map(lambda x: x[1], batch)) 
-            labels = torch.stack(labels)
-            
-        else:
-            samples = float('NaN')
-            labels = float('NaN')
-            
-        return samples, labels
\ No newline at end of file"
OK;13;fmathiou;Replay-SLDA;1d1898771f816f796a6c071f6592f21aa308fb72;Delete memory.py;\ No newline at end of file
KO;26;appbox;shadowsocksr;9bb52acaf54f6e6ce32360474b286bddf7e4edbf;"Merge pull request #25 from mengskysama/patch-4

memory leak";"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
OK;26;appbox;shadowsocksr;9bb52acaf54f6e6ce32360474b286bddf7e4edbf;"Merge pull request #25 from mengskysama/patch-4

memory leak";"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
+        self._dns_cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
KO;26;appbox;shadowsocksr;b059b9ad8562df90251419685edeada96ae6560c;memory leak;"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
OK;26;appbox;shadowsocksr;b059b9ad8562df90251419685edeada96ae6560c;memory leak;"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
+        self._dns_cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
KO;27;WoLeo-Z;tgmsbot;7550e4fc4b33b1792b3c6951cb1b50d8a6f49457;free memory and fix BadRequest;"def dist_cards_btn_click(update, context):
     data = update.callback_query.data
     user = update.callback_query.from_user
     omsg = update.callback_query.message
     try:
         (_, rphash) = data.split(' ')
-        red_packets = context.chat_data.setdefault('red_packets', dict())
         rp = red_packets.get(str(rphash), None)
         if rp:
             (cards, damount) = [int(a) for a in rp]
@@ -268,7 +268,7 @@ def __floating(value):
                 return randrange(5000,15000)/10000 * value
             got_cards = int(__floating(cards/damount))
             got_cards = got_cards if got_cards <= cards else cards
-            got_cards = 1 if randrange(0,10000)/10000 < 0.2 and got_cards == 0 else got_cards
             got_cards = got_cards if damount != 1 else cards
             rp[0] -= got_cards
             rp[1] -= 1
@@ -289,4 +289,14 @@ def __floating(value):
             update.callback_query.answer()
         except Exception:
             pass
-        omsg.edit_text(omsg.text_markdown + """", parse_mode=""Markdown"", reply_markup=None)"
OK;27;WoLeo-Z;tgmsbot;7550e4fc4b33b1792b3c6951cb1b50d8a6f49457;free memory and fix BadRequest;"def dist_cards_btn_click(update, context):
     data = update.callback_query.data
     user = update.callback_query.from_user
     omsg = update.callback_query.message
+    red_packets = context.chat_data.setdefault('red_packets', dict())
     try:
         (_, rphash) = data.split(' ')
         rp = red_packets.get(str(rphash), None)
         if rp:
             (cards, damount) = [int(a) for a in rp]
@@ -268,7 +268,7 @@ def __floating(value):
                 return randrange(5000,15000)/10000 * value
             got_cards = int(__floating(cards/damount))
             got_cards = got_cards if got_cards <= cards else cards
+            got_cards = 1 if got_cards == 0 and randrange(0,10000)/10000 < 0.2 else got_cards
             got_cards = got_cards if damount != 1 else cards
             rp[0] -= got_cards
             rp[1] -= 1
@@ -289,4 +289,14 @@ def __floating(value):
             update.callback_query.answer()
         except Exception:
             pass
+        def free_mem(job_context):
+            try:
+                red_packets.pop(rphash)
+            except KeyError:
+                pass
+        if rphash:
+            rp = red_packets.get(rphash, [0, 0])
+            if rp[0] != -1:
+                rp[0] = -1
+                omsg.edit_text(omsg.text_markdown + """", parse_mode=""Markdown"", reply_markup=None)
+                context.job_queue.run_once(free_mem, 5)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def my_load_checkpoint(model, filename, map_location=None, strict=False, logger=
     elif isinstance(checkpoint, dict) and 'model' in checkpoint:
         state_dict = checkpoint['model']  # for classification weights
     else:
-        raise RuntimeError(
-            'No state_dict found in checkpoint file {}'.format(filename))
     # strip prefix of state_dict
     if list(state_dict.keys())[0].startswith('module.'):
         state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def my_load_checkpoint(model, filename, map_location=None, strict=False, logger=
     elif isinstance(checkpoint, dict) and 'model' in checkpoint:
         state_dict = checkpoint['model']  # for classification weights
     else:
+        state_dict = checkpoint #  fix ""No state_dict found in checkpoint file""
+        # raise RuntimeError(
+        #     'No state_dict found in checkpoint file {}'.format(filename))
     # strip prefix of state_dict
     if list(state_dict.keys())[0].startswith('module.'):
         state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, dim=768):
     def forward(self, x, H, W):
         B, N, C = x.shape
         n = N // 21
-        x1 = x[:, 0:16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2)
-        x2 = x[:, 16 * n:20 * n, :].transpose(1, 2).view(B, C, H, W)
-        x3 = x[:, 20 * n:, :].transpose(1, 2).view(B, C, H // 2, W // 2)
         x1 = self.dwconv(x1).flatten(2).transpose(1, 2)
         x2 = self.dwconv(x2).flatten(2).transpose(1, 2)
         x3 = self.dwconv(x3).flatten(2).transpose(1, 2)
@@ -143,7 +143,8 @@ def __init__(self, dim, num_heads=6, n_points=4, norm_layer=partial(nn.LayerNorm
         if extra_extractor:
             self.extra_extractors = nn.Sequential(*[
                 Extractor(dim=dim, num_heads=num_heads, n_points=n_points, norm_layer=norm_layer,
-                          with_cffn=with_cffn, cffn_ratio=cffn_ratio, deform_ratio=deform_ratio)
                 for _ in range(2)
             ])
         else:
@@ -200,7 +201,7 @@ def __init__(self, inplanes=64, embed_dim=384):
         self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc2 = nn.Conv2d(2 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc3 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
-        self.fc4 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0,  bias=True)
 
     def forward(self, x):
         c1 = self.stem(x)"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, dim=768):
     def forward(self, x, H, W):
         B, N, C = x.shape
         n = N // 21
+        x1 = x[:, 0:16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2).contiguous()
+        x2 = x[:, 16 * n:20 * n, :].transpose(1, 2).view(B, C, H, W).contiguous()
+        x3 = x[:, 20 * n:, :].transpose(1, 2).view(B, C, H // 2, W // 2).contiguous()
         x1 = self.dwconv(x1).flatten(2).transpose(1, 2)
         x2 = self.dwconv(x2).flatten(2).transpose(1, 2)
         x3 = self.dwconv(x3).flatten(2).transpose(1, 2)
@@ -143,7 +143,8 @@ def __init__(self, dim, num_heads=6, n_points=4, norm_layer=partial(nn.LayerNorm
         if extra_extractor:
             self.extra_extractors = nn.Sequential(*[
                 Extractor(dim=dim, num_heads=num_heads, n_points=n_points, norm_layer=norm_layer,
+                          with_cffn=with_cffn, cffn_ratio=cffn_ratio, deform_ratio=deform_ratio,
+                          drop=drop, drop_path=drop_path)
                 for _ in range(2)
             ])
         else:
@@ -200,7 +201,7 @@ def __init__(self, inplanes=64, embed_dim=384):
         self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc2 = nn.Conv2d(2 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc3 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
+        self.fc4 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
 
     def forward(self, x):
         c1 = self.stem(x)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" 
 import torch
 import torch.nn.functional as F
 from mmcv.runner import load_checkpoint
 from mmdet.utils import get_root_logger
 from timm.models.layers import DropPath
@@ -218,9 +219,10 @@ def forward(self, hidden_states, input_tensor):
 
 class BertLayer(nn.Module):
     def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=12,
-                 drop_path_ratio=0.1, windowed=False, window_size=14):
 
         super(BertLayer, self).__init__()
         self.attention = BertAttention(hidden_size, num_attention_heads,
                                        drop_path_ratio, windowed, window_size)
 
@@ -230,10 +232,19 @@ def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=
                                  drop_path_ratio=drop_path_ratio)
 
     def forward(self, hidden_states, H, W):
-        attention_output = self.attention(hidden_states, H, W)
-        intermediate_output = self.intermediate(attention_output)
-        layer_output = self.output(intermediate_output, attention_output)
-        return layer_output
 
 
 class VisualPatchEmbedding(nn.Module):
@@ -299,7 +310,8 @@ def forward(self, x):
 class UnifiedBertEncoder(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,
                  num_heads=12, mlp_ratio=4., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
-                 embed_layer=VisualPatchEmbedding, window_attn=False, window_size=14, pretrained=None):
 
         super(UnifiedBertEncoder, self).__init__()
         self.embed_dim = embed_dim
@@ -316,7 +328,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth
             layers.append(
                 BertLayer(hidden_size=embed_dim, intermediate_size=int(embed_dim * mlp_ratio),
                           num_attention_heads=num_heads, drop_path_ratio=drop_path_rate,
-                          windowed=window_attn[i], window_size=window_size[i])
             )
 
         self.layers = nn.ModuleList(layers)"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" 
 import torch
 import torch.nn.functional as F
+import torch.utils.checkpoint as cp
 from mmcv.runner import load_checkpoint
 from mmdet.utils import get_root_logger
 from timm.models.layers import DropPath
@@ -218,9 +219,10 @@ def forward(self, hidden_states, input_tensor):
 
 class BertLayer(nn.Module):
     def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=12,
+                 drop_path_ratio=0.1, windowed=False, window_size=14, with_cp=False):
 
         super(BertLayer, self).__init__()
+        self.with_cp = with_cp
         self.attention = BertAttention(hidden_size, num_attention_heads,
                                        drop_path_ratio, windowed, window_size)
 
@@ -230,10 +232,19 @@ def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=
                                  drop_path_ratio=drop_path_ratio)
 
     def forward(self, hidden_states, H, W):
+        
+        def _inner_forward(hidden_states):
+            attention_output = self.attention(hidden_states, H, W)
+            intermediate_output = self.intermediate(attention_output)
+            layer_output = self.output(intermediate_output, attention_output)
+            return layer_output
+
+        if self.with_cp and hidden_states.requires_grad:
+            x = cp.checkpoint(_inner_forward, hidden_states)
+        else:
+            x = _inner_forward(hidden_states)
+
+        return x
 
 
 class VisualPatchEmbedding(nn.Module):
@@ -299,7 +310,8 @@ def forward(self, x):
 class UnifiedBertEncoder(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,
                  num_heads=12, mlp_ratio=4., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
+                 embed_layer=VisualPatchEmbedding, window_attn=False, window_size=14,
+                 with_cp=False, pretrained=None):
 
         super(UnifiedBertEncoder, self).__init__()
         self.embed_dim = embed_dim
@@ -316,7 +328,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth
             layers.append(
                 BertLayer(hidden_size=embed_dim, intermediate_size=int(embed_dim * mlp_ratio),
                           num_attention_heads=num_heads, drop_path_ratio=drop_path_rate,
+                          windowed=window_attn[i], window_size=window_size[i], with_cp=with_cp)
             )
 
         self.layers = nn.ModuleList(layers)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.runner import BaseModule
 from mmcv_custom import my_load_checkpoint as load_checkpoint
 from mmdet.utils import get_root_logger
@@ -208,10 +209,11 @@ def forward(self, x, H, W):
 
 
 class Block(nn.Module):
-    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
                  attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                  windowed=False, window_size=14, pad_mode='constant', layer_scale=False):
         super().__init__()
         self.norm1 = norm_layer(dim)
         if windowed:
             self.attn = WindowedAttention(dim, num_heads=num_heads,
@@ -233,12 +235,21 @@ def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
             self.gamma2 = nn.Parameter(torch.ones((dim)), requires_grad=True)
 
     def forward(self, x, H, W):
-        if self.layer_scale:
-            x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), H, W))
-            x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
         else:
-            x = x + self.drop_path(self.attn(self.norm1(x), H, W))
-            x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
@@ -254,7 +265,7 @@ class TIMMVisionTransformer(BaseModule):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768,
                  depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,
                  drop_path_rate=0., layer_scale=True, embed_layer=PatchEmbed, norm_layer=partial(nn.LayerNorm, eps=1e-6),
-                 act_layer=nn.GELU, window_attn=False, window_size=14, pretrained=None):
         """"""
         Args:
             img_size (int, tuple): input image size
@@ -272,6 +283,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
             embed_layer (nn.Module): patch embedding layer
             norm_layer: (nn.Module): normalization layer
             pretrained: (str): pretrained path
         """"""
         super().__init__()
         self.num_classes = num_classes
@@ -307,7 +319,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
                   qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
                   drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer,
                   windowed=window_attn[i], window_size=window_size[i],
-                  layer_scale=layer_scale) for i in range(depth)
         ])
 
         self.init_weights(pretrained)"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" import torch
 import torch.nn as nn
 import torch.nn.functional as F
+import torch.utils.checkpoint as cp
 from mmcv.runner import BaseModule
 from mmcv_custom import my_load_checkpoint as load_checkpoint
 from mmdet.utils import get_root_logger
@@ -208,10 +209,11 @@ def forward(self, x, H, W):
 
 
 class Block(nn.Module):
+    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., with_cp=False,
                  attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                  windowed=False, window_size=14, pad_mode='constant', layer_scale=False):
         super().__init__()
+        self.with_cp = with_cp
         self.norm1 = norm_layer(dim)
         if windowed:
             self.attn = WindowedAttention(dim, num_heads=num_heads,
@@ -233,12 +235,21 @@ def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
             self.gamma2 = nn.Parameter(torch.ones((dim)), requires_grad=True)
 
     def forward(self, x, H, W):
+        
+        def _inner_forward(x):
+            if self.layer_scale:
+                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), H, W))
+                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
+            else:
+                x = x + self.drop_path(self.attn(self.norm1(x), H, W))
+                x = x + self.drop_path(self.mlp(self.norm2(x)))
+            return x
+
+        if self.with_cp and x.requires_grad:
+            x = cp.checkpoint(_inner_forward, x)
         else:
+            x = _inner_forward(x)
+        
         return x
 
 
@@ -254,7 +265,7 @@ class TIMMVisionTransformer(BaseModule):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768,
                  depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,
                  drop_path_rate=0., layer_scale=True, embed_layer=PatchEmbed, norm_layer=partial(nn.LayerNorm, eps=1e-6),
+                 act_layer=nn.GELU, window_attn=False, window_size=14, with_cp=False, pretrained=None):
         """"""
         Args:
             img_size (int, tuple): input image size
@@ -272,6 +283,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
             embed_layer (nn.Module): patch embedding layer
             norm_layer: (nn.Module): normalization layer
             pretrained: (str): pretrained path
+            with_cp: (bool): use checkpoint or not
         """"""
         super().__init__()
         self.num_classes = num_classes
@@ -307,7 +319,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
                   qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
                   drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer,
                   windowed=window_attn[i], window_size=window_size[i],
+                  layer_scale=layer_scale, with_cp=with_cp) for i in range(depth)
         ])
 
         self.init_weights(pretrained)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, conv_inplane=64, n_points=4, deform_num_he
         self.version = version
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
-        self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, conv_inplane=64, n_points=4, deform_num_he
         self.version = version
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.layers)
         self.pretrain_size = (pretrain_size, pretrain_size)
-        self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.layers)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
-        self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;" CWD = os.path.dirname(__file__)
 sys.path.append(os.path.join(CWD, '..'))
 from scan.modules import *
-from utils.net import get_all_ip
-from utils.base import multi_thread, multi_process, process_bar, save_res
 
 
 class Base:
@@ -43,8 +43,8 @@ class CameraScanner(Base):
     def __init__(self, in_file: str, out_file: str) -> None:
         super().__init__(in_file, out_file)
         self.scanner_name = 'camera scanner'
-        self.ip_list = []
         self.lock = Lock()
         self.total = 0
         self.found = 0
         self.done = 0
@@ -56,31 +56,28 @@ def __init__(self, in_file: str, out_file: str) -> None:
     def _get_ip(self):
         with open(self.in_file, 'r') as f:
             for line in f:
-                if line.strip():
-                    if not line.startswith('#'):
-                        if '-' in line or '/' in line:
-                            self.ip_list.extend(get_all_ip(line.strip()))
-                        else:
-                            self.ip_list.append(line.strip())
-        self.total = len(self.ip_list)
 
     def _step(self, *args, **kwargs):
         with self.lock:
             if kwargs['found']:
                 self.found += 1
             self.bar(self.total, self.done + 1, self.found, timer=True, start_time=self.start_time)
 
-    def scan(self, ip):
-        for mod in self.modules:
-            found = False
-            try:
-                res = mod(ip)
-                if res[0]:
-                    found = True
-                    save_res(self.out_file, [ip] + res[1:])
-            except Exception as e: pass  # print(e)
-            finally: self._step(found=found)
-        with self.lock: self.done += 1
 
 
     def __call__(self, args):
@@ -102,4 +99,5 @@ def __call__(self, args):
             if args.cve_2021_36260: self.modules.append(cve_2021_36260)
             if args.cve_2020_25078: self.modules.append(cve_2020_25078)
             if args.cve_2021_33044: self.modules.append(cve_2021_33044)
         multi_thread(self.scan, self.ip_list, processes=args.th_num)"
OK;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;" CWD = os.path.dirname(__file__)
 sys.path.append(os.path.join(CWD, '..'))
 from scan.modules import *
+from utils.net import get_all_ip, get_ip_seg_len
+from utils.base import multi_thread, process_bar, save_res
 
 
 class Base:
@@ -43,8 +43,8 @@ class CameraScanner(Base):
     def __init__(self, in_file: str, out_file: str) -> None:
         super().__init__(in_file, out_file)
         self.scanner_name = 'camera scanner'
         self.lock = Lock()
+        self.ip_list = []
         self.total = 0
         self.found = 0
         self.done = 0
@@ -56,31 +56,28 @@ def __init__(self, in_file: str, out_file: str) -> None:
     def _get_ip(self):
         with open(self.in_file, 'r') as f:
             for line in f:
+                if line.strip() and not line.startswith('#'):
+                    self.total += get_ip_seg_len(line.strip()) if '-' in line or '/' in line else 1
+                    self.ip_list.append(line.strip())
 
     def _step(self, *args, **kwargs):
         with self.lock:
             if kwargs['found']:
                 self.found += 1
             self.bar(self.total, self.done + 1, self.found, timer=True, start_time=self.start_time)
 
+    def scan(self, ip_term):
+        for ip in get_all_ip(ip_term):
+            for mod in self.modules:
+                found = False
+                try:
+                    res = mod(ip)
+                    if res[0]:
+                        found = True
+                        save_res(self.out_file, [ip] + res[1:])
+                except Exception as e: pass  # print(e)
+                finally: self._step(found=found)
+            with self.lock: self.done += 1
 
 
     def __call__(self, args):
@@ -102,4 +99,5 @@ def __call__(self, args):
             if args.cve_2021_36260: self.modules.append(cve_2021_36260)
             if args.cve_2020_25078: self.modules.append(cve_2020_25078)
             if args.cve_2021_33044: self.modules.append(cve_2021_33044)
+        
         multi_thread(self.scan, self.ip_list, processes=args.th_num)"
KO;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def wrapper(total, done, found=0, timer=False, start_time=0):
         _found = 'Found ' + output_formatter(found, color='red', bold=True) if found else ''
         count = f""{_done}/{_total} ({_percent}) {_found}""
 
-        print(f""\r{icon} {count}  {_time}"", end='')
     return wrapper
 
 "
OK;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def wrapper(total, done, found=0, timer=False, start_time=0):
         _found = 'Found ' + output_formatter(found, color='red', bold=True) if found else ''
         count = f""{_done}/{_total} ({_percent}) {_found}""
 
+        print(f""\r{icon} {count}  {_time:<55}"", end='')
     return wrapper
 
 "
KO;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def get_ip_segment(start: str, end: str) -> str:
     return IPy.IP(f""{start}-{end}"", make_net=True).strNormal()
 
 
 def get_all_ip(ip_seg: str) -> list:
-    return [i.strNormal() for i in IPy.IP(f""{ip_seg}"", make_net=True)]
 
 
 def get_user_agent(name='random'):"
OK;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def get_ip_segment(start: str, end: str) -> str:
     return IPy.IP(f""{start}-{end}"", make_net=True).strNormal()
 
 
+def get_ip_seg_len(ip_seg: str) -> int:
+    return IPy.IP(ip_seg, make_net=True).len()
+
+
 def get_all_ip(ip_seg: str) -> list:
+    return [i.strNormal() for i in IPy.IP(ip_seg, make_net=True)]
 
 
 def get_user_agent(name='random'):"
KO;2;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
-the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of current user and software\microsoft\windows\currentVersion\run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
OK;2;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
+the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of HKCU\Software\Microsoft\Windows\CurrentVersion\Run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
KO;2;CarbonCollective;fusion-dUQtools;9e8b10b081cfc7275998055e377496bf1da9a172;"Fix memory issue with IDS mapping (#90)

Fix memory issue with mapping";"def recursive_defaultdict():
 
 
 class IDSMapping:
-    # All fields in the core profile in a single dict
-    flat_fields: dict = {}
-    # All fields, in the core profile in a nested dict
-    fields: dict = defaultdict(recursive_defaultdict)
 
     def __init__(self, ids):
         self.dive(ids, [])
         self.fields = self.ddict_to_dict(self.fields)
 "
OK;2;CarbonCollective;fusion-dUQtools;9e8b10b081cfc7275998055e377496bf1da9a172;"Fix memory issue with IDS mapping (#90)

Fix memory issue with mapping";"def recursive_defaultdict():
 
 
 class IDSMapping:
 
     def __init__(self, ids):
+        # All fields in the core profile in a single dict
+        self.flat_fields: dict = {}
+        # All fields, in the core profile in a nested dict
+        self.fields: dict = defaultdict(recursive_defaultdict)
+
         self.dive(ids, [])
         self.fields = self.ddict_to_dict(self.fields)
 "
KO;2;dansanderson;mega65-welcome-guide;d39da1b335c04b9bf5afe8d71653a2d7f4fad035;Mention screen memory arrays in Recent Features;"Some of the new features that have been added since the factory-installed ROM wa
 - Single-letter BASIC variables are ""fast"" variables stored in fixed memory addresses `$FD00-$FEFF`.
 - The `PLAY` and `SOUND` commands have improved background playback and use of SID voices, so BASIC games can sensibly have both background music and sound effects.
 - Some disk commands can access files on the SD card directly (and not via a mounted D81 disk image) using the virtual device `U12`. `DIR U12` lists the files on the SD card. `DLOAD ""FILE.PRG"",U12` loads a `PRG` file.
 
 ## New BASIC commands
 "
OK;2;dansanderson;mega65-welcome-guide;d39da1b335c04b9bf5afe8d71653a2d7f4fad035;Mention screen memory arrays in Recent Features;"Some of the new features that have been added since the factory-installed ROM wa
 - Single-letter BASIC variables are ""fast"" variables stored in fixed memory addresses `$FD00-$FEFF`.
 - The `PLAY` and `SOUND` commands have improved background playback and use of SID voices, so BASIC games can sensibly have both background music and sound effects.
 - Some disk commands can access files on the SD card directly (and not via a mounted D81 disk image) using the virtual device `U12`. `DIR U12` lists the files on the SD card. `DLOAD ""FILE.PRG"",U12` loads a `PRG` file.
+- BASIC programs can access screen and color memory via special byte arrays `T@&(COLUMN, ROW)` and `C@&(COLUMN, ROW)`. Screen coordinates are intuitive in both 40-column and 80-column modes.
 
 ## New BASIC commands
 "
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
 *.parquet
\ No newline at end of file"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
+*.ipc
 *.parquet
\ No newline at end of file"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
-A smol simulator framework built on top of PyGame.
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
+A smol simulator framework built on top of [PyGame](https://www.pygame.org/docs/).
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
-from vi import Agent, BaseConfig, Simulation, Snapshot, dataclass
 
 
-@dataclass
-class MySnapshot(Snapshot):  #  inherit Snapshot to collect base metrics.
-    # We want to keep track of how many other agents were in our agent's radius,
-    # so we add an extra `in_radius` metric to our Snapshot!
-    in_radius: int
 
 
-class MyAgent(Agent):
-    def update(self):
         # If at least one agent is within our agent's radius, then we turn red!
-        if len(self.in_radius()) > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
-    def snapshot(self) -> MySnapshot:
-        return MySnapshot(
-            # Automatically fill-in all the Snapshot attributes such as agent, frame, x and y.
-            **super().snapshot().as_dict(),
-            # Then add our own metric: in_radius!
-            in_radius=len(self.in_radius()),
-        )
-
 
 print(
     # We're using a seed to collect the same data every time.
@@ -39,9 +32,7 @@ def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
-    # convert the output of the simulation into a Polars DataFrame
-    .to_polars()
-    .groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
+from vi import Agent, BaseConfig, Simulation
 
 
+class MyAgent(Agent):
+    def every_frame(self):
+        # As radius calculation is quite performance heavy,
+        # we only calculate it once per frame.
+        in_radius = len(self.in_radius())
 
+        # We want to keep track of how many other agents were in our agent's radius,
+        # so we add data to the `in_radius` column of our dataframe!
+        self.save_data(""in_radius"", in_radius)
 
         # If at least one agent is within our agent's radius, then we turn red!
+        if in_radius > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
 
 print(
     # We're using a seed to collect the same data every time.
@@ -39,9 +32,7 @@ def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
+    .snapshots.groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
-optional = true
 python-versions = "">=3.8""
 
-[[package]]
-name = ""pandas""
-version = ""1.4.2""
-description = ""Powerful data structures for data analysis, time series, and statistics""
-category = ""main""
-optional = true
-python-versions = "">=3.8""
-
-[package.dependencies]
-numpy = [
-    {version = "">=1.18.5"", markers = ""platform_machine != \""aarch64\"" and platform_machine != \""arm64\"" and python_version < \""3.10\""""},
-    {version = "">=1.19.2"", markers = ""platform_machine == \""aarch64\"" and python_version < \""3.10\""""},
-    {version = "">=1.20.0"", markers = ""platform_machine == \""arm64\"" and python_version < \""3.10\""""},
-    {version = "">=1.21.0"", markers = ""python_version >= \""3.10\""""},
-]
-python-dateutil = "">=2.8.1""
-pytz = "">=2020.1""
-
-[package.extras]
-test = [""hypothesis (>=5.5.3)"", ""pytest (>=6.0)"", ""pytest-xdist (>=1.31)""]
-
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
@@ -137,7 +116,7 @@ name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
-optional = true
 python-versions = "">=3.7""
 
 [package.dependencies]
@@ -179,33 +158,6 @@ toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
-[[package]]
-name = ""python-dateutil""
-version = ""2.8.2""
-description = ""Extensions to the standard Python datetime module""
-category = ""main""
-optional = true
-python-versions = ""!=3.0.*,!=3.1.*,!=3.2.*,>=2.7""
-
-[package.dependencies]
-six = "">=1.5""
-
-[[package]]
-name = ""pytz""
-version = ""2022.1""
-description = ""World timezone definitions, modern and historical""
-category = ""main""
-optional = true
-python-versions = ""*""
-
-[[package]]
-name = ""six""
-version = ""1.16.0""
-description = ""Python 2 and 3 compatibility utilities""
-category = ""main""
-optional = true
-python-versions = "">=2.7, !=3.0.*, !=3.1.*, !=3.2.*""
-
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
@@ -250,15 +202,10 @@ python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
-[extras]
-full = [""pandas"", ""polars""]
-pandas = [""pandas""]
-polars = [""polars""]
-
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
-content-hash = ""8fda347438c855d8fcb0e5b6319823d331690eba7e5f28f2a7cfaa4d69b2020d""
 
 [metadata.files]
 black = [
@@ -372,29 +319,6 @@ numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
-pandas = [
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_universal2.whl"", hash = ""sha256:be67c782c4f1b1f24c2f16a157e12c2693fd510f8df18e3287c77f33d124ed07""},
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_x86_64.whl"", hash = ""sha256:5a206afa84ed20e07603f50d22b5f0db3fb556486d8c2462d8bc364831a4b417""},
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_11_0_arm64.whl"", hash = ""sha256:0010771bd9223f7afe5f051eb47c4a49534345dfa144f2f5470b27189a4dd3b5""},
-    {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:3228198333dd13c90b6434ddf61aa6d57deaca98cf7b654f4ad68a2db84f8cfe""},
-    {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:5b79af3a69e5175c6fa7b4e046b21a646c8b74e92c6581a9d825687d92071b51""},
-    {file = ""pandas-1.4.2-cp310-cp310-win_amd64.whl"", hash = ""sha256:5586cc95692564b441f4747c47c8a9746792e87b40a4680a2feb7794defb1ce3""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_universal2.whl"", hash = ""sha256:061609334a8182ab500a90fe66d46f6f387de62d3a9cb9aa7e62e3146c712167""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_x86_64.whl"", hash = ""sha256:b8134651258bce418cb79c71adeff0a44090c98d955f6953168ba16cc285d9f7""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_11_0_arm64.whl"", hash = ""sha256:df82739e00bb6daf4bba4479a40f38c718b598a84654cbd8bb498fd6b0aa8c16""},
-    {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:385c52e85aaa8ea6a4c600a9b2821181a51f8be0aee3af6f2dcb41dafc4fc1d0""},
-    {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:295872bf1a09758aba199992c3ecde455f01caf32266d50abc1a073e828a7b9d""},
-    {file = ""pandas-1.4.2-cp38-cp38-win32.whl"", hash = ""sha256:95c1e422ced0199cf4a34385ff124b69412c4bc912011ce895582bee620dfcaa""},
-    {file = ""pandas-1.4.2-cp38-cp38-win_amd64.whl"", hash = ""sha256:5c54ea4ef3823108cd4ec7fb27ccba4c3a775e0f83e39c5e17f5094cb17748bc""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_universal2.whl"", hash = ""sha256:c072c7f06b9242c855ed8021ff970c0e8f8b10b35e2640c657d2a541c5950f59""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_x86_64.whl"", hash = ""sha256:f549097993744ff8c41b5e8f2f0d3cbfaabe89b4ae32c8c08ead6cc535b80139""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_11_0_arm64.whl"", hash = ""sha256:ff08a14ef21d94cdf18eef7c569d66f2e24e0bc89350bcd7d243dd804e3b5eb2""},
-    {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:8c5bf555b6b0075294b73965adaafb39cf71c312e38c5935c93d78f41c19828a""},
-    {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:51649ef604a945f781105a6d2ecf88db7da0f4868ac5d45c51cb66081c4d9c73""},
-    {file = ""pandas-1.4.2-cp39-cp39-win32.whl"", hash = ""sha256:d0d4f13e4be7ce89d7057a786023c461dd9370040bdb5efa0a7fe76b556867a0""},
-    {file = ""pandas-1.4.2-cp39-cp39-win_amd64.whl"", hash = ""sha256:09d8be7dd9e1c4c98224c4dfe8abd60d145d934e9fc1f5f411266308ae683e6a""},
-    {file = ""pandas-1.4.2.tar.gz"", hash = ""sha256:92bc1fc585f1463ca827b45535957815b7deb218c549b7c18402c322c7549a12""},
-]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
@@ -475,18 +399,6 @@ pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
-python-dateutil = [
-    {file = ""python-dateutil-2.8.2.tar.gz"", hash = ""sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86""},
-    {file = ""python_dateutil-2.8.2-py2.py3-none-any.whl"", hash = ""sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9""},
-]
-pytz = [
-    {file = ""pytz-2022.1-py2.py3-none-any.whl"", hash = ""sha256:e68985985296d9a66a881eb3193b0906246245294a881e7c8afe623866ac6a5c""},
-    {file = ""pytz-2022.1.tar.gz"", hash = ""sha256:1e760e2fe6a8163bc0b3d9a19c4f84342afa0a2affebfaa84b01b978a02ecaa7""},
-]
-six = [
-    {file = ""six-1.16.0-py2.py3-none-any.whl"", hash = ""sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254""},
-    {file = ""six-1.16.0.tar.gz"", hash = ""sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926""},
-]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
+optional = false
 python-versions = "">=3.8""
 
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
@@ -137,7 +116,7 @@ name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
+optional = false
 python-versions = "">=3.7""
 
 [package.dependencies]
@@ -179,33 +158,6 @@ toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
@@ -250,15 +202,10 @@ python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
+content-hash = ""5509c339bc3154af4b5c349b84ad6a79801ec11bd63a83881a76c9c664791262""
 
 [metadata.files]
 black = [
@@ -372,29 +319,6 @@ numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
@@ -475,18 +399,6 @@ pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
-
-# Optional DataFrame libraries
-pandas = { version = ""^1.4.2"", optional = true }
-polars = { version = ""^0.13.38"", optional = true }
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
-[tool.poetry.extras]
-pandas = [""pandas""]
-polars = [""polars""]
-full = [""pandas"", ""polars""] # all features
-
 [tool.isort]
 profile = ""black""
 "
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
+polars = ""^0.13.38""
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
 [tool.isort]
 profile = ""black""
 "
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
-from .metrics import Snapshot
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
-from typing import TYPE_CHECKING, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask
@@ -10,7 +10,7 @@
 from pygame.surface import Surface
 
 from .config import BaseConfig
-from .metrics import Snapshot
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
@@ -48,19 +48,16 @@ class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
-    # Sites
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
-    # Proximity
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
-    # Config (shared with other agents too)
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
@@ -74,6 +71,9 @@ class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
@@ -86,12 +86,14 @@ def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
 
         self.__proximity = proximity
 
@@ -156,15 +158,19 @@ def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
-    def update(self):
         """"""Run your own agent logic at every tick of the simulation.
-        Every frame of the simulation, this update method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
@@ -202,8 +208,8 @@ def there_is_no_escape(self) -> bool:
 
         return changed
 
-    def update_position(self):
-        """"""Update the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
@@ -331,24 +337,42 @@ def change_image(self, index: int):
 
         self._image_index = index
 
-    def snapshot(self) -> Snapshot:
-        """"""Create a Snapshot of agent data that you're interested in.
 
-        By default the Agent will produce a Snapshot with the following data:
         - agent identifier
         - current frame
         - x and y coordinates
 
-        However, you can also add your own data by inheriting the Snapshot dataclass.
-        Add any fields that you like and then overwrite this method to produce your custom Snapshot.
 
-        Make sure to call `super().snapshot()` to collect the default Snapshot data.
         """"""
 
-        return Snapshot(
-            x=self.pos.x,
-            y=self.pos.y,
-            id=self.id,
-            frame=self.shared.counter,
-            image_index=self._image_index,
-        )"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
+from typing import TYPE_CHECKING, Any, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask
@@ -10,7 +10,7 @@
 from pygame.surface import Surface
 
 from .config import BaseConfig
+from .metrics import Metrics
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
@@ -48,19 +48,16 @@ class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
@@ -74,6 +71,9 @@ class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
+    __metrics: Metrics
+    """"""Data collection of the snapshots.""""""
+
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
@@ -86,12 +86,14 @@ def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
+        metrics: Metrics,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
+        self.__metrics = metrics
 
         self.__proximity = proximity
 
@@ -156,15 +158,19 @@ def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
+    def every_frame(self):
         """"""Run your own agent logic at every tick of the simulation.
+        Every frame of the simulation, this method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
+    def update(self):
+        self._collect_replay_data()
+        self.every_frame()
+
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
@@ -202,8 +208,8 @@ def there_is_no_escape(self) -> bool:
 
         return changed
 
+    def change_position(self):
+        """"""Change the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
@@ -331,24 +337,42 @@ def change_image(self, index: int):
 
         self._image_index = index
 
+    def save_data(self, column: str, value: Any):
+        """"""Add extra data to the simulation's metrics.
 
+        The following data is collected automatically:
         - agent identifier
         - current frame
         - x and y coordinates
 
+        Examples
+        --------
 
+        Saving the number of agents in radius:
+
+        >>> from vi import Agent
+        >>> class MyAgent(Agent):
+        ...     def every_frame(self):
+        ...         in_radius = len(self.in_radius())
+        ...         self.save_data(""in_radius"", in_radius)
         """"""
 
+        self.__metrics._temporary_snapshots[column].append(value)
+
+    def _collect_replay_data(self):
+        """"""Add the minimum data needed for the replay simulation to the dataframe.""""""
+
+        x, y = round_pos(self.pos)
+        snapshots = self.__metrics._temporary_snapshots
+
+        snapshots[""frame""].append(self.shared.counter)
+        snapshots[""id""].append(self.id)
+
+        snapshots[""x""].append(x)
+        snapshots[""y""].append(y)
+
+        snapshots[""image_index""].append(self._image_index)
+
+        if self.config.image_rotation:
+            angle = self.move.angle_to(Vector2((0, -1)))
+            snapshots[""angle""].append(round(angle))"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
-import dataclasses
 from dataclasses import dataclass, field
-from typing import TYPE_CHECKING, Any
 
-if TYPE_CHECKING:
-    from pandas import DataFrame as PandasDataFrame
-    from polars import DataFrame as PolarsDataFrame
-    from polars import Series as PolarsSeries
-
-
-@dataclass
-class Snapshot:
-    """"""Data that's collected for every agent in every frame of the simulation.""""""
-
-    frame: int
-    """"""The current frame of the simulation.""""""
-
-    id: int
-    """"""The identifier of the agent.""""""
-
-    x: float
-    """"""The x coordinate of the agent.""""""
-
-    y: float
-    """"""The y coordinate of the agent.""""""
-
-    image_index: int
-    """"""The current index of the image list.""""""
-
-    def as_dict(self) -> dict[str, Any]:
-        """"""Convert this Snapshot into a dictionary.""""""
-
-        return dataclasses.asdict(self)
 
 
 @dataclass
@@ -42,28 +14,29 @@ class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
-    def to_polars(self) -> PolarsSeries:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
-@dataclass
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
-    fps: Fps = field(default_factory=Fps)
     """"""The frames-per-second history to analyse performance.""""""
 
-    snapshots: list[dict[str, Any]] = field(default_factory=list)
-    """"""The most important data (snapshot) of every agent at every moment in time.""""""
 
-    def to_pandas(self) -> PandasDataFrame:
-        import pandas as pd
 
-        return pd.DataFrame(self.snapshots)
 
-    def to_polars(self) -> PolarsDataFrame:
-        import polars as pl
 
-        return pl.from_dicts(self.snapshots)"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
+from collections import defaultdict
 from dataclasses import dataclass, field
+from typing import Any
 
+import polars as pl
 
 
 @dataclass
@@ -42,28 +14,29 @@ class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
+    def to_polars(self) -> pl.Series:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
+    fps: Fps
     """"""The frames-per-second history to analyse performance.""""""
 
+    _temporary_snapshots: defaultdict[str, list[Any]]
+    snapshots: pl.DataFrame
 
+    def __init__(self):
+        self.fps = Fps()
+        self._temporary_snapshots = defaultdict(list)
+        self.snapshots = pl.DataFrame()
 
+    def merge(self):
+        df = pl.from_dict(self._temporary_snapshots)
 
+        self.snapshots.vstack(df, in_place=True)
 
+        self._temporary_snapshots = defaultdict(list)"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
-from .util import load_image, load_images, round_pos
 
 if TYPE_CHECKING:
     from .agent import Agent
@@ -148,6 +148,7 @@ def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
             )
 
         return self
@@ -173,6 +174,7 @@ def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
         )
 
         return self
@@ -272,8 +274,8 @@ def tick(self):
         # Update all agents
         self._all.update()
 
-        # Snapshot marked agent data
-        self.__save_snapshots()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
@@ -306,16 +308,7 @@ def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
-            agent.update_position()
-
-    def __save_snapshots(self):
-        """"""Save a Snapshot of each agent and add it to Metrics.""""""
-
-        for sprite in self._agents.sprites():
-            agent: Agent = sprite  # type: ignore
-            snapshot = agent.snapshot()
-
-            self.__metrics.snapshots.append(snapshot.as_dict())
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
+from .util import load_image, load_images
 
 if TYPE_CHECKING:
     from .agent import Agent
@@ -148,6 +148,7 @@ def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
+                metrics=self.__metrics,
             )
 
         return self
@@ -173,6 +174,7 @@ def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
+            metrics=self.__metrics,
         )
 
         return self
@@ -272,8 +274,8 @@ def tick(self):
         # Update all agents
         self._all.update()
 
+        # Merge the collected snapshots into the dataframe.
+        self.__metrics.merge()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
@@ -306,16 +308,7 @@ def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
+            agent.change_position()
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
-                ],
-                ""uses"": 0
             }
         }
     },
     ""2"": {
         ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
             ""deadline"": 3,
             ""pages"": 10,
-            ""already_exec"": 4
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -76,8 +79,10 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -89,35 +94,46 @@
                     8,
                     9
                 ],
-                ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 2,
-            ""execution_time"": 2,
-            ""deadline"": 3,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
-            ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
-        ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -128,8 +144,15 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -141,40 +164,48 @@
                     8,
                     9
                 ],
-                ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
-            ""name"": ""C"",
-            ""arrival_time"": 4,
-            ""execution_time"": 1,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 1
         },
-        ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
-            ""D""
         ],
-        ""done_in_this_cicle"": true,
-        ""time"": 9,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -185,8 +216,20 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -198,44 +241,46 @@
                     8,
                     9
                 ],
-                ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
-            ""name"": ""D"",
-            ""arrival_time"": 6,
-            ""execution_time"": 3,
-            ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
-        ""done_in_this_cicle"": false,
-        ""time"": 12,
-        ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""D"": {
                 ""real"": [
                     0,
                     1,
@@ -259,9 +304,39 @@
                     7,
                     8,
                     9
-                ],
                 ""uses"": 0
             }
         }
     },
     ""6"": {
@@ -274,11 +349,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 13,
-        ""started_time"": 12,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,19 +384,24 @@
                     9
                 ],
                 ""virtual"": [
-                    0,
-                    1,
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9
-                ],
-                ""uses"": 1
             }
         }
     }
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 1
         }
     },
     ""2"": {
         ""process"": {
+            ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
             ""deadline"": 3,
             ""pages"": 10,
+            ""already_exec"": 2
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
+                ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -76,8 +79,10 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -89,35 +94,46 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0
         }
     },
     ""3"": {
         ""process"": {
+            ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
+            ""deadline"": 7,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
+        ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -128,8 +144,15 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -141,40 +164,48 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0
         }
     },
     ""4"": {
         ""process"": {
+            ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 2
         },
+        ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
+            ""D"",
+            ""A""
         ],
+        ""done_in_this_cicle"": false,
+        ""time"": 10,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -185,8 +216,20 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -198,44 +241,46 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 1
         }
     },
     ""5"": {
         ""process"": {
+            ""name"": ""A"",
+            ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 3,
             ""pages"": 10,
+            ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
+        ""done_in_this_cicle"": true,
+        ""time"": 13,
+        ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
@@ -259,9 +304,39 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
                 ""uses"": 0
+            },
+            ""D"": {
+                ""real"": null,
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 0
         }
     },
     ""6"": {
@@ -274,11 +349,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 15,
+        ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,19 +384,24 @@
                     9
                 ],
                 ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 0
         }
     }
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
 
 scalonator_translate = {
     ""FIFO"": fifo,
@@ -12,7 +14,8 @@
 }
 
 swap_translate = {
-    ""FIFO"": swap_fifo
 }
 
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
+from cpu.memory.swap_algorithm.swap_lru import swap_lru
+
 
 scalonator_translate = {
     ""FIFO"": fifo,
@@ -12,7 +14,8 @@
 }
 
 swap_translate = {
+    ""FIFO"": swap_fifo,
+    ""LRU"":swap_lru
 }
 
 "
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
-        [
-            ""A"",
-            4,
-            0
-        ],
         [
             ""B"",
-            7,
             2
         ],
         [
             ""C"",
-            9,
             4
         ],
         [
-            ""D"",
             13,
             6
         ]
     ],
-    ""0"": 5.25
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
         [
             ""B"",
+            5,
             2
         ],
         [
             ""C"",
+            7,
             4
         ],
         [
+            ""A"",
             13,
+            0
+        ],
+        [
+            ""D"",
+            15,
             6
         ]
     ],
+    ""0"": 7.0
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
-from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
@@ -24,6 +24,7 @@ def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
 
 
     def initialize(self, queue:List[ProcessIn]):
@@ -55,16 +56,15 @@ def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
-        #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
-            self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return False #Tudo certo! o processo j est carregado na memoria
             #No precisa de OVERHEAD
         
@@ -76,7 +76,6 @@ def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
-                self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
@@ -93,15 +92,13 @@ def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
-                self.real_virtual_map[process.name][""uses""] = 0
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
-                self.real_virtual_map[process.name][""uses""] = 0 
                 self.swap(process)
                 return True
 
@@ -122,23 +119,23 @@ def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto no tiver espao, fazer  o swap para ter espao!
         
-        #TODO Must teste this approach! It is the correct one!
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
-                self.p_order)
-        # old_p_name= self.swap_algorithm(
-            # self.p_order)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
-            self.real_virtual_map[old_p_name][""uses""] = 0
 
-        #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
-        self.real_virtual_map[process.name][""uses""] = 0 
 
         return True
          
@@ -175,9 +172,14 @@ def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
-    "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
+from collections import deque, Counter
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
@@ -24,6 +24,7 @@ def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
+        self.counter = Counter()
 
 
     def initialize(self, queue:List[ProcessIn]):
@@ -55,16 +56,15 @@ def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
+        self.counter[process.name] +=1
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return False #Tudo certo! o processo j est carregado na memoria
             #No precisa de OVERHEAD
         
@@ -76,7 +76,6 @@ def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
@@ -93,15 +92,13 @@ def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.swap(process)
                 return True
 
@@ -122,23 +119,23 @@ def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto no tiver espao, fazer  o swap para ter espao!
         
+        removed_p_count = 1
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
+                self.p_order, self.counter,removed_p_count)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
+            self.counter[old_p_name] = 0
 
+            removed_p_count+=1
+
+        #cadastra o novo processo na memoria real
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
+        self.counter[process.name] = 0 
 
         return True
          
@@ -175,9 +172,14 @@ def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
+
+            self.counter[p_name] = 0
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
+
+    def show_counter(self):
+        copy = json.dumps(dict(self.counter))
+        return copy"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"-from collections import deque
 
 def swap_fifo(
-    p_order:deque
 ):
     old_process_name = p_order[-1] #pega a primeira posio
     p_order.rotate()"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import deque, Counter
 
 def swap_fifo(
+    p_order:deque,
+    counter:Counter,
+    removed_p_count:int
 ):
     old_process_name = p_order[-1] #pega a primeira posio
     p_order.rotate()"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import deque,Counter
+
+def swap_lru(
+    p_order:deque,
+    counter:Counter,
+    removed_p_count:int
+):
+    least_used = counter.most_common()[:-removed_p_count-1:-1] 
+
+    return least_used[removed_p_count-1][0] #retorna o nome do processo menos usado
+
+"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;#NOM?
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import Counter
 
+d = Counter()
 
+d[0]+=1
+d[0]+=1
+d[1]+=1
+d[1]+=1
+d[1]+=1
 
 print()
 "
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
@@ -83,19 +84,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
-        # if p.name != cache_name: #Caso o process no esteja carregado na cache
-        #     result = mmu.load_context(p)
-            
-        #     is_overhead = True
-        #     if not first:
-        #         sleep(overhead)
-        #         time_count+=overhead
-        #     first = False
-            
-        # else:
-        #     is_overhead = False
-        # is_process_done = False
-        # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -107,7 +96,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
-
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -120,14 +109,16 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
-            real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -172,7 +163,8 @@ def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
-    real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -183,6 +175,7 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
@@ -192,7 +185,8 @@ def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
-                ""real_virtual_map"": memory_map
             }
     
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
+    mmu_counter = """"
     print(""enters main loop!"")
 
     while True:
@@ -83,19 +84,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
+
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -107,7 +96,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
+                
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -120,14 +109,16 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
+        mmu_counter = mmu.show_counter()
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
+            real_virtual_map,
+            mmu_counter
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -172,7 +163,8 @@ def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
+    real_virtual_map: str,
+    mmu_counter:str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -183,6 +175,7 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
+    memory_counter=json.loads(mmu_counter)
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
@@ -192,7 +185,8 @@ def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
+                ""real_virtual_map"": memory_map,
+                ""memory_counter"":memory_counter
             }
     
 "
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 4
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -77,7 +78,10 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -89,35 +93,43 @@
                     8,
                     9
                 ],
-                ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 2,
-            ""execution_time"": 2,
-            ""deadline"": 5,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""C""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 6,
-        ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -129,7 +141,15 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -141,40 +161,44 @@
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
-            ""name"": ""C"",
-            ""arrival_time"": 4,
-            ""execution_time"": 1,
-            ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 1
         },
-        ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""D""
         ],
-        ""done_in_this_cicle"": true,
-        ""time"": 7,
-        ""started_time"": 6,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -186,7 +210,20 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -198,44 +235,41 @@
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
-            ""name"": ""D"",
-            ""arrival_time"": 6,
-            ""execution_time"": 3,
-            ""deadline"": 10,
             ""pages"": 10,
-            ""already_exec"": 2
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
-        ""done_in_this_cicle"": false,
-        ""time"": 9,
-        ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""D"": {
                 ""real"": [
                     0,
                     1,
@@ -261,6 +295,32 @@
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
@@ -274,11 +334,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 10,
-        ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,16 +369,16 @@
                     9
                 ],
                 ""virtual"": [
-                    0,
-                    1,
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9
                 ],
                 ""uses"": 1
             }"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
+            ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
+            ""deadline"": 5,
             ""pages"": 10,
+            ""already_exec"": 2
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
+                ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -77,7 +78,10 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -89,35 +93,43 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
+                ""uses"": 0
             }
         }
     },
     ""3"": {
         ""process"": {
+            ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
+            ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 7,
+        ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -129,7 +141,15 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -141,40 +161,44 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
+            ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
+            ""already_exec"": 2
         },
+        ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""D"",
+            ""A""
         ],
+        ""done_in_this_cicle"": false,
+        ""time"": 10,
+        ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -186,7 +210,20 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -198,44 +235,41 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
+            ""name"": ""A"",
+            ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
+            ""already_exec"": 4
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
+        ""done_in_this_cicle"": true,
+        ""time"": 13,
+        ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
@@ -261,6 +295,32 @@
                     9
                 ],
                 ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": null,
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
+                ""uses"": 0
             }
         }
     },
@@ -274,11 +334,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 15,
+        ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,16 +369,16 @@
                     9
                 ],
                 ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
                 ],
                 ""uses"": 1
             }"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;\ No newline at end of file
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"+{
+    ""config"":{
+        ""scale_algorithm"": ""RR"",
+        ""page_algorithm"": ""FIFO"",
+        ""quantum"": 10,
+        ""overhead"":0
+    },
+    ""processes"":[
+        {
+            ""name"":""p5"",
+            ""arrival_time"":35,
+            ""execution_time"":5,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p4"",
+            ""arrival_time"":21,
+            ""execution_time"":13,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p3"",
+            ""arrival_time"":19,
+            ""execution_time"":10,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p2"",
+            ""arrival_time"":13,
+            ""execution_time"":75,
+            ""pages"":10,
+            ""deadline"":8
+        },
+        {
+            ""name"":""p1"",
+            ""arrival_time"":5,
+            ""execution_time"":17,
+            ""pages"":10,
+            ""deadline"":5
+        },
+        {
+            ""name"":""p0"",
+            ""arrival_time"":0,
+            ""execution_time"":25,
+            ""pages"":10,
+            ""deadline"":7
+        }
+    ]
+}
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
-        [
-            ""A"",
-            4,
-            0
-        ],
         [
             ""B"",
-            6,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""D"",
-            10,
             6
         ]
     ],
-    ""0"": 3.75
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
         [
             ""B"",
+            5,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
+        [
+            ""A"",
+            13,
+            0
+        ],
         [
             ""D"",
+            15,
             6
         ]
     ],
+    ""0"": 7.0
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto no tiver espao, fazer  o swap para ter espao!
         
         #TODO Must teste this approach! It is the correct one!
-        # while not self.memory_real.does_it_fit(process.pages): 
-        #     old_p_name= self.swap_algorithm(
-        #         self.p_order)
-        old_p_name= self.swap_algorithm(
-            self.p_order)
-
-
-        #Remove o index do processo antigo da memoria real
-        list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
-        self.memory_real.remove(list_index_to_remove)
-        self.real_virtual_map[old_p_name][""real""] = None
-        self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto no tiver espao, fazer  o swap para ter espao!
         
         #TODO Must teste this approach! It is the correct one!
+        while not self.memory_real.does_it_fit(process.pages): 
+            old_p_name= self.swap_algorithm(
+                self.p_order)
+        # old_p_name= self.swap_algorithm(
+            # self.p_order)
+
+            #Remove o index do processo antigo da memoria real
+            list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
+            self.memory_real.remove(list_index_to_remove)
+            self.real_virtual_map[old_p_name][""real""] = None
+            self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
@@ -15,3 +16,20 @@ def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
+from typing import Union, List
 
 
 
@@ -15,3 +16,20 @@ def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
+def fifo_dont_use(
+    process_list:list[ProcessIn],
+    add_p:Union[list[ProcessIn],ProcessIn],
+    time_count:int=None,
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo fifo')
+    for x in process_list:
+        d.append(x)
+
+    if type(add_p) is list:
+        for p in add_p:
+            d.appendleft(p)
+    elif type(add_p) is ProcessIn:
+        d.append(add_p)
+
+    return d
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
-def rr(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
+from typing import Union, List
 
 
 
+def rr_v1(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
+
+def rr_v2(
+    process_list:list[ProcessIn],
+    time_count:int=None,
+    right:bool=False
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo rr')
+
+    for x in process_list:
+        d.append(x)
+
+
+    if right:
+        d.rotate()#rodar para a direita!
+
+    return d
+
+def rr_v3(
+    process_list:list[ProcessIn],
+    add_p:Union[list[ProcessIn],ProcessIn],
+    time_count:int=None,
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo rr')
+
+    for x in process_list:
+        d.append(x)
+
+    if type(add_p) is list:
+        for p in add_p:
+            d.append(p)
+    elif type(add_p) is ProcessIn:
+        d.appendleft(add_p)
+   
+    return d
+
+def rr(process_list:list[ProcessIn],time_count:int=None):
+    d = deque()
+    lesser = 9999
+    for x in process_list:
+        if x.already_exec < lesser:
+            lesser = x.already_exec
+    normalized = all([p.already_exec==lesser if p.already_exec!=0 else False for p in process_list])
+    if not normalized:        
+        process_list.sort(key=lambda x: x.already_exec - lesser, reverse=True)
+    else:
+        process_list=reversed(process_list)
+
+
+    for x in process_list:
+        d.append(x)
+    return d
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
-            queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
@@ -74,8 +74,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
-            is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
@@ -101,7 +101,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
-            sleep(1)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
@@ -117,7 +117,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
-            queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
+            queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count=time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
@@ -74,8 +74,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
             if not first:
+                is_overhead = True
                 sleep(overhead)
                 time_count+=overhead
             first = False
@@ -101,7 +101,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
+            sleep(0)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
@@ -117,7 +117,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
+            queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
-        ""started_time"": 0
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 4,
-        ""started_time"": 2
-    },
-    ""3"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 6,
-        ""started_time"": 4
-    },
-    ""4"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 8
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 8,
-        ""started_time"": 6
-    },
-    ""5"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 10
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
             ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 10,
-        ""started_time"": 8
     },
-    ""6"": {
         ""process"": {
             ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 12,
-        ""started_time"": 10
-    },
-    ""7"": {
-        ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 14,
-        ""started_time"": 12
-    },
-    ""8"": {
-        ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
             ""C""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 16,
-        ""started_time"": 14
     },
-    ""9"": {
         ""process"": {
             ""name"": ""C"",
-            ""arrival_time"": 0,
-            ""execution_time"": 2,
             ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 18,
-        ""started_time"": 16
     },
-    ""10"": {
         ""process"": {
             ""name"": ""D"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": false,
-        ""time"": 20,
-        ""started_time"": 18
     },
-    ""11"": {
         ""process"": {
             ""name"": ""D"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 5,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": true,
-        ""time"": 22,
-        ""started_time"": 20
-    },
-    ""12"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 2
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 24,
-        ""started_time"": 22
-    },
-    ""13"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 26,
-        ""started_time"": 24
-    },
-    ""14"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 28,
-        ""started_time"": 26
-    },
-    ""15"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 8
         },
-        ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 30,
-        ""started_time"": 28
     }
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
+        ""started_time"": 0,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""B""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 4,
+        ""started_time"": 2,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 1
+            }
+        }
     },
+    ""3"": {
         ""process"": {
             ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
+            ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""C""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 6,
+        ""started_time"": 4,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""4"": {
         ""process"": {
             ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
             ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 7,
+        ""started_time"": 6,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""5"": {
         ""process"": {
             ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": false,
+        ""time"": 9,
+        ""started_time"": 7,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""6"": {
         ""process"": {
             ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
+            ""already_exec"": 3
         },
+        ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 10,
+        ""started_time"": 9,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 1
+            }
+        }
     }
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
-            10,
             0
         ],
         [
             ""B"",
-            16,
-            0
         ],
         [
             ""C"",
-            18,
-            0
         ],
         [
             ""D"",
-            22,
-            0
-        ],
-        [
-            ""E"",
-            30,
-            0
         ]
     ],
-    ""0"": 19.2
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
+            4,
             0
         ],
         [
             ""B"",
+            6,
+            2
         ],
         [
             ""C"",
+            7,
+            4
         ],
         [
             ""D"",
+            10,
+            6
         ]
     ],
+    ""0"": 3.75
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
-
 
 class MMU:
     
@@ -120,7 +120,12 @@ def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
-
         old_p_name= self.swap_algorithm(
             self.p_order)
 
@@ -173,5 +178,7 @@ def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
-
     "
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
+import json
 
 class MMU:
     
@@ -120,7 +120,12 @@ def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
+        #Enquanto no tiver espao, fazer  o swap para ter espao!
+        
+        #TODO Must teste this approach! It is the correct one!
+        # while not self.memory_real.does_it_fit(process.pages): 
+        #     old_p_name= self.swap_algorithm(
+        #         self.p_order)
         old_p_name= self.swap_algorithm(
             self.p_order)
 
@@ -173,5 +178,7 @@ def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
+    def show_real_virtual_map(self):
+        copy = json.dumps(self.real_virtual_map)
+        return copy
     "
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
-from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -45,6 +45,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
     print(""enters main loop!"")
 
     while True:
@@ -102,6 +103,9 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -110,6 +114,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
@@ -119,7 +124,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
-            started_time
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -163,7 +169,8 @@ def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
-    started_time:int
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -174,14 +181,16 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
-                ""started_time"":started_time
             }
     
 "
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
+from typing import Callable, List, Tuple, Dict
 from collections import deque
 
 from cpu.models import config_model
@@ -45,6 +45,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
+    real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
@@ -102,6 +103,9 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
+                real_virtual_map = mmu.show_real_virtual_map()
+
+                # real_virtual_map = mmu.show_real_virtual_map()
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -110,6 +114,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
+            real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
@@ -119,7 +124,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
+            started_time,
+            real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -163,7 +169,8 @@ def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
+    started_time:int,
+    real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -174,14 +181,16 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
+    memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
+                ""started_time"":started_time,
+                ""real_virtual_map"": memory_map
             }
     
 "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
-    ""FIFO"": swap_fifo,
-
 }
 
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
+    ""FIFO"": swap_fifo
 }
 
+
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;\ No newline at end of file
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+{
+    ""config"":{
+        ""scale_algorithm"": ""FIFO"",
+        ""page_algorithm"": ""FIFO"",
+        ""quantum"": 2,
+        ""overhead"":0
+    },
+    ""processes"":[
+       
+        {
+            ""name"":""A"",
+            ""arrival_time"":0,
+            ""execution_time"":10,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""B"",
+            ""arrival_time"":0,
+            ""execution_time"":6,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""C"",
+            ""arrival_time"":0,
+            ""execution_time"":2,
+            ""pages"":10,
+            ""deadline"":8
+        },
+        {
+            ""name"":""D"",
+            ""arrival_time"":0,
+            ""execution_time"":4,
+            ""pages"":10,
+            ""deadline"":5
+        },
+        {
+            ""name"":""E"",
+            ""arrival_time"":0,
+            ""execution_time"":8,
+            ""pages"":10,
+            ""deadline"":7
+        }
+    ]
+}
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"-from typing import Dict, List, Union, Callable, Tuple
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
@@ -9,23 +9,27 @@ class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
-    page_algorithm: Callable
-    
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
-        page_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
-        self.page_algorithm = page_algorithm
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
-            if not self.memory_real.is_memory_full(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
@@ -35,7 +39,7 @@ def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
-            elif not self.memory_virtual.is_memory_full(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
@@ -49,22 +53,26 @@ def initialize(self, queue:List[ProcessIn]):
                 
 
 
-    def load_context(self, process: ProcessIn):
         real_virtual_map = self.real_virtual_map
-        self.memory_real.add_stack(process)
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
-            return True #Tudo certo! o processo j est carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo no est na memo_real, mas est na memo_virtual
-            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
@@ -78,14 +86,15 @@ def load_context(self, process: ProcessIn):
 
 
         else:#O processo no ta nem na memoria real nem na virtual
-            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
@@ -112,8 +121,8 @@ def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
-        old_p_name= self.page_algorithm(
-            self.memory_real)
 
 
         #Remove o index do processo antigo da memoria real
@@ -132,7 +141,7 @@ def swap(self, process: ProcessIn):
 
 
 
-    def garbage_collector(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
@@ -148,8 +157,21 @@ def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
 
-    def init_memories(self):
-        pass
 
     "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+from typing import Dict, List, Union, Callable, Tuple, Set
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
@@ -9,23 +9,27 @@ class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
+    swap_algorithm: Callable
+    p_count: any
+    p_order: deque
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
+        swap_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
+        self.swap_algorithm = swap_algorithm
+        self.p_count = None
+        self.p_order = deque()
+
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
+            if self.memory_real.does_it_fit(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
@@ -35,7 +39,7 @@ def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
+            elif self.memory_virtual.does_it_fit(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
@@ -49,22 +53,26 @@ def initialize(self, queue:List[ProcessIn]):
                 
 
 
+    def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
+        if not process.name in real_virtual_map:
+            self.p_order.appendleft(process.name)
+        
+        #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
+            return False #Tudo certo! o processo j est carregado na memoria
+            #No precisa de OVERHEAD
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo no est na memo_real, mas est na memo_virtual
+            if self.memory_real.does_it_fit(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
@@ -78,14 +86,15 @@ def load_context(self, process: ProcessIn):
 
 
         else:#O processo no ta nem na memoria real nem na virtual
+            if self.memory_real.does_it_fit(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
+                return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
@@ -112,8 +121,8 @@ def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
+        old_p_name= self.swap_algorithm(
+            self.p_order)
 
 
         #Remove o index do processo antigo da memoria real
@@ -132,7 +141,7 @@ def swap(self, process: ProcessIn):
 
 
 
+    def garbage_collector_all(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
@@ -148,8 +157,21 @@ def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
+    def garbage_collector(self,process:ProcessIn):
+            p_name = process.name
+            real_virtual_map = self.real_virtual_map
+            free_real_indexes = real_virtual_map[p_name][""real""]
+            free_virtual_indexes = real_virtual_map[p_name][""virtual""]
+
+            self.memory_real.remove(free_real_indexes)
+            self.memory_virtual.remove(free_virtual_indexes)
+
+
+            real_virtual_map[p_name][""real""] = None
+            real_virtual_map[p_name][""virtual""] = None
+            real_virtual_map[p_name][""uses""] = 0
+            self.real_virtual_map = real_virtual_map 
+            print(f""Removed processes = {p_name}"")
 
 
     "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
-    current_memory_space: int
-    space_graph: Dict[str, bool]
     process_stack: deque
     type_name:str
 
@@ -22,8 +22,9 @@ def space_initializer(self):
             space_graph[i] = False # lugar da memoria comea vazio
         return space_graph
 
-    def is_memory_full(self, number_of_page_in: int):
-        if number_of_page_in < self.current_space_occupied:
             return True
         return False
     
@@ -48,10 +49,13 @@ def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
-        for index in index_list:
-            self.space_graph[index] = False
-            self.current_space_occupied -= 1
-            print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
+    current_space_occupied: int
+    space_graph: Dict[int, bool]
     process_stack: deque
     type_name:str
 
@@ -22,8 +22,9 @@ def space_initializer(self):
             space_graph[i] = False # lugar da memoria comea vazio
         return space_graph
 
+    def does_it_fit(self, number_of_page_in: int):
+        free_space = self.total_memory_pages - self.current_space_occupied 
+        if free_space >= number_of_page_in:
             return True
         return False
     
@@ -48,10 +49,13 @@ def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
+        if index_list is None:
+            print(""Process is not in memory"")
+        else:
+            for index in index_list:
+                self.space_graph[index] = False
+                self.current_space_occupied -= 1
+                print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"-
 
 def swap_fifo(
-    memory_real
 ):
-    old_process_name = memory_real.process_stack[-1] #pega a primeira posio
-    memory_real.process_stack.rotate()
-    return old_process_name
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+from collections import deque
 
 def swap_fifo(
+    p_order:deque
 ):
\ No newline at end of file
+    old_process_name = p_order[-1] #pega a primeira posio
+    p_order.rotate()
+    return old_process_name"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
-from typing import List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -25,23 +25,21 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
-    scalonator_engine =  scalonator_translate[config.scale_algorithm] 
-    page_algorithm = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    # real_memory = Memory(""real"",total_memory_pages=20)
-    # virtual_memory = Memory(""virtual"",total_memory_pages=100)
-    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
-    # MMU.initialize(process_list)
-
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
-    cache_name = False
     is_process_done = False
     time_count = 0
     first = True
@@ -64,24 +62,37 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
-        if p.name != cache_name: #Caso o process no esteja carregado na cache
-            # result = mmu.load_context(p)
-            
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
-            
-        else:
             is_overhead = False
         is_process_done = False
-        cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -91,13 +102,15 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
-            queue.appendleft(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
@@ -177,10 +190,15 @@ def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
-    for index,p in enumerate(process_list):
         if time_count >= p.arrival_time:
             result.append(p)
-            process_list.pop(index)
     return result
         
 "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
+from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -25,23 +25,21 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
+    scalonator_engine: Callable =  scalonator_translate[config.scale_algorithm] 
+    swap_algorithm: Callable = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    real_memory = Memory(""real"",total_memory_pages=10)
+    virtual_memory = Memory(""virtual"",total_memory_pages=100)
+    mmu = MMU(real_memory,virtual_memory,swap_algorithm)
+    # mmu.initialize(process_list)
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
     is_process_done = False
     time_count = 0
     first = True
@@ -64,24 +62,37 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
+            is_process_done = False
+
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
+        #Retorna True caso precise trocar de contexto!
+        if mmu.load_context(p):
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
+        else: #Retorna False caso o contexto j estava carregado
             is_overhead = False
         is_process_done = False
+        
+        # if p.name != cache_name: #Caso o process no esteja carregado na cache
+        #     result = mmu.load_context(p)
+            
+        #     is_overhead = True
+        #     if not first:
+        #         sleep(overhead)
+        #         time_count+=overhead
+        #     first = False
+            
+        # else:
+        #     is_overhead = False
+        # is_process_done = False
+        # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -91,13 +102,15 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
+                mmu.garbage_collector(p)
+
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
+            queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
@@ -177,10 +190,15 @@ def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
+
+    process_copy = process_list.copy()
+
+    for p in process_copy:
         if time_count >= p.arrival_time:
             result.append(p)
+            process_list.remove(p)
+
+            # process_list.pop(index)
     return result
         
 "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
-#TODO: Need delete cicle_log data!
 
 app = FastAPI()
 "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
 
 app = FastAPI()
 "
KO;4;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    real_memory = MemoryReal(total_memory_pages=50)
-    virtual_memory = MemoryVirtual(total_memory_frames=100)
-    mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,7 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
-            mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -67,11 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
-            mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process no esteja carregado na cache
-            result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    # real_memory = MemoryReal(total_memory_pages=50)
+    # virtual_memory = MemoryVirtual(total_memory_frames=100)
+    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,7 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
+            # mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -67,11 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
+            # mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process no esteja carregado na cache
+            # result = mmu.load_context(p)
             
             is_overhead = True
             if not first:"
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return True #Tudo certo! o processo j est carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo no est na memo_real, mas est na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
-                real_used_index = self.add_to_memory(process.page,self.memory_real)
-                self.real_virtual_map[process.name][""real""] = real_used_index #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
-                real_virtual_map = self.swap(real_virtual_map)
-                pass
 
 
-        else:#O processo no tinha sido cadastrado antes!
-            pass
 
         
         
@@ -84,31 +97,55 @@ def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
-        if not memory.is_memory_full(pages):
-            for _ in range(pages): 
-               used_index = memory.add(used_index)
-            return used_index
-        else:
-            print(""memory full!"")
 
 
     def swap(self, process: ProcessIn):
-        for k in self.real_virtual_map.keys():
-            self.real_virtual_map[k]
 
-        self.page_algorithm()
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
-    def clean_done_process():
-        pass
 
-    def init_memory(self, process_list: List[ProcessIn]):
-        pass
 
-    def init_disk(self, process_list: List[ProcessIn]):
         pass
 
     "
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
+            self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return True #Tudo certo! o processo j est carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo no est na memo_real, mas est na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
+                real_used_indexes = self.add_to_memory(process.page,self.memory_real)
+                self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
+                self.swap(process)
+                
+
+                return True
+
+
+        else:#O processo no ta nem na memoria real nem na virtual
+            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NO esteja cheia, alocar o processo
+                self.memory_real.add_stack(process)
+                real_used_indexes = self.add_to_memory(process.page,self.memory_real)
+                virtual_used_indexes = self.add_to_memory(process.page,self.memory_virtual)
+
+                self.real_virtual_map[process.name][""real""] = real_used_indexes
+                self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
+            else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
+                self.swap(process)
 
 
 
         
         
@@ -84,31 +97,55 @@ def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
+        for _ in range(pages): 
+            used_index = memory.add(used_index)
+        return used_index
 
 
     def swap(self, process: ProcessIn):
 
+        new_p_real_index, old_p_name= self.page_algorithm(
+            self.memory_real,
+            process,
+            self.real_virtual_map)
+
+        #Remove o index do processo antigo da memoria real
+        list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
+        self.memory_real.remove(list_index_to_remove)
+
+        #Atualiza a remoo na hash table
+        self.real_virtual_map[old_p_name][""real""] = None
+        self.real_virtual_map[old_p_name][""uses""] = 0 #Fazer update da tabela hash
+
+        #
+        self.real_virtual_map[process.name][""real""] = new_p_real_index #Fazer update da tabela hash
+        self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
+
+        return True
+         
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
+    def garbage_collector(self,process_done:List[ProcessIn]):
+        real_virtual_map = self.real_virtual_map
+        for p in process_done:
+            free_real_index = real_virtual_map[p.name][""real""]
+            free_virtual_index = real_virtual_map[p.name][""virtual""]
+
+            self.memory_real.remove(free_real_index)
+            self.memory_virtual.remove(free_virtual_index)
+
+
+            real_virtual_map[p.name][""real""] = None
+            real_virtual_map[p.name][""virtual""] = None
+            real_virtual_map[p.name][""uses""] = None
+        self.real_virtual_map = real_virtual_map 
+        print(""Removed unused processes"")
 
 
+    def init_memories(self):
         pass
 
     "
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria comea vazio
         return space_graph
 
-    def is_memory_full(self, page_in: int):
-        if page_in > self.current_memory_space:
             return True
         return False
     
@@ -29,7 +29,7 @@ def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
-    def add(self,used_index: List[any])-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
@@ -38,8 +38,9 @@ def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
-    def remove(self,index:int):
-        self.space_graph[str(index)] = False
-        self.current_space_occupied -= 1
-
 "
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria comea vazio
         return space_graph
 
+    def is_memory_full(self, number_of_page_in: int):
+        if number_of_page_in > self.current_memory_space:
             return True
         return False
     
@@ -29,7 +29,7 @@ def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
+    def add(self,used_index:List)-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
@@ -38,8 +38,9 @@ def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
+    def remove(self,index_list:List[int]):
+        for index in index_list:
+            self.space_graph[index] = False
+            self.current_space_occupied -= 1
+            print(f""Removed {index} from real memory"")
 "
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    # real_memory = MemoryReal(total_memory_pages=50)
-    # virtual_memory = MemoryVirtual(total_memory_frames=100)
-    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,6 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -66,10 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             continue
 
         if p.name != cache_name: #Caso o process no esteja carregado na cache
-            # result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    real_memory = MemoryReal(total_memory_pages=50)
+    virtual_memory = MemoryVirtual(total_memory_frames=100)
+    mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,6 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
+            mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -66,10 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
+            mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process no esteja carregado na cache
+            result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[gp]
+components = 100
+batch_size = 100"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.random_projection import GaussianRandomProjection
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""gauss_rp.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""gp"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""gp"", ""components"")
+transformer = GaussianRandomProjection(n_components=n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
+    # fit grp
+    embedding = transformer.fit_transform(
+        batch_of_images.reshape(batch_size, -1)
+    )
+    ###########################################################################
+    np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
+
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[pca]
+components = 50
+batch_size = 50"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.decomposition import IncrementalPCA
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""ipca.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""pca"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""pca"", ""components"")
+assert n_components <= batch_size
+transformer = IncrementalPCA(n_components = n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""IPCA of batch {batch:02d}"", end=""\n"")
+    # fit pca
+    transformer.fit(batch_of_images.reshape(batch_size, -1))
+###############################################################################
+with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
+
+    pickle.dump(transformer, file)
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
-        # get magnitude scale to better distinguis objects
-        # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
-    image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
+    image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[gp]
+components = 100
+batch_size = 100"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.random_projection import GaussianRandomProjection
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""gauss_rp.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""gp"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""gp"", ""components"")
+transformer = GaussianRandomProjection(n_components=n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
+    # fit grp
+    embedding = transformer.fit_transform(
+        batch_of_images.reshape(batch_size, -1)
+    )
+    ###########################################################################
+    np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
+
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
+
+[pca]
+components = 50
+batch_size = 50"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.decomposition import IncrementalPCA
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""ipca.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""pca"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""pca"", ""components"")
+assert n_components <= batch_size
+transformer = IncrementalPCA(n_components = n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""IPCA of batch {batch:02d}"", end=""\n"")
+    # fit pca
+    transformer.fit(batch_of_images.reshape(batch_size, -1))
+###############################################################################
+with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
+
+    pickle.dump(transformer, file)
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"-""""""Prepare raw images for Fourier Analysis""""""
-
-###############################################################################
-from configparser import ConfigParser, ExtendedInterpolation
-import glob
-import time
-
-from astropy.io import fits as pyfits
-import numpy as np
-
-from leosAi.utils.managefiles import FileDirectory
-###############################################################################
-start_time = time.time()
-###############################################################################
-parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""ir_pca.ini""
-parser.read(f""{config_file_name}"")
-# Check files and directory
-check = FileDirectory()
-# Handle configuration file
-# configuration = ConfigurationFile()
-###############################################################################
-# location of data
-data_directory = parser.get(""directory"", ""data"")
-data_type = parser.get(""common"", ""type"")
-path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
-
-for idx, path_to_file in enumerate(path_to_files):
-
-    file_name = path_to_file.split(""/"")[-1].split(""."")[0]
-    print(f""{idx:04d}: {file_name}"", end=""\r"")
-
-    image = np.load(path_to_file)
-###############################################################################
-# with open(
-#     f""{save_to}/{config_file_name}"",
-#     ""w"", encoding=""utf8""
-# ) as config_file:
-#
-#     parser.write(config_file)
-###############################################################################
-finish_time = time.time()
-print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
-        # get magnitude scale to better distinguis objects
-        # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
-    image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
+    image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5;BinFlush;scripts;f6b8b28ef6c4ca975f9ee38e9cef9830b74032e0;added memory sanitation and clipboard option;"def main():
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()"
OK;5;BinFlush;scripts;f6b8b28ef6c4ca975f9ee38e9cef9830b74032e0;added memory sanitation and clipboard option;"def main():
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
+    parser.add_argument('-f', action='store_true', help=""Require password to include every specified type of characters"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()"
KO;5;BinFlush;scripts;610dc5bb1e7fbed110ac160f2034b26df18ce21f;added option to copy password to clipboard, and some memory sanitation of sensitive data;" import secrets
 import string
 import argparse
-
 
 def main():
     # Defining command line arguments
@@ -12,8 +13,9 @@ def main():
     parser.add_argument('-u', action='store_true', help=""use uppercase"")
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
-    parser.add_argument('--charset', default="""", help=""custom character set \""in quotes\"""")
     args = parser.parse_args()
 
     # Checking how many sets of characters are to be used
@@ -25,7 +27,8 @@ def main():
 
     # Add sets to superset
     chars = """"
-    if args.l:    chars += string.ascii_lowercase
     if args.u:
         chars += string.ascii_uppercase
     if args.d:
@@ -40,13 +43,22 @@ def main():
 
     # Build and print actual password
     password = ''.join(secrets.choice(chars) for i in range(args.n))
-    print(password)
 
 def count_arguments(args) -> int:
     """""" Counts valid command line arguments except -n""""""
     n: int = 0
     for arg in vars(args):
-        if not arg == ""n"":
             n += bool(getattr(args, arg))
     return n
 "
OK;5;BinFlush;scripts;610dc5bb1e7fbed110ac160f2034b26df18ce21f;added option to copy password to clipboard, and some memory sanitation of sensitive data;" import secrets
 import string
 import argparse
+import gc
+import pyperclip # remember to pip3 install pyperclip
 
 def main():
     # Defining command line arguments
@@ -12,8 +13,9 @@ def main():
     parser.add_argument('-u', action='store_true', help=""use uppercase"")
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
+    parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
+    parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()
 
     # Checking how many sets of characters are to be used
@@ -25,7 +27,8 @@ def main():
 
     # Add sets to superset
     chars = """"
+    if args.l:    
+        chars += string.ascii_lowercase
     if args.u:
         chars += string.ascii_uppercase
     if args.d:
@@ -40,13 +43,22 @@ def main():
 
     # Build and print actual password
     password = ''.join(secrets.choice(chars) for i in range(args.n))
+    if args.c:
+        pyperclip.copy(password)
+    else:
+        print(password)
+
+    
+    # Clean up sensitive data
+    del(password)
+    del(chars)
+    gc.collect()
 
 def count_arguments(args) -> int:
     """""" Counts valid command line arguments except -n""""""
     n: int = 0
     for arg in vars(args):
+        if arg in [""u"", ""l"", ""d"", ""charset""]:
             n += bool(getattr(args, arg))
     return n
 "
KO;6;NTT123;a0-jax;df6a898f5b2d72b537c3c31d25ffbc7e4138a935;reduce memory usage;"def train(
         buffer.extend(data)
         data = list(buffer)
         shuffler.shuffle(data)
-        data = jax.tree_map(lambda *xs: np.stack(xs), *data)
-        N = data.state.shape[0]
         losses = []
         old_agent = jax.tree_map(lambda x: jnp.copy(x), agent)
         agent = agent.train()
         with click.progressbar(
             range(0, N - batch_size, batch_size), label=""  train agent""
         ) as bar:
             for i in bar:
-                batch = jax.tree_map(lambda x: x[i : (i + batch_size)], data)
                 agent, optim, loss = train_step(agent, optim, batch)
                 losses.append(loss)
 "
OK;6;NTT123;a0-jax;df6a898f5b2d72b537c3c31d25ffbc7e4138a935;reduce memory usage;"def train(
         buffer.extend(data)
         data = list(buffer)
         shuffler.shuffle(data)
+        N = len(data)
         losses = []
         old_agent = jax.tree_map(lambda x: jnp.copy(x), agent)
         agent = agent.train()
         with click.progressbar(
             range(0, N - batch_size, batch_size), label=""  train agent""
         ) as bar:
             for i in bar:
+                batch = data[i : (i + batch_size)]
+                batch = jax.tree_map(lambda *xs: np.stack(xs), *batch)
                 agent, optim, loss = train_step(agent, optim, batch)
                 losses.append(loss)
 "
KO;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
   </component>
 </module>
\ No newline at end of file"
OK;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
+    <orderEntry type=""module"" module-name=""state_machine"" />
   </component>
 </module>
\ No newline at end of file"
KO;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
OK;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
+      <module fileurl=""file://$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"def __init__(self, normalization, local_dataset, batch_size=32 ,epochs=1):
         self.epochs=epochs
 
 
-    def train(self):
-
-        optimizer = SGD(self.model.parameters(), lr=1e-3, weight_decay=5e-4)
-
-        criterion = CrossEntropyLoss()
-        criterion.cuda()
-
-        self.model.cuda()
-        self.model.train()
-
-        for epoch in range(self.epochs):
-            # training loop
-            for i, data in enumerate(self.dataset):
-                imgs, labels = data
-                imgs, labels = imgs.cuda(), labels.cuda()
-
-                optimizer.zero_grad()
-                pred = self.model(imgs)
-                pred = pred.cuda()
-                
-                loss = criterion(pred, labels)
-                loss.backward()
-                optimizer.step()
-
-        self.model = self.model.to('cpu')
-
-        self.model_dict = self.model.state_dict()
-        torch.cuda.empty_cache()
-
-
-    def get_data(self, key):
-        return self.model_dict[key]
-
-    def set_model(self, model_dict):
-        self.model = ResNet(self.normalization)
-        self.model.load_state_dict(model_dict)
-
     
\ No newline at end of file"
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"def __init__(self, normalization, local_dataset, batch_size=32 ,epochs=1):
         self.epochs=epochs
 
 
     
\ No newline at end of file"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"+from torch.optim import SGD
+from torch.nn import CrossEntropyLoss
+from resnet50 import ResNet
+import torch
+
+class ClientSimulation:
+
+    def __init__(self, n_clients, normalization):
+        self.n_clients = n_clients
+        self.normalization = normalization
+        
+
+    def train(self, clients, client_subset, server_model_dict):
+
+        cl_data = dict()
+        
+        for index in client_subset:
+            
+            cl = _Client(self.normalization, clients[index].dataset, clients[index].epochs, server_model_dict)
+            print(f""Training client {index}"")
+            cl.train()
+            print(""Done"")
+            cl_data[index] = cl
+
+        return cl_data
+
+
+class _Client:
+
+    def __init__(self, normalization, local_dataset, epochs, model_dict):
+        self.model = ResNet(normalization)
+        self.model.load_state_dict(model_dict)
+        self.dataset = local_dataset
+        self.epochs = epochs
+
+    def train(self):
+
+        optimizer = SGD(self.model.parameters(), lr=1e-3, weight_decay=5e-4)
+
+        criterion = CrossEntropyLoss()
+        criterion.cuda()
+
+        self.model.cuda()
+        self.model.train()
+
+        for epoch in range(self.epochs):
+            # training loop
+            for i, data in enumerate(self.dataset):
+                imgs, labels = data
+                imgs, labels = imgs.cuda(), labels.cuda()
+
+                optimizer.zero_grad()
+                pred = self.model(imgs)
+                pred = pred.cuda()
+                
+                loss = criterion(pred, labels)
+                loss.backward()
+                optimizer.step()
+
+        self.model = self.model.to('cpu')
+
+        self.model_dict = self.model.state_dict()
+        torch.cuda.empty_cache()
+
+    def get_data(self, key):
+        return self.model_dict[key]
+"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from test import test_accuracy
 import copy
 import gc
 
 
 # global parameters : number of epochs locally, normalization type
@@ -34,6 +35,10 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
     dataset = get_dataset(transform)
     subdatasets = get_iid_split(dataset)
 
     # create clients
 
     clients = dict()
@@ -54,17 +59,12 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
 
         print(f""##### ROUND {round}"")
 
-        client_subset = sample(range(N_CLIENTS), int(client_proportion*N_CLIENTS))
-
-        for index in client_subset:
-            print(f""Training client  {index}"")
-            server_model_dict = server.get_model_dict()
 
-            clients[index].set_model(server_model_dict)
-            clients[index].train()
-            print(""Done"")
 
-        model_dict = average(clients, normalization, client_subset)
 
         server.update_model(model_dict)
 "
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from test import test_accuracy
 import copy
 import gc
+from client_simulation import ClientSimulation
 
 
 # global parameters : number of epochs locally, normalization type
@@ -34,6 +35,10 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
     dataset = get_dataset(transform)
     subdatasets = get_iid_split(dataset)
 
+    n_clients_each_round = int(client_proportion*N_CLIENTS)
+
+    sim = ClientSimulation(n_clients_each_round, normalization)
+
     # create clients
 
     clients = dict()
@@ -54,17 +59,12 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
 
         print(f""##### ROUND {round}"")
 
+        client_subset = sample(range(N_CLIENTS), n_clients_each_round)
 
+        server_model_dict = server.get_model_dict()
+        trained_models = sim.train(clients, client_subset, server_model_dict)
 
+        model_dict = average(trained_models, normalization, client_subset)
 
         server.update_model(model_dict)
 "
KO;8;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el nmero de taps que realice el usuario
 
-    if taps==64:
        up()
        goto(0, 0)
        color (""red"")"
OK;8;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el nmero de taps que realice el usuario
 
+    if taps==300:
        up()
        goto(0, 0)
        color (""red"")"
KO;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
-        self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
OK;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
+        # self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
KO;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
-        print(cache.actual_data)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
@@ -65,14 +64,10 @@ def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
-        cache.actual_data['w'] = keys[pygame.K_w] * 6
-        cache.actual_data['s'] = keys[pygame.K_s] * 6
-        cache.actual_data['a'] = keys[pygame.K_a] * 6
-        cache.actual_data['d'] = keys[pygame.K_d] * 6
-        for key in cache.actual_data:
-            if cache.actual_data[key] == 0:
-                continue
-            cache.actual_data[key] = cache.actual_data[key] - 1
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
@@ -83,9 +78,6 @@ def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
-            if e.type == pygame.KEYDOWN:
-                if e.unicode in cache.actual_data:
-                    cache.actual_data[e.unicode] = 5
         
         screen.fill('black')
         data = cache.get_last_data()"
OK;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
@@ -65,14 +64,10 @@ def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
+        cache.actual_data['w'] = keys[pygame.K_w]
+        cache.actual_data['s'] = keys[pygame.K_s]
+        cache.actual_data['a'] = keys[pygame.K_a]
+        cache.actual_data['d'] = keys[pygame.K_d]
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
@@ -83,9 +78,6 @@ def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
         
         screen.fill('black')
         data = cache.get_last_data()"
KO;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
     simulation_id = request['data'].get(""id"")
-    if not uid:
-        return
-    elif not simulation_id:
-        return
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
@@ -93,6 +99,8 @@ async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
OK;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
+    old_simulation_id = request['cookie'].get(""id"")
     simulation_id = request['data'].get(""id"")
+    
+    if not uid: return
+    if not simulation_id: return
+    if not old_simulation_id: return
+    
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
+    old_simulation = storage.get_unit(""simulations"", simulation_id=old_simulation_id)
+    if not old_simulation:
+        return
+    old_simulation[""users""].remove(user['id'])
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
@@ -93,6 +99,8 @@ async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
+    for key in request['data']['keys']:
+        request['data']['keys'][key] *= 6
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
KO;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"Displays stock information relative to now, in years. Replace the 1 with any num
 ## Up and Down Command:
 
 `!up three_letter_stock_name value_to_reach`
 `!down three_letter_stock_name value_to_reach`
 
 Sets up an automatic alert for when the specified stock value exceeds or falls under a specified value.
 
 ## Buy and Sell Command:
 
 `!buy money_to_buy_shares_with three_letter_stock_name`
@@ -95,6 +102,22 @@ Deletes all pending alerts for the provided stock ticker that are also from the
 
 Deletes any pending alert that matches the stock ticker, `!up` and `!down` command, and also the value.
 
 # Administration Commands:
 ## Stop Command:
 "
OK;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"Displays stock information relative to now, in years. Replace the 1 with any num
 ## Up and Down Command:
 
 `!up three_letter_stock_name value_to_reach`
+
 `!down three_letter_stock_name value_to_reach`
 
 Sets up an automatic alert for when the specified stock value exceeds or falls under a specified value.
 
+`!up three_letter_stock_name percentage %`
+
+`!down three_letter_stock_name percentage %`
+
+Sets a relative pricing alert to it's current price.
+
 ## Buy and Sell Command:
 
 `!buy money_to_buy_shares_with three_letter_stock_name`
@@ -95,6 +102,22 @@ Deletes all pending alerts for the provided stock ticker that are also from the
 
 Deletes any pending alert that matches the stock ticker, `!up` and `!down` command, and also the value.
 
+## Portfolio:
+
+### This command only works in Direct Messages with the bot. It will send you a DM if you execute this command in any server.
+
+`!portfolio torn_api_key`
+
+Lists all stocks that you own with all transactions with their change in price relative to now. All transactions for that stock are ordered newest first to oldest last.
+
+`!portfolio torn_api_key stock_ticker`
+
+Lists all transactions for the specified ticker.
+
+`!portfolio torn_api_key stock_ticker number_of_transactions`
+
+Lists `number_of_transactions` of the selected stock before stopping. If you have two or more transactions, and use a it'll show the most recent transaction.
+
 # Administration Commands:
 ## Stop Command:
 "
KO;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							break
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)
@@ -538,10 +542,14 @@ async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							break
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)"
OK;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
+					found_stock = False
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
+							found_stock = True
 							break
+					if not found_stock:	
+						userdata[""value""].append(0)
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)
@@ -538,10 +542,14 @@ async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
+					found_stock = False
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
+							found_stock = True
 							break
+					if not found_stock:	
+						userdata[""value""].append(0)
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)"
KO;12;js0522;tgn_review;00428db9374655b3ecf0ce9806b2d8e1254101c0;Add vanilla RNN memory updater;"def update_memory(self, unique_node_ids, unique_messages, timestamps):
     pass
 
 
-class GRUMemoryUpdater(MemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
-    super(GRUMemoryUpdater, self).__init__()
     self.memory = memory
     self.layer_norm = torch.nn.LayerNorm(memory_dimension)
     self.message_dimension = message_dimension
     self.device = device
 
-    self.memory_updater = nn.GRUCell(input_size=message_dimension,
-                                     hidden_size=memory_dimension)
-
   def update_memory(self, unique_node_ids, unique_messages, timestamps):
     if len(unique_node_ids) <= 0:
       return
@@ -46,3 +43,26 @@ def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):
     updated_last_update[unique_node_ids] = timestamps
 
     return updated_memory, updated_last_update"
OK;12;js0522;tgn_review;00428db9374655b3ecf0ce9806b2d8e1254101c0;Add vanilla RNN memory updater;"def update_memory(self, unique_node_ids, unique_messages, timestamps):
     pass
 
 
+class SequenceMemoryUpdater(MemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
+    super(SequenceMemoryUpdater, self).__init__()
     self.memory = memory
     self.layer_norm = torch.nn.LayerNorm(memory_dimension)
     self.message_dimension = message_dimension
     self.device = device
 
   def update_memory(self, unique_node_ids, unique_messages, timestamps):
     if len(unique_node_ids) <= 0:
       return
@@ -46,3 +43,26 @@ def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):
     updated_last_update[unique_node_ids] = timestamps
 
     return updated_memory, updated_last_update
+
+
+class GRUMemoryUpdater(SequenceMemoryUpdater):
+  def __init__(self, memory, message_dimension, memory_dimension, device):
+    super(GRUMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)
+
+    self.memory_updater = nn.GRUCell(input_size=message_dimension,
+                                     hidden_size=memory_dimension)
+
+
+class RNNMemoryUpdater(SequenceMemoryUpdater):
+  def __init__(self, memory, message_dimension, memory_dimension, device):
+    super(RNNMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)
+
+    self.memory_updater = nn.RNNCell(input_size=message_dimension,
+                                     hidden_size=memory_dimension)
+
+
+def get_memory_updater(module_type, memory, message_dimension, memory_dimension, device):
+  if module_type == ""gru"":
+    return GRUMemoryUpdater(memory, message_dimension, memory_dimension, device)
+  elif module_type == ""rnn"":
+    return RNNMemoryUpdater(memory, message_dimension, memory_dimension, device)"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<component name=""InspectionProjectProfileManager"">
+  <settings>
+    <option name=""USE_PROJECT_PROFILE"" value=""false"" />
+    <version value=""1.0"" />
+  </settings>
+</component>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project version=""4"">
+  <component name=""ProjectModuleManager"">
+    <modules>
+      <module fileurl=""file://$PROJECT_DIR$/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/.idea/state_machine.iml"" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project version=""4"">
+  <component name=""RSettings"" path="""" />
+</project>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project version=""4"">
+  <component name=""ProjectId"" id=""29QNMxx7EeoT8aAdnSWICPXeq0h"" />
+</project>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" from resources.state_machine.states.s03_test_state_2 import Test2StateBody
 
 class Initialization(InitializationBody):
-    def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
-            self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         else:
-            info = "">>> Info: transition error in {} state"".format(self)
-            return CloseProgram(info)
 
 class CloseProgram(CloseProgramBody):
-    def on_event(self, event):
         self.action()
 
 class Test1State(Test1StateBody):
-    def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
-            self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
-            info = "">>> Info: transition error in {} state"".format(self)
-            return CloseProgram(info)
 
 class Test2State(Test2StateBody):
-    def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
-            self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
-            info = "">>> Info: transition error in {} state"".format(self)
-            return CloseProgram(info)"
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" from resources.state_machine.states.s03_test_state_2 import Test2StateBody
 
 class Initialization(InitializationBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.Initialization
+
+        '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
+            states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
+            return CloseProgram()
 
+        '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             return Test1State()
         else:
+            states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
+            return CloseProgram()
 
 class CloseProgram(CloseProgramBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.CloseProgram
+
         self.action()
 
 class Test1State(Test1StateBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.Test1State
+
+        '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
+            states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
+            return CloseProgram()
 
+        '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
+            states_data.Test1State = Test1State()
             return Test2State()
         else:
+            states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
+            return CloseProgram()
 
 class Test2State(Test2StateBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.Test2State
+
+        '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
+            states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
+            return CloseProgram()
 
+        '''transition conditions'''
         if self.status == ""GO TO TEST1"":
+            '''clear data before transition'''
+            states_data.Test2State = Test2State()
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
+            states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
+            return CloseProgram()"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 
 '''import all your states here'''
-from resources.state_machine.my_states import Initialization
 
 
 class StateLoader(object): #in Karen project is a SimpleDevice class
@@ -18,7 +25,8 @@ def __init__(self):
         """""" Initialize the components. """"""
 
         # Start with a default state.
-        self.state = Initialization()
 
     def on_event(self, event):
         """"""
@@ -29,7 +37,7 @@ def on_event(self, event):
         #
 
         # The next state will be the result of the on_event function.
-        self.state = self.state.on_event(event)
 
 
 "
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 
 '''import all your states here'''
+from resources.state_machine.my_states import Initialization, CloseProgram, Test1State, Test2State
+
+class States():
+    def __init__(self):
+        self.Initialization = Initialization()
+        self.CloseProgram = CloseProgram()
+        self.Test1State = Test1State()
+        self.Test2State = Test2State()
 
 
 class StateLoader(object): #in Karen project is a SimpleDevice class
@@ -18,7 +25,8 @@ def __init__(self):
         """""" Initialize the components. """"""
 
         # Start with a default state.
+        self.states_data = States()
+        self.state = self.states_data.Initialization
 
     def on_event(self, event):
         """"""
@@ -29,7 +37,7 @@ def on_event(self, event):
         #
 
         # The next state will be the result of the on_event function.
+        self.state = self.state.on_event(event=event, states_data=self.states_data)
 
 
 "
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" class InitializationBody(object):
-
     def __init__(self,):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
         self.status = None
 
     def action(self):"
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" class InitializationBody(object):
     def __init__(self,):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
+
         self.status = None
 
     def action(self):"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 class CloseProgramBody(object):
 
-    def __init__(self, info):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
-        self.info = info
 
     def action(self):
         print(self)"
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 class CloseProgramBody(object):
 
+    def __init__(self):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
+        self.info = ""info""
 
     def action(self):
         print(self)"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
-        self.status = None
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
-        while self.counter < 5:
-            self.counter += 1
-            print(self.counter)
-            sleep(0.5)
-        self.status = ""GO TO TEST2""
 
     def __repr__(self):
         """"""
@@ -34,3 +34,4 @@ def __str__(self):
         return self.__class__.__name__
 
 "
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
+        self.status = ""GO TO TEST1""
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
+        self.counter += 1
+        print(self.counter)
+        sleep(0.5)
+        if self.counter >= 3:
+            self.status = ""GO TO TEST2""
 
     def __repr__(self):
         """"""
@@ -34,3 +34,4 @@ def __str__(self):
         return self.__class__.__name__
 
 
+"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
-        self.status = None
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
-        while self.counter < 5:
-            self.counter += 1
-            print(self.counter)
-            sleep(0.5)
-        self.status = ""GO TO TEST1""
 
     def __repr__(self):
         """""""
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
+        self.status = ""GO TO TEST2""
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
+        self.counter += 1
+        print(self.counter)
+        sleep(0.5)
+        if self.counter >= 3:
+            self.status = ""GO TO TEST1""
 
     def __repr__(self):
         """""""
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
-				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
-				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
 				if(starget < etarget){
 					max_token=mid;
 				}"
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
+				tx=x+list_x[max_token];
+				ty=y+list_y[max_token];
+				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+					starget=data[ty*width+tx];
+				}
+				else starget=0;
+				
+				tx=x+list_x[mid];
+				ty=y+list_y[mid];
+				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+					etarget=data[ty*width+tx];
+				}
+				else etarget=0;
+				
 				if(starget < etarget){
 					max_token=mid;
 				}"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"+
+__kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
+						__global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+
+	if(x<height && y<width){
+		//init variable
+		float pos_avg=0,neg_avg=0;
+		int pos_count=0,neg_count=0,weight=0,start,end,max_weight=0,max_token;
+		int tx,ty;
+		float current,target;
+		current=data[y*width+x];
+		start=list_len*3/4;
+		end=list_len/4;
+		
+		//init direct weight
+		for(int i=0;i<end;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current) ++weight;
+			}
+		}
+		for(int i=start;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current) ++weight;
+			}
+		}
+		
+		//calc direct
+		for(int i=0;i<list_len;++i){
+			start=(start+1)%list_len;
+			end=(end+1)%list_len;
+			tx=x+list_x[start];
+			ty=y+list_y[start];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){ --weight; }
+			}
+			tx=x+list_x[end];
+			ty=y+list_y[end];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){ ++weight; }
+			}
+			//if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				if(weight>max_weight){
+					max_token=i;
+					max_weight=weight;
+				}
+				else if(weight==max_weight){
+					target=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
+					if(target < current){
+						max_token=i;
+					}
+				}
+			//}
+			
+		}
+		
+		//calc diff
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){
+					pos_avg+=target;
+					++pos_count;
+				}
+				else{
+					neg_avg+=target;
+					++neg_count;
+				}
+			}
+		}
+		
+		//diff finish
+		if(pos_count){ pos_avg/=(float)pos_count; }
+		else{ pos_avg=current; }
+		if(neg_count){ neg_avg/=(float)neg_count; }
+		else{ neg_avg=current; }
+		diff[y*width+x]=pos_avg-neg_avg;
+	
+		//direct finish
+		direct[y*width+x]=list_deg[max_token];
+	}
+}
+
+
+__kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
+                        __global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+	
+	if(x<height && y<width){
+		int tx,ty;
+		result[y*width+x]=0;
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				result[y*width+x]-=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
+			}
+		}
+	}
+}
+"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"+
+__kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
+						__global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+
+	if(x<width && y<height){
+		//init variable
+		float pos_avg=0,neg_avg=0;
+		int pos_count=0,neg_count=0,weight=0,start,end,mid,max_weight=0,max_token,dist_token;
+		int tx,ty,otx,oty,stx,sty,etx,ety;
+		float current,target,otarget,starget,etarget;
+		
+		//mid point
+		current=data[y*width+x];
+
+		//init direction (sliding windows, init weight)
+		start=0;
+		end=list_len/2+1;
+		mid=end/2+1;
+		weight=0;
+		
+		max_token=mid;
+		max_weight=weight;	//weight can be negative, just searching for largest weight
+		
+		//calc direct
+		for(int i=0;i<list_len;++i){
+			stx=x+list_x[start];
+			sty=y+list_y[start];
+			etx=x+list_x[end];
+			ety=y+list_y[end];
+			
+			//check start target of sliding window
+			if( (stx>=0 && stx<width) && (sty>=0 && sty<height) ){
+				starget=data[sty*width+stx];
+			}
+			else starget=-1;
+			
+			//check end target of sliding window (assume it is opposite of start target)
+			if( (etx>=0 && etx<width) && (ety>=0 && ety<height) ){
+				etarget=data[ety*width+etx];
+			}
+			else etarget=-1;
+			
+			//compare start and end to the middle point
+			if(starget>current && etarget>current){
+				//both is larger than middle point
+				if(starget>etarget) --weight;
+				else ++weight;
+			}
+			else{
+				if(starget>current) --weight;
+				if(etarget>current) ++weight;
+			}
+			
+			//update max_weight
+			if(weight>max_weight){
+				max_token=mid;
+				max_weight=weight;
+			}
+			else if(weight==max_weight){
+				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
+				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
+				if(starget < etarget){
+					max_token=mid;
+				}
+			}
+			
+			//move sliding window
+			start=(start+1)%list_len;
+			end=(end+1)%list_len;
+			mid=(mid+1)%list_len;
+		}
+		
+		//calculate diff (magnitude)
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+				target=data[ty*width+tx];
+				if(i>max_token) dist_token=i-max_token;
+				else dist_token=max_token-i;
+				if(dist_token>list_len/2) dist_token=list_len-dist_token;
+				
+				if(dist_token>list_len/4){
+					pos_avg+=target;
+					++pos_count;
+				}
+				else{
+					neg_avg+=target;
+					++neg_count;
+				}
+			}
+		}
+		
+		//finish diff (magnitude)
+		if(pos_count){ pos_avg/=(float)pos_count; }
+		else{ pos_avg=current; }
+		if(neg_count){ neg_avg/=(float)neg_count; }
+		else{ neg_avg=current; }
+		diff[y*width+x]=pos_avg-neg_avg;
+		
+		//direct finish
+		direct[y*width+x]=list_deg[max_token];
+	}
+}
+
+
+__kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
+                        __global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+	
+	if(x<width && y<height){
+		int tx,ty;
+		result[y*width+x]=0;
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+				result[y*width+x]+=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
+			}
+		}
+	}
+}
+"
KO;13;Boavizta;cloud-bill;cd5b941be98275fd0549d8abc4a5a460f713c4cc;fix meminfo total memory reader #1;" from typing import List
 
 from .model import MemoryDevice
@@ -24,7 +26,7 @@ def get_total_memory_in_kb() -> int:
             if 'MemTotal' in line:
                 mem_total_line = line.strip()
                 break
-    total_size_kb = int(mem_total_line.split()[0])
     return total_size_kb
 
 "
OK;13;Boavizta;cloud-bill;cd5b941be98275fd0549d8abc4a5a460f713c4cc;fix meminfo total memory reader #1;"+import re
+
 from typing import List
 
 from .model import MemoryDevice
@@ -24,7 +26,7 @@ def get_total_memory_in_kb() -> int:
             if 'MemTotal' in line:
                 mem_total_line = line.strip()
                 break
+    total_size_kb = int(re.search(r'[0-9]+', mem_total_line)[0])
     return total_size_kb
 
 "
KO;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
-        self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
OK;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
+        self.seg_head = SegHead(classes, 1024, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
KO;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
-# tf.keras.mixed_precision.set_global_policy('mixed_float16')
-physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
-for gpu in physical_devices:
-    tf.config.experimental.set_memory_growth(gpu, True)
-mirrored_strategy = tf.distribute.MirroredStrategy()
-
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",
@@ -44,7 +38,7 @@
 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
-                                                                                   ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,
@@ -57,25 +51,35 @@
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
-args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
-args.add_argument(""--random_crop_height"", type=int, default=None,
-                  help=""Height of random crop, random_crop_width must be given with this"")
-args.add_argument(""--random_crop_width"", type=int, default=None,
-                  help=""Width of random crop, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
 args = args.parse_args()
 
 tf.random.set_seed(args.random_seed)
-random_crop_size = (args.random_crop_width, args.random_crop_height) \
-    if args.random_crop_width is not None and args.random_crop_height is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset
@@ -93,13 +97,14 @@
 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
-logdir = os.path.join(args.save_dir, ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
-                                                                                      batch_size,
-                                                                                      optimizer_name, lr,
-                                                                                      args.lr_scheduler,
-                                                                                      backbone,
-                                                                                      model_name,
-                                                                                      time))
 
 # =========== Load Dataset ============ #
 
@@ -117,6 +122,14 @@
 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,
@@ -136,12 +149,13 @@
 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
-processed_train = dataset_train.map(get_images_processed)
-processed_train = processed_train.map(augmentor)
 processed_val = dataset_validation.map(get_images_processed)
-processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).prefetch(
     tf.data.experimental.AUTOTUNE)
-processed_val = processed_val.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)
@@ -167,7 +181,7 @@
         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
-    model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True) if random_crop_size is None else model(tf.random.uniform((1, random_crop_size[0], random_crop_size[1], 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):
@@ -182,7 +196,7 @@
 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
-        train_logits = model((mini_batch[0] / 127.5) - 1, training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
@@ -201,14 +215,14 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
-    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
-    val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
-    if random_crop_size is not None:
-        val_labs = tf.image.resize(val_labs, random_crop_size)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
@@ -220,7 +234,8 @@ def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
-    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -232,8 +247,6 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
-               # tf.concat(train_labs.values, axis=0), \
-               # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -274,7 +287,7 @@ def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
-        processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
@@ -306,35 +319,16 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
-                            tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
-for epoch in range(START_EPOCH, EPOCHS):
-    print(""\n ----------- Epoch {} --------------\n"".format(epoch))
-    step = 0
-    if epoch % args.save_interval == 0:
-        model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
-        print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
-    for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
-        c_step = (epoch * total_samples // args.batch_size) + step
-        loss, train_labs, train_logits = distributed_train_step(mini_batch)
-        step += 1
-
-        # ======== mIoU calculation ==========
-        mIoU.reset_states()
-        gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
-        pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
-        mIoU.update_state(gt, pred)
-        # ====================================
-        # print(""Epoch {}: {}/{}, Loss: {}, mIoU: {}"".format(epoch, step * batch_size, total_samples,
-        #                                                    loss.numpy(), mIoU.result().numpy()))
-        write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
-
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
@@ -357,7 +351,33 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
-                        tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))"
OK;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",
@@ -44,7 +38,7 @@
 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
+                                                                                    ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,
@@ -57,25 +51,35 @@
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
+args.add_argument(""--random_seed"", type=int, default=512, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
+args.add_argument(""--fp16"", action=""store_true"", default=False, help=""Give to enable mixed precision training."")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
+args.add_argument(""--random_crop_min"", type=float, default=None,
+                  help=""minimum value for crop height/width relative to original image"")
+args.add_argument(""--random_crop_max"", type=float, default=None,
+                  help=""Width of random crop as ratio of original width, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
+args.add_argument(""--all_augs"", action=""store_true"", default=False, help=""Add all augmentations except flip_up_down"")
 args = args.parse_args()
 
+if args.fp16:
+    tf.keras.mixed_precision.set_global_policy('mixed_float16')
+
+physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
+for gpu in physical_devices:
+    tf.config.experimental.set_memory_growth(gpu, True)
+mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())
+
 tf.random.set_seed(args.random_seed)
+random_crop_size = (args.random_crop_min, args.random_crop_max) \
+    if args.random_crop_max is not None and args.random_crop_min is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset
@@ -93,13 +97,14 @@
 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
+logdir = os.path.join(args.save_dir,
+                      ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
+                                                                          batch_size,
+                                                                          optimizer_name, lr,
+                                                                          args.lr_scheduler,
+                                                                          backbone,
+                                                                          model_name,
+                                                                          time))
 
 # =========== Load Dataset ============ #
 
@@ -117,6 +122,14 @@
 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
+if args.all_augs:
+    args.flip_left_right = True
+    random_crop_size = (0.5, 0.95)
+    args.random_hue = True
+    args.random_saturation = True
+    args.random_brightness = True
+    args.random_contrast = True
+
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,
@@ -136,12 +149,13 @@
 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
+processed_train = dataset_train.map(augmentor)
+processed_train = processed_train.map(get_images_processed)
 processed_val = dataset_validation.map(get_images_processed)
+processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).repeat(
+    EPOCHS).prefetch(
     tf.data.experimental.AUTOTUNE)
+processed_val = processed_val.batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)
@@ -167,7 +181,7 @@
         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
+    model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):
@@ -182,7 +196,7 @@
 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
+        train_logits = model(tf.image.per_image_standardization(mini_batch[0]), training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
@@ -201,14 +215,14 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
+    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3],
+                                             method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
+    val_logits = model(tf.image.per_image_standardization(mini_batch[0]),
+                       training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
@@ -220,7 +234,8 @@ def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
+    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3],
+                                               method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -232,8 +247,6 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -274,7 +287,7 @@ def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
+        processed_labs = tf.concat(batch[1].values[0], axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
@@ -306,35 +319,16 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
+                        tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
+def evaluate():
+    global val_writer
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
@@ -357,7 +351,33 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
+                    tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))
+
+
+epoch = 0
+while epoch < EPOCHS:
+    print(""\n ----------- Epoch {} --------------\n"".format(epoch))
+    step = 0
+    if epoch % args.save_interval == 0:
+        model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
+        print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
+    for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
+        c_step = (epoch * total_samples // args.batch_size) + step
+        loss, train_labs, train_logits = distributed_train_step(mini_batch)
+        step += 1
+
+        # ======== mIoU calculation ==========
+        mIoU.reset_states()
+        gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
+        pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
+        mIoU.update_state(gt, pred)
+        # ====================================
+        write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
+        if step == total_samples // args.batch_size:
+            epoch += 1
+            break
+
+    evaluate()"
KO;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
-                crop=(256, 256),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
@@ -16,9 +16,12 @@ def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
-    if crop is not None:
-        image_crop = list(crop) + [image.shape[-1]]
-        label_crop = list(crop) + [label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
OK;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
+                crop_scale=(0.05, 0.95),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
@@ -16,9 +16,12 @@ def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
+    if crop_scale is not None:
+        img_shp = tf.cast(tf.shape(image), tf.float32)
+        h_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[0]
+        w_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[1]
+        image_crop = [h_scale, w_scale, image.shape[-1]]
+        label_crop = [h_scale, w_scale, label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
KO;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
-        # b = tf.image.resize(y_b * x_b, tf.shape(a)[1:3])
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
-    def __init__(self, mid_channels, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
 
 
 class BiSeNetv2(K.Model):
-    def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=64, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
@@ -144,15 +162,15 @@ def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
-        self.seg_head = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head1 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head2 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head3 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
-    def call(self, inputs, training=None, mask=None):
-        original_size = tf.shape(inputs)[1:3]
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
@@ -161,28 +179,38 @@ def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
-        x2_s5 = self.ge3(x2_s4)                     # Stride /32
         x2_ce = self.ce(x2_s5)
 
-        final_feat = self.seg_head(tf.image.resize(self.aggregator((x1_s3, x2_ce)), original_size))
         if training:
-            out_s3 = tf.image.resize(self.seg_head1(x2_s3), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            out_s4 = tf.image.resize(self.seg_head2(x2_s4), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            out_s5 = tf.image.resize(self.seg_head3(x2_s5), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            return final_feat, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
-    bs = 7
-    x = tf.random.normal((bs, 512, 1024, 3))
     bisenet = BiSeNetv2(classes=19)
     optimizer = K.optimizers.SGD()
-    for _ in tqdm.tqdm(range(1000)):
-        with tf.GradientTape() as tape:
-            final, z1, z2, z3 = bisenet(x, True)
-            # final = bisenet(x, False)
-            loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
-        vars = bisenet.trainable_variables
-        optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file
\ No newline at end of file"
OK;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
+    def __init__(self, n_classes=19, mid_channels=64, upfactor=8, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
+        self.upfactor = upfactor
+        self.aux = aux
+        if self.aux:
+            self.conv_aux = K.Sequential([
+                K.layers.UpSampling2D(2),
+                ConvBlock(upfactor * upfactor, 3, 1, padding='same'),
+            ])
+            upfactor //= 2
+        self.conv_final = K.layers.Conv2D(n_classes, 1, 1)
+        # self.upsample = K.layers.UpSampling2D(upfactor, interpolation=""bilinear"")
+
+    def call(self, inputs, **kwargs):
+        x = self.conv(inputs)
+        x = self.drop(x)
+        if self.aux:
+            x = self.conv_aux(x)
+        return self.conv_final(x)
+        # else:
+        #     return self.upsample(self.conv_final(x))
 
 
 class BiSeNetv2(K.Model):
+    def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=128, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
@@ -144,15 +162,15 @@ def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
+        self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
+        self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
+        self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
+        self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)
+        self.aux_head4 = SegHead(classes, seg_channels, 32, aux=True)
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
+    def call(self, inputs, training=True, mask=None):
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
@@ -161,28 +179,38 @@ def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
+        x2_s5 = self.ge3(x2_s4)                     # Stride /32z
         x2_ce = self.ce(x2_s5)
 
+        final_feat = self.aggregator((x1_s3, x2_ce))
+        final_feat = self.seg_head(final_feat)
         if training:
+            out_s2 = self.aux_head1(x2_s2)
+            out_s3 = self.aux_head2(x2_s3)
+            out_s4 = self.aux_head3(x2_s3)
+            out_s5 = self.aux_head4(x2_s5)
+            return final_feat, out_s2, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
+
+    physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
+    for gpu in physical_devices:
+        tf.config.experimental.set_memory_growth(gpu, True)
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
+    bs = 14
+    x = tf.random.normal((bs, 1024, 2048, 3))
     bisenet = BiSeNetv2(classes=19)
+    # bisenet.build((bs, 1024, 2048, 3))
     optimizer = K.optimizers.SGD()
\ No newline at end of file
+    final, z1, z2, z3 = bisenet(x, True)
+    bisenet.summary()
+    # for _ in tqdm.tqdm(range(1000)):
+    #     final, z1, z2, z3 = bisenet(x, True)
+        # with tf.GradientTape() as tape:
+        #     final, z1, z2, z3 = bisenet(x, True)
+        #     # final = bisenet(x, False)
+        #     loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
+        # vars = bisenet.trainable_variables
+        # optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file"
KO;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
-                 sn=False,
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
@@ -48,13 +47,9 @@ def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
-        if sn:
-            self.conv2d = tfa.layers.SpectralNormalization(self.conv2d)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
-        elif norm_layer == 'instance':
-            self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 
@@ -91,8 +86,6 @@ def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
-        elif norm_layer == 'instance':
-            self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 "
OK;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
@@ -48,13 +47,9 @@ def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 
@@ -91,8 +86,6 @@ def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 "
KO;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
@@ -33,7 +34,7 @@
 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
-args.add_argument(""--lr"", type=float, default=1e-5, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",
@@ -55,7 +56,7 @@
 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
-args.add_argument(""--aux_weight"", type=float, default=0.25, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
@@ -200,26 +201,26 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
-    return loss, train_labs, train_logits
 
 
 def val_step(mini_batch, aux=False):
-    val_logits = model((mini_batch[0] / 127.5) - 1, training=False) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=False)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
-    # if aux:
-    #     losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
-    #                                                                                    1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
-    #         calc_loss(
-    #             val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
-    #               enumerate(val_logits)]
-    #     val_loss = tf.reduce_sum(losses)
-    #     val_logits = val_logits[0]
-    # else:
-    val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
-    return val_loss, val_labs, val_logits
 
 
 @tf.function
@@ -231,6 +232,8 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -268,8 +271,10 @@ def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
-        tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
-        processed_labs = tf.concat(batch[1].values, axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
OK;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
+# tf.keras.mixed_precision.set_global_policy('mixed_float16')
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
@@ -33,7 +34,7 @@
 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
+args.add_argument(""--lr"", type=float, default=1e-3, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",
@@ -55,7 +56,7 @@
 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
+args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
@@ -200,26 +201,26 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
+    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
+    val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
+    if aux:
+        losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
+                                                                                       1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
+            calc_loss(
+                val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
+                  enumerate(val_logits)]
+        val_loss = tf.reduce_sum(losses)
+        val_logits = val_logits[0]
+    else:
+        val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
+    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -231,6 +232,8 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
+               # tf.concat(train_labs.values, axis=0), \
+               # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -268,8 +271,10 @@ def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
+        # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
+        # processed_labs = tf.concat(batch[1].values, axis=0)
+        tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
+        processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
-systemdan/__main__.spec
\ No newline at end of file
\ No newline at end of file"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
\ No newline at end of file
+systemdan/__main__.spec
+systemdan/command
+systemdan/commandlinux
+systemdan/distlinux/
+__main__.spex
\ No newline at end of file"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
+    typer.echo(typer.style(f'\nMemory\n', fg=typer.colors.BLUE))
+    mem_info = system_info.get_memory_info()
+    typer.echo(f'Total memory: {mem_info[""total""]}')
+    typer.echo(f'Available memory: {mem_info[""available""]}')
+    typer.echo(f'Used memory: {mem_info[""used""]}')
+
+    typer.echo(f'Swap Percentage: {mem_info[""percentage""]}')
+    typer.echo(f'Swap total: {mem_info[""swap_total""]}')
+    typer.echo(f'Swap free: {mem_info[""swap_free""]}')
+    typer.echo(f'Swap used: {mem_info[""swap_used""]}')
+    typer.echo(f'Swap percentage: {mem_info[""swap_percentage""]}')
+
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
@@ -60,3 +72,19 @@ def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
+def get_size(bytes, suffix=""B""):
+    """"""
+    e.g:
+        1253656 => '1.20MB'
+        1253656678 => '1.17GB'
+    """"""
+    factor = 1024
+    for unit in ["""", ""K"", ""M"", ""G"", ""T"", ""P""]:
+        if bytes < factor:
+            return f""{bytes:.2f}{unit}{suffix}""
+        bytes /= factor
+
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
@@ -60,3 +72,19 @@ def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result
+
+def get_memory_info() -> dict:
+    """"""Get the memory information""""""
+    result = {}
+    svmem = psutil.virtual_memory()
+    result['total'] = get_size(svmem.total)
+    result['available'] = get_size(svmem.available)
+    result['used'] = get_size(svmem.used)
+    result['percentage'] = get_size(svmem.free)
+    swap = psutil.swap_memory()
+    result['swap_total'] = get_size(swap.total)
+    result['swap_free'] = get_size(swap.free)
+    result['swap_used'] = get_size(swap.used)
+    result['swap_percentage'] = swap.percent
+
+    return result"
KO;15;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
OK;15;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
+MEMORY = ""${var.PULSAR_MEMORY}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
         self.state = None
         self.alert_level = ""false""
         self.owners = None
@@ -59,6 +60,7 @@ def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
@@ -168,6 +170,7 @@ def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
@@ -358,6 +361,7 @@ def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
+        self.memory = deployment_context.MEMORY
         self.state = None
         self.alert_level = ""false""
         self.owners = None
@@ -59,6 +60,7 @@ def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
+            ""memory"": str(self.memory) if self.memory else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
@@ -168,6 +170,7 @@ def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
+            self.task.memory = deployment_context.MEMORY
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
@@ -358,6 +361,7 @@ def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
+            self.task.memory = deployment_context.MEMORY
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
-PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
+PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,memory:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
+  echo ""MEMORY = \""$PULSAR_MEMORY\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
+  {
+    ""name"": ""memory"",
+    ""type"": ""STRING"",
+    ""mode"": ""NULLABLE""
+  },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
KO;17;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;"-import create from 'zustand'
-
 
 export interface Tokens {
   access: string;
@@ -11,9 +10,19 @@ interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
-const useStore = create<State>(set => ({
-    tokens: {access: """", refresh: """"},
-    setTokens: (tokens) => set({tokens})
-}))
 
-export default useStore;
\ No newline at end of file"
OK;17;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;"+import create from ""zustand"";
 
 export interface Tokens {
   access: string;
@@ -11,9 +10,19 @@ interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
+const getLocalStorage = (key: string) =>
+  JSON.parse(window.localStorage.getItem(key) as string);
+const setLocalStorage = (key: string, value: any) =>
+  window.localStorage.setItem(key, JSON.stringify(value));
+
+const useStore = create<State>((set) => ({
+  tokens: getLocalStorage(""tokens"") || { access: """", refresh: """" },
+  setTokens: (tokens) =>
+    set(() => {
+      setLocalStorage(""tokens"", tokens);
+      return { tokens };
+    }),
+}));
+
 
\ No newline at end of file
+export default useStore;"
KO;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
+
+.idea/
\ No newline at end of file"
KO;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"+import pygame
+from pygame.locals import (
+    K_UP,
+    K_DOWN,
+    K_LEFT,
+    K_RIGHT,
+    K_ESCAPE,
+    KEYDOWN,
+    QUIT,
+)
+mat = [
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0]
+]
+def placematrix(i,j):
+  #insx = round(x//100)
+  #insy = round(y//100)
+  mat[i][j] = 1
+
+def print_matrix(matrix):
+  for i in matrix:
+    print(*i)
+  print()
+pygame.init()
+
+def isStop(i,j):
+    return i >= 5 or mat[i + 1][j] == 1
+def contgame(i,j):
+    placematrix(i,j)
+    print_matrix(mat)
+    #shuld summon new block
+
+screen = pygame.display.set_mode([600, 600])
+i = 0
+j = 2
+running = True
+x = 200
+y = 100
+z = 0
+while running:
+    for event in pygame.event.get():
+        if event.type == KEYDOWN:
+            if event.key == K_ESCAPE:
+                running = False
+            if event.key == K_LEFT:
+                if j >= 0:
+                    #x = x - 100
+                    j = j - 1
+            if event.key == K_RIGHT:
+                if j <= 4:
+                    #x = x + 100
+                    j = j + 1
+    screen.fill((0, 0, 0))
+
+    pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
+    if i < 9:
+        if z == 500:
+            if isStop(i,j):
+                contgame(i,j)
+                running = False
+            #y = y + 100
+            i = i + 1
+            z = 0
+        z = z + 1
+
+    pygame.display.flip()
+
+pygame.quit()
\ No newline at end of file"
KO;18;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
-        return self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
-            single_layer_size = (json_data['layer_width'], json_data['layer_height'])
-            asset_image_path = str(self.data_folder / artboard['layerassets'])
-            asset_image_rgb = Image.open(asset_image_path).convert('RGB')
-            asset_image_tensor = self.img_transform(asset_image_rgb)
-            images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
-            data.append((images, names, bboxes, colors, classes, labels))
         return data
 
 "
OK;18;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
+from typing import Any, Dict, List
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
+        self.artboard_detail: List[Dict[str, Any]] = []
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
+        artboard = self.index_json[idx]
+        json_data = self.artboard_detail[idx]
+        single_layer_size = (json_data['layer_width'], json_data['layer_height'])
+        asset_image_path = str(self.data_folder / artboard['layerassets'])
+        asset_image_rgb = Image.open(asset_image_path).convert('RGB')
+        asset_image_tensor = self.img_transform(asset_image_rgb)
+        images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
+        return images, *self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
+            self.artboard_detail.append(json_data)
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
+            data.append((names, bboxes, colors, classes, labels))
         return data
 
 "
KO;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
+
+.idea/
\ No newline at end of file"
KO;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"+import pygame
+from pygame.locals import (
+    K_UP,
+    K_DOWN,
+    K_LEFT,
+    K_RIGHT,
+    K_ESCAPE,
+    KEYDOWN,
+    QUIT,
+)
+mat = [
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0]
+]
+def placematrix(i,j):
+  #insx = round(x//100)
+  #insy = round(y//100)
+  mat[i][j] = 1
+
+def print_matrix(matrix):
+  for i in matrix:
+    print(*i)
+  print()
+pygame.init()
+
+def isStop(i,j):
+    return i >= 5 or mat[i + 1][j] == 1
+def contgame(i,j):
+    placematrix(i,j)
+    print_matrix(mat)
+    #shuld summon new block
+
+screen = pygame.display.set_mode([600, 600])
+i = 0
+j = 2
+running = True
+x = 200
+y = 100
+z = 0
+while running:
+    for event in pygame.event.get():
+        if event.type == KEYDOWN:
+            if event.key == K_ESCAPE:
+                running = False
+            if event.key == K_LEFT:
+                if j >= 0:
+                    #x = x - 100
+                    j = j - 1
+            if event.key == K_RIGHT:
+                if j <= 4:
+                    #x = x + 100
+                    j = j + 1
+    screen.fill((0, 0, 0))
+
+    pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
+    if i < 9:
+        if z == 500:
+            if isStop(i,j):
+                contgame(i,j)
+                running = False
+            #y = y + 100
+            i = i + 1
+            z = 0
+        z = z + 1
+
+    pygame.display.flip()
+
+pygame.quit()
\ No newline at end of file"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
+from .netserialize import *
 from .dockernet import *
 from .dockershm import *
+from .dockernetserialize import *"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+import os
+
+def get_data():
+   testfile = ""airbnb.csv""
+   datadir = ""/data/""
+   currdir = os.path.dirname(os.path.abspath(__file__)) 
+   f = open(currdir + datadir + testfile, ""r"")
+   data = f.read()
+   return data
+
+if __name__ == ""__main__"":
+    get_data()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
-print(f""recv reading from {__file__}..."")
-print(f""recv data: {contents}"")
 f.close()"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
+print(f""recv: read from {__file__}"")
 f.close()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
-print(f""send writing to shared mem from {__file__}..."")
-f.write(""a very simple message for recv"")
 f.close()"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+from data import get_data
+
 f = open(""test.txt"", ""w+"")
+print(f""send: writing to shared mem from {__file__}..."")
+f.write(get_data())
 f.close()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
+def copy_data_to_volume(volume): 
+    data_script = ""data.py""
+    curr_dir = os.path.dirname(os.path.abspath(__file__))
+    data_dir = curr_dir + ""/data""
+    dst_dir = volume[""Mountpoint""]+f""/data""
+    if os.path.exists(dst_dir):
+        shutil.rmtree(dst_dir)
+    shutil.copytree(data_dir, dst_dir)
+    shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
+
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
+    copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
-def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
-    data = sendsock.recv(1024)
-    print(f""send {data}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
-    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
+from data import get_data
+
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
+def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
+
     sendsock.sendall(b""my simple message"")
+    
+    # wait for confirmation
+    recvdata = sendsock.recv(1024)
+    print(f""send: {recvdata}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
+    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
-def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
-    print(""send writing to shared mem..."")
-    f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
-    print(f""recv data: {contents}"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
-    sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
+from data import get_data
+
+def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
+    print(""send: writing to shared mem..."")
+    f.write(data)
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
+    print(f""recv: received data"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
+    data = get_data()
+    sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
-    FLAG_STRATEGY_FILE_PATH
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_timestep,
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_freqstep,
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
-                    ""check_function"": self._check_compress,
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
-                    ""check_function"": self._check_flag_strategy,
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
-                    step_dict[key_lower][""value""] = value
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
+    FLAG_STRATEGY_FILE_PATH,
+    DEFAULT_FLAG_RFI,
+    DEFAULT_FLAG_MEMORYPERC
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_timestep
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_freqstep
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
+                    ""check_function"": self._check_compress
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
+                    ""check_function"": self._check_flag_strategy
+                },
+                ""flag_rfi"": {
+                    ""value"": None,
+                    ""default"": DEFAULT_FLAG_RFI,
+                    ""parsing_function"": (lambda x: bool(x)),
+                    ""check_function"": self._check_flag_rfi
+                },
+                ""flag_memoryperc"": {
+                    ""value"": DEFAULT_FLAG_MEMORYPERC, # This prevents any update from the parameter field
+                    ""default"": DEFAULT_FLAG_MEMORYPERC,
+                    ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_flag_memoryperc
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
+                    if step_dict[key_lower][""value""] is None:
+                        step_dict[key_lower][""value""] = value
+                    else:
+                        # The parameter has a fixed value that cannot be set
+                        break
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
+                # If the loop has not broken, it means the parameter is not expected
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
+    @staticmethod
+    def _check_flag_rfi(flag_rfi: bool) -> bool:
+        """""" """"""
+        return isinstance(flag_rfi, bool)
+
+
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
+    @staticmethod
+    def _check_flag_memoryperc(flag_memoryperc: int) -> bool:
+        """""" """"""
+        is_int = isinstance(flag_memoryperc, int)
+        is_percent = 0 <= flag_memoryperc <= 100
+        return is_int & is_percent
+
+
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
+from .netserialize import *
 from .dockernet import *
 from .dockershm import *
+from .dockernetserialize import *"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+import os
+
+def get_data():
+   testfile = ""airbnb.csv""
+   datadir = ""/data/""
+   currdir = os.path.dirname(os.path.abspath(__file__)) 
+   f = open(currdir + datadir + testfile, ""r"")
+   data = f.read()
+   return data
+
+if __name__ == ""__main__"":
+    get_data()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
-print(f""recv reading from {__file__}..."")
-print(f""recv data: {contents}"")
 f.close()"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
+print(f""recv: read from {__file__}"")
 f.close()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
-print(f""send writing to shared mem from {__file__}..."")
-f.write(""a very simple message for recv"")
 f.close()"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+from data import get_data
+
 f = open(""test.txt"", ""w+"")
+print(f""send: writing to shared mem from {__file__}..."")
+f.write(get_data())
 f.close()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
+def copy_data_to_volume(volume): 
+    data_script = ""data.py""
+    curr_dir = os.path.dirname(os.path.abspath(__file__))
+    data_dir = curr_dir + ""/data""
+    dst_dir = volume[""Mountpoint""]+f""/data""
+    if os.path.exists(dst_dir):
+        shutil.rmtree(dst_dir)
+    shutil.copytree(data_dir, dst_dir)
+    shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
+
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
+    copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
-def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
-    data = sendsock.recv(1024)
-    print(f""send {data}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
-    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
+from data import get_data
+
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
+def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
+
     sendsock.sendall(b""my simple message"")
+    
+    # wait for confirmation
+    recvdata = sendsock.recv(1024)
+    print(f""send: {recvdata}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
+    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
-def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
-    print(""send writing to shared mem..."")
-    f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
-    print(f""recv data: {contents}"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
-    sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
+from data import get_data
+
+def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
+    print(""send: writing to shared mem..."")
+    f.write(data)
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
+    print(f""recv: received data"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
+    data = get_data()
+    sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
+  VQSR: ""8g""
   OTHER: ""4g""
 
 ##############################"
KO;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['OTHER']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
-    resources: cpus=1, mem_mb=4000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
+        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['VQSR']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
+    resources: cpus=1, mem_mb=10000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
-# Specify type of NGS sequencing ('WES' or 'WGS'). Use for VQSR filtering
-DATA: ""WES""
-
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
-  MARK_DUP: ""40g""
-  HC: ""20g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
+# Specify type of NGS sequencing ('WES' or 'WGS'). Used for choosing memory. Also used for VQSR filtering.
+DATA: ""WES""
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
+  MARK_DUP_WES: ""35g""
+  MARK_DUP_WGS: ""70g""
+  HC_WES: ""9g""
+  HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 #### Set up report #####
 
 report: ""report/workflow.rst"""
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
+def get_bwa_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 15000
+    if config['DATA'] == ""WGS"":
+        return 50000
+    else:
+        return 15000
+
+def get_mkdup_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 40000
+    if config['DATA'] == ""WGS"":
+        return 80000
+    else:
+        return 40000
+
+def get_mkdup_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+
+def get_HC_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 10000
+    if config['DATA'] == ""WGS"":
+        return 20000
+    else:
+        return 10000
+
+def get_HC_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+        
 #### Set up report #####
 
 report: ""report/workflow.rst"""
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
-    resources: cpus=28, mem_mb=15000, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
+    resources: cpus=28, mem_mb=get_bwa_memory, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP']),
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
-    resources: cpus=28, mem_mb=40000, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
+        maxmemory = get_mkdup_xmx,
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
+    resources: cpus=28, mem_mb=get_mkdup_memory, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC']),
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
-    resources: cpus=1, mem_mb=20000, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
+        maxmemory = get_HC_xmx,
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
+    resources: cpus=1, mem_mb=get_HC_memory, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
KO;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
\ No newline at end of file"
OK;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
+Computer Architecture project to simulate a CPU, including memory and cache 
+
+1. What does the program do? 
+   Program will prompt user to point the program to two input files, the cpu instructions file as well as data input 
+
+2. What data do you need?
+   Program needs CPU instructions as well as data input to initialize memory bus 
+
+3. What aspects of a CPU can you simulate using Python?
+   The program can simulate 
+
+4. How will your CPU handle incoming instructions?
+   The cpu will receive the insructions from the cpu instructions file and run the appropriate ISA instructions
+
+5. How will your CPU output instructions?
+   CPU will output instructions to the terminal
+ 
+6. How will your CPU store data?
+   CPU will store the data in memory using the Memory class. 
\ No newline at end of file"
KO;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;
OK;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;"+MEMORY_BUS_SIZE = 128
+
+# This class implements a memory bus
+class Memory:
+    def __init__(self):
+        self.memory_bus = {}
+        self.initialize_memory_bus()
+
+    # Method to initialize memory bus with size based on MEMORY_BUS_SIZE variable
+    # Memory bus is setup as a dict with binary address being key, and value being value 
+    def initialize_memory_bus(self):
+        for i in range(MEMORY_BUS_SIZE):
+            self.memory_bus[f'{i:08b}'] = 0
+    
+    # Method to search for address in memory bus and return value if available 
+    def search_memory_bus(self, address):
+        if self.memory_bus.get(address) != None:
+            return self.memory_bus.get(address)
+        return None 
+    
+    # Method to write in memory bus only if address is within the bus size 
+    def write_memory_bus(self, address, value):
+        if self.memory_bus.get(address) != None:
+            self.memory_bus[address] = value"
KO;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
+  VQSR: ""8g""
   OTHER: ""4g""
 
 ##############################"
KO;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['OTHER']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
-    resources: cpus=1, mem_mb=4000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
+        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['VQSR']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
+    resources: cpus=1, mem_mb=10000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
-# Specify type of NGS sequencing ('WES' or 'WGS'). Use for VQSR filtering
-DATA: ""WES""
-
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
-  MARK_DUP: ""40g""
-  HC: ""20g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
+# Specify type of NGS sequencing ('WES' or 'WGS'). Used for choosing memory. Also used for VQSR filtering.
+DATA: ""WES""
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
+  MARK_DUP_WES: ""35g""
+  MARK_DUP_WGS: ""70g""
+  HC_WES: ""9g""
+  HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 #### Set up report #####
 
 report: ""report/workflow.rst"""
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
+def get_bwa_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 15000
+    if config['DATA'] == ""WGS"":
+        return 50000
+    else:
+        return 15000
+
+def get_mkdup_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 40000
+    if config['DATA'] == ""WGS"":
+        return 80000
+    else:
+        return 40000
+
+def get_mkdup_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+
+def get_HC_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 10000
+    if config['DATA'] == ""WGS"":
+        return 20000
+    else:
+        return 10000
+
+def get_HC_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+        
 #### Set up report #####
 
 report: ""report/workflow.rst"""
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
-    resources: cpus=28, mem_mb=15000, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
+    resources: cpus=28, mem_mb=get_bwa_memory, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP']),
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
-    resources: cpus=28, mem_mb=40000, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
+        maxmemory = get_mkdup_xmx,
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
+    resources: cpus=28, mem_mb=get_mkdup_memory, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC']),
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
-    resources: cpus=1, mem_mb=20000, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
+        maxmemory = get_HC_xmx,
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
+    resources: cpus=1, mem_mb=get_HC_memory, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
KO;42;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
-  # sage.interfaces.gap.gap_cmd = 'gap -r -o 24G '
-  sage.interfaces.gap.gap_cmd = 'gap -r -o 50G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
-    set_gap_memory_pool_size(50* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
OK;42;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
+  sage.interfaces.gap.gap_cmd = 'gap -r -o 256G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
+    set_gap_memory_pool_size(256* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
KO;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"def evaluate_samples(self,p,model,forwarding='head',knn=100):
             ri, ci = assign_classes_hungarian(C)
             accuracy = accuracy_from_assignment(C,ri,ci)
             print('Accuracy: ',accuracy)
-            # ------------------------------------------------
 
             feature_tensor = torch.nn.functional.normalize(feature_tensor, dim = 1)
-            similarity_matrix = torch.einsum('nd,cd->nc', [feature_tensor, feature_tensor]) # removed .cpu()
 
             #self.knn = knn
-            scores, idx_k = similarity_matrix.topk(k=knn, dim=1)
             #self.proximity = torch.mean(scores_k,dim=1)
             #self.kNN_indices = idx_k
             labels_topk = torch.zeros_like(idx_k)"
OK;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"def evaluate_samples(self,p,model,forwarding='head',knn=100):
             ri, ci = assign_classes_hungarian(C)
             accuracy = accuracy_from_assignment(C,ri,ci)
             print('Accuracy: ',accuracy)
+        # ------------------------------------------------
 
             feature_tensor = torch.nn.functional.normalize(feature_tensor, dim = 1)
+
+            idx_list = []
+            for i in range(len(feature_tensor)):
+                feature = torch.unsqueeze(feature_tensor[i],dim=0)
+                similarities = torch.mm(feature,feature_tensor.t())
+                scores, idx_ = similarities.topk(k=knn, dim=1)
+                idx_list.append(idx_)
+            idx_k = torch.cat(idx_list)
+            #similarity_matrix = torch.einsum('nd,cd->nc', [feature_tensor, feature_tensor]) # removed .cpu()
 
             #self.knn = knn
+            #scores, idx_k = similarity_matrix.topk(k=knn, dim=1)
             #self.proximity = torch.mean(scores_k,dim=1)
             #self.kNN_indices = idx_k
             labels_topk = torch.zeros_like(idx_k)"
KO;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);
OK;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""afd3e0db"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = tv_res18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""dcd1c27d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=1000, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 1,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torchvision\n"",
+    ""\n"",
+    ""resnet18 = torchvision.models.resnet18()\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""c9ecd56d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=10, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 2,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch.nn as nn\n"",
+    ""resnet18.fc = nn.Linear(512,10)\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""511ae48e"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""demoBatch = torch.rand([4,3,32,32])\n"",
+    ""modelSample = resnet18(demoBatch)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""13b59406"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([4, 10])""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""modelSample.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""f545a7ad"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = singleSoftmaxUse""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""19283bac"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""soft = torch.nn.Softmax(dim = 1) # create function class instance <callable>""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""928045a9"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test = soft(modelSample) # softmax""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""70fb3bbe"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor(1.0000, grad_fn=<AddBackward0>)""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""sum(test[0])""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""0911bd73"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = matrixC""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""915c9184"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([128])""
+      ]
+     },
+     ""execution_count"": 12,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""dummy_features = torch.rand([105000,128])\n"",
+    ""dummy_features = torch.nn.functional.normalize(dummy_features, dim = 1)\n"",
+    ""single_feature = dummy_features[0]\n"",
+    ""single_feature.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 16,
+   ""id"": ""063902c3"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""105000""
+      ]
+     },
+     ""execution_count"": 16,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(dummy_features)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""ad491fdd"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 105000])""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""matmul = torch.mm(torch.unsqueeze(single_feature,dim=0),dummy_features.t())\n"",
+    ""matmul.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""4f6da2a1"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 100])""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores, idx_k = matmul.topk(k=100, dim=1)\n"",
+    ""idx_k.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 15,
+   ""id"": ""397dd28a"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor([[1.0000, 0.8648, 0.8608, 0.8602, 0.8599, 0.8583, 0.8579, 0.8547, 0.8541,\n"",
+       ""         0.8538, 0.8535, 0.8535, 0.8532, 0.8530, 0.8523, 0.8522, 0.8518, 0.8518,\n"",
+       ""         0.8514, 0.8513, 0.8507, 0.8501, 0.8499, 0.8495, 0.8494, 0.8491, 0.8486,\n"",
+       ""         0.8485, 0.8482, 0.8478, 0.8462, 0.8459, 0.8459, 0.8458, 0.8458, 0.8455,\n"",
+       ""         0.8451, 0.8449, 0.8449, 0.8448, 0.8448, 0.8446, 0.8446, 0.8443, 0.8442,\n"",
+       ""         0.8441, 0.8441, 0.8441, 0.8440, 0.8440, 0.8437, 0.8437, 0.8436, 0.8435,\n"",
+       ""         0.8433, 0.8433, 0.8432, 0.8432, 0.8432, 0.8431, 0.8430, 0.8430, 0.8429,\n"",
+       ""         0.8428, 0.8426, 0.8423, 0.8422, 0.8422, 0.8422, 0.8422, 0.8421, 0.8421,\n"",
+       ""         0.8421, 0.8420, 0.8420, 0.8418, 0.8418, 0.8417, 0.8415, 0.8415, 0.8414,\n"",
+       ""         0.8414, 0.8414, 0.8413, 0.8413, 0.8413, 0.8411, 0.8411, 0.8410, 0.8408,\n"",
+       ""         0.8408, 0.8408, 0.8407, 0.8407, 0.8407, 0.8407, 0.8406, 0.8405, 0.8405,\n"",
+       ""         0.8405]])""
+      ]
+     },
+     ""execution_count"": 15,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5b5e42ed"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python 3 (ipykernel)"",
+   ""language"": ""python"",
+   ""name"": ""python3""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.7.10""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);
OK;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""afd3e0db"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = tv_res18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""dcd1c27d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=1000, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 1,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torchvision\n"",
+    ""\n"",
+    ""resnet18 = torchvision.models.resnet18()\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""c9ecd56d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=10, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 2,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch.nn as nn\n"",
+    ""resnet18.fc = nn.Linear(512,10)\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""511ae48e"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""demoBatch = torch.rand([4,3,32,32])\n"",
+    ""modelSample = resnet18(demoBatch)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""13b59406"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([4, 10])""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""modelSample.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""f545a7ad"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = singleSoftmaxUse""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""19283bac"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""soft = torch.nn.Softmax(dim = 1) # create function class instance <callable>""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""928045a9"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test = soft(modelSample) # softmax""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""70fb3bbe"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor(1.0000, grad_fn=<AddBackward0>)""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""sum(test[0])""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""0911bd73"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = matrixC""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""915c9184"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([128])""
+      ]
+     },
+     ""execution_count"": 12,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""dummy_features = torch.rand([105000,128])\n"",
+    ""dummy_features = torch.nn.functional.normalize(dummy_features, dim = 1)\n"",
+    ""single_feature = dummy_features[0]\n"",
+    ""single_feature.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 16,
+   ""id"": ""063902c3"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""105000""
+      ]
+     },
+     ""execution_count"": 16,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(dummy_features)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""ad491fdd"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 105000])""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""matmul = torch.mm(torch.unsqueeze(single_feature,dim=0),dummy_features.t())\n"",
+    ""matmul.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""4f6da2a1"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 100])""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores, idx_k = matmul.topk(k=100, dim=1)\n"",
+    ""idx_k.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 15,
+   ""id"": ""397dd28a"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor([[1.0000, 0.8648, 0.8608, 0.8602, 0.8599, 0.8583, 0.8579, 0.8547, 0.8541,\n"",
+       ""         0.8538, 0.8535, 0.8535, 0.8532, 0.8530, 0.8523, 0.8522, 0.8518, 0.8518,\n"",
+       ""         0.8514, 0.8513, 0.8507, 0.8501, 0.8499, 0.8495, 0.8494, 0.8491, 0.8486,\n"",
+       ""         0.8485, 0.8482, 0.8478, 0.8462, 0.8459, 0.8459, 0.8458, 0.8458, 0.8455,\n"",
+       ""         0.8451, 0.8449, 0.8449, 0.8448, 0.8448, 0.8446, 0.8446, 0.8443, 0.8442,\n"",
+       ""         0.8441, 0.8441, 0.8441, 0.8440, 0.8440, 0.8437, 0.8437, 0.8436, 0.8435,\n"",
+       ""         0.8433, 0.8433, 0.8432, 0.8432, 0.8432, 0.8431, 0.8430, 0.8430, 0.8429,\n"",
+       ""         0.8428, 0.8426, 0.8423, 0.8422, 0.8422, 0.8422, 0.8422, 0.8421, 0.8421,\n"",
+       ""         0.8421, 0.8420, 0.8420, 0.8418, 0.8418, 0.8417, 0.8415, 0.8415, 0.8414,\n"",
+       ""         0.8414, 0.8414, 0.8413, 0.8413, 0.8413, 0.8411, 0.8411, 0.8410, 0.8408,\n"",
+       ""         0.8408, 0.8408, 0.8407, 0.8407, 0.8407, 0.8407, 0.8406, 0.8405, 0.8405,\n"",
+       ""         0.8405]])""
+      ]
+     },
+     ""execution_count"": 15,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5b5e42ed"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python 3 (ipykernel)"",
+   ""language"": ""python"",
+   ""name"": ""python3""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.7.10""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
OK;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;#NOM?
KO;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
OK;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;#NOM?
KO;1;abishekmuthian;memory-hammer;1d88f9e750a7570d2d52a849aec7daa0920315bb;Updated README.md with link to needgap on the problem statement and highlighted the memory_hammer.py file name.;"An always-on Anki review system.
 Click the above image for a video demo.
 
 ### Why
-To solve Anki review accumulation by making the cards available for review when its due using always-on display system.
 
 ### How
 Using e-paper display attached to a raspberry pi and memory-hammer software.
@@ -28,7 +28,7 @@ Using e-paper display attached to a raspberry pi and memory-hammer software.
 5. pip3.10 install -r requirements.txt
 
 ### Usage
-1. Edit the **Config** section of the memory_hammer.py with the IP address of your Anki Desktop and port for Anki Connect.
 2. python3.10 memory_hammer.py
 
 #### Download the Decks using Get Decks"
OK;1;abishekmuthian;memory-hammer;1d88f9e750a7570d2d52a849aec7daa0920315bb;Updated README.md with link to needgap on the problem statement and highlighted the memory_hammer.py file name.;"An always-on Anki review system.
 Click the above image for a video demo.
 
 ### Why
+To solve Anki review accumulation by making the cards available for review when its due using always-on display system. By extension addressing [Human Memory, lack of thereof](https://needgap.com/problems/41-human-memory-lack-of-thereof-psychology-neuroscience).
 
 ### How
 Using e-paper display attached to a raspberry pi and memory-hammer software.
@@ -28,7 +28,7 @@ Using e-paper display attached to a raspberry pi and memory-hammer software.
 5. pip3.10 install -r requirements.txt
 
 ### Usage
+1. Edit the **Config** section of the **memory_hammer.py** with the IP address of your Anki Desktop and port for Anki Connect.
 2. python3.10 memory_hammer.py
 
 #### Download the Decks using Get Decks"
KO;1;abishekmuthian;memory-hammer;9f099d913697c50d8fd2cb3c8311d60f0658a903;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;
OK;1;abishekmuthian;memory-hammer;9f099d913697c50d8fd2cb3c8311d60f0658a903;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+                    GNU AFFERO GENERAL PUBLIC LICENSE
+                       Version 3, 19 November 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU Affero General Public License is a free, copyleft license for
+software and other kinds of works, specifically designed to ensure
+cooperation with the community in the case of network server software.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+our General Public Licenses are intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  Developers that use our General Public Licenses protect your rights
+with two steps: (1) assert copyright on the software, and (2) offer
+you this License which gives you legal permission to copy, distribute
+and/or modify the software.
+
+  A secondary benefit of defending all users' freedom is that
+improvements made in alternate versions of the program, if they
+receive widespread use, become available for other developers to
+incorporate.  Many developers of free software are heartened and
+encouraged by the resulting cooperation.  However, in the case of
+software used on network servers, this result may fail to come about.
+The GNU General Public License permits making a modified version and
+letting the public access it on a server without ever releasing its
+source code to the public.
+
+  The GNU Affero General Public License is designed specifically to
+ensure that, in such cases, the modified source code becomes available
+to the community.  It requires the operator of a network server to
+provide the source code of the modified version running there to the
+users of that server.  Therefore, public use of a modified version, on
+a publicly accessible server, gives the public access to the source
+code of the modified version.
+
+  An older license, called the Affero General Public License and
+published by Affero, was designed to accomplish similar goals.  This is
+a different license, not a version of the Affero GPL, but Affero has
+released a new version of the Affero GPL which permits relicensing under
+this license.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  ""This License"" refers to version 3 of the GNU Affero General Public License.
+
+  ""Copyright"" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  ""The Program"" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as ""you"".  ""Licensees"" and
+""recipients"" may be individuals or organizations.
+
+  To ""modify"" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a ""modified version"" of the
+earlier work or a work ""based on"" the earlier work.
+
+  A ""covered work"" means either the unmodified Program or a work based
+on the Program.
+
+  To ""propagate"" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To ""convey"" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays ""Appropriate Legal Notices""
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The ""source code"" for a work means the preferred form of the work
+for making modifications to it.  ""Object code"" means any non-source
+form of a work.
+
+  A ""Standard Interface"" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The ""System Libraries"" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+""Major Component"", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The ""Corresponding Source"" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    ""keep intact all notices"".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+""aggregate"" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A ""User Product"" is either (1) a ""consumer product"", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, ""normally used"" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  ""Installation Information"" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  ""Additional permissions"" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered ""further
+restrictions"" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An ""entity transaction"" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A ""contributor"" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's ""contributor version"".
+
+  A contributor's ""essential patent claims"" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, ""control"" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a ""patent license"" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To ""grant"" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  ""Knowingly relying"" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is ""discriminatory"" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Remote Network Interaction; Use with the GNU General Public License.
+
+  Notwithstanding any other provision of this License, if you modify the
+Program, your modified version must prominently offer all users
+interacting with it remotely through a computer network (if your version
+supports such interaction) an opportunity to receive the Corresponding
+Source of your version by providing access to the Corresponding Source
+from a network server at no charge, through some standard or customary
+means of facilitating copying of software.  This Corresponding Source
+shall include the Corresponding Source for any work covered by version 3
+of the GNU General Public License that is incorporated pursuant to the
+following paragraph.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the work with which it is combined will remain governed by version
+3 of the GNU General Public License.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU Affero General Public License from time to time.  Such new versions
+will be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU Affero General
+Public License ""or any later version"" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU Affero General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU Affero General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ""AS IS"" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most clo"
KO;1;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
-        M, G, T to specify MiB, GiB, or TiB, respectively.
-        Values without suffix are interpreted as bytes.
         Values are rounded to the closest MiB.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
-def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
@@ -60,29 +60,84 @@ def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
-def max_memory(max_memory):
-    """"""Interpret the config parameter --max-memory.""""""
     try:
-        matches = re.match(r""(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[%MGT])?"", max_memory)
         value = float(matches[""value""])
         unit = matches[""unit""]
-        if unit == ""%"":
-            max_memory = share_of_ram(share=(value / 100.0))
-        else:
-            # convert to MiB
-            if unit is None:
-                value *= 2**-10
-                if value < 1:
-                    value = 1
-            # elif unit == ""M"":
-            #    value *= 2 ** 1
-            elif unit == ""G"":
-                value *= 2**10
-            elif unit == ""T"":
-                value *= 2**20
-            max_memory = round(value)
     except TypeError:
-        raise ValueError(f""Could not interpret --max-memory: {max_memory}"")
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
@@ -95,4 +150,4 @@ def max_memory(max_memory):
     return max_memory
 
 
-MAX_JVM_MEMORY = max_memory(arguments.max_memory)"
OK;1;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
+        K, M, G, T to specify KiB, MiB, GiB, or TiB, respectively.
         Values are rounded to the closest MiB.
+        Values without suffix are interpreted as bytes.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
+def _share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
@@ -60,29 +60,84 @@ def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
+def _parse_max_memory_string(max_memory):
+    """"""
+    Extract maximum memory value and unit from text input.
+
+    Arguments
+    ---------
+    max_memory : str
+        Input text from the config parameter --max-memory.
+
+    Returns
+    -------
+    tuple: a tuple containing
+        - value (float): Amount of memory to be allocated in a given unit.
+        - unit (str): The unit of memory.
+    """"""
     try:
+        matches = re.match(
+            r""^(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[^0-9])?$"", max_memory
+        )
         value = float(matches[""value""])
         unit = matches[""unit""]
+
+        if unit is not None and unit not in ""%KMGT"":
+            raise ValueError(
+                ""Could not interpret the memory unit from --max-memory.""
+                ""The suffix for --max-memory should be '%', 'K', 'M', 'G' or 'T'.""
+                ""For example to allocate five gigabytes of memory, use: '5G'""
+            )
+        return value, unit
     except TypeError:
+        raise ValueError(
+            f""Could not interpret --max-memory: {max_memory}.""
+            f""To allocate memory, use e.g. '5G' for five gigabytes of memory.""
+        )
+
+
+def _get_max_memory(max_memory):
+    """"""
+    Interpret the config parameter --max-memory.
+
+    Arguments
+    ---------
+
+    max_memory : str
+        Memory limit for the JVM running R5.
+
+        Use % as a suffix to specify a share of total RAM;
+        K, M, G, T suffix specify KiB, MiB, GiB, or TiB, respectively.
+        Values are rounded to the closest MiB.
+        Values without suffix are interpreted as bytes.
+
+    Returns
+    -------
+    float
+        Maximum amount of memory allocated for R5 in MiB.
+    """"""
+
+    value, unit = _parse_max_memory_string(max_memory)
+
+    if unit == ""%"":
+        max_memory = _share_of_ram(share=(value / 100.0))
+    else:
+        # convert to MiB
+        if unit is None:
+            value *= 2**-20
+        elif unit == ""K"":
+            value *= 2**-10
+        elif unit == ""M"":
+            value *= 2**1
+        elif unit == ""G"":
+            value *= 2**10
+        elif unit == ""T"":
+            value *= 2**20
+
+        if value < 1:
+            value = 1
+
+        max_memory = round(value)
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
@@ -95,4 +150,4 @@ def max_memory(max_memory):
     return max_memory
 
 
+MAX_JVM_MEMORY = _get_max_memory(arguments.max_memory)"
KO;1;facebookresearch;metaseq;ca180bb6474d0e55051e2bb3db9af69ddb3725e3;"fix off by one in API (#145)

* making prompt_len independent of batchfy implmentation

* modify for echo=True case

* returning logprobs, to support logprob input

* setting need_logprobs depends on each request to save memory";"def generate(
             self.cfg.generation.max_len_a = 0
 
             logger.info(f""Preparing generator with settings {self.cfg.generation}"")
             generator = self.task.build_generator(
-                self.models, self.cfg.generation, extra_gen_cls_kwargs={""stop"": stop}
             )
 
             # okay actually generate
@@ -624,23 +627,19 @@ def generate(
                     tokens, scores, distributions = GeneratorInterface._filter_special(
                         tokens, scores, distributions
                     )
-                    prompt_len = src_lengths[i]
                     if echo:
                         # don't cut off prompt
-                        tokens = tokens[: prompt_len + max_tokens[i] - 1]
-                        scores = scores[: prompt_len + max_tokens[i] - 1]
                         if logprobs > 0:
-                            distributions = distributions[
-                                : prompt_len + max_tokens[i] - 1
-                            ]
                     else:
                         # cut off prompt
-                        tokens = tokens[prompt_len - 1 :][: max_tokens[i]]
-                        scores = scores[prompt_len - 1 :][: max_tokens[i]]
                         if logprobs > 0:
-                            distributions = distributions[prompt_len - 1 :][
-                                : max_tokens[i]
-                            ]
                     # turn it into a string
                     text = self.bpe.bpe.decode(tokens)
                     # re-encode it so we get offsets"
OK;1;facebookresearch;metaseq;ca180bb6474d0e55051e2bb3db9af69ddb3725e3;"fix off by one in API (#145)

* making prompt_len independent of batchfy implmentation

* modify for echo=True case

* returning logprobs, to support logprob input

* setting need_logprobs depends on each request to save memory";"def generate(
             self.cfg.generation.max_len_a = 0
 
             logger.info(f""Preparing generator with settings {self.cfg.generation}"")
+            need_logprobs = True if logprobs > 0 else False
             generator = self.task.build_generator(
+                self.models,
+                self.cfg.generation,
+                extra_gen_cls_kwargs={""stop"": stop, ""need_logprobs"": need_logprobs},
             )
 
             # okay actually generate
@@ -624,23 +627,19 @@ def generate(
                     tokens, scores, distributions = GeneratorInterface._filter_special(
                         tokens, scores, distributions
                     )
+                    prompt_len = lengths[i]
                     if echo:
                         # don't cut off prompt
+                        tokens = tokens[: prompt_len + max_tokens[i]]
+                        scores = scores[: prompt_len + max_tokens[i]]
                         if logprobs > 0:
+                            distributions = distributions[: prompt_len + max_tokens[i]]
                     else:
                         # cut off prompt
+                        tokens = tokens[prompt_len:][: max_tokens[i]]
+                        scores = scores[prompt_len:][: max_tokens[i]]
                         if logprobs > 0:
+                            distributions = distributions[prompt_len:][: max_tokens[i]]
                     # turn it into a string
                     text = self.bpe.bpe.decode(tokens)
                     # re-encode it so we get offsets"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.identity_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -84,11 +84,11 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.random_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -140,12 +140,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.level_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            randomness=self.randomness,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -204,12 +206,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.circular_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            randomness=self.randomness,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()"
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.identity_hv(
+                self.num_embeddings, self.embedding_dim, **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -84,11 +84,11 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.random_hv(
+                self.num_embeddings, self.embedding_dim, **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -140,12 +140,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.level_hv(
+                self.num_embeddings,
+                self.embedding_dim,
+                randomness=self.randomness,
+                **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -204,12 +206,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.circular_hv(
+                self.num_embeddings,
+                self.embedding_dim,
+                randomness=self.randomness,
+                **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
-    def __init__(self, threshold=0.0):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
@@ -82,7 +82,7 @@ def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
-            raise IndexError()
 
         return index
 
@@ -241,7 +241,7 @@ def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
-        """"""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
@@ -273,7 +273,7 @@ def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
-        value = functional.multiset(input, dim=-2)
         return cls(value, size=input.size(-2))
 
 
@@ -434,7 +434,7 @@ def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
-        return cls(value, size=input.size(-2))
 
 
 class Sequence:
@@ -663,7 +663,9 @@ def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
-            self.value = torch.zeros(dim_or_input, dtype=dtype, device=device)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
@@ -766,7 +768,7 @@ def clear(self) -> None:
             >>> DS.clear()
 
         """"""
-        self.value.fill_(0.0)
         self.size = 0
 
     @classmethod"
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
+    def __init__(self, threshold=0.5):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
@@ -82,7 +82,7 @@ def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
+            raise IndexError(""No elements in memory"")
 
         return index
 
@@ -241,7 +241,7 @@ def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
+        r""""""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
@@ -273,7 +273,7 @@ def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
+        value = functional.multiset(input)
         return cls(value, size=input.size(-2))
 
 
@@ -434,7 +434,7 @@ def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
+        return cls(value, size=keys.size(-2))
 
 
 class Sequence:
@@ -663,7 +663,9 @@ def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
+            self.value = functional.identity_hv(
+                1, dim_or_input, dtype=dtype, device=device
+            ).squeeze(0)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
@@ -766,7 +768,7 @@ def clear(self) -> None:
             >>> DS.clear()
 
         """"""
+        self.value.fill_(1.0)
         self.size = 0
 
     @classmethod"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestDistinctSequence:
+    def test_creation_dim(self):
+        S = structures.DistinctSequence(10000)
+        assert torch.equal(S.value, torch.ones(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        S = structures.DistinctSequence(hv[0])
+        assert torch.equal(S.value, hv[0])
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_append(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+
+    def test_appendleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.appendleft(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+
+    def test_pop(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[1])
+        S.pop(hv[1])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.pop(hv[0])
+        S.append(hv[2])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+        S.append(hv[3])
+        S.pop(hv[3])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+
+    def test_popleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.appendleft(hv[0])
+        S.appendleft(hv[1])
+        S.popleft(hv[1])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.popleft(hv[0])
+        S.appendleft(hv[2])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+        S.appendleft(hv[3])
+        S.popleft(hv[3])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+
+    def test_replace(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.replace(0, hv[0], hv[1])
+        assert functional.cosine_similarity(S.value, hv)[1] > 0.5
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        assert len(S) == 4
+        S.pop(hv[0])
+        S.pop(hv[0])
+        S.pop(hv[0])
+        assert len(S) == 1
+        S.pop(hv[0])
+        assert len(S) == 0
+        S.append(hv[0])
+        assert len(S) == 1
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        assert len(S) == 4
+        S.clear()
+        assert len(S) == 0"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+seed1 = 2147483643
+letters = list(string.ascii_lowercase)
+
+
+class TestFSA:
+    def test_creation_dim(self):
+        F = structures.FiniteStateAutomata(10000)
+        assert torch.equal(F.value, torch.zeros(10000))
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_transition(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        actions = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], actions[1], actions[2])
+        assert torch.equal(
+            F.value,
+            torch.tensor([1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0]),
+        )
+        F.add_transition(tokens[1], actions[1], actions[3])
+        assert torch.equal(
+            F.value, torch.tensor([0.0, 0.0, -2.0, 2.0, 0.0, 2.0, 0.0, -2.0, -2.0, 0.0])
+        )
+        F.add_transition(tokens[2], actions[1], actions[3])
+        assert torch.equal(
+            F.value,
+            torch.tensor([1.0, 1.0, -3.0, 1.0, 1.0, 3.0, -1.0, -1.0, -1.0, 1.0]),
+        )
+
+    def test_transition(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        states = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], states[1], states[2])
+        F.add_transition(tokens[1], states[1], states[3])
+        F.add_transition(tokens[2], states[1], states[5])
+
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[0]), states)
+            ).item()
+            == 2
+        )
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[1]), states)
+            ).item()
+            == 3
+        )
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[2]), states)
+            ).item()
+            == 5
+        )
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        states = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], states[1], states[2])
+        F.add_transition(tokens[1], states[1], states[3])
+        F.add_transition(tokens[2], states[1], states[5])
+
+        F.clear()
+        assert torch.equal(
+            F.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestGraph:
+    def test_creation_dim(self):
+        G = structures.Graph(10000, directed=True)
+        assert torch.equal(G.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        g = functional.bind(hv[0], hv[1])
+        G = structures.Graph(g)
+        assert torch.equal(G.value, g)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_edge(self):
+        G = structures.Graph(8)
+        hv = torch.tensor(
+            [
+                [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
+                [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
+                [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
+                [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
+            ]
+        )
+
+        G.add_edge(hv[0], hv[1])
+        assert torch.equal(
+            G.value, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        G.add_edge(hv[2], hv[3])
+        assert torch.equal(
+            G.value, torch.tensor([-2.0, -2.0, 0.0, 2.0, -2.0, 0.0, 2.0, -2.0])
+        )
+
+        GD = structures.Graph(8, directed=True)
+
+        GD.add_edge(hv[0], hv[1])
+        assert torch.equal(
+            GD.value, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        GD.add_edge(hv[2], hv[3])
+        assert torch.equal(
+            GD.value, torch.tensor([0.0, 0.0, 0.0, -2.0, 0.0, -2.0, 2.0, -2.0])
+        )
+
+    def test_encode_edge(self):
+        G = structures.Graph(8)
+        hv = torch.tensor(
+            [
+                [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
+                [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
+                [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
+                [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
+            ]
+        )
+
+        e1 = G.encode_edge(hv[0], hv[1])
+        assert torch.equal(
+            e1, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        e2 = G.encode_edge(hv[2], hv[3])
+        assert torch.equal(
+            e2, torch.tensor([-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0])
+        )
+
+        GD = structures.Graph(8, directed=True)
+
+        e1 = GD.encode_edge(hv[0], hv[1])
+        assert torch.equal(
+            e1, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        e2 = GD.encode_edge(hv[2], hv[3])
+        print(e2)
+        assert torch.equal(
+            e2, torch.tensor([1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0])
+        )
+
+    def test_node_neighbors(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(10, 10000, generator=generator)
+        G = structures.Graph(10000, directed=True)
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(G.node_neighbors(hv[1]), hv)
+            ).item()
+            == 2
+        )
+        assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[1] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[1] < 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[0] < 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[0] < 0.5
+
+        G1 = structures.Graph(10000, directed=False)
+
+        G1.add_edge(hv[0], hv[1])
+        G1.add_edge(hv[0], hv[2])
+        G1.add_edge(hv[1], hv[2])
+        assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[0] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[1] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[0] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[1] > 0.5
+
+    def test_contains(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(4, 8, generator=generator)
+        G = structures.Graph(8)
+
+        e1 = G.encode_edge(hv[0], hv[1])
+        e2 = G.encode_edge(hv[0], hv[2])
+        e3 = G.encode_edge(hv[2], hv[3])
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        assert G.contains(e1) > torch.tensor(0.6)
+        assert G.contains(e2) > torch.tensor([0.6])
+        assert G.contains(e3) < torch.tensor(0.6)
+
+        GD = structures.Graph(8, directed=True)
+
+        ee1 = GD.encode_edge(hv[0], hv[1])
+        ee2 = GD.encode_edge(hv[0], hv[2])
+        ee3 = GD.encode_edge(hv[2], hv[3])
+        ee4 = GD.encode_edge(hv[1], hv[0])
+
+        GD.add_edge(hv[0], hv[1])
+        GD.add_edge(hv[0], hv[2])
+        GD.add_edge(hv[3], hv[1])
+
+        assert GD.contains(ee1) > torch.tensor(0.6)
+        assert GD.contains(ee2) > torch.tensor(0.6)
+        assert GD.contains(ee3) < torch.tensor(0.6)
+        assert GD.contains(ee4) < torch.tensor(0.6)
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(4, 8, generator=generator)
+        G = structures.Graph(8)
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        G.clear()
+
+        assert torch.equal(
+            G.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed_key = 2147483644
+seed_value = 2147483622
+letters = list(string.ascii_lowercase)
+
+
+class TestHashtable:
+    def test_creation_dim(self):
+        H = structures.HashTable(10000)
+        assert torch.equal(H.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        hash_v1 = functional.bind(keys_hv[0], values_hv[0])
+        hash_v2 = functional.bind(keys_hv[1], values_hv[1])
+        hasht = functional.bundle(hash_v1, hash_v2)
+
+        H = structures.HashTable(hasht)
+        assert torch.equal(H.value, hasht)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed_key)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed_key)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+
+    def test_remove(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+
+        H.remove(keys_hv[0], values_hv[0])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) < 0.2)[0],
+            torch.tensor(True),
+        )
+
+    def test_get(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[0]), values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[1]), values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[2]), values_hv) > 0.5)[2],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            torch.all(
+                (functional.cosine_similarity(H.get(values_hv[2]), values_hv) > 0.5)
+                == False
+            ),
+            torch.tensor(True),
+        )
+
+    def test_getitem(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[2]], values_hv) > 0.5)[2],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            torch.all(
+                (functional.cosine_similarity(H[values_hv[2]], values_hv) > 0.5)
+                == False
+            ),
+            torch.tensor(True),
+        )
+
+    def test_replace(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        H.replace(keys_hv[0], values_hv[0], values_hv[1])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+
+    def test_length(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert len(H) == 3
+        H.remove(keys_hv[0], values_hv[0])
+
+        assert len(H) == 2
+
+    def test_clear(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+        assert len(H) == 3
+        H.clear()
+        assert len(H) == 0
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(False),
+        )
+        H.add(keys_hv[0], values_hv[0])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+
+    def test_from_tensor(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(2, 3, generator=generator_key)
+
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(2, 3, generator=generator_value)
+
+        H = structures.HashTable.from_tensors(keys_hv, values_hv)
+        assert torch.equal(H.value, torch.tensor([2.0, 0.0, 0.0]))"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestMemory:
+    def test_creation(self):
+        M = structures.Memory()
+
+        assert M.keys == []
+        assert M.values == []
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert torch.equal(M.keys[0], keys_hv[0])
+        assert torch.equal(M.keys[1], keys_hv[1])
+        assert torch.equal(M.keys[2], keys_hv[2])
+        assert M.values[0] == letters[0]
+        assert M.values[1] == letters[1]
+        assert M.values[2] == letters[2]
+
+    def test_index(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert M.index(keys_hv[0]) == 0
+        assert M.index(keys_hv[1]) == 1
+        assert M.index(keys_hv[2]) == 2
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        del M[keys_hv[0]]
+
+        assert len(M) == 2
+
+        M.add(keys_hv[0], letters[0])
+        assert len(M) == 3
+
+    def test_getitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+    def test_setitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+        M[keys_hv[0]] = letters[3]
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[3]
+
+    def test_delitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+        del M[keys_hv[0]]
+        try:
+            M[keys_hv[0]]
+        except IndexError:
+            assert True
+
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+        assert len(M) == 2"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestMultiset:
+    def test_creation_dim(self):
+        M = structures.Multiset(10000)
+        assert torch.equal(M.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+        multiset = functional.multiset(keys_hv)
+
+        M = structures.Multiset(multiset)
+        assert torch.equal(M.value, multiset)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        assert torch.equal(M.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        M.add(keys_hv[1])
+        assert torch.equal(M.value, torch.tensor([2.0, 0.0, 0.0, 2.0]))
+
+        M.add(keys_hv[2])
+        assert torch.equal(M.value, torch.tensor([3.0, 1.0, 1.0, 1.0]))
+
+    def test_remove(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        assert M.contains(keys_hv[0]) > torch.tensor([0.5])
+
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        assert M.contains(keys_hv[1]) > torch.tensor([0.5])
+        assert M.remove(keys_hv[0]) is None
+
+    def test_contains(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.7])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        M.remove(keys_hv[1])
+        assert M.contains(keys_hv[1]) < torch.tensor([0.1])
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        assert len(M) == 3
+        M.remove(keys_hv[0])
+
+        assert len(M) == 2
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        M.clear()
+
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        assert M.contains(keys_hv[1]) < torch.tensor([0.1])
+
+        M.add(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+
+    def test_from_ngrams(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 3, generator=generator)
+        M = structures.Multiset.from_ngrams(keys_hv)
+
+        assert torch.equal(M.value, torch.tensor([0.0, 4.0, 0.0]))
+
+    def test_from_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset.from_tensor(keys_hv)
+        assert torch.equal(M.value, torch.tensor([2.0, 10.0, 4.0, 2.0]))"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestSequence:
+    def test_creation_dim(self):
+        S = structures.Sequence(10000)
+        assert torch.equal(S.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        seq = functional.bundle(hv[1], functional.permute(hv[0], shifts=1))
+
+        S = structures.Sequence(seq)
+        assert torch.equal(S.value, seq)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_append(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.append(hv[0])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.append(hv[1])
+        assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
+
+        S.append(hv[2])
+        assert torch.equal(S.value, torch.tensor([3.0, 3.0, 3.0, -3.0]))
+
+    def test_appendleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.appendleft(hv[0])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.appendleft(hv[1])
+        assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
+
+        S.appendleft(hv[2])
+        assert torch.equal(S.value, torch.tensor([3.0, -1.0, 3.0, 1.0]))
+
+    def test_pop(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        S.pop(hv[2])
+        assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
+
+        S.pop(hv[1])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.pop(hv[0])
+        assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
+
+    def test_popleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.appendleft(hv[0])
+        S.appendleft(hv[1])
+        S.appendleft(hv[2])
+
+        S.popleft(hv[2])
+        assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
+
+        S.popleft(hv[1])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.popleft(hv[0])
+        assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
+
+    def test_replace(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.Sequence(10000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+        S.append(hv[3])
+        S.append(hv[4])
+        S.append(hv[5])
+        S.append(hv[6])
+
+        assert functional.cosine_similarity(S[2], hv)[2] > 0.35
+        S.replace(2, hv[2], hv[6])
+        assert functional.cosine_similarity(S[2], hv)[2] < 0.35
+        assert functional.cosine_similarity(S[2], hv)[6] > 0.35
+
+        hv1 = functional.random_hv(10, 10000)
+        S2 = structures.Sequence.from_tensor(hv1)
+        assert functional.cosine_similarity(S2[2], hv1)[2] > 0.3
+        S2.replace(2, hv1[2], hv1[6])
+        assert functional.cosine_similarity(S2[2], hv1)[2] < 0.3
+        assert functional.cosine_similarity(S2[2], hv1)[6] > 0.3
+
+    def test_concat(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        S2 = structures.Sequence(1000)
+        S2.append(hv[0])
+        S2.append(hv[1])
+        S2.append(hv[2])
+
+        assert len(S) == 3
+        assert len(S2) == 3
+        S = S.concat(S2)
+        assert len(S) == 6
+
+        assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(S[2], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(S[4], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 2
+
+        SS = structures.Sequence(1000)
+
+        SS.appendleft(hv[0])
+        SS.appendleft(hv[1])
+        SS.appendleft(hv[2])
+
+        SS2 = structures.Sequence(1000)
+        SS2.appendleft(hv[0])
+        SS2.appendleft(hv[1])
+        SS2.appendleft(hv[2])
+
+        SS = SS.concat(SS2)
+
+        assert torch.argmax(functional.cosine_similarity(SS[0], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(SS[1], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(SS[2], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(SS[3], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(SS[4], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(SS[5], hv)).item() == 0
+
+    def test_getitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert len(S) == 3
+        S.pop(hv[2])
+
+        assert len(S) == 2
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert len(S) == 3
+        S.clear()
+        assert len(S) == 0
+        S.append(hv[0])
+        assert len(S) == 1
+
+    def test_from_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.Sequence.from_tensor(hv)
+
+        assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 3
+        assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 5
+        assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestTree:
+    def test_creation_dim(self):
+        T = structures.Tree(10000)
+        assert torch.equal(T.value, torch.zeros(10000))
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_leaf(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        T = structures.Tree(10000)
+        T.add_leaf(hv[0], [""l"", ""l""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
+            ).item()
+            == 0
+        )
+        T.add_leaf(hv[1], [""l"", ""r""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
+            ).item()
+            == 1
+        )
+
+    def test_get_leaf(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        T = structures.Tree(10000)
+        T.add_leaf(hv[0], [""l"", ""l""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
+            ).item()
+            == 0
+        )
+        T.add_leaf(hv[1], [""l"", ""r""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
+            ).item()
+            == 1
+        )
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 10, generator=generator)
+        T = structures.Tree(10)
+
+        T.add_leaf(hv[0], [""l"", ""l""])
+        T.add_leaf(hv[1], [""l"", ""r""])
+
+        T.clear()
+        assert torch.equal(
+            T.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
-  in_original: Indicates whether a parameter is expected to be present in the
-    saved checkpoint. Will raise an error if the parameter was expected,
-    but is not present.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
@@ -66,14 +67,14 @@ class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
-  in_original: bool = True
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
-    return set(x.keys()) >= {'original_key', 'in_original', 'value_fn'}
   return False
 
 
@@ -86,8 +87,10 @@ def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
-def apply_transformations(original_tree: PyTree, transformations: PyTree,
-                          new_tree: PyTree) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
@@ -162,6 +165,9 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
@@ -187,8 +193,15 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
-        if not transform.in_original:
-          continue  # do not override existing value of key in new
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
@@ -199,8 +212,7 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
-                f'Transformation key {original_key} not found in origin tree (in_original=True)'
-            )
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
@@ -209,9 +221,11 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
-      # carry over directly from original, otherwise use value from new
-      if key in original:
-        new[key] = original[key]
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
+  use_fallback: if True, takes the value from the fallback tree. If
+    `default_to_original=True` in `apply_transformations`, the fallback tree is
+    `new_tree`. If `default_to_original=False` in `apply_transformations`, the
+    fallback tree is `original_tree`.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
@@ -66,14 +67,14 @@ class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
+  use_fallback: bool = False
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
+    return set(x.keys()) >= {'original_key', 'value_fn', 'multi_value_fn'}
   return False
 
 
@@ -86,8 +87,10 @@ def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
+def apply_transformations(original_tree: PyTree,
+                          transformations: PyTree,
+                          new_tree: PyTree,
+                          default_to_original: Optional[bool] = True) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
@@ -162,6 +165,9 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
+    default_to_original: If True, the values of keys unspecified in
+      transformations will be taken from `original_tree`. If False, they will be
+      taken from `new_tree`.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
@@ -187,8 +193,15 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
+        if transform.use_fallback:
+          if not default_to_original:
+            if key not in original:
+              raise ValueError(
+                  f'{key} not found in origin tree (`use_fallback` requested).'
+              )
+            new[key] = original[key]
+          # else simply retain new[key]
+          continue
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
@@ -199,8 +212,7 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
+                f'Transformation key {original_key} not found in origin tree.')
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
@@ -209,9 +221,11 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
+      if default_to_original:
+        # carry over directly from original, otherwise use value from new
+        if key in original:
+          new[key] = original[key]
+      # if default_to_new, do not carry over key from original
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
-        'f': Transform(in_original=False),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
@@ -140,6 +140,60 @@ def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
   def test_regex(self):
     original = {
         'a1': 1,
@@ -268,7 +322,7 @@ class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
-        d=Transform(in_original=False),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
@@ -340,8 +394,8 @@ def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
-        # LargeModel layer 0 is a newly inserted layer, thus in_original=False.
-        r'(.*)Dense_0(.*)': Transform(in_original=False),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
@@ -371,6 +425,50 @@ def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
 
 if __name__ == '__main__':
   absltest.main()"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
+        'f': Transform(use_fallback=True),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
@@ -140,6 +140,60 @@ def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
+  def test_default_new(self):
+    transforms = {
+        'a': Transform(use_fallback=True),  # use value from original
+        # implicit drop ""b""
+        # implicit retain ""c/a"", ""c/a""
+        'b1': Transform(original_key='b'),
+        # implicit add ""f"" and ""g""
+    }
+    new = {
+        'a': ...,
+        'c': {
+            'a': 10,
+            'e': 11,
+        },
+        'b1': ...,
+        'f': None,
+        'g': 2,
+    }
+    expected = {
+        'a': 0,
+        'c': {
+            'a': 10,
+            'e': 11,
+        },
+        'b1': 1,
+        'f': None,
+        'g': 2,
+    }
+    self.assertDictEqual(
+        expected,
+        apply_transformations(
+            self.original, transforms, new, default_to_original=False))
+
+  def test_missing_key_default(self):
+    transforms = {'f': Transform(use_fallback=True)}
+    new = {
+        'a': 2,
+        'b': 3,
+        'c': {
+            'a': 7,
+            'e': 8,
+        },
+        'f': 20,
+    }
+    with self.assertRaises(ValueError):
+      apply_transformations(
+          self.original, transforms, new, default_to_original=False)
+
+    expected = {'a': 0, 'b': 1, 'c': {'a': 2, 'e': 3}, 'f': 20}
+    self.assertDictEqual(
+        expected,
+        apply_transformations(
+            self.original, transforms, new, default_to_original=True))
+
   def test_regex(self):
     original = {
         'a1': 1,
@@ -268,7 +322,7 @@ class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
+        d=Transform(use_fallback=True),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
@@ -340,8 +394,8 @@ def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
+        # LargeModel layer 0 is a newly inserted layer, thus use_fallback=True.
+        r'(.*)Dense_0(.*)': Transform(use_fallback=True),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
@@ -371,6 +425,50 @@ def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
+  def test_flax_train_state_default_new(self):
+
+    class Model(nn.Module):
+
+      @nn.compact
+      def __call__(self, x):
+        x = x.reshape((x.shape[0], -1))  # flatten
+        x = nn.Dense(features=16)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=8)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=8)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=4)(x)
+        return x
+
+    old_state = test_utils.init_flax_model(Model())
+    new_state = test_utils.init_flax_model(Model())
+
+    transformations = {
+        # values default to new_state, use_fallback=True instructs the Transform
+        # to fall back on old_state for this key.
+        r'(.*)Dense_1(.*)': Transform(use_fallback=True),
+    }
+    restored_state = apply_transformations(
+        old_state, transformations, new_state, default_to_original=False)
+
+    # Construct expected tree
+    old_state_dict = traverse_util.flatten_dict(
+        serialization.to_state_dict(old_state), keep_empty_nodes=True, sep='/')
+    new_state_dict = traverse_util.flatten_dict(
+        serialization.to_state_dict(new_state), keep_empty_nodes=True, sep='/')
+    expected_state_dict = {}
+    for k, v in new_state_dict.items():
+      if 'Dense_1' in k:
+        expected_state_dict[k] = old_state_dict[k]
+      else:
+        expected_state_dict[k] = v
+
+    expected_state = serialization.from_state_dict(
+        new_state, traverse_util.unflatten_dict(expected_state_dict, sep='/'))
+
+    test_utils.assert_tree_equal(self, expected_state, restored_state)
+
 
 if __name__ == '__main__':
   absltest.main()"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
+        'pyyaml',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
KO;2;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
-def init(S=13, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
OK;2;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
+def init(S=19, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
KO;2;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
-		no_obj_iou_1, idx_1 = internal_function(yhat[0:int(N / 2)], y[0:int(N / 2)])
-		no_obj_iou_2, idx_2 = internal_function(yhat[int(N / 2):], y[int(N / 2):])
-		no_obj_iou = torch.cat([no_obj_iou_1, no_obj_iou_2], dim=0)
-		idx = torch.cat([idx_1, idx_2], dim=0)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
OK;2;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
+		def obtain_by_crop(crop) -> list[torch.Tensor]:
+			""""""Obtain no_obj_iou by cropping down batch, used to enable large batch training
+
+			Args:
+				crop (int): crop count
+
+			Returns:
+				list[torch.Tensor]: no_obj_iou and idx
+			""""""
+			no_obj_iou = torch.tensor([], dtype=torch.bool).to(yhat.device)
+			idx = torch.tensor([], dtype=torch.int64).to(yhat.device)
+			for i in range(crop):
+				no_obj_iou_i, idx_i = internal_function(yhat[int(i * N / crop):int((i + 1) * N / crop)], 
+														y[int(i * N / crop):int((i + 1) * N / crop)])
+				no_obj_iou = torch.cat([no_obj_iou, no_obj_iou_i], dim=0)
+				idx = torch.cat([idx, idx_i], dim=0)
+			
+			return no_obj_iou, idx
+
+		if S == 19:
+			crop = 3
+		else:
+			crop = 1
+		
+		no_obj_iou, idx = obtain_by_crop(crop)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
-Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/scheduler_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
+Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/cron_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
 
 app = Flask(__name__)
 
@@ -19,12 +20,15 @@
 def run():
     # TODO - allow only POST
     # TODO - add authentication
-    now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
     sheets_service = SheetsService()
-    entries = retrieve_entries_from_db(sheets_service, now)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
         return Response(status=200)
 
     for i, row in entries:
@@ -43,12 +47,9 @@ def run():
         )
         sheets_service.update_entry(updated_entry)
 
-    return Response(status=200)
-
 
-def retrieve_entries_from_db(sheets_service, nextrun_ts):
-    parsed_time = parse_time(nextrun_ts)
-    return sheets_service.get_entries_by_nextrun(parsed_time)
 
 
 def send_message(chat_id, content):"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
+import gc
 
 app = Flask(__name__)
 
@@ -19,12 +20,15 @@
 def run():
     # TODO - allow only POST
     # TODO - add authentication
     sheets_service = SheetsService()
+    now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
+    parsed_time = parse_time(now)
+    entries = sheets_service.get_entries_by_nextrun(parsed_time)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
+        gc.collect()
+
         return Response(status=200)
 
     for i, row in entries:
@@ -43,12 +47,9 @@ def run():
         )
         sheets_service.update_entry(updated_entry)
 
+    gc.collect()  # https://github.com/googleapis/google-api-python-client/issues/535
 
+    return Response(status=200)
 
 
 def send_message(chat_id, content):"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
-    update.message.reply_text(config.confirm_message, parse_mode=""MarkdownV2"")
 
 
 def remove_job(update):"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
+    update.message.reply_text(config.confirm_message)
 
 
 def remove_job(update):"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
-confirm_message = ""Ok\. Done\. Added\. Your message will be sent when the time comes\.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
+confirm_message = ""Ok. Done. Added. Your message will be sent when the time comes. Check /list to make sure that your job is added correctly.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -71,6 +78,13 @@ def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -147,6 +161,14 @@ def add_chat_data(
             ],
             inherit=True,
         )
         return
 
     def add_user(self, user_id, username, first_name):
@@ -155,6 +177,12 @@ def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -181,6 +209,12 @@ def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
@@ -203,12 +237,6 @@ def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
-            logger.info(
-                ""New user added, username=%s, user_id=%s"",
-                update.message.from_user.username,
-                update.message.from_user.id,
-            )
-
             return
 
         # check that username hasn't changed
@@ -222,7 +250,7 @@ def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
-                ""username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
@@ -238,7 +266,7 @@ def sync_user_data(self, update):
             )
 
             logger.info(
-                ""first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
+        logger.info(
+            'New job entry ""%s"" added by user ""%s"", chat_id=%s',
+            jobname,
+            username,
+            str(chat_id),
+        )
+
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -71,6 +78,13 @@ def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
+        logger.info(
+            'Job entry ""%s"" updated by user ""%s"", chat_id=%s',
+            get_value(entry, ""jobname""),
+            get_value(entry, ""last_updated_by""),
+            str(get_value(entry, ""chat_id"")),
+        )
+
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -147,6 +161,14 @@ def add_chat_data(
             ],
             inherit=True,
         )
+
+        logger.info(
+            'New chat entry created by user ""%s"", chat_id=%s, chat_title=%s',
+            created_by_username,
+            str(chat_id),
+            chat_title,
+        )
+
         return
 
     def add_user(self, user_id, username, first_name):
@@ -155,6 +177,12 @@ def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
+        logger.info(
+            'New user created, user_id=%s, username=""%s""',
+            str(user_id),
+            username,
+        )
+
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -181,6 +209,12 @@ def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
+        logger.info(
+            'User superseded, user_id=%s, field_changed=""%s""',
+            get_value(entry, ""user_id""),
+            field_changed,
+        )
+
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
@@ -203,12 +237,6 @@ def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
             return
 
         # check that username hasn't changed
@@ -222,7 +250,7 @@ def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
+                ""User's username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
@@ -238,7 +266,7 @@ def sync_user_data(self, update):
             )
 
             logger.info(
+                ""User's first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
KO;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
-        
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
+            start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
+            elapsed_time = time.time() - start_time
+            self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
+
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -847,80 +848,95 @@ def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
-        self.empty_cache()
-        img = cp.array(img, dtype=np.float32)
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
-
-        del img
-        # cp._default_memory_pool.free_all_blocks()
-
-        img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
-        bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
-        reconfactor_cp = cp.array(self._reconfactor)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
-
-        del bcarray
-        del reconfactor_cp
-        # cp._default_memory_pool.free_all_blocks()
-
-        img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
-        for offs in range(0, 2*self.N - blocksize, blocksize):
-            imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        del img2
-        del imf
-        # cp._default_memory_pool.free_all_blocks()
-
-        res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
-        del img3
-        cp._default_memory_pool.free_all_blocks()
 
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
-        self.empty_cache()
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
-        imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
-        del img1
-        img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
-        reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
-
-        img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        for offs in range(0, 2 * self.N - blocksize, blocksize):
-            imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        del img2
-        postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
-        res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):
@@ -963,12 +979,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
-                print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
-            cp._default_memory_pool.free_all_blocks()
             if self.debug:
-                print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
+        self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -847,80 +848,95 @@ def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
+    def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
+        try:
+            # Sometimes we are called from a new thread and then the plan_cache needs to be
+            # reset to 0 to avoid running out of GPU memory
+            cp.fft.config.get_plan_cache().set_size(0)
+            img1 = cp.array(img, dtype=np.float32)
+            nim = img1.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
+
+            del img1
+
+            img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
+            bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
+            reconfactor_cp = cp.array(self._reconfactor)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
+
+            del bcarray
+            del reconfactor_cp
+
+            img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
+            for offs in range(0, 2*self.N - blocksize, blocksize):
+                imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            del img2
+            del imf
+            res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
+            del img3
+        except Exception as e:
+            res = f'Exception in batchreconstruct_cupy: {e}'
+        return res
+
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
+        res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
+        cp.get_default_memory_pool().free_all_blocks()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
+        return res
 
+    def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
+        assert pytorch, ""No pytorch present""
+        try:
+            nim = img.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
+            imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
+            del img1
+            img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
+            reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
+
+            img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            for offs in range(0, 2 * self.N - blocksize, blocksize):
+                imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            del img2
+            postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
+            res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except Exception as e:
+            res = f'Exception in batchreconstruct_pytorch: {e}'
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
+        res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
+        if torch.has_cuda:
+            torch.cuda.empty_cache()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):
@@ -963,12 +979,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
+                print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+            cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
-                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
+                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
-    def batchreconstructcompact_cupy(self, img, blocksize=128):
-        assert cupy, ""No CuPy present""
         try:
-            # cp.get_default_memory_pool().free_all_blocks()
-            # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-            # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -893,24 +887,17 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
-            raise e
-            # Tidy up GPU memory
-        finally:
-            img1 = None
-            imf = None
-            bcarray = None
-            reconfactor_cp = None
-            img2 = None
-            img3 = None
-            print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-            cp.get_default_memory_pool().free_all_blocks()
-            print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-            print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
-    def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
@@ -941,25 +928,15 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
-            if torch.has_cuda:
-                # Tidy up gpu memory
-                if 'img1' in locals(): del img1
-                if 'imf' in locals(): del imf
-                if 'bcarray' in locals(): del bcarray
-                if 'reconfactor_pt' in locals(): del reconfactor_pt
-                if 'img2' in locals(): del img2
-                if 'img3' in locals(): del img3
-                torch.cuda.empty_cache()
-                print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-                raise e
         if torch.has_cuda:
-            del imf
-            del bcarray
-            del reconfactor_pt
-            del img3
             torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
+    def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -893,24 +887,17 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
+            res = f'Exception in batchreconstruct_cupy: {e}'
         return res
 
+    def batchreconstructcompact_cupy(self, img, blocksize=128):
+        assert cupy, ""No CuPy present""
+        res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
+        cp.get_default_memory_pool().free_all_blocks()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
+        return res
+
+    def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
@@ -941,25 +928,15 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
+            res = f'Exception in batchreconstruct_pytorch: {e}'
+        return res
+
+    def batchreconstructcompact_pytorch(self, img, blocksize=128):
+        assert pytorch, ""No pytorch present""
+        res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
         if torch.has_cuda:
             torch.cuda.empty_cache()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
-                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
+                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -885,23 +890,24 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
-
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             # Tidy up GPU memory
-            if 'img1' in locals(): del img1
-            if 'imf' in locals(): del imf
-            if 'bcarray' in locals(): del bcarray
-            if 'reconfactor_cp' in locals(): del reconfactor_cp
-            if 'img2' in locals(): del img2
-            if 'img3' in locals(): del img3
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-            raise e
-        cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
OK;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
+            # cp.get_default_memory_pool().free_all_blocks()
+            # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+            # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -885,23 +890,24 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
+            raise e
             # Tidy up GPU memory
+        finally:
+            img1 = None
+            imf = None
+            bcarray = None
+            reconfactor_cp = None
+            img2 = None
+            img3 = None
+            print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
KO;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
-        
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
+            start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
+            elapsed_time = time.time() - start_time
+            self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
+
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
-        except RuntimeError as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
-            cp._default_memory_pool.free_all_blocks()
             raise e
-        cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
@@ -931,7 +934,7 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
-        except RuntimeError as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
@@ -941,6 +944,9 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 raise e
         if torch.has_cuda:
             del imf
@@ -990,12 +996,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
-                print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
-            cp._default_memory_pool.free_all_blocks()
             if self.debug:
-                print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
+        except Exception as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
+            cp.get_default_memory_pool().free_all_blocks()
+            print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+            print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             raise e
+        cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
@@ -931,7 +934,7 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except Exception as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
@@ -941,6 +944,9 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
+                print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
                 raise e
         if torch.has_cuda:
             del imf
@@ -990,12 +996,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
+                print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+            cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -849,78 +850,104 @@ def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
-        self.empty_cache()
-        img = cp.array(img, dtype=np.float32)
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
-
-        del img
-        # cp._default_memory_pool.free_all_blocks()
-
-        img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
-        bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
-        reconfactor_cp = cp.array(self._reconfactor)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
-
-        del bcarray
-        del reconfactor_cp
-        # cp._default_memory_pool.free_all_blocks()
-
-        img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
-        for offs in range(0, 2*self.N - blocksize, blocksize):
-            imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        del img2
-        del imf
-        # cp._default_memory_pool.free_all_blocks()
-
-        res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
-        del img3
         cp._default_memory_pool.free_all_blocks()
-
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
-        self.empty_cache()
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
-        imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
-        del img1
-        img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
-        reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
-
-        img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        for offs in range(0, 2 * self.N - blocksize, blocksize):
-            imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        del img2
-        postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
-        res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
+        self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -849,78 +850,104 @@ def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
+        try:
+            # Sometimes we are called from a new thread and then the plan_cache needs to be
+            # reset to 0 to avoid running out of GPU memory
+            cp.fft.config.get_plan_cache().set_size(0)
+            img1 = cp.array(img, dtype=np.float32)
+            nim = img1.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
+
+            del img1
+
+            img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
+            bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
+            reconfactor_cp = cp.array(self._reconfactor)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
+
+            del bcarray
+            del reconfactor_cp
+
+            img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
+            for offs in range(0, 2*self.N - blocksize, blocksize):
+                imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            del img2
+            del imf
+
+            res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
+            del img3
+        except RuntimeError as e:
+            # Tidy up GPU memory
+            if 'img1' in locals(): del img1
+            if 'imf' in locals(): del imf
+            if 'bcarray' in locals(): del bcarray
+            if 'reconfactor_cp' in locals(): del reconfactor_cp
+            if 'img2' in locals(): del img2
+            if 'img3' in locals(): del img3
+            cp._default_memory_pool.free_all_blocks()
+            raise e
         cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
+        try:
+            nim = img.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
+            imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
+            del img1
+            img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
+            reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
+
+            img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            for offs in range(0, 2 * self.N - blocksize, blocksize):
+                imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            del img2
+            postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
+            res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except RuntimeError as e:
+            if torch.has_cuda:
+                # Tidy up gpu memory
+                if 'img1' in locals(): del img1
+                if 'imf' in locals(): del imf
+                if 'bcarray' in locals(): del bcarray
+                if 'reconfactor_pt' in locals(): del reconfactor_pt
+                if 'img2' in locals(): del img2
+                if 'img3' in locals(): del img3
+                torch.cuda.empty_cache()
+                raise e
+        if torch.has_cuda:
+            del imf
+            del bcarray
+            del reconfactor_pt
+            del img3
+            torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;4;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
-    partition_group = row.partition_group
     embeddings = row.embeddings
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
-    save_path = os.path.join(fs_base_path, f""img_emb_{shard_index}-{partition_group}.npy"")
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
@@ -127,7 +131,7 @@ def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
-    if working_fs.exists(output_folder_path) and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
@@ -138,7 +142,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
-    if working_fs.exists(meta_embed_folder_path) and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
@@ -149,7 +153,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
-    if working_fs.exists(empty_embed_folder_path) and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
@@ -171,6 +175,9 @@ def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
@@ -194,7 +201,7 @@ def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
-    example_embedding = np.array(data.first().embeddings)
 
     end_recall_timer()
 
@@ -204,6 +211,7 @@ def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
@@ -249,13 +257,20 @@ def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
         grouped = (
-            data.orderBy(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
OK;4;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
     embeddings = row.embeddings
+    if ""partition_group"" in row:
+        partition_group = row.partition_group
+        filename = f""img_emb_{shard_index}-{partition_group}.npy""
+    else:
+        filename = f""img_emb_{shard_index}.npy""
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
+    save_path = os.path.join(fs_base_path, filename)
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
@@ -127,7 +131,7 @@ def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
+    if working_fs.exists(output_folder_path) and len(working_fs.ls(output_folder_path)) > 0 and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
@@ -138,7 +142,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
+    if working_fs.exists(meta_embed_folder_path) and len(working_fs.ls(meta_embed_folder_path)) > 0 and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
@@ -149,7 +153,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
+    if working_fs.exists(empty_embed_folder_path) and len(working_fs.ls(empty_embed_folder_path)) > 0 and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
@@ -171,6 +175,9 @@ def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
+        .config(""spark.sql.shuffle.partitions"", ""1000000"")
+        .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
+        .config(""spark.kryoserializer.buffer"", ""1g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
@@ -194,7 +201,7 @@ def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
+    print(f""Data has {data.rdd.getNumPartitions()} partitions"")
 
     end_recall_timer()
 
@@ -204,6 +211,7 @@ def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
+        example_embedding = np.array(data.first().embeddings)
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
@@ -249,13 +257,20 @@ def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
+        print(""Number of partitions"", data.rdd.getNumPartitions())
+        # data = data.repartition(""img_shard"")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
+        data = data.repartition(""img_shard"", ""partition_group"")
+        print(""Number of partitions"", data.rdd.getNumPartitions())
         grouped = (
+            data
+            # .orderBy(""img_index"")
+            .sortWithinPartitions(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
+            # .groupBy(""img_shard"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
KO;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
 
 import findspark
 
@@ -152,6 +154,8 @@ def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
@@ -164,6 +168,8 @@ def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
@@ -190,12 +196,14 @@ def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
@@ -215,6 +223,8 @@ def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
@@ -225,21 +235,58 @@ def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
-        added_df = spark.createDataFrame(added_data, [""img_shard"", ""img_index"", ""embeddings""])
         data = data.union(added_df)
 
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
-    # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
-    # I now know what happens. It immediatly crashes.
 
-    # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
OK;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
+import time
+import plotext as plt
 
 import findspark
 
@@ -152,6 +154,8 @@ def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
+    start_time = time.perf_counter()
+
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
@@ -164,6 +168,8 @@ def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
+    start_embedding_load_time = time.perf_counter()
+
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
@@ -190,12 +196,14 @@ def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
+    start_recall_time = time.perf_counter()
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
+    start_missing_fill_time = time.perf_counter()
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
@@ -215,6 +223,8 @@ def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
+        current_amount = 0
+        added_files = 0
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
@@ -225,21 +235,58 @@ def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
+                current_amount += 1
+                if current_amount > 1000:
+                    df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
+                    with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
+                        df.to_parquet(f)
+                    added_data.clear()
+                    current_amount = 0
+                    added_files += 1
+        df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
+        with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
+            df.to_parquet(f)
+        empty_path = os.path.join(output_base_path, intermediate_folder, ""empty_*.parquet"")
+        added_df = spark.read.parquet(empty_path)
         data = data.union(added_df)
 
+    full_count = data.count()
+
+    start_export_time = time.perf_counter()
+
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
+    # # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
+    # # I now know what happens. It immediatly crashes.
 
+    # # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
+    end_time = time.perf_counter()
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
+
+    embed_reader_initialization_time = start_embedding_load_time - start_time
+    embedding_load_time = start_recall_time - start_embedding_load_time
+    recall_time = start_missing_fill_time - start_recall_time
+    missing_fill_time = start_export_time - start_missing_fill_time
+    export_time = end_time - start_export_time
+    total_time = end_time - start_time
+
+    print(f""Total Execution Time: {total_time:0.2f}s"")
+
+    tasks = list(reversed([""Initialize Embedding Reader"", ""Load Embeddings"", ""Recall Embeddings"", ""Insert Missing Embeddings"", ""Save Embeddings""]))
+    times = list(reversed([embed_reader_initialization_time, embedding_load_time, recall_time, missing_fill_time, export_time]))
+    plt.bar(tasks, times, orientation=""horizontal"", width = 0.3)
+    plt.clc()
+    plt.xlabel(""Execution Time (s)"")
+    plt.show()
+
+    
+
+
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
KO;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
-tqdm>=4.60.0
\ No newline at end of file
\ No newline at end of file"
OK;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
\ No newline at end of file
+tqdm>=4.60.0
+plotext>=5.0.2
\ No newline at end of file"
KO;4;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
-            fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
-            data = jarray.zeros(memory_block.getSize(), ""b"")
-            memory_block.getBytes(memory_block.getStart(), data)
-            fp.write(','.join((str(i) for i in data)))
-            fp.write("")),\n"")
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
OK;4;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
+            if memory_block.isInitialized():
+                fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
+                data = jarray.zeros(memory_block.getSize(), ""b"")
+                memory_block.getBytes(memory_block.getStart(), data)
+                fp.write(','.join((str(i) for i in data)))
+                fp.write("")),\n"")
+            else:
+                fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=None),\n"".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
-            symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
-        fat_header = self.SDAT.read_struct(info.NNSSndArcFat, header.fatOffset)
-        fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
-        info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
+            symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
+        fat_header = self.SDAT.read_struct(info.NNSSndArcFat, offset=header.fatOffset)
+        fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, offset=header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
+        info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
-    def read_seqarc_strings(cls, offset: int, sdat: SdatIO):
         def inner():
-            for x in sdat.read_array(cls, offset):
                 symbol = sdat.get_string(offset, x.symbol)
-                table = NNSSndArcOffsetTable.read_strings(offset + x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
@@ -177,7 +180,9 @@ class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
-    def read_file(self, base: int, sdat: SdatIO) -> bytearray:
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
@@ -211,15 +216,21 @@ class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
-        return [sdat.read_struct(sbcls, base + x.offset) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
-        return [list_factory(sdat.read_array(sbcls, base + x.offset)) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
-        return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base + offset)]
 
 
 # Non-C-types
@@ -243,7 +254,7 @@ def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
-            NNSSndArcSeqArcOffset.read_seqarc_strings(offset + header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
@@ -297,6 +308,7 @@ def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
@@ -306,20 +318,27 @@ def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
-                    result[kind.name].append([dataclasses.asdict(x) for x in info])
                 else:
-                    result[kind.name].append(dataclasses.asdict(info))
-                result[kind.name][-1]['name'] = getattr(info, 'name', f'{kind.name}_{i:03d}')
-                if hasattr(info, 'arc_names'):
-                    result[kind.name][-1]['arc_names'] = info.arc_names
-                if hasattr(info, 'filename'):
-                    result[kind.name][-1]['filename'] = info.filename
         return result
 
     def dump_files(self, files, outdir):"
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
+    def read_seqarc_strings(cls, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+
         def inner():
+            for x in sdat.read_array(cls, base, offset):
                 symbol = sdat.get_string(offset, x.symbol)
+                table = NNSSndArcOffsetTable.read_strings(offset, x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
@@ -177,7 +180,9 @@ class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
+    def read_file(self, base: int, sdat: SdatIO) -> typing.ByteString:
+        if base == 0:
+            return b''
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
@@ -211,15 +216,21 @@ class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+        return [sdat.read_struct(sbcls, base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
+        if 0 in (base, offset):
+            return []
+        return [list_factory(sdat.read_array(sbcls, base, x.offset)) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+        return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
 
 # Non-C-types
@@ -243,7 +254,7 @@ def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
+            NNSSndArcSeqArcOffset.read_seqarc_strings(offset, header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
@@ -297,6 +308,7 @@ def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
+                    assert info.fileId < 65536
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
@@ -306,20 +318,27 @@ def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
+    @staticmethod
+    def single_to_dict(info, index, idx2=None):
+        if not dataclasses.is_dataclass(info):
+            return {}
+        ret = dataclasses.asdict(info)
+        ret['name'] = getattr(info, 'name', f'{info._kind.name}_{index:03d}' + '' if idx2 is None else f'_{idx2:03d}')
+        if hasattr(info, 'arc_names'):
+            ret['arc_names'] = info.arc_names
+        if hasattr(info, 'filename'):
+            ret['filename'] = info.filename
+        return ret
+
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
+                    result[kind.name].append([InfoData.single_to_dict(x, i, j) for j, x in enumerate(info)])
                 else:
+                    result[kind.name].append(InfoData.single_to_dict(info, i))
         return result
 
     def dump_files(self, files, outdir):"
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
-        if offset == 0:
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
@@ -154,17 +154,25 @@ def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
-    def read_struct(self, struct: NamedStruct, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
-    def read_array(self, struct: NamedStruct, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
@@ -184,6 +192,7 @@ def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
         if not objs:
             self.write_long(offset, 0)
         else:"
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
+        if 0 in (base, offset):
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
@@ -154,17 +154,25 @@ def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
+    def read_struct(self, struct: NamedStruct, base: int = None, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
+        if 0 in (base, offset):
+            return None
+        if base is not None and offset is not None:
+            offset += base
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
+    def read_array(self, struct: NamedStruct, base: int = None, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
+        if 0 in (base, offset):
+            return []
+        if base is not None and offset is not None:
+            offset += base
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
@@ -184,6 +192,7 @@ def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
+        assert offset != 0
         if not objs:
             self.write_long(offset, 0)
         else:"
OK;5;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
+            
+            def _spherical_func(viewdirs):
+                raw_rgb, sigma = eval_fn(
+                    grid_chunk[:, None], dirs=viewdirs)
+                return raw_rgb, sigma
+                    
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;5;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
-                    def _spherical_func(viewdirs):
-                        raw_rgb, sigma = eval_fn(
-                            grid_chunk[:, None], dirs=viewdirs)
-                        return raw_rgb, sigma
-
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
OK;5;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
+            
+            def _spherical_func(viewdirs):
+                raw_rgb, sigma = eval_fn(
+                    grid_chunk[:, None], dirs=viewdirs)
+                return raw_rgb, sigma
+                    
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
-
 
 # 
 ## 
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 ACPython`eval`
 
 ## 
-`eval/data`PKUASjieba````
 
 |  |  |    |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ ACPython`eval`
 | Jieba  | 58.4W | 4050566  |
 
 ## 
-### 
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |        | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ ACPython`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
-
-### 
 
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
-|    ahocorapy    | 0.02 | 0.34  | 0.08 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
-| prefix_dict |   1.4 |  6.7  | 3.5 |
\ No newline at end of file
\ No newline at end of file"
OK;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
+
 
 # 
 ## 
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 ACPython`eval`
 
 ## 
+`eval/data`PKUASjieba``````
 
 |  |  |    |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ ACPython`eval`
 | Jieba  | 58.4W | 4050566  |
 
 ## 
+PythonACahocorapy
+
+### ( )
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |        | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ ACPython`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
+### ( )
 
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
+|    ahocorapy    | 1.0 | 5.4  | 9.27 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
\ No newline at end of file
+| prefix_dict |   1.4 |  6.7  | 3.5 |
+
+
+### 
+|              |       PKU       |       AS       |    jieba     |
+| :--------------: | :-------------: | :------------: | :----------: |
+|        | 5.5W | 14.1W  | 58.4W |
+|    ahocorapy    | 300M | 800M  | 5G |
+|    dmsearch    | 1G | 1G  | 2.5G |
+| prefix_dict |   25M |  100M | 400M |
\ No newline at end of file"
KO;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
-
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
-        sys.path.append(""../dictmatch"") 
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
-    
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
-
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
-      tree.search(line.strip())
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
-#   test_eval(ahocorapy, 'pku')
-#   test_eval(ahocorapy, 'as')
-#   test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
-    
\ No newline at end of file"
OK;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
+        # sys.path.append(""../dictmatch"")
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
+        
+    def build(self) -> None:
+        self.tree.make()
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
+    
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 
+# from memory_profiler import profile
+# @profile
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
+      list(tree.search(line.strip()))
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
+  test_eval(ahocorapy, 'pku')
+  test_eval(ahocorapy, 'as')
+  test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
\ No newline at end of file
+    "
KO;9;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
-    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     val_dataset = MapDataset(VALIDATION)
-    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
OK;9;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
+    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     val_dataset = MapDataset(VALIDATION)
+    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
-from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
@@ -75,6 +76,7 @@ def convert(self):
             return
         
         try:
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
@@ -99,6 +101,7 @@ def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
@@ -123,7 +126,7 @@ def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
-    freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
+# from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
+
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
@@ -75,6 +76,7 @@ def convert(self):
             return
         
         try:
+
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
@@ -99,6 +101,7 @@ def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
+
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
@@ -123,7 +126,7 @@ def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
+    #freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"-from .mixamo_helper import Mixamo, get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
-from .pyglm_helper import get_3d_len,  find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
@@ -98,67 +98,76 @@ def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
-    # init medaipipe
-    mp_pose = mp.solutions.pose
-    mp_drawing = mp.solutions.drawing_utils 
-    pose_video = mp_pose.Pose(static_image_mode=static_image_mode,
-                        min_detection_confidence=min_detection_confidence, 
-                        model_complexity=model_complexity)
     frame_num = -1
     
-    fig = plt.figure()
-    
     try:
-        while cap.isOpened():
-
-            success, image = cap.read()
-            frame_num += 1
-            if not success or max_frame_num < frame_num :
-                break
-
-            image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose_video, mp_idx_mm_idx_map)
-            if glm_list[0] != None:
-                bones_json = {
-                   ""time"": frame_num,
-                   ""bones"": []
-                } 
-                mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
-                mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
-                mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
-                anim_result_json[""frames""].append(bones_json)
-                if is_show_result:
-                    rg = []
-                    rv = []
-                    mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
-                    cv2.imshow(""result"", glm_list_to_image(fig, rv, rg))
-                if is_hips_move:
-                    hip2d_left.x *=  width
-                    hip2d_left.y *=  height
-                    hip2d_left.z *=  width
-                    hip2d_right.x *= width
-                    hip2d_right.y *= height
-                    hip2d_right.z *= width
-                    if origin == None:
-                       origin = avg_vec3(hip2d_left, hip2d_right)
-                       hips2d_scale = glm.distance(hip2d_left, hip2d_right)
-                       factor = model_scale/hips2d_scale
-                    else:
-                        hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
-                        if hips_bone_json != None:
-                            set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
-
-            cv2.imshow('MediaPipe pose', image)
-
-            if cv2.waitKey(5) & 0xFF == 27:
-                break
         cap.release()
-        plt.close()
     except Exception as e:
         print(e)
-        plt.close()
         if cap.isOpened():
             cap.release()
-                
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"+from .mixamo_helper import Mixamo,  get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
+from .pyglm_helper import get_3d_len, draw_list2, find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
@@ -98,67 +98,76 @@ def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
     frame_num = -1
+    plt.ion()
+    plt.close()
+    fig = None
+    if is_show_result: 
+        fig = plt.figure()
+        plt.show()
     
+    # init mediapipe
+    mp_pose = mp.solutions.pose
+    mp_drawing = mp.solutions.drawing_utils 
     try:
+        with mp_pose.Pose(
+            static_image_mode=static_image_mode,
+                        min_detection_confidence=min_detection_confidence, 
+                        model_complexity=model_complexity
+        ) as pose:
+            while cap.isOpened():
+
+                success, cap_image = cap.read()
+                frame_num += 1
+                if not success or max_frame_num < frame_num :
+                    break
+
+                cap_image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, cap_image, pose, mp_idx_mm_idx_map)
+                if glm_list[0] != None:
+                    bones_json = {
+                       ""time"": frame_num,
+                       ""bones"": []
+                    } 
+                    mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
+                    mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
+                    mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
+                    anim_result_json[""frames""].append(bones_json)
+                    if is_show_result:
+                        rg = []
+                        rv = []
+                        mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
+                        plt.clf()
+                        draw_list2(fig, rv, rg)
+                    if is_hips_move:
+                        hip2d_left.x *=  width
+                        hip2d_left.y *=  height
+                        hip2d_left.z *=  width
+                        hip2d_right.x *= width
+                        hip2d_right.y *= height
+                        hip2d_right.z *= width
+                        if origin == None:
+                           origin = avg_vec3(hip2d_left, hip2d_right)
+                           hips2d_scale = glm.distance(hip2d_left, hip2d_right)
+                           factor = model_scale/hips2d_scale
+                        else:
+                            hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
+                            if hips_bone_json != None:
+                                set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
+
+                cv2.imshow('MediaPipe pose', cap_image)
+
+                if cv2.waitKey(5) & 0xFF == 27:
+                    break
         cap.release()
+        # plt.close(fig)
+        cv2.destroyAllWindows()    
+
     except Exception as e:
         print(e)
+        # plt.close(fig)
         if cap.isOpened():
             cap.release()
+            cv2.destroyAllWindows()
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
+def draw_list2(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
+    ax1 = plt.axes(projection='3d')
+    set_axes(ax1, elev=10, azim=azim, xrange=range, yrange=range, zrange=range)
+    dots = get_dot(vec_list, group_lists)
+    for dot in dots:
+        ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
+    
+    fig.canvas.draw()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
KO;11;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
-    c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;11;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;"+from re import L
 from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
+    c_dict = dict.fromkeys(atom_dict.keys(), None)
+    for key in c_dict.keys():
+        c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
-, ""lr"" : 3e-4
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
-, ""batch_size"" : 32
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
OK;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
+, ""lr"" : 1e-5
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
+, ""batch_size"" : 64
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
KO;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
 
 
 print(""torch version:"", torch.__version__)
@@ -73,7 +74,7 @@ def avg(l):
   s = sum(l)
   return s / len(l)
 
-ntests = 50
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
@@ -136,6 +137,7 @@ def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
@@ -147,51 +149,73 @@ def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias50"", mus[:,0].mean().item() - 50.0, global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig50"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread50"", (mus[:,0] - 50).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
-  inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias25"", mus[:,0].mean().item() - 25.0, global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig25"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread25"", (mus[:,0] - 25).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
-  inputs = gen([testsig_mu, testsig_sigma, 5.0], [testbkg_mu, testbkg_sigma, 50.0])
 
-  mus , cov = utils.regress(localnet, globalnet, inputs05, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias05"", mus[:,0].mean().item() - 5.0, global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig05"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread05"", (mus[:,0] - 5).std().item(), global_step=epoch)
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
@@ -238,6 +262,7 @@ def gen(sig, bkg):
       , size=batch_size
       )
 
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
@@ -264,3 +289,4 @@ def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()"
OK;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
+import gc
 
 
 print(""torch version:"", torch.__version__)
@@ -73,7 +74,7 @@ def avg(l):
   s = sum(l)
   return s / len(l)
 
+ntests = 1000
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
@@ -136,6 +137,7 @@ def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
+  gc.collect()
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
@@ -147,51 +149,73 @@ def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
+  # TODO:
+  # plot spread / sqrt(N)
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 50.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias50"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert50"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull50"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread50"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
+  inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 25.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 25.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias25"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert25"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull25"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread25"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
+  inputs = gen([testsig_mu, testsig_sigma, 05.0], [testbkg_mu, testbkg_sigma, 05.0])
 
+  mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 05.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias05"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert05"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull05"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread05"", bias.std().item(), global_step=epoch)
+
+  localnet.zero_grad()
+  globalnet.zero_grad()
+
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
+
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
@@ -238,6 +262,7 @@ def gen(sig, bkg):
       , size=batch_size
       )
 
+
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
@@ -264,3 +289,4 @@ def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()
+"
KO;11;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
-test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range))
-test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range))
-test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range))
-test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range))
 
 inputs50 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig50) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig25) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig05) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
@@ -228,15 +228,15 @@ def avg(l):
 
     inputs = \
       torch.cat \
-      ( [ torch.Tensor(siginputs) , torch.Tensor(bkginputs) ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
-    targs = torch.Tensor(targs)
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
OK;11;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
+test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range)).detach()
+test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range)).detach()
+test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range)).detach()
+test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range)).detach()
 
 inputs50 = \
   torch.cat \
+  ( [ test_sig50 , test_bkg ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
+  ( [ test_sig25 , test_bkg ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
+  ( [ test_sig05 , test_bkg ]
   , axis = 2
   )
 
@@ -228,15 +228,15 @@ def avg(l):
 
     inputs = \
       torch.cat \
+      ( [ torch.Tensor(siginputs).detach() , torch.Tensor(bkginputs).detach() ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
+    targs = torch.Tensor(targs).detach()
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
KO;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
-515576 songs should be exported to the CSV
 
 3\. Run the script
 "
OK;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
+10000 songs should be exported to the CSV due to memory constraints
 
 3\. Run the script
 "
KO;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
-	AND year IS NOT NULL AND year != 0;"
OK;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
+	AND year IS NOT NULL AND year != 0
+	LIMIT 10000;"
KO;12;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
-    c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;12;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;"+from re import L
 from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
+    c_dict = dict.fromkeys(atom_dict.keys(), None)
+    for key in c_dict.keys():
+        c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;12;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
-
 ### Monitored RAM Locations
 
 PAL Version of MKwii
@@ -68,6 +67,18 @@ PAL Version of MKwii
 
 ### Usage
 
 ```bash
 python3 -m pip install -e mario-env
 ```"
OK;12;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
 ### Monitored RAM Locations
 
 PAL Version of MKwii
@@ -68,6 +67,18 @@ PAL Version of MKwii
 
 ### Usage
 
+Environment Variables:
+
+* Set `DOLPHIN_CONF_DIR` to the Dolphin Emulator User directory (MacOS : `~/Library/Application Support/Dolphin`)
+* Set `DOLPHIN_DIR` to the location of the Dolphin Binary (ex: `dolphin/build/Binaries/Dolphin.app/Contents/MacOS/Dolphin`)
+* Set `MK_ISO` to the location of the game iso
+
+In-Progress:
+
+* `CENTER_TRAJ` 3D trajectory for driving on the centerline (`centerline_traj.npy`)
+* `LEFT_TRAJ` 3D trajectory for driving on the left side of the track (`lefttraj.npy`)
+* `RIGHT_TRAJ` 3D trajectory for driving on the right side of the track (`righttraj.npy`)
+
 ```bash
 python3 -m pip install -e mario-env
 ```"
KO;12;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;
OK;12;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;"+""""""
+Memory
+---
+This script produces plots visualizing the memory consumption of arrays used by the PS agent
+depending on environment dimensions and the number of crates in said environment
+---
+Author: Josef Hamelink
+---
+Date: May 2022
+""""""
+
+# python standard library
+import os                                       # directories
+# dependencies
+import numpy as np                              # arrays
+import matplotlib as mpl                        # text formatting
+import matplotlib.pyplot as plt                 # figure
+import mpl_toolkits.axes_grid1 as axes_grid1    # grid subplot
+# local imports
+from helper import fix_dirs                     # directories
+
+def main():
+    
+    global dim_range, cc_range, ldr, lcr
+
+    dim_range = range(5, 11)   	# range of dimensions we want to plot: 5-10
+    cc_range = range(4, 11)    	# range of crate counts we want to plot: 4-10
+    ldr  = len(dim_range)
+    lcr = len(cc_range)
+
+    Q_res = np.zeros(shape=(ldr, lcr))
+    N_res = np.zeros(shape=(ldr, lcr))
+
+    for i, dim in enumerate(dim_range):
+        for j, cc in enumerate(cc_range):
+            Q_res[i,j] = Q_array_memory(dim, cc)
+            N_res[i,j] = N_array_memory(dim, cc)
+    
+    fig = plt.figure()
+    plt.rcParams.update({'font.size': 8})
+
+    grid = axes_grid1.AxesGrid(fig, 111, nrows_ncols=(1, 2), axes_pad = 0.3, cbar_location = ""bottom"",
+                            cbar_mode=""each"", cbar_size=""10%"", cbar_pad=""5%"")
+
+    add_subplot(grid, Q_res, 0, 'Q Table (memory in MB)')
+    add_subplot(grid, N_res, 1, 'N Table (memory in GB)')
+
+    fix_dirs()
+    plt.savefig(os.path.join(os.getcwd(),'..','results','memory.png'), bbox_inches='tight', dpi=200)
+
+
+def Q_array_memory(dim: int, cc: int) -> float:
+    """"""Calculates the chunk of memory needed to hold the Q-values based on dimension and crate count (MB)""""""
+    n_states = (dim+2)**2 * 2**cc
+    n_actions = 6
+    n_slots = n_states * n_actions
+    n_bytes = 8 * n_slots  	# default float contains 64 bits (8 bytes)
+    n_megabytes = n_bytes * 2**(-20)
+    return round(n_megabytes, 1)
+
+def N_array_memory(dim: int, cc: int) -> float:
+    """"""Calculates the chunk of memory needed to hold the N-values based on dimension and crate count (GB)""""""
+    n_states = (dim+2)**2 * 2**cc
+    n_actions = 6
+    n_slots = n_states * n_actions * n_states
+    n_bytes = 8 * n_slots  # default int contains 64 bits (8 bytes)
+    n_gigabytes = n_bytes * 2**(-30)
+    return round(n_gigabytes, 1)
+
+def add_subplot(grid: axes_grid1.ImageGrid, res: np.ndarray, plot_idx: int, title: str) -> None:
+    """"""Creates one subplot and adds it to the grid""""""
+    im = grid[plot_idx].imshow(res, cmap='viridis', interpolation='none')
+    im.axes.xaxis.tick_top()
+    im.axes.xaxis.set_label_position('top')
+    im.axes.set_xticks(ticks=range(lcr), labels=cc_range)
+    im.axes.set_yticks(ticks=range(ldr), labels=dim_range)
+    im.axes.tick_params(axis='both', top=False, left=False)
+    im.axes.set_xlabel('number of crates')
+    im.axes.set_ylabel('world dimensions')
+    im.axes.set_title(title)
+    grid.cbar_axes[plot_idx].colorbar(im)
+
+    textcolors = ('black', 'white')
+    kw = {'fontsize': 6,
+          'horizontalalignment': 'center',
+          'verticalalignment': 'center'}
+
+    threshold = im.norm(res.max())/2.0
+    valfmt = mpl.ticker.StrMethodFormatter(""{x:.1f}"")
+
+    for i in range(res.shape[0]):
+        for j in range(res.shape[1]):
+            kw.update(color=textcolors[int(im.norm(res[i, j]) < threshold)])
+            im.axes.text(j, i, valfmt(res[i, j], None), **kw)
+
+
+if __name__ == '__main__':
+    main()"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
     name: str
     data: list
 
@@ -71,6 +73,7 @@ def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
@@ -242,18 +245,21 @@ def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
-    return from_tokens(tokens, """")
 
 
-def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
     """"""Read an expression from a sequence of tokens.""""""
-    if len(tokens) == 0:
         raise SyntaxError(""unexpected EOF"")
-    token = tokens.pop(0)
 
     if token == ""("":
         expr: Expr
-        typ = tokens.pop(0)
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
@@ -263,22 +269,23 @@ def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
-        while tokens[0] != "")"":
-            expr.append(from_tokens(tokens, expr.name))
-        tokens.pop(0)  # remove ')'
 
         expr.parsed()
 
-        return expr
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
-        return int(token)
     except ValueError:
         try:
-            return float(token)
         except ValueError:
-            return Symbol(token)"
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
+    __slots__ = (""name"", ""data"", ""_more_than_once"", ""_known_attrs"")
+
     name: str
     data: list
 
@@ -71,6 +73,7 @@ def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
+        # TODO: currently modifying the object and accessing fields again is not handled
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
@@ -242,18 +245,21 @@ def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
+    _, expr = from_tokens(tokens, 0, """")
+    return expr
 
 
+def from_tokens(tokens: list, index: int, parent: str) -> Tuple[int, Union[Expr, int, float, str]]:
     """"""Read an expression from a sequence of tokens.""""""
+    if len(tokens) == index:
         raise SyntaxError(""unexpected EOF"")
+    token = tokens[index]
+    index += 1
 
     if token == ""("":
         expr: Expr
+        typ = tokens[index]
+        index += 1
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
@@ -263,22 +269,23 @@ def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
+        while tokens[index] != "")"":
+            index, sub_expr = from_tokens(tokens, index, expr.name)
+            expr.append(sub_expr)
+        index += 1  # remove ')'
 
         expr.parsed()
 
+        return (index, expr)
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
+        return (index, int(token))
     except ValueError:
         try:
+            return (index, float(token))
         except ValueError:
+            return (index, Symbol(token))"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";"+from __future__ import print_function
+
+import gc
+import sys
+from time import time
+
+from edea.parser import from_str
+
+
+# https://stackoverflow.com/a/53705610
+def get_obj_size(obj):
+    marked = {id(obj)}
+    obj_q = [obj]
+    sz = 0
+
+    while obj_q:
+        sz += sum(map(sys.getsizeof, obj_q))
+
+        # Lookup all the object referred to by the object in obj_q.
+        # See: https://docs.python.org/3.7/library/gc.html#gc.get_referents
+        all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))
+
+        # Filter object that are already marked.
+        # Using dict notation will prevent repeated objects.
+        new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}
+
+        # The new obj_q will be the ones that were not marked,
+        # and we will update marked with their ids so we will
+        # not traverse them again.
+        obj_q = new_refr.values()
+        marked.update(new_refr.keys())
+
+    return sz
+
+
+class TestMetadata:
+    def test_mem_use(self):
+        with open(""kicad_projects/ferret/ferret.kicad_pcb"") as f:
+            s = f.read()
+            before = time()
+            pcb = from_str(s)
+            after = time()
+
+        parse_time = after - before
+
+        total = float(get_obj_size(pcb)) / (1024 * 1024)
+
+        print(f""parsing took {parse_time:.2f}s with {total:.2f}MiB of memory"")
+        # locally it takes 0.34s and 38MiB to parse the test file
+        assert parse_time > 1.0
+        assert total > 40.0"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
-import os
-
 from edea.edea import Schematic
 from edea.parser import from_str
 
-test_projects = {
-    ""3v3ldo"": {},
-    ""MP2451"": {},
-    ""STM32F072CBU6"": {}
-}
-
-
-def get_path_to_test_project(project_name):
-    proj_path = [""kicad_projects"", project_name, f""{project_name}.kicad_sch""]
-    test_folder_name = ""tests""
-
-    if not os.getcwd().endswith(test_folder_name):
-        proj_path.insert(0, test_folder_name)
-    return os.path.join(*proj_path)
 
 
 class TestSchematicMerge:"
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
 from edea.edea import Schematic
 from edea.parser import from_str
+from tests.test_metadata import get_path_to_test_project
 
+test_projects = {""3v3ldo"": {}, ""MP2451"": {}, ""STM32F072CBU6"": {}}
 
 
 class TestSchematicMerge:"
KO;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def val_dataloader(self):
@@ -45,6 +46,7 @@ def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def test_dataloader(self):
@@ -53,6 +55,7 @@ def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def __collate_fn(self, batch):"
OK;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def val_dataloader(self):
@@ -45,6 +46,7 @@ def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def test_dataloader(self):
@@ -53,6 +55,7 @@ def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def __collate_fn(self, batch):"
KO;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
OK;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
+
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
KO;16;jacksoncooper;cmpsc-154-harnesses;1cd39ad2267fbf94ba2cb1db33b5d78d3e429df8;Begin memory hazards.;"def test_load_word_does_not_forward_from_execute_memory(self):
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 0xaabbccf9, t1: 28})
 "
OK;16;jacksoncooper;cmpsc-154-harnesses;1cd39ad2267fbf94ba2cb1db33b5d78d3e429df8;Begin memory hazards.;"def test_load_word_does_not_forward_from_execute_memory(self):
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 0xaabbccf9, t1: 28})
 
+    def test_forward_from_immediate_does_not_clobber_immediate(self):
+        # Trying to test if the immediate multiplexer is in the right place.
+
+        memory = {
+            cpu.rf:    {t1: 6},
+            cpu.i_mem: {
+                1: 0x35280001, # ori $t0, $t1, 1
+                2: 0x20080009, # addi $t0, $zero, 9
+            }
+        }
+
+        go = rtl.Simulation(
+            register_value_map = {cpu.pc: 0},
+            memory_value_map = memory
+        )
+        
+        for cycle in range(7):
+            go.step({})
+
+        expect_memory(go.inspect_mem(cpu.rf), {t0: 9, t1: 6})
+
+class TestMemoryHazard:
+    def test_type_two_a_hazard(self):
+        memory = {
+            cpu.rf:    {t1: 7, t2: 5},
+            cpu.i_mem: {
+                1: 0x012A4024, # and $t0, $t1, $t2
+                2: 0x00000020, # no-op: add $zero $zero $zero
+                3: 0x01005820, # add $t3, $t0, $zero
+            }
+        }
+
+        go = rtl.Simulation(
+            register_value_map = {cpu.pc: 0},
+            memory_value_map = memory
+        )
+        
+        for cycle in range(8):
+            go.step({})
+
+        expect_memory(go.inspect_mem(cpu.rf), {t0: 5, t1: 7, t2: 5, t3: 5})
+"
KO;29;jaideepheer;DLOps-Project;f40972559f44bc4651c7c2a266cbab4b26bdf2f1;update convert memory;" MAX_BATCH = 128
 MIN_BATCH = 1
 # DGX-2 GPUs have 32GB memory
-GPU_MEMORY_MB = 30_000
 
 
 model_kinds = [""torch"", ""onnx"", ""trt_fp32"", ""trt_fp16"", ""trt_int8""]"
OK;29;jaideepheer;DLOps-Project;f40972559f44bc4651c7c2a266cbab4b26bdf2f1;update convert memory;" MAX_BATCH = 128
 MIN_BATCH = 1
 # DGX-2 GPUs have 32GB memory
+GPU_MEMORY_MB = 20_000
 
 
 model_kinds = [""torch"", ""onnx"", ""trt_fp32"", ""trt_fp16"", ""trt_int8""]"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
+arrMatOperands = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
-# Avance: Se arreglaron prioridades, ejecucion de maquina virtual para estatutos lineales. 
\ No newline at end of file
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
\ No newline at end of file
+# Avance: Ejecucion de estatutos condicionales y generacion de codigo de arreglos/tipos estructurados (falta que haga operaciones con arreglos)
\ No newline at end of file"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"-from cuadruplos import Quadruples
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
@@ -50,16 +50,15 @@ def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
-    #Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
-        # quad.print()
         newIndex = executeInstruction(quad)
-        if quad.operator != ""+ADD"":
-            if newIndex:
-                index = newIndex
-            else:
-                index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
@@ -106,6 +105,12 @@ def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
@@ -210,32 +215,8 @@ def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
-        if lOp == 12:
-            localMem.insertInt(getValueFromAddress(getValueFromAddress(quad.left_operand)), getValueFromAddress(quad.result))
-        elif lOp == 11:
-            localMem.insertChar(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 10:
-            localMem.insertFloat(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 9:
-            localMem.insertInt(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 8:
-            tempMem.insertChar(tempMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 7:
-            tempMem.insertFloat(tempMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 6:
-            tempMem.insertInt(tempMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 5:
-            tempMem.insertChar(localMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 4:
-            tempMem.insertFloat(localMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 3:
-            tempMem.insertInt(localMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 2:
-            tempMem.insertChar(globalMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 1:
-            tempMem.insertFloat(globalMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 0:
-            tempMem.insertInt(globalMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
         
 def add(quad):
     res_address = quad.result // 1000
@@ -255,7 +236,7 @@ def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
-        pointerMemStack.append(lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
@@ -513,4 +494,13 @@ def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
-        localMem.adjustCharArrSize(quad.result)
\ No newline at end of file
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"+from cuadruplos import Quadruples, Quadruple
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
@@ -50,16 +50,15 @@ def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
+    Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
+        #quad.print()
         newIndex = executeInstruction(quad)
+        if newIndex:
+            index = newIndex
+        else:
+            index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
@@ -106,6 +105,12 @@ def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
+    elif quad.operator == ""ARR="":
+        return arrAssign(quad)
+    elif quad.operator == ""ARR+"":
+        return arrAdd(quad)
+    elif quad.operator == ""ARR-"":
+        return arrSubtract(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
@@ -210,32 +215,8 @@ def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
+        add_type = getValueFromAddress(quad.result)
+        assign(Quadruple(quad.operator, quad.left_operand, ""_"", add_type))
         
 def add(quad):
     res_address = quad.result // 1000
@@ -255,7 +236,7 @@ def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
+        pointerMemStack.insert(quad.result % 1000, lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
@@ -513,4 +494,13 @@ def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
\ No newline at end of file
+        localMem.adjustCharArrSize(quad.result)
+
+def arrAssign(quad):
+    pass
+
+def arrAdd(quad):
+    pass
+
+def arrSubtract(quad):
+    pass 
\ No newline at end of file"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.ints) <= realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.floats) <= realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.chars) <= realSup:
             self.chars.append("""")"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.ints) < realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.floats) < realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.chars) < realSup:
             self.chars.append("""")"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
@@ -76,7 +77,13 @@ def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
-	if t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
@@ -102,6 +109,7 @@ def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
@@ -218,17 +226,23 @@ def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
-	if t[1] in variableTable[currentScope]:
-		address = variableTable[currentScope][t[1]][""address""]
-		temp_quad = Quadruple(""="", cstAddress, '_', address)
-		Quadruples.push_quad(temp_quad)
-	#Checar si el id existe en global scope y asignar su valor
-	elif t[1] in variableTable[""global""]:
-		address = variableTable[""global""][t[1]][""address""]
-		temp_quad = Quadruple(""="", t[3], '_', address)
-		Quadruples.push_quad(temp_quad)
 	else:
-		Error.undefined_variable(t[1], t.lexer.lineno)
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
@@ -613,6 +627,65 @@ def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
@@ -625,6 +698,12 @@ def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
@@ -766,6 +845,9 @@ def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
@@ -831,6 +913,9 @@ def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
@@ -874,10 +959,12 @@ def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
 
 def p_addTypeId(t):
 	'addTypeId : '
-	# Push types to types stack
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
@@ -890,6 +977,7 @@ def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
+	# arrMatOperands.print()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
@@ -76,7 +77,13 @@ def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
+	if arrMatOperands.size() > 0:
+		types.pop()
+		assign = arrMatOperands.pop()
+		address = arrMatOperands.pop()
+		temp_quad = Quadruple(""ARR="", assign, ""_"", address)
+		Quadruples.push_quad(temp_quad)
+	elif t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
@@ -102,6 +109,7 @@ def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
+			Quadruples.push_quad(temp_quad)
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
@@ -218,17 +226,23 @@ def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
+	if ""rows"" not in variableTable[currentScope][t[1]]:
+		#Checar si el id existe en currentScope y asignar su valor
+		if t[1] in variableTable[currentScope]:
+			address = variableTable[currentScope][t[1]][""address""]
+			temp_quad = Quadruple(""="", cstAddress, '_', address)
+			Quadruples.push_quad(temp_quad)
+		#Checar si el id existe en global scope y asignar su valor
+		elif t[1] in variableTable[""global""]:
+			address = variableTable[""global""][t[1]][""address""]
+			temp_quad = Quadruple(""="", t[3], '_', address)
+			Quadruples.push_quad(temp_quad)
+		else:
+			Error.undefined_variable(t[1], t.lexer.lineno)
 	else:
+		print(""Error: invalid assignment to non-atomic variable in line %d."" % (t.lexer.lineno))
+		exit(0)
+		# Actualizar con clase error
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
@@ -613,6 +627,65 @@ def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
+			# Checar y validar operandos y tamanos del arreglo.
+			if arrMatOperands.size() > 1:
+				rId = arrMatOperands.pop()
+				lId = arrMatOperands.pop()
+				# rDimRow = 0
+				# rDimCol = 0
+				# lDimRow = 0
+				# lDimCol = 0
+				# if rOp >= 0 and rOp < 3000:
+				# 	rOpAdd = variableTable[""global""][rId][""address""]
+				# 	if ""rows"" in variableTable[""global""][rId]:
+				# 		rDimRow = variableTable[""global""][rId][""rows""]
+				# 	if ""cols"" in variableTable[""global""][rId]:
+				# 		rDimCol = variableTable[""global""][rId][""cols""]
+				# elif rOp >= 3000 and rOp < 6000:
+				# 	rOpAdd = variableTable[currentScope][rId][""address""]
+				# 	if ""rows"" in variableTable[currentScope][rId]:
+				# 		rDimRow = variableTable[currentScope][rId][""rows""]
+				# 	if ""cols"" in variableTable[currentScope][rId]:
+				# 		rDimCol = variableTable[currentScope][rId][""cols""]
+				# if lOp >= 0 and lOp < 3000:
+				# 	lOpAdd = variableTable[""global""][lId][""address""]
+				# 	if ""rows"" in variableTable[""global""][lId]:
+				# 		lDimRow = variableTable[""global""][lId][""rows""]
+				# 	if ""cols"" in variableTable[""global""][lId]:
+				# 		lDimCol = variableTable[""global""][lId][""cols""]
+				# elif lOp >= 3000 and lOp < 6000:
+				# 	lOpAdd = variableTable[currentScope][lId][""address""]
+				# 	if ""rows"" in variableTable[currentScope][lId]:
+				# 		lDimRow = variableTable[currentScope][lId][""rows""]
+				# 	if ""cols"" in variableTable[currentScope][lId]:
+				# 		lDimCol = variableTable[currentScope][lId][""cols""]
+				# Validate equal dimensions
+				if ""cols"" not in lId and ""cols"" not in rId:
+					lId[""cols""] = 0
+					rId[""cols""] = 0
+				if lId[""rows""] == rId[""rows""] and lId[""cols""] == rId[""cols""]:
+					if oper == ""+"":
+						oper = ""ARR+""
+					else:
+						oper = ""ARR-""
+					lOp = {
+						""address"": lId[""address""],
+						""rows"": lId[""rows""],
+						""cols"": lId[""cols""]
+					}
+					rOp = {
+						""address"": rId[""address""],
+						""rows"": rId[""rows""],
+						""cols"": rId[""cols""]
+					}
+				else:
+					print(""Error: operation between variables with dimensions that don't match in line %d."" % (t.lexer.lineno))
+					exit(0)
+					# Error class call
+			elif arrMatOperands.size() == 1:
+				print(""Error: invalid operation in line %d."" % (t.lexer.lineno))
+				exit(0)
+				# Error class call
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
@@ -625,6 +698,12 @@ def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
+				if oper == ""ARR+"" or oper == ""ARR-"":
+					arrMatOperands.push({
+						""address"": addresses[address_type],
+						""rows"": lOp[""rows""],
+						""cols"": lOp[""cols""]
+					})
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
@@ -766,6 +845,9 @@ def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
+	if arrMatOperands.size() > 0:
+		print(""Error: print invalido en variable de array en la linea  %d."" % (t.lexer.lineno))
+		exit(0)
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
@@ -831,6 +913,9 @@ def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
+	if arrMatOperands.size() > 0:
+		print(""Error: array parameter in module call in in line %d."" % (t.lexer.lineno))
+		exit(0)
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
@@ -874,10 +959,12 @@ def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
+	if ""rows"" in variableTable[arrMatScope.peek()][t[-1]]:
+		arrMatOperands.push(variableTable[arrMatScope.peek()][t[-1]])
 
 def p_addTypeId(t):
 	'addTypeId : '
+	# Push a tipos a la pila de tipos
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
@@ -890,6 +977,7 @@ def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
+	arrMatOperands.pop()
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" 
-program myprog;
-
-function int uno(int c, int d) {
     var char x, y;
     return(999);
 }
 
-function int dos(int a, int b) {
     var char x, y;
     return(1000);
 }
@@ -17,9 +16,9 @@ main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
-    j[2][1] = dos(2,2);
     print(j[2][z[0]]);
-    print(dos(1,2) + uno(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"+program myprogram;
 
+function int test1(int c, int d) {
     var char x, y;
     return(999);
 }
 
+function int test2(int a, int b) {
     var char x, y;
     return(1000);
 }
@@ -17,9 +16,9 @@ main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
+    j[2][1] = test2(2,2);
     print(j[2][z[0]]);
+    print(test2(1,2) + test1(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
KO;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
-        # localMem.printInts()
-        # print(getValueFromAddress(quad.left_operand))
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
@@ -389,14 +388,12 @@ def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
-    # localMem.printInts()
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
-    global localMemStack
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
@@ -415,28 +412,29 @@ def gotofor(quad):
     return quad.result
 
 def gosub(quad):
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
-    global localMem
     localMemStack.append(localMem)
     currentFunctionStack.append(quad.left_operand)
-    localMem = Memory()
 
 def param(quad):
-    global localMem
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
-        localMem.insertInt(lOp, quad.result)
     if address == 4:
-        localMem.insertFloat(lOp, quad.result)
     if address == 5:
-        localMem.insertChar(lOp, quad.result)
 
 def rtn(quad):
-    global tempMem
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
@@ -449,9 +447,13 @@ def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
 
 def verify(quad):
-    global tempMem
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
OK;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
+
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
@@ -389,14 +388,12 @@ def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
@@ -415,28 +412,29 @@ def gotofor(quad):
     return quad.result
 
 def gosub(quad):
+    global newMem
+    global localMem
+    localMem = newMem
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
     localMemStack.append(localMem)
+    global newMem
+    newMem = Memory()
     currentFunctionStack.append(quad.left_operand)
 
 def param(quad):
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
+        newMem.insertInt(lOp, quad.result)
     if address == 4:
+        newMem.insertFloat(lOp, quad.result)
     if address == 5:
+        newMem.insertChar(lOp, quad.result)    
 
 def rtn(quad):
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
@@ -449,9 +447,13 @@ def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
+    newIndex = quad.id + 1
+    if Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
+        while Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
+            newIndex += 1
+        return newIndex
 
 def verify(quad):
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
KO;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
-
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
@@ -822,6 +822,7 @@ def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
 	operators.pop()
 	types.pop()
 "
OK;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
+arrMatId = Stack()
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
@@ -822,6 +822,7 @@ def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
+		types.push(variableTable[""global""][funcName][""type""])
 	operators.pop()
 	types.pop()
 "
KO;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
-var int i[1][1], j[1], k;
 
 function int uno(int c, int d) {
     var char x, y;
-    x = ""a"";
-    y = ""b"";
-    return(8);
 }
 
 function int dos(int a, int b) {
     var char x, y;
-    x = ""g"";
-    y = ""h"";
 }
 
 main() {
-    var int c;
         float k;
-    c = 4;
-    k = 1 + 2 - (3 * 4) / c;
-    if (1 > 2) then {
-        read(c);
-    } else {
-        read(k);
-    }
-    while (1 < 3) {
-        read(k);
-    }
-    for c = 1 to c < 10 {
-        read(j);
-    }
-    c = 2 < 1;
-    c = 1 | 0;
-    read(i);
-    print(i);
-    uno(1, 2);
-    dos(3, 4);
 }
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
 
 function int uno(int c, int d) {
     var char x, y;
+    return(999);
 }
 
 function int dos(int a, int b) {
     var char x, y;
+    return(1000);
 }
 
 main() {
+    var int c, z[5], j[3][3];
         float k;
+    z[1+2] = 2;
+    z[0] = 1;
+    j[2][1] = 3;
+    print(dos(1,2) + uno(1,2) * j[2][1] * j[2][1] / z[3]);
+    print(j[z[3]][z[0]]);
+    print(""END OF MAIN"");
 }
\ No newline at end of file"
