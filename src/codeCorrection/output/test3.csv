Page;Username;Repo;Commit;Bug;Code
1;CorentinJ;Real-Time-Voice-Cloning;ded7b37234e229d9bde0a9a506f7c65605803731;"Low memory inference fix (#536)

* For low_mem, use spawned workers instead of forked workers (resolves #36)
Used implementation from @lilydjwg: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/36#issuecomment-529380190

* Different method of passing the seed for low_mem inference
Resolves #491, #529, #535";"@@ -2,6 +2,7 @@
 from synthesizer.hparams import hparams
 from multiprocess.pool import Pool  # You're free to use either one
 #from multiprocessing import Pool   # 
+from multiprocess.context import SpawnContext
 from synthesizer import audio
 from pathlib import Path
 from typing import Union, List
@@ -97,16 +98,16 @@ def synthesize_spectrograms(self, texts: List[str],
             # Low memory inference mode: load the model upon every request. The model has to be 
             # loaded in a separate process to be able to release GPU memory (a simple workaround 
             # to tensorflow's intricacies)
-            specs, alignments = Pool(1).starmap(Synthesizer._one_shot_synthesize_spectrograms, 
-                                                [(self.checkpoint_fpath, embeddings, texts)])[0]
+            specs, alignments = Pool(1, context=SpawnContext()).starmap(Synthesizer._one_shot_synthesize_spectrograms,
+                                                [(self.checkpoint_fpath, embeddings, texts, self._seed)])[0]
     
         return (specs, alignments) if return_alignments else specs
 
     @staticmethod
-    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts):
+    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts, seed):
         # Load the model and forward the inputs
         tf.compat.v1.reset_default_graph()
-        model = Tacotron2(checkpoint_fpath, hparams, seed=self._seed)
+        model = Tacotron2(checkpoint_fpath, hparams, seed=seed)
         specs, alignments = model.my_synthesize(embeddings, texts)
         
         # Detach the outputs (not doing so will cause the process to hang)"
2;ultralytics;yolov5;6e4661773e08aee5e9673f68aacc1817b8a6fca9;"AutoBatch checks against failed solutions (#8159)

* AutoBatch checks against failed solutions

@kalenmike this is a simple improvement to AutoBatch to verify that returned solutions have not already failed, i.e. return batch-size 8 when 8 already produced CUDA out of memory.

This is a halfway fix until I can implement a 'final solution' that will actively verify the solved-for batch size rather than passively assume it works.

* Update autobatch.py

* Update autobatch.py";"@@ -8,7 +8,7 @@
 import numpy as np
 import torch
 
-from utils.general import LOGGER, colorstr
+from utils.general import LOGGER, colorstr, emojis
 from utils.torch_utils import profile
 
 
@@ -26,32 +26,41 @@ def autobatch(model, imgsz=640, fraction=0.9, batch_size=16):
     #     model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)
     #     print(autobatch(model))
 
+    # Check device
     prefix = colorstr('AutoBatch: ')
     LOGGER.info(f'{prefix}Computing optimal batch size for --imgsz {imgsz}')
     device = next(model.parameters()).device  # get model device
     if device.type == 'cpu':
         LOGGER.info(f'{prefix}CUDA not detected, using default CPU batch-size {batch_size}')
         return batch_size
 
+    # Inspect CUDA memory
     gb = 1 << 30  # bytes to GiB (1024 ** 3)
     d = str(device).upper()  # 'CUDA:0'
     properties = torch.cuda.get_device_properties(device)  # device properties
-    t = properties.total_memory / gb  # (GiB)
-    r = torch.cuda.memory_reserved(device) / gb  # (GiB)
-    a = torch.cuda.memory_allocated(device) / gb  # (GiB)
-    f = t - (r + a)  # free inside reserved
+    t = properties.total_memory / gb  # GiB total
+    r = torch.cuda.memory_reserved(device) / gb  # GiB reserved
+    a = torch.cuda.memory_allocated(device) / gb  # GiB allocated
+    f = t - (r + a)  # GiB free
     LOGGER.info(f'{prefix}{d} ({properties.name}) {t:.2f}G total, {r:.2f}G reserved, {a:.2f}G allocated, {f:.2f}G free')
 
+    # Profile batch sizes
     batch_sizes = [1, 2, 4, 8, 16]
     try:
         img = [torch.zeros(b, 3, imgsz, imgsz) for b in batch_sizes]
-        y = profile(img, model, n=3, device=device)
+        results = profile(img, model, n=3, device=device)
     except Exception as e:
         LOGGER.warning(f'{prefix}{e}')
 
-    y = [x[2] for x in y if x]  # memory [2]
-    batch_sizes = batch_sizes[:len(y)]
-    p = np.polyfit(batch_sizes, y, deg=1)  # first degree polynomial fit
+    # Fit a solution
+    y = [x[2] for x in results if x]  # memory [2]
+    p = np.polyfit(batch_sizes[:len(y)], y, deg=1)  # first degree polynomial fit
     b = int((f * fraction - p[1]) / p[0])  # y intercept (optimal batch size)
-    LOGGER.info(f'{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%)')
+    if None in results:  # some sizes failed
+        i = results.index(None)  # first fail index
+        if b >= batch_sizes[i]:  # y intercept above failure point
+            b = batch_sizes[max(i - 1, 0)]  # select prior safe point
+
+    fraction = np.polyval(p, b) / t  # actual fraction predicted
+    LOGGER.info(emojis(f'{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%) âœ…'))
     return b"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -149,30 +149,28 @@ def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criter
 
     # noinspection PyMethodMayBeStatic
     def compute_lens(self, data: Union[List[Dict[str, Any]], str], dataset: TransformableDataset,
-                     input_ids='token_input_ids', length_field='token'):
+                     input_ids='token_input_ids'):
         """"""
 
         Args:
             data: Samples to be measured or path to dataset during training time.
             dataset: During training time, use this dataset to measure the length of each sample inside.
             input_ids: Field name corresponds to input ids.
-            length_field: Fall back to this field during prediction as input_ids may not be generated yet.
 
         Returns:
 
             Length list of this samples
 
         """"""
+        if not dataset.cache:
+            warnings.warn(f'Caching for the dataset is not enabled, '
+                          f'try `dataset.purge_cache()` if possible. The dataset is {dataset}.')
         if isinstance(data, str):
-            if not dataset.cache:
-                warnings.warn(f'Caching for the dataset is not enabled, '
-                              f'try `dataset.purge_cache()` if possible. The dataset is {dataset}.')
             timer = CountdownTimer(len(dataset))
             for each in dataset:
                 timer.log('Preprocessing and caching samples [blink][yellow]...[/yellow][/blink]')
             timer.erase()
-            return [len(x[input_ids]) for x in dataset]
-        return [len(x[length_field]) for x in data]
+        return [len(x[input_ids]) for x in dataset]
 
     def feed_batch(self,
                    h: torch.FloatTensor,"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -85,11 +85,10 @@ def build_dataloader(self,
                          gradient_accumulation=1,
                          **kwargs) -> DataLoader:
         dataset = CRFConstituencyParsing.build_dataset(self, data, transform)
-        if isinstance(data, str):
-            dataset.purge_cache()
+        dataset.purge_cache()
         if self.vocabs.mutable:
             CRFConstituencyParsing.build_vocabs(self, dataset, logger)
-        if dataset.cache:
+        if isinstance(data, str):
             timer = CountdownTimer(len(dataset))
             # noinspection PyCallByClass
             BiaffineDependencyParser.cache_dataset(self, dataset, timer, training, logger)"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -114,18 +114,17 @@ def build_dataloader(self, data, transform: TransformList = None, training=False
                          logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:
         transform.insert(0, append_bos)
         dataset = BiaffineDependencyParser.build_dataset(self, data, transform)
-        if isinstance(data, str):
-            dataset.purge_cache()
+        dataset.purge_cache()
         if self.vocabs.mutable:
             BiaffineDependencyParser.build_vocabs(self, dataset, logger, transformer=True)
-        if dataset.cache:
+        if isinstance(data, str):
             timer = CountdownTimer(len(dataset))
             BiaffineDependencyParser.cache_dataset(self, dataset, timer, training, logger)
         max_seq_len = self.config.get('max_seq_len', None)
         if max_seq_len and isinstance(data, str):
             dataset.prune(lambda x: len(x['token_input_ids']) > 510, logger)
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, length_field='FORM'),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),
                                                      shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset,"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -59,8 +59,7 @@ def __init__(self, trn: str = None, dev: str = None, tst: str = None, sampler_bu
     def build_dataloader(self, data, transform: Callable = None, training=False, device=None,
                          logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:
         dataset = BiaffineSecondaryParser.build_dataset(self, data, transform)
-        if isinstance(data, str):
-            dataset.purge_cache()
+        dataset.purge_cache()
         if self.vocabs.mutable:
             BiaffineSecondaryParser.build_vocabs(self, dataset, logger, transformer=True)
         return PadSequenceDataLoader("
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -84,12 +84,12 @@ def build_dataloader(self,
                          **kwargs) -> DataLoader:
         args = dict((k, self.config[k]) for k in
                     ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)
-        dataset = self.build_dataset(data, cache=cache, transform=transform, **args)
+        dataset = self.build_dataset(data, cache=True, transform=transform, **args)
         dataset.append_transform(self.vocabs)
         if self.vocabs.mutable:
             self.build_vocabs(dataset, logger)
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, 'token_input_ids', 'token'),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),
                                                      shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset)"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -81,6 +81,7 @@ def build_dataloader(self, data,
         transform = copy(transform)
         transform.append(unpack_ner)
         dataset = BiaffineNamedEntityRecognizer.build_dataset(self, data, self.vocabs, transform)
+        dataset.purge_cache()
         if self.vocabs.mutable:
             BiaffineNamedEntityRecognizer.build_vocabs(self, dataset, logger, self.vocabs)
         return PadSequenceDataLoader("
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -122,11 +122,12 @@ def build_dataloader(self,
                     ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)
         dataset = self.build_dataset(data, cache=cache, transform=transform, **args)
         dataset.append_transform(self.vocabs)
+        dataset.purge_cache()
         if self.vocabs.mutable:
             self.build_vocabs(dataset, logger)
         return PadSequenceDataLoader(
             batch_sampler=self.sampler_builder.build(
-                self.compute_lens(data, dataset, 'token_input_ids', 'token'),
+                self.compute_lens(data, dataset),
                 shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset)"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -121,12 +121,12 @@ def build_dataloader(self,
                          **kwargs) -> DataLoader:
         args = dict((k, self.config[k]) for k in
                     ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)
-        dataset = self.build_dataset(data, cache=cache, transform=transform, **args)
+        dataset = self.build_dataset(data, cache=True, transform=transform, **args)
         dataset.append_transform(self.vocabs)
         if self.vocabs.mutable:
             self.build_vocabs(dataset, logger)
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, 'token_input_ids', 'token'),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),
                                                      shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset)"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -114,18 +114,14 @@ def build_metric(self, **kwargs):
     def build_dataloader(self, data, transform: TransformList = None, training=False, device=None,
                          logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:
         dataset = BiaffineSemanticDependencyParser.build_dataset(self, data, transform)
-        if isinstance(data, str):
-            dataset.purge_cache()
-            length_field = 'token'
-        else:
-            length_field = 'FORM'
+        dataset.purge_cache()
         if self.vocabs.mutable:
             BiaffineSemanticDependencyParser.build_vocabs(self, dataset, logger, transformer=True)
-        if dataset.cache:
+        if isinstance(data, str):
             timer = CountdownTimer(len(dataset))
             BiaffineSemanticDependencyParser.cache_dataset(self, dataset, timer, training, logger)
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, length_field=length_field),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),
                                                      shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset,"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -67,6 +67,7 @@ def __init__(self,
     def build_dataloader(self, data, transform: Callable = None, training=False, device=None,
                          logger: logging.Logger = None, cache=False, gradient_accumulation=1, **kwargs) -> DataLoader:
         dataset = self.build_dataset(data, transform=[transform, self.vocabs])
+        dataset.purge_cache()
         if self.vocabs.mutable:
             SpanBIOSemanticRoleLabeler.build_vocabs(self, dataset, logger)
         return PadSequenceDataLoader("
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -75,6 +75,7 @@ def __init__(self, trn: str = None, dev: str = None, tst: str = None, sampler_bu
     def build_dataloader(self, data, transform: Callable = None, training=False, device=None,
                          logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:
         dataset = self.build_dataset(data, isinstance(data, list), logger, transform)
+        dataset.purge_cache()
         return PadSequenceDataLoader(
             batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset), shuffle=training,
                                                      gradient_accumulation=gradient_accumulation),"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -74,7 +74,7 @@ def build_dataloader(self,
                          tokenizer: PreTrainedTokenizer = None,
                          **kwargs) -> DataLoader:
         assert tokenizer
-        dataset = TextTokenizingDataset(data, cache=isinstance(data, str), delimiter=self.config.sent_delimiter,
+        dataset = TextTokenizingDataset(data, cache=True, delimiter=self.config.sent_delimiter,
                                         generate_idx=isinstance(data, list),
                                         max_seq_len=self.config.max_seq_len,
                                         sent_delimiter=self.config.sent_delimiter,
@@ -87,7 +87,7 @@ def build_dataloader(self,
                                             FieldLength('text_input_ids', 'text_input_ids_length', delta=-2),
                                             generate_token_span_tuple])
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, 'text_input_ids', 'text'),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, 'text_input_ids'),
                                                      shuffle=training),
             device=device,
             dataset=dataset)"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -104,10 +104,11 @@ def build_dataloader(self, data, transform: TransformList = None, training=False
             transform.insert(0, self.transform)
         transform.append(self.last_transform())
         dataset = self.build_dataset(data, cache=cache, transform=transform, **args)
+        dataset.purge_cache()
         if self.vocabs.mutable:
             self.build_vocabs(dataset, logger)
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, 'token_input_ids'),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),
                                                      shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset)"
2;hankcs;HanLP;672d662c9a627f8c9fb6f4d997f317da6442ce8b;Deprecated `length_field`. Since the memory consumption is dominated by encoders, input_ids is always the field that determines the length of a sample.;"@@ -76,13 +76,14 @@ def build_dataloader(self, data, transform: Callable = None, training=False, dev
         if isinstance(data, str) and not self.config.punct:
             _transform.append(PunctuationMask('token', 'punct_mask'))
         dataset = UniversalDependenciesParser.build_dataset(self, data, _transform)
+        dataset.purge_cache()
         if self.vocabs.mutable:
             UniversalDependenciesParser.build_vocabs(self, dataset, logger, transformer=True)
         max_seq_len = self.config.get('max_seq_len', None)
         if max_seq_len and isinstance(data, str):
             dataset.prune(lambda x: len(x['token_input_ids']) > max_seq_len, logger)
         return PadSequenceDataLoader(
-            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, length_field='token'),
+            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),
                                                      shuffle=training, gradient_accumulation=gradient_accumulation),
             device=device,
             dataset=dataset,"
3;facebookresearch;fairseq;eb2d7862c29990e5be35ee227a6952ae21d621a1;"fix ema memory leak (#3384)

Summary:
fixes memory leak in ema module by making sure the update happens in no_grad regime

X-link: https://github.com/fairinternal/fairseq-py/pull/3384

Reviewed By: arbabu123

Differential Revision: D36352890

Pulled By: alexeib

fbshipit-source-id: 0f3575ac356a13483e00ed431375b2c798621a3a";"@@ -98,7 +98,7 @@ def _step_internal(self, new_model):
         ema_params = (
             self.fp32_params if self.config.ema_fp32 else self.model.state_dict()
         )
-        for key, param in new_model.state_dict().items():
+        for key, param in new_model.named_parameters():
             if isinstance(param, dict):
                 continue
             try:
@@ -107,6 +107,7 @@ def _step_internal(self, new_model):
                 ema_param = (
                     param.float().clone() if param.ndim == 1 else copy.deepcopy(param)
                 )
+                ema_params[key] = ema_param
 
             if param.shape != ema_param.shape:
                 raise ValueError(
@@ -118,15 +119,21 @@ def _step_internal(self, new_model):
                 # Do not decay a model.version pytorch param
                 continue
 
-            if key in self.skip_keys:
-                ema_param = param.to(dtype=ema_param.dtype).clone()
-                ema_params[key].copy_(ema_param)
+            if key in self.skip_keys or not param.requires_grad:
+                ema_params[key].copy_(param.to(dtype=ema_param.dtype).data)
+                ema_param = ema_params[key]
             else:
                 ema_param.mul_(decay)
-                ema_param.add_(param.to(dtype=ema_param.dtype), alpha=1 - decay)
+                ema_param.add_(param.data.to(dtype=ema_param.dtype), alpha=1 - decay)
+
             ema_state_dict[key] = ema_param
+
+        for key, param in new_model.named_buffers():
+            ema_state_dict[key] = param
+
         self.restore(ema_state_dict, build_fp32_params=False)
 
+    @torch.no_grad()
     def step(self, new_model):
         self._step_internal(new_model)
 "
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -279,6 +279,17 @@ def truncate_emb(key):
                     logger.info(""Overwriting "" + prefix + ""classification_heads."" + k)
                     state_dict[prefix + ""classification_heads."" + k] = v
 
+    def set_beam_size(self, beam):
+        """"""Set beam size for efficient beamable enc-dec attention.""""""
+        beamable = False
+        for layer in self.decoder.layers:
+            if layer.encoder_attn is not None:
+                if hasattr(layer.encoder_attn, ""set_beam_size""):
+                    layer.encoder_attn.set_beam_size(beam)
+                    beamable = True
+        if beamable:
+            self.encoder.reorder_encoder_out = self.encoder._reorder_encoder_out
+
 
 class BARTClassificationHead(nn.Module):
     """"""Head for sentence-level classification tasks."""""""
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -287,9 +287,6 @@ def extract_features_scriptable(
         padding_mask: Optional[Tensor] = None
         if encoder_out is not None and len(encoder_out[""encoder_out""]) > 0:
             enc = encoder_out[""encoder_out""][0]
-            assert (
-                enc.size()[1] == bs
-            ), f""Expected enc.shape == (t, {bs}, c) got {enc.shape}""
         if encoder_out is not None and len(encoder_out[""encoder_padding_mask""]) > 0:
             padding_mask = encoder_out[""encoder_padding_mask""][0]
 "
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -313,6 +313,11 @@ def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
             ""src_lengths"": src_lengths,  # B x 1
         }
 
+    @torch.jit.export
+    def _reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
+        """"""Dummy re-order function for beamable enc-dec attention""""""
+        return encoder_out
+
     def max_positions(self):
         """"""Maximum input length supported by the encoder.""""""
         if self.embed_positions is None:"
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -84,7 +84,7 @@ def __init__(
             self.bias_k = self.bias_v = None
 
         self.add_zero_attn = add_zero_attn
-
+        self.beam_size = 1
         self.reset_parameters()
 
         self.onnx_trace = False
@@ -286,9 +286,8 @@ def forward(
         if key is not None:
             src_len, key_bsz, _ = key.size()
             if not torch.jit.is_scripting():
-                assert key_bsz == bsz
                 assert value is not None
-                assert src_len, bsz == value.shape[:2]
+                assert src_len, key_bsz == value.shape[:2]
 
         if (
             not self.onnx_trace
@@ -351,6 +350,11 @@ def forward(
                 assert value is None
                 k = v = None
             else:
+                if self.beam_size > 1 and bsz == key.size(1):
+                    # key is [T, bsz*beam_size, C], reduce to [T, bsz, C]
+                    key = key.view(key.size(0), -1, self.beam_size, key.size(2))[:, :, 0, :]
+                    if key_padding_mask is not None:
+                        key_padding_mask = key_padding_mask.view(-1, self.beam_size, key_padding_mask.size(1))[:, 0, :]
                 k = self.k_proj(key)
                 v = self.v_proj(key)
 
@@ -383,16 +387,18 @@ def forward(
             .view(tgt_len, bsz * self.num_heads, self.head_dim)
             .transpose(0, 1)
         )
+        kv_bsz = bsz  # need default value for scripting
         if k is not None:
+            kv_bsz = k.size(1)
             k = (
                 k.contiguous()
-                .view(-1, bsz * self.num_heads, self.head_dim)
+                .view(-1, kv_bsz * self.num_heads, self.head_dim)
                 .transpose(0, 1)
             )
         if v is not None:
             v = (
                 v.contiguous()
-                .view(-1, bsz * self.num_heads, self.head_dim)
+                .view(-1, kv_bsz * self.num_heads, self.head_dim)
                 .transpose(0, 1)
             )
 
@@ -401,7 +407,8 @@ def forward(
             if ""prev_key"" in saved_state:
                 _prev_key = saved_state[""prev_key""]
                 assert _prev_key is not None
-                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)
+                kv_bsz = _prev_key.size(0)
+                prev_key = _prev_key.view(kv_bsz * self.num_heads, -1, self.head_dim)
                 if static_kv:
                     k = prev_key
                 else:
@@ -411,7 +418,8 @@ def forward(
             if ""prev_value"" in saved_state:
                 _prev_value = saved_state[""prev_value""]
                 assert _prev_value is not None
-                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)
+                assert kv_bsz == _prev_value.size(0)
+                prev_value = _prev_value.view(kv_bsz * self.num_heads, -1, self.head_dim)
                 if static_kv:
                     v = prev_value
                 else:
@@ -424,13 +432,13 @@ def forward(
             key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(
                 key_padding_mask=key_padding_mask,
                 prev_key_padding_mask=prev_key_padding_mask,
-                batch_size=bsz,
+                batch_size=kv_bsz,
                 src_len=k.size(1),
                 static_kv=static_kv,
             )
 
-            saved_state[""prev_key""] = k.view(bsz, self.num_heads, -1, self.head_dim)
-            saved_state[""prev_value""] = v.view(bsz, self.num_heads, -1, self.head_dim)
+            saved_state[""prev_key""] = k.view(kv_bsz, self.num_heads, -1, self.head_dim)
+            saved_state[""prev_value""] = v.view(kv_bsz, self.num_heads, -1, self.head_dim)
             saved_state[""prev_key_padding_mask""] = key_padding_mask
             # In this branch incremental_state is never None
             assert incremental_state is not None
@@ -444,7 +452,7 @@ def forward(
             key_padding_mask = None
 
         if key_padding_mask is not None:
-            assert key_padding_mask.size(0) == bsz
+            assert key_padding_mask.size(0) == kv_bsz
             assert key_padding_mask.size(1) == src_len
 
         if self.add_zero_attn:
@@ -467,7 +475,15 @@ def forward(
                     dim=1,
                 )
 
-        attn_weights = torch.bmm(q, k.transpose(1, 2))
+        if self.encoder_decoder_attention and bsz != kv_bsz:
+            attn_weights = torch.einsum(
+                ""bxhtd,bhsd->bxhts"",
+                q.view((kv_bsz, -1, self.num_heads) + q.size()[1:]),
+                k.view((kv_bsz, self.num_heads) + k.size()[1:]),
+            )
+            attn_weights = attn_weights.reshape((-1,) + attn_weights.size()[-2:])
+        else:
+            attn_weights = torch.bmm(q, k.transpose(1, 2))
         attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
 
         assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
@@ -482,8 +498,12 @@ def forward(
             # don't attend to padding symbols
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
             if not is_tpu:
+                attn_weights = attn_weights.view(kv_bsz, -1, self.num_heads, tgt_len, src_len)
                 attn_weights = attn_weights.masked_fill(
-                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),
+                    key_padding_mask.unsqueeze(1)
+                    .unsqueeze(2)
+                    .unsqueeze(3)
+                    .to(torch.bool),
                     float(""-inf""),
                 )
             else:
@@ -502,7 +522,15 @@ def forward(
         attn_probs = self.dropout_module(attn_weights)
 
         assert v is not None
-        attn = torch.bmm(attn_probs, v)
+        if self.encoder_decoder_attention and bsz != kv_bsz:
+            attn = torch.einsum(
+                ""bxhts,bhsd->bxhtd"",
+                attn_probs.view((kv_bsz, -1, self.num_heads,) + attn_probs.size()[1:]),
+                v.view((kv_bsz, self.num_heads,) + v.size()[1:]),
+            )
+            attn = attn.reshape((-1,) + attn.size()[-2:])
+        else:
+            attn = torch.bmm(attn_probs, v)
         assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
         if self.onnx_trace and attn.size(1) == 1:
             # when ONNX tracing a single decoder step (sequence length == 1)
@@ -578,14 +606,22 @@ def reorder_incremental_state(
             for k in input_buffer.keys():
                 input_buffer_k = input_buffer[k]
                 if input_buffer_k is not None:
-                    if self.encoder_decoder_attention and input_buffer_k.size(
-                        0
-                    ) == new_order.size(0):
-                        break
-                    input_buffer[k] = input_buffer_k.index_select(0, new_order)
+                    if self.encoder_decoder_attention:
+                        if input_buffer_k.size(0) * self.beam_size == new_order.size(0):
+                            return incremental_state
+                        elif self.beam_size > 1:
+                            input_buffer[k] = input_buffer_k.index_select(0, new_order.reshape(-1, self.beam_size)[:, 0] // self.beam_size)
+                        else:
+                            input_buffer[k] = input_buffer_k.index_select(0, new_order)
+                    else:
+                        input_buffer[k] = input_buffer_k.index_select(0, new_order)
             incremental_state = self._set_input_buffer(incremental_state, input_buffer)
         return incremental_state
 
+    def set_beam_size(self, beam_size):
+        """"""Used for effiecient beamable enc-dec attention""""""
+        self.beam_size = beam_size
+
     def _get_input_buffer(
         self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]
     ) -> Dict[str, Optional[Tensor]]:"
3;facebookresearch;fairseq;806855bf660ea748ed7ffb42fe8dcc881ca3aca0;"Fast Beamable enc-dec attention

Summary:
Implements beamable encoder-decoder cross attention. This removes the need to duplicate the encoder states beam_size # of times during inference. Which gives both a big memory improvement enabling larger batch sizes while on GPU and also compute efficiency by greatly reducing time spent in reorder_encoder_out.

This is inspired from work in [fastseq](https://arxiv.org/abs/2106.04718) which has more in-depth analysis.

There was an old [PR](https://github.com/pytorch/fairseq/pull/1958) for fairseq as well to implement this feature but was not merged and eventually closed. I revive+refactor that PR and also add support for dynamically changing the beam_size while calling `hub_interface.generate()`

## Benchmarking

**CPU Performance** (On-demand devserver)
batch size: 1 | beam size: 4
50.4s/it -> 22.3s/it | **2.25X Speedup**

batch size: 2 | beam size: 4
53.1s/it -> 25.8s/it | **2.06X Speedup**

batch size: 1 | beam size: 8
65.8s/it -> 23.8s/it | **2.76X Speedup**

**GPU Performance**

Reported in detail [here](https://github.com/pytorch/fairseq/issues/1957)

Currently this optimization is only enabled for our custom BART model used in the workplace summarization demo to unblock landing this fast.
This should be up-streamed to TransformerModel after syncing with fairseq folk.

Reviewed By: xwhan

Differential Revision: D35722467

fbshipit-source-id: a420f73ff5b9ec0cdf40c59464b6ed1794114906";"@@ -80,6 +80,7 @@ def __init__(
         self.beam_size = beam_size
         # the max beam size is the dictionary size - 1, since we never select pad
         self.beam_size = min(beam_size, self.vocab_size - 1)
+        self.model.set_decoder_beam_size(self.beam_size)
         self.max_len_a = max_len_a
         self.max_len_b = max_len_b
         self.min_len = min_len
@@ -768,6 +769,13 @@ def max_decoder_positions(self):
             + [sys.maxsize]
         )
 
+    def set_decoder_beam_size(self, beam_size):
+        """"""Set beam size for efficient beamable enc-dec attention.""""""
+        if beam_size > 1:
+            for model in self.models:
+                if hasattr(model, 'set_beam_size'):
+                    model.set_beam_size(beam_size)
+
     @torch.jit.export
     def forward_encoder(self, net_input: Dict[str, Tensor]):
         if not self.has_encoder():"
4;ray-project;ray;1483c4553ccee2840275634d6a5271d87fb763a3;"use smaller instance for scheduling tests (#25635)

m5.16xlarge instances have 64 CPU and 256GB memory, which are overkill for scheduling tests that do not have a lot of computations. Use smaller instance m5.4xlarge to save cost and make allocating instances easier.";"@@ -5,15 +5,18 @@ max_workers: 999
 
 head_node_type:
     name: head_node
-    instance_type: m5.16xlarge
+    instance_type: m5.4xlarge
     resources:
+      # Assume the node has 64 CPU instead of 16.
+      # This should be fine since each task has little
+      # computation in scheduling tests.
       cpu: 64
       custom_resources:
         node: 1
 
 worker_node_types:
     - name: worker_node
-      instance_type: m5.16xlarge
+      instance_type: m5.4xlarge
       min_workers: 31
       max_workers: 31
       use_spot: false"
4;ray-project;ray;ce103b4ffafe7b9d9a1306d7a1b1be9b85f14600;Eagerly clears object memory before Python GC kicks in when consuming DatasetPipeline (#25461);"@@ -27,6 +27,7 @@ def batch_blocks(
     stats: Union[DatasetStats, DatasetPipelineStats],
     *,
     prefetch_blocks: int = 0,
+    clear_block_after_read: bool = False,
     batch_size: Optional[int] = None,
     batch_format: str = ""native"",
     drop_last: bool = False,
@@ -41,6 +42,11 @@ def batch_blocks(
     Args:
         prefetch_blocks: The number of blocks to prefetch ahead of the
             current block during the scan.
+        clear_block_after_read: Whether to clear the block from object store
+            manually (i.e. without waiting for Python's automatic GC) after it
+            is read. Doing so will reclaim memory faster and hence reduce the
+            memory footprint. However, the caller has to ensure the safety, i.e.
+            the block will never be accessed again.
         batch_size: Record batch size, or None to let the system pick.
         batch_format: The format in which to return each batch.
             Specify ""native"" to use the current block format (promoting
@@ -74,7 +80,9 @@ def batch_block(block: ObjectRef[Block]):
         prefetcher = ActorBlockPrefetcher()
     else:
         prefetcher = WaitBlockPrefetcher()
-    for block_window in _sliding_window(blocks, prefetch_blocks + 1):
+    for block_window in _sliding_window(
+        blocks, prefetch_blocks + 1, clear_block_after_read
+    ):
         block_window = list(block_window)
         with stats.iter_wait_s.timer():
             prefetcher.prefetch_blocks(block_window)
@@ -109,7 +117,7 @@ def _format_batch(batch: Block, batch_format: str) -> BatchType:
     return batch
 
 
-def _sliding_window(iterable: Iterable, n: int):
+def _sliding_window(iterable: Iterable, n: int, clear_block_after_read: bool = False):
     """"""Creates an iterator consisting of n-width sliding windows over
     iterable. The sliding windows are constructed lazily such that an
     element on the base iterator (iterable) isn't consumed until the
@@ -122,6 +130,11 @@ def _sliding_window(iterable: Iterable, n: int):
         iterable: The iterable on which the sliding window will be
             created.
         n: The width of the sliding window.
+        clear_block_after_read: Whether to clear the leftmost block
+            from object store manually (i.e. without waiting for Python's
+            automatic GC) when it's out of the sliding window (i.e. been
+            consumed), so as to reclaim memory faster. The caller has to
+            ensure safety, i.e. the block will never be accessed again.
 
     Returns:
         An iterator of n-width windows over iterable.
@@ -133,6 +146,9 @@ def _sliding_window(iterable: Iterable, n: int):
     if len(window) > 0:
         yield tuple(window)
     for elem in it:
+        block_ref = window.popleft()
+        if clear_block_after_read:
+            ray.internal.internal_api.free(block_ref, local_only=False)
         window.append(elem)
         yield tuple(window)
 "
4;ray-project;ray;ce103b4ffafe7b9d9a1306d7a1b1be9b85f14600;Eagerly clears object memory before Python GC kicks in when consuming DatasetPipeline (#25461);"@@ -173,11 +173,18 @@ def iter_batches(
         Returns:
             An iterator over record batches.
         """"""
+        if self._executed[0]:
+            raise RuntimeError(""Pipeline cannot be read multiple times."")
         time_start = time.perf_counter()
+        # When the DatasetPipeline actually did transformations (i.e. the self._stages
+        # isn't empty), there will be output blocks created. In this case, those output
+        # blocks are safe to clear right after read, because we know they will never be
+        # accessed again, given that DatasetPipeline can be read at most once.
         yield from batch_blocks(
             self._iter_blocks(),
             self._stats,
             prefetch_blocks=prefetch_blocks,
+            clear_block_after_read=(len(self._stages) > 0),
             batch_size=batch_size,
             batch_format=batch_format,
             drop_last=drop_last,"
4;ray-project;ray;ce103b4ffafe7b9d9a1306d7a1b1be9b85f14600;Eagerly clears object memory before Python GC kicks in when consuming DatasetPipeline (#25461);"@@ -0,0 +1,108 @@
+import time
+
+import pytest
+
+import ray
+from ray.tests.conftest import *  # noqa
+
+from ray.internal.internal_api import memory_summary
+
+
+def check_no_spill(ctx, pipe, prefetch_blocks: int = 0):
+    # Run .iter_batches() for 10 secs, and we expect no object spilling.
+    end_time = time.time() + 10
+    for batch in pipe.iter_batches(prefetch_blocks=prefetch_blocks):
+        if time.time() > end_time:
+            break
+    meminfo = memory_summary(ctx.address_info[""address""], stats_only=True)
+    assert ""Spilled"" not in meminfo, meminfo
+
+
+def test_iter_batches_no_spilling_upon_no_transformation(shutdown_only):
+    # The object store is about 300MB.
+    ctx = ray.init(num_cpus=1, object_store_memory=300e6)
+    # The size of dataset is 500*(80*80*4)*8B, about 100MB.
+    ds = ray.data.range_tensor(500, shape=(80, 80, 4), parallelism=100)
+
+    check_no_spill(ctx, ds.repeat())
+    check_no_spill(ctx, ds.repeat(), 5)
+
+    check_no_spill(ctx, ds.window(blocks_per_window=20))
+    check_no_spill(ctx, ds.window(blocks_per_window=20), 5)
+
+
+def test_iter_batches_no_spilling_upon_prior_transformation(shutdown_only):
+    # The object store is about 500MB.
+    ctx = ray.init(num_cpus=1, object_store_memory=500e6)
+    # The size of dataset is 500*(80*80*4)*8B, about 100MB.
+    ds = ray.data.range_tensor(500, shape=(80, 80, 4), parallelism=100)
+
+    # Repeat, with transformation prior to the pipeline.
+    check_no_spill(ctx, ds.map_batches(lambda x: x).repeat())
+    check_no_spill(ctx, ds.map_batches(lambda x: x).repeat(), 5)
+
+    # Window, with transformation prior to the pipeline.
+    check_no_spill(ctx, ds.map_batches(lambda x: x).window(blocks_per_window=20))
+    check_no_spill(ctx, ds.map_batches(lambda x: x).window(blocks_per_window=20), 5)
+
+
+def test_iter_batches_no_spilling_upon_post_transformation(shutdown_only):
+    # The object store is about 500MB.
+    ctx = ray.init(num_cpus=1, object_store_memory=500e6)
+    # The size of dataset is 500*(80*80*4)*8B, about 100MB.
+    ds = ray.data.range_tensor(500, shape=(80, 80, 4), parallelism=100)
+
+    # Repeat, with transformation post the pipeline creation.
+    check_no_spill(ctx, ds.repeat().map_batches(lambda x: x))
+    check_no_spill(ctx, ds.repeat().map_batches(lambda x: x), 5)
+
+    # Window, with transformation post the pipeline creation.
+    check_no_spill(ctx, ds.window(blocks_per_window=20).map_batches(lambda x: x))
+    check_no_spill(ctx, ds.window(blocks_per_window=20).map_batches(lambda x: x), 5)
+
+
+def test_iter_batches_no_spilling_upon_transformations(shutdown_only):
+    # The object store is about 700MB.
+    ctx = ray.init(num_cpus=1, object_store_memory=700e6)
+    # The size of dataset is 500*(80*80*4)*8B, about 100MB.
+    ds = ray.data.range_tensor(500, shape=(80, 80, 4), parallelism=100)
+
+    # Repeat, with transformation before and post the pipeline.
+    check_no_spill(ctx, ds.map_batches(lambda x: x).repeat().map_batches(lambda x: x))
+    check_no_spill(
+        ctx, ds.map_batches(lambda x: x).repeat().map_batches(lambda x: x), 5
+    )
+
+    # Window, with transformation before and post the pipeline.
+    check_no_spill(
+        ctx,
+        ds.map_batches(lambda x: x)
+        .window(blocks_per_window=20)
+        .map_batches(lambda x: x),
+    )
+    check_no_spill(
+        ctx,
+        ds.map_batches(lambda x: x)
+        .window(blocks_per_window=20)
+        .map_batches(lambda x: x),
+        5,
+    )
+
+
+def test_iter_batches_no_spilling_upon_shuffle(shutdown_only):
+    # The object store is about 500MB.
+    ctx = ray.init(num_cpus=1, object_store_memory=500e6)
+    # The size of dataset is 500*(80*80*4)*8B, about 100MB.
+    ds = ray.data.range_tensor(500, shape=(80, 80, 4), parallelism=100)
+
+    check_no_spill(ctx, ds.repeat().random_shuffle_each_window())
+    check_no_spill(ctx, ds.repeat().random_shuffle_each_window(), 5)
+
+    check_no_spill(ctx, ds.window(blocks_per_window=20).random_shuffle_each_window())
+    check_no_spill(ctx, ds.window(blocks_per_window=20).random_shuffle_each_window(), 5)
+
+
+if __name__ == ""__main__"":
+    import sys
+
+    sys.exit(pytest.main([""-v"", __file__]))"
5;python;mypy;405efd23e483c214cb1293700ef2008aacf8e2ca;"[mypyc] Use PyObject_CallNoArgs and PyObject_CallOneArg (#12862)

Use PyObject_CallNoArgs and PyObject_CallOneArg to replace 
PyObject_CallFunctionObjArgs in lib-rt, since the new API costs 
less memory according to python/cpython#13890 (comment)

Also remove the macro in pythonsupport.h since the two API are 
already covered by pythoncapi_compat.h.";"@@ -7,7 +7,7 @@
 
 void CPy_Raise(PyObject *exc) {
     if (PyObject_IsInstance(exc, (PyObject *)&PyType_Type)) {
-        PyObject *obj = PyObject_CallFunctionObjArgs(exc, NULL);
+        PyObject *obj = PyObject_CallNoArgs(exc);
         if (!obj)
             return;
         PyErr_SetObject(exc, obj);"
5;python;mypy;405efd23e483c214cb1293700ef2008aacf8e2ca;"[mypyc] Use PyObject_CallNoArgs and PyObject_CallOneArg (#12862)

Use PyObject_CallNoArgs and PyObject_CallOneArg to replace 
PyObject_CallFunctionObjArgs in lib-rt, since the new API costs 
less memory according to python/cpython#13890 (comment)

Also remove the macro in pythonsupport.h since the two API are 
already covered by pythoncapi_compat.h.";"@@ -54,7 +54,7 @@ int CPy_YieldFromErrorHandle(PyObject *iter, PyObject **outp)
     if (PyErr_GivenExceptionMatches(exc_type, PyExc_GeneratorExit)) {
         _m = _PyObject_GetAttrId(iter, &PyId_close);
         if (_m) {
-            res = PyObject_CallFunctionObjArgs(_m, NULL);
+            res = PyObject_CallNoArgs(_m);
             Py_DECREF(_m);
             if (!res)
                 return 2;
@@ -360,7 +360,7 @@ CPyDataclass_SleightOfHand(PyObject *dataclass_dec, PyObject *tp,
     }
 
     /* Run the @dataclass descriptor */
-    res = PyObject_CallFunctionObjArgs(dataclass_dec, tp, NULL);
+    res = PyObject_CallOneArg(dataclass_dec, tp);
     if (!res) {
         goto fail;
     }"
5;python;mypy;405efd23e483c214cb1293700ef2008aacf8e2ca;"[mypyc] Use PyObject_CallNoArgs and PyObject_CallOneArg (#12862)

Use PyObject_CallNoArgs and PyObject_CallOneArg to replace 
PyObject_CallFunctionObjArgs in lib-rt, since the new API costs 
less memory according to python/cpython#13890 (comment)

Also remove the macro in pythonsupport.h since the two API are 
already covered by pythoncapi_compat.h.";"@@ -350,15 +350,15 @@ CPyGen_SetStopIterationValue(PyObject *value)
         return 0;
     }
     /* Construct an exception instance manually with
-     * PyObject_CallFunctionObjArgs and pass it to PyErr_SetObject.
+     * PyObject_CallOneArg and pass it to PyErr_SetObject.
      *
      * We do this to handle a situation when ""value"" is a tuple, in which
      * case PyErr_SetObject would set the value of StopIteration to
      * the first element of the tuple.
      *
      * (See PyErr_SetObject/_PyErr_CreateException code for details.)
      */
-    e = PyObject_CallFunctionObjArgs(PyExc_StopIteration, value, NULL);
+    e = PyObject_CallOneArg(PyExc_StopIteration, value);
     if (e == NULL) {
         return -1;
     }
@@ -410,10 +410,6 @@ _CPyObject_HasAttrId(PyObject *v, _Py_Identifier *name) {
     _PyObject_CallMethodIdObjArgs((self), (name), NULL)
 #define _PyObject_CallMethodIdOneArg(self, name, arg) \
     _PyObject_CallMethodIdObjArgs((self), (name), (arg), NULL)
-#define PyObject_CallNoArgs(callable) \
-    PyObject_CallFunctionObjArgs((callable), NULL)
-#define PyObject_CallOneArg(callable, arg) \
-    PyObject_CallFunctionObjArgs((callable), (arg), NULL)
 #endif
 
 #endif"
6;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"@@ -764,8 +764,10 @@ def assemble(self):
         if is_win:
             # Set checksum to appease antiviral software. Also set build timestamp to current time to increase entropy
             # (but honor SOURCE_DATE_EPOCH environment variable for reproducible builds).
+            logger.info(""Fixing EXE headers"")
             build_timestamp = int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))
-            winutils.fixup_exe_headers(build_name, build_timestamp)
+            winutils.set_exe_build_timestamp(build_name, build_timestamp)
+            winutils.update_exe_pe_checksum(build_name)
         elif is_darwin:
             # If the version of macOS SDK used to build bootloader exceeds that of macOS SDK used to built Python
             # library (and, by extension, bundled Tcl/Tk libraries), force the version declared by the frozen executable"
6;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"@@ -148,18 +148,19 @@ def convert_dll_name_to_str(dll_name):
         return dll_name
 
 
-def fixup_exe_headers(exe_path, timestamp=None):
+def set_exe_build_timestamp(exe_path, timestamp):
     """"""
-    Set executable's checksum and build timestamp in its headers.
-
-    This optional checksum is supposed to protect the executable against corruption but some anti-viral software have
-    taken to flagging anything without it set correctly as malware. See issue #5579.
+    Modifies the executable's build timestamp by updating values in the corresponding PE headers.
     """"""
     import pefile
-    pe = pefile.PE(exe_path, fast_load=False)  # full load because we need all headers
-    # Set build timestamp.
-    # See: https://0xc0decafe.com/malware-analyst-guide-to-pe-timestamps
-    if timestamp is not None:
+
+    with pefile.PE(exe_path, fast_load=True) as pe:
+        # Manually perform a full load. We need it to load all headers, but specifying it in the constructor triggers
+        # byte statistics gathering that takes forever with large files. So we try to go around that...
+        pe.full_load()
+
+        # Set build timestamp.
+        # See: https://0xc0decafe.com/malware-analyst-guide-to-pe-timestamps
         timestamp = int(timestamp)
         # Set timestamp field in FILE_HEADER
         pe.FILE_HEADER.TimeDateStamp = timestamp
@@ -169,7 +170,189 @@ def fixup_exe_headers(exe_path, timestamp=None):
         for debug_entry in debug_entries:
             if debug_entry.struct.TimeDateStamp:
                 debug_entry.struct.TimeDateStamp = timestamp
-    # Set PE checksum
-    pe.OPTIONAL_HEADER.CheckSum = pe.generate_checksum()
-    pe.close()
-    pe.write(exe_path)
+
+        # Generate updated EXE data
+        data = pe.write()
+
+    # Rewrite the exe
+    with open(exe_path, 'wb') as fp:
+        fp.write(data)
+
+
+def update_exe_pe_checksum(exe_path):
+    """"""
+    Compute the executable's PE checksum, and write it to PE headers.
+
+    This optional checksum is supposed to protect the executable against corruption but some anti-viral software have
+    taken to flagging anything without it set correctly as malware. See issue #5579.
+    """"""
+    import pefile
+
+    # Compute checksum using our equivalent of the MapFileAndCheckSumW - for large files, it is significantly faster
+    # than pure-pyton pefile.PE.generate_checksum(). However, it requires the file to be on disk (i.e., cannot operate
+    # on a memory buffer).
+    try:
+        checksum = compute_exe_pe_checksum(exe_path)
+    except Exception as e:
+        raise RuntimeError(""Failed to compute PE checksum!"") from e
+
+    # Update the checksum
+    with pefile.PE(exe_path, fast_load=True) as pe:
+        pe.OPTIONAL_HEADER.CheckSum = checksum
+
+        # Generate updated EXE data
+        data = pe.write()
+
+    # Rewrite the exe
+    with open(exe_path, 'wb') as fp:
+        fp.write(data)
+
+
+def compute_exe_pe_checksum(exe_path):
+    """"""
+    This is a replacement for the MapFileAndCheckSumW function. As noted in MSDN documentation, the Microsoft's
+    implementation of MapFileAndCheckSumW internally calls its ASCII variant (MapFileAndCheckSumA), and therefore
+    cannot handle paths that contain characters that are not representable in the current code page.
+    See: https://docs.microsoft.com/en-us/windows/win32/api/imagehlp/nf-imagehlp-mapfileandchecksumw
+
+    This function is based on Wine's implementation of MapFileAndCheckSumW, and due to being based entirely on
+    the pure widechar-API functions, it is not limited by the current code page.
+    """"""
+    # ctypes bindings for relevant win32 API functions
+    import ctypes
+    from ctypes import windll, wintypes
+
+    INVALID_HANDLE = wintypes.HANDLE(-1).value
+
+    GetLastError = ctypes.windll.kernel32.GetLastError
+    GetLastError.argtypes = ()
+    GetLastError.restype = wintypes.DWORD
+
+    CloseHandle = windll.kernel32.CloseHandle
+    CloseHandle.argtypes = (
+        wintypes.HANDLE,  # hObject
+    )
+    CloseHandle.restype = wintypes.BOOL
+
+    CreateFileW = windll.kernel32.CreateFileW
+    CreateFileW.argtypes = (
+        wintypes.LPCWSTR,  # lpFileName
+        wintypes.DWORD,  # dwDesiredAccess
+        wintypes.DWORD,  # dwShareMode
+        wintypes.LPVOID,  # lpSecurityAttributes
+        wintypes.DWORD,  # dwCreationDisposition
+        wintypes.DWORD,  # dwFlagsAndAttributes
+        wintypes.HANDLE,  # hTemplateFile
+    )
+    CreateFileW.restype = wintypes.HANDLE
+
+    CreateFileMappingW = windll.kernel32.CreateFileMappingW
+    CreateFileMappingW.argtypes = (
+        wintypes.HANDLE,  # hFile
+        wintypes.LPVOID,  # lpSecurityAttributes
+        wintypes.DWORD,  # flProtect
+        wintypes.DWORD,  # dwMaximumSizeHigh
+        wintypes.DWORD,  # dwMaximumSizeLow
+        wintypes.LPCWSTR,  # lpName
+    )
+    CreateFileMappingW.restype = wintypes.HANDLE
+
+    MapViewOfFile = windll.kernel32.MapViewOfFile
+    MapViewOfFile.argtypes = (
+        wintypes.HANDLE,  # hFileMappingObject
+        wintypes.DWORD,  # dwDesiredAccess
+        wintypes.DWORD,  # dwFileOffsetHigh
+        wintypes.DWORD,  # dwFileOffsetLow
+        wintypes.DWORD,  # dwNumberOfBytesToMap
+    )
+    MapViewOfFile.restype = wintypes.LPVOID
+
+    UnmapViewOfFile = windll.kernel32.UnmapViewOfFile
+    UnmapViewOfFile.argtypes = (
+        wintypes.LPCVOID,  # lpBaseAddress
+    )
+    UnmapViewOfFile.restype = wintypes.BOOL
+
+    GetFileSizeEx = windll.kernel32.GetFileSizeEx
+    GetFileSizeEx.argtypes = (
+        wintypes.HANDLE,  # hFile
+        wintypes.PLARGE_INTEGER,  # lpFileSize
+    )
+
+    CheckSumMappedFile = windll.imagehlp.CheckSumMappedFile
+    CheckSumMappedFile.argtypes = (
+        wintypes.LPVOID,  # BaseAddress
+        wintypes.DWORD,  # FileLength
+        wintypes.PDWORD,  # HeaderSum
+        wintypes.PDWORD,  # CheckSum
+    )
+    CheckSumMappedFile.restype = wintypes.LPVOID
+
+    # Open file
+    hFile = CreateFileW(
+        ctypes.c_wchar_p(exe_path),
+        0x80000000,  # dwDesiredAccess = GENERIC_READ
+        0x00000001 | 0x00000002,  # dwShareMode = FILE_SHARE_READ | FILE_SHARE_WRITE,
+        None,  # lpSecurityAttributes = NULL
+        3,  # dwCreationDisposition = OPEN_EXISTING
+        0x80,  # dwFlagsAndAttributes = FILE_ATTRIBUTE_NORMAL
+        None  # hTemplateFile = NULL
+    )
+    if hFile == INVALID_HANDLE:
+        err = GetLastError()
+        raise RuntimeError(f""Failed to open file {exe_path}! Error code: {err}"")
+
+    # Query file size
+    fileLength = wintypes.LARGE_INTEGER(0)
+    if GetFileSizeEx(hFile, fileLength) == 0:
+        err = GetLastError()
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to query file size file! Error code: {err}"")
+    fileLength = fileLength.value
+    if fileLength > (2**32 - 1):
+        raise RuntimeError(""Executable size exceeds maximum allowed executable size on Windows (4 GiB)!"")
+
+    # Map the file
+    hMapping = CreateFileMappingW(
+        hFile,
+        None,  # lpFileMappingAttributes = NULL
+        0x02,  # flProtect = PAGE_READONLY
+        0,  # dwMaximumSizeHigh = 0
+        0,  # dwMaximumSizeLow = 0
+        None  # lpName = NULL
+    )
+    if not hMapping:
+        err = GetLastError()
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to map file! Error code: {err}"")
+
+    # Create map view
+    baseAddress = MapViewOfFile(
+        hMapping,
+        4,  # dwDesiredAccess = FILE_MAP_READ
+        0,  # dwFileOffsetHigh = 0
+        0,  # dwFileOffsetLow = 0
+        0  # dwNumberOfBytesToMap = 0
+    )
+    if baseAddress == 0:
+        err = GetLastError()
+        CloseHandle(hMapping)
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to create map view! Error code: {err}"")
+
+    # Finally, compute the checksum
+    headerSum = wintypes.DWORD(0)
+    checkSum = wintypes.DWORD(0)
+    ret = CheckSumMappedFile(baseAddress, fileLength, ctypes.byref(headerSum), ctypes.byref(checkSum))
+    if ret is None:
+        err = GetLastError()
+
+    # Cleanup
+    UnmapViewOfFile(baseAddress)
+    CloseHandle(hMapping)
+    CloseHandle(hFile)
+
+    if ret is None:
+        raise RuntimeError(f""CheckSumMappedFile failed! Error code: {err}"")
+
+    return checkSum.value"
6;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"@@ -0,0 +1,2 @@
+(Windows) Optimize EXE PE headers fix-up process in an attempt to reduce
+the processing time and the memory footprint with large onefile builds."
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -0,0 +1 @@
+Reduce the amount of state we pull from the DB."
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -245,6 +245,8 @@ def __init__(self, hs: ""HomeServer""):
         self.store = hs.get_datastores().main
         self.state = hs.get_state_handler()
 
+        self._storage_controllers = hs.get_storage_controllers()
+
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
 
@@ -602,7 +604,9 @@ async def send_read_receipt(self, receipt: ReadReceipt) -> None:
         room_id = receipt.room_id
 
         # Work out which remote servers should be poked and poke them.
-        domains_set = await self.state.get_current_hosts_in_room(room_id)
+        domains_set = await self._storage_controllers.state.get_current_hosts_in_room(
+            room_id
+        )
         domains = [
             d
             for d in domains_set"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -59,6 +59,7 @@ class FollowerTypingHandler:
 
     def __init__(self, hs: ""HomeServer""):
         self.store = hs.get_datastores().main
+        self._storage_controllers = hs.get_storage_controllers()
         self.server_name = hs.config.server.server_name
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
@@ -131,15 +132,17 @@ async def _push_remote(self, member: RoomMember, typing: bool) -> None:
             return
 
         try:
-            users = await self.store.get_users_in_room(member.room_id)
             self._member_last_federation_poke[member] = self.clock.time_msec()
 
             now = self.clock.time_msec()
             self.wheel_timer.insert(
                 now=now, obj=member, then=now + FEDERATION_PING_INTERVAL
             )
 
-            for domain in {get_domain_from_id(u) for u in users}:
+            hosts = await self._storage_controllers.state.get_current_hosts_in_room(
+                member.room_id
+            )
+            for domain in hosts:
                 if domain != self.server_name:
                     logger.debug(""sending typing update to %s"", domain)
                     self.federation.build_and_send_edu("
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -172,10 +172,6 @@ async def get_current_users_in_room(
         entry = await self.resolve_state_groups_for_events(room_id, latest_event_ids)
         return await self.store.get_joined_users_from_state(room_id, entry)
 
-    async def get_current_hosts_in_room(self, room_id: str) -> FrozenSet[str]:
-        event_ids = await self.store.get_latest_event_ids_in_room(room_id)
-        return await self.get_hosts_in_room_at_events(room_id, event_ids)
-
     async def get_hosts_in_room_at_events(
         self, room_id: str, event_ids: Collection[str]
     ) -> FrozenSet[str]:"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -71,6 +71,7 @@ def _invalidate_state_caches(
             self._attempt_to_invalidate_cache(""is_host_joined"", (room_id, host))
         if members_changed:
             self._attempt_to_invalidate_cache(""get_users_in_room"", (room_id,))
+            self._attempt_to_invalidate_cache(""get_current_hosts_in_room"", (room_id,))
             self._attempt_to_invalidate_cache(
                 ""get_users_in_room_with_profiles"", (room_id,)
             )"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -23,6 +23,7 @@
     List,
     Mapping,
     Optional,
+    Set,
     Tuple,
 )
 
@@ -482,3 +483,10 @@ async def get_current_state_event(
             room_id, StateFilter.from_types((key,))
         )
         return state_map.get(key)
+
+    async def get_current_hosts_in_room(self, room_id: str) -> Set[str]:
+        """"""Get current hosts in room based on current state.""""""
+
+        await self._partial_state_room_tracker.await_full_state(room_id)
+
+        return await self.stores.main.get_current_hosts_in_room(room_id)"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -893,6 +893,43 @@ async def _check_host_room_membership(
 
         return True
 
+    @cached(iterable=True, max_entries=10000)
+    async def get_current_hosts_in_room(self, room_id: str) -> Set[str]:
+        """"""Get current hosts in room based on current state.""""""
+
+        # First we check if we already have `get_users_in_room` in the cache, as
+        # we can just calculate result from that
+        users = self.get_users_in_room.cache.get_immediate(
+            (room_id,), None, update_metrics=False
+        )
+        if users is not None:
+            return {get_domain_from_id(u) for u in users}
+
+        if isinstance(self.database_engine, Sqlite3Engine):
+            # If we're using SQLite then let's just always use
+            # `get_users_in_room` rather than funky SQL.
+            users = await self.get_users_in_room(room_id)
+            return {get_domain_from_id(u) for u in users}
+
+        # For PostgreSQL we can use a regex to pull out the domains from the
+        # joined users in `current_state_events` via regex.
+
+        def get_current_hosts_in_room_txn(txn: LoggingTransaction) -> Set[str]:
+            sql = """"""
+                SELECT DISTINCT substring(state_key FROM '@[^:]*:(.*)$')
+                FROM current_state_events
+                WHERE
+                    type = 'm.room.member'
+                    AND membership = 'join'
+                    AND room_id = ?
+            """"""
+            txn.execute(sql, (room_id,))
+            return {d for d, in txn}
+
+        return await self.db_pool.runInteraction(
+            ""get_current_hosts_in_room"", get_current_hosts_in_room_txn
+        )
+
     async def get_joined_hosts(
         self, room_id: str, state_entry: ""_StateCacheEntry""
     ) -> FrozenSet[str]:"
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -30,16 +30,16 @@
 
 class FederationSenderReceiptsTestCases(HomeserverTestCase):
     def make_homeserver(self, reactor, clock):
-        mock_state_handler = Mock(spec=[""get_current_hosts_in_room""])
-        # Ensure a new Awaitable is created for each call.
-        mock_state_handler.get_current_hosts_in_room.return_value = make_awaitable(
-            [""test"", ""host2""]
-        )
-        return self.setup_test_homeserver(
-            state_handler=mock_state_handler,
+        hs = self.setup_test_homeserver(
             federation_transport_client=Mock(spec=[""send_transaction""]),
         )
 
+        hs.get_storage_controllers().state.get_current_hosts_in_room = Mock(
+            return_value=make_awaitable({""test"", ""host2""})
+        )
+
+        return hs
+
     @override_config({""send_federation"": True})
     def test_send_receipts(self):
         mock_send_transaction = ("
7;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -129,10 +129,12 @@ async def check_host_in_room(room_id: str, server_name: str) -> bool:
 
         hs.get_event_auth_handler().check_host_in_room = check_host_in_room
 
-        def get_joined_hosts_for_room(room_id: str):
+        async def get_current_hosts_in_room(room_id: str):
             return {member.domain for member in self.room_members}
 
-        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room
+        hs.get_storage_controllers().state.get_current_hosts_in_room = (
+            get_current_hosts_in_room
+        )
 
         async def get_users_in_room(room_id: str):
             return {str(u) for u in self.room_members}"
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -0,0 +1 @@
+Reduce the amount of state we pull from the DB."
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -245,6 +245,8 @@ def __init__(self, hs: ""HomeServer""):
         self.store = hs.get_datastores().main
         self.state = hs.get_state_handler()
 
+        self._storage_controllers = hs.get_storage_controllers()
+
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
 
@@ -602,7 +604,9 @@ async def send_read_receipt(self, receipt: ReadReceipt) -> None:
         room_id = receipt.room_id
 
         # Work out which remote servers should be poked and poke them.
-        domains_set = await self.state.get_current_hosts_in_room(room_id)
+        domains_set = await self._storage_controllers.state.get_current_hosts_in_room(
+            room_id
+        )
         domains = [
             d
             for d in domains_set"
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -59,6 +59,7 @@ class FollowerTypingHandler:
 
     def __init__(self, hs: ""HomeServer""):
         self.store = hs.get_datastores().main
+        self._storage_controllers = hs.get_storage_controllers()
         self.server_name = hs.config.server.server_name
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
@@ -131,15 +132,17 @@ async def _push_remote(self, member: RoomMember, typing: bool) -> None:
             return
 
         try:
-            users = await self.store.get_users_in_room(member.room_id)
             self._member_last_federation_poke[member] = self.clock.time_msec()
 
             now = self.clock.time_msec()
             self.wheel_timer.insert(
                 now=now, obj=member, then=now + FEDERATION_PING_INTERVAL
             )
 
-            for domain in {get_domain_from_id(u) for u in users}:
+            hosts = await self._storage_controllers.state.get_current_hosts_in_room(
+                member.room_id
+            )
+            for domain in hosts:
                 if domain != self.server_name:
                     logger.debug(""sending typing update to %s"", domain)
                     self.federation.build_and_send_edu("
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -172,10 +172,6 @@ async def get_current_users_in_room(
         entry = await self.resolve_state_groups_for_events(room_id, latest_event_ids)
         return await self.store.get_joined_users_from_state(room_id, entry)
 
-    async def get_current_hosts_in_room(self, room_id: str) -> FrozenSet[str]:
-        event_ids = await self.store.get_latest_event_ids_in_room(room_id)
-        return await self.get_hosts_in_room_at_events(room_id, event_ids)
-
     async def get_hosts_in_room_at_events(
         self, room_id: str, event_ids: Collection[str]
     ) -> FrozenSet[str]:"
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -71,6 +71,7 @@ def _invalidate_state_caches(
             self._attempt_to_invalidate_cache(""is_host_joined"", (room_id, host))
         if members_changed:
             self._attempt_to_invalidate_cache(""get_users_in_room"", (room_id,))
+            self._attempt_to_invalidate_cache(""get_current_hosts_in_room"", (room_id,))
             self._attempt_to_invalidate_cache(
                 ""get_users_in_room_with_profiles"", (room_id,)
             )"
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -23,6 +23,7 @@
     List,
     Mapping,
     Optional,
+    Set,
     Tuple,
 )
 
@@ -482,3 +483,10 @@ async def get_current_state_event(
             room_id, StateFilter.from_types((key,))
         )
         return state_map.get(key)
+
+    async def get_current_hosts_in_room(self, room_id: str) -> Set[str]:
+        """"""Get current hosts in room based on current state.""""""
+
+        await self._partial_state_room_tracker.await_full_state(room_id)
+
+        return await self.stores.main.get_current_hosts_in_room(room_id)"
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -893,6 +893,43 @@ async def _check_host_room_membership(
 
         return True
 
+    @cached(iterable=True, max_entries=10000)
+    async def get_current_hosts_in_room(self, room_id: str) -> Set[str]:
+        """"""Get current hosts in room based on current state.""""""
+
+        # First we check if we already have `get_users_in_room` in the cache, as
+        # we can just calculate result from that
+        users = self.get_users_in_room.cache.get_immediate(
+            (room_id,), None, update_metrics=False
+        )
+        if users is not None:
+            return {get_domain_from_id(u) for u in users}
+
+        if isinstance(self.database_engine, Sqlite3Engine):
+            # If we're using SQLite then let's just always use
+            # `get_users_in_room` rather than funky SQL.
+            users = await self.get_users_in_room(room_id)
+            return {get_domain_from_id(u) for u in users}
+
+        # For PostgreSQL we can use a regex to pull out the domains from the
+        # joined users in `current_state_events` via regex.
+
+        def get_current_hosts_in_room_txn(txn: LoggingTransaction) -> Set[str]:
+            sql = """"""
+                SELECT DISTINCT substring(state_key FROM '@[^:]*:(.*)$')
+                FROM current_state_events
+                WHERE
+                    type = 'm.room.member'
+                    AND membership = 'join'
+                    AND room_id = ?
+            """"""
+            txn.execute(sql, (room_id,))
+            return {d for d, in txn}
+
+        return await self.db_pool.runInteraction(
+            ""get_current_hosts_in_room"", get_current_hosts_in_room_txn
+        )
+
     async def get_joined_hosts(
         self, room_id: str, state_entry: ""_StateCacheEntry""
     ) -> FrozenSet[str]:"
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -30,16 +30,16 @@
 
 class FederationSenderReceiptsTestCases(HomeserverTestCase):
     def make_homeserver(self, reactor, clock):
-        mock_state_handler = Mock(spec=[""get_current_hosts_in_room""])
-        # Ensure a new Awaitable is created for each call.
-        mock_state_handler.get_current_hosts_in_room.return_value = make_awaitable(
-            [""test"", ""host2""]
-        )
-        return self.setup_test_homeserver(
-            state_handler=mock_state_handler,
+        hs = self.setup_test_homeserver(
             federation_transport_client=Mock(spec=[""send_transaction""]),
         )
 
+        hs.get_storage_controllers().state.get_current_hosts_in_room = Mock(
+            return_value=make_awaitable({""test"", ""host2""})
+        )
+
+        return hs
+
     @override_config({""send_federation"": True})
     def test_send_receipts(self):
         mock_send_transaction = ("
8;matrix-org;synapse;44de53bb79f961147386ea2a8bfbeb54b007cd41;"Reduce state pulled from DB due to sending typing and receipts over federation (#12964)

Reducing the amount of state we pull from the DB is useful as fetching state is expensive in terms of DB, CPU and memory.";"@@ -129,10 +129,12 @@ async def check_host_in_room(room_id: str, server_name: str) -> bool:
 
         hs.get_event_auth_handler().check_host_in_room = check_host_in_room
 
-        def get_joined_hosts_for_room(room_id: str):
+        async def get_current_hosts_in_room(room_id: str):
             return {member.domain for member in self.room_members}
 
-        self.datastore.get_joined_hosts_for_room = get_joined_hosts_for_room
+        hs.get_storage_controllers().state.get_current_hosts_in_room = (
+            get_current_hosts_in_room
+        )
 
         async def get_users_in_room(room_id: str):
             return {str(u) for u in self.room_members}"
9;pjialin;py12306;390fcecdd24b9786519ff3eb99c40488b754769d;"Merge pull request #317 from out0fmemory/master

å¢žåŠ æ—¶é—´åˆ¤æ–­é€»è¾‘";"@@ -117,8 +117,8 @@ QUERY_JOBS = [
         # 'job_name':  'bj -> sz',  # ä»»åŠ¡åç§°ï¼Œä¸å¡«é»˜è®¤ä¼šä»¥è½¦ç«™åå‘½åï¼Œä¸å¯é‡å¤
         'account_key': 0,  # å°†ä¼šä½¿ç”¨æŒ‡å®šè´¦å·ä¸‹å•
         'left_dates': [  # å‡ºå‘æ—¥æœŸ :Array
-            ""2019-01-25"",
-            ""2019-01-26"",
+            ""2020-01-25"",
+            ""2020-01-26"",
         ],
         'stations': {  # è½¦ç«™ æ”¯æŒå¤šä¸ªè½¦ç«™åŒæ—¶æŸ¥è¯¢  :Dict or :List
             'left': 'åŒ—äº¬',"
9;pjialin;py12306;390fcecdd24b9786519ff3eb99c40488b754769d;"Merge pull request #317 from out0fmemory/master

å¢žåŠ æ—¶é—´åˆ¤æ–­é€»è¾‘";"@@ -1,5 +1,6 @@
 import sys
 from datetime import timedelta
+from datetime import datetime
 
 from py12306.app import app_available_check
 from py12306.cluster.cluster import Cluster
@@ -66,6 +67,8 @@ class Job:
     INDEX_LEFT_TIME = 8
     INDEX_ARRIVE_TIME = 9
 
+    max_buy_time = 32
+
     def __init__(self, info, query):
         self.cluster = Cluster()
         self.query = query
@@ -136,11 +139,29 @@ def start(self):
                 QueryLog.add_log('\n').flush(sep='\t\t', publish=False)
             if Const.IS_TEST: return
 
+    def judge_date_legal(self, date):
+        date_now = datetime.datetime.now()
+        date_query = datetime.datetime.strptime(str(date), ""%Y-%m-%d"")
+        diff = (date_query - date_now).days
+        if date_now.day == date_query.day:
+            diff = 0
+        if diff < 0:
+            msg = 'ä¹˜è½¦æ—¥æœŸé”™è¯¯ï¼Œæ¯”å½“å‰æ—¶é—´è¿˜æ—©ï¼ï¼'
+            QueryLog.add_quick_log(msg).flush(publish=False)
+            raise RuntimeError(msg)
+        elif diff > self.max_buy_time:
+            msg = 'ä¹˜è½¦æ—¥æœŸé”™è¯¯ï¼Œè¶…å‡ºä¸€ä¸ªæœˆé¢„å”®æœŸï¼ï¼'
+            QueryLog.add_quick_log(msg).flush(publish=False)
+            raise RuntimeError(msg)
+        else:
+            pass
+
     def query_by_date(self, date):
         """"""
         é€šè¿‡æ—¥æœŸè¿›è¡ŒæŸ¥è¯¢
         :return:
         """"""
+        self.judge_date_legal(date)
         from py12306.helpers.cdn import Cdn
         QueryLog.add_log(('\n' if not is_main_thread() else '') + QueryLog.MESSAGE_QUERY_START_BY_DATE.format(date,
                                                                                                               self.left_station,"
9;pjialin;py12306;998b387875d5620e734285e8c11250e03c661a8a;"Merge pull request #288 from out0fmemory/master

ä¿®æ”¹äººå‘˜çš„æ³¨é‡Šï¼Œå­˜åœ¨ä¸æ³¨æ„æƒ…å†µä¸‹æœ‰å¤šä½™çš„äººå‘˜ä¿¡æ¯";"@@ -132,7 +132,7 @@ QUERY_JOBS = [
         # }],
         'members': [  # ä¹˜å®¢å§“åï¼Œä¼šæ ¹æ®å½“å‰è´¦å·è‡ªåŠ¨è¯†åˆ«ä¹˜å®¢ç±»åž‹ è´­ä¹°å„¿ç«¥ç¥¨ è®¾ç½®ä¸¤ä¸ªç›¸åŒçš„å§“åå³å¯ï¼Œç¨‹åºä¼šè‡ªåŠ¨è¯†åˆ« å¦‚  ['å¼ ä¸‰', 'å¼ ä¸‰']
             ""å¼ ä¸‰"",
-            ""*çŽ‹äº”"", #åœ¨å§“åå‰åŠ *è¡¨ç¤ºå­¦ç”Ÿè´­ä¹°æˆäººç¥¨
+            #""*çŽ‹äº”"", #åœ¨å§“åå‰åŠ *è¡¨ç¤ºå­¦ç”Ÿè´­ä¹°æˆäººç¥¨
             # 7,  # æ”¯æŒé€šè¿‡åºå·ç¡®å®šå”¯ä¸€ä¹˜å®¢ï¼Œåºå·æŸ¥çœ‹å¯é€šè¿‡  python main.py -t ç™»å½•æˆåŠŸä¹‹åŽåœ¨ runtime/user/ ä¸‹æ‰¾åˆ°å¯¹åº”çš„ ç”¨æˆ·å_passengers.json æ–‡ä»¶ï¼Œæ‰¾åˆ°å¯¹åº”çš„ code å¡«å…¥
         ],
         'allow_less_member': 0,  # æ˜¯å¦å…è®¸ä½™ç¥¨ä¸è¶³æ—¶æäº¤éƒ¨åˆ†ä¹˜å®¢"
10;awslabs;aws-shell;62eb740dc62a6fc442fc7ea428c4da8072d4ce3f;"Merge pull request #158 from joguSD/memory-history

Don't append 'aws' twice when invoking the .edit command";"@@ -80,6 +80,11 @@ def _get_editor_command(self):
         else:
             return compat.default_editor()
 
+    def _generate_edit_history(self, application):
+        history = list(application.history)
+        commands = [h for h in history if not h.startswith(('.', '!'))]
+        return '\n'.join(commands)
+
     def run(self, command, application):
         """"""Open application's history buffer in an editor.
 
@@ -91,10 +96,8 @@ def run(self, command, application):
         :param application: The application object.
 
         """"""
-        all_commands = '\n'.join(
-            ['aws ' + h for h in list(application.history)
-             if not h.startswith(('.', '!'))])
         with temporary_file('w') as f:
+            all_commands = self._generate_edit_history(application)
             f.write(all_commands)
             f.flush()
             editor = self._get_editor_command()
@@ -222,7 +225,7 @@ class AWSShell(object):
     """"""
 
     def __init__(self, completer, model_completer, docs,
-                 input=None, output=None):
+                 input=None, output=None, popen_cls=None):
         self.completer = completer
         self.model_completer = model_completer
         self.history = InMemoryHistory()
@@ -238,6 +241,10 @@ def __init__(self, completer, model_completer, docs,
         self._input = input
         self._output = output
 
+        if popen_cls is None:
+            popen_cls = subprocess.Popen
+        self._popen_cls = popen_cls
+
         # These attrs come from the config file.
         self.config_obj = None
         self.config_section = None
@@ -309,7 +316,7 @@ def run(self):
                         initial_document=Document(self.current_docs,
                                                   cursor_position=0))
                     self.cli.request_redraw()
-                    p = subprocess.Popen(full_cmd, shell=True, env=self._env)
+                    p = self._popen_cls(full_cmd, shell=True, env=self._env)
                     p.communicate()
 
     def stop_input_and_refresh_cli(self):"
10;awslabs;aws-shell;62eb740dc62a6fc442fc7ea428c4da8072d4ce3f;"Merge pull request #158 from joguSD/memory-history

Don't append 'aws' twice when invoking the .edit command";"@@ -26,17 +26,35 @@ def run(self, command, context):
     assert call_args == [(['.foo', 'a', 'b', 'c'], context)]
 
 
+class PopenLogger(object):
+    def __call__(self, cmd):
+        self.cmd = cmd
+        filename = cmd[1]
+        with open(filename, 'r') as f:
+            self.contents = f.read()
+        return mock.Mock()
+
+
 def test_edit_handler():
     env = {'EDITOR': 'my-editor'}
     popen_cls = mock.Mock()
-    context = mock.Mock()
-    context.history = []
-    handler = app.EditHandler(popen_cls, env)
-    handler.run(['.edit'], context)
+    application = mock.Mock()
+    application.history = [
+        '!ls',
+        '.edit',
+        'aws ec2 describe-instances',
+        'aws ec2 allocate-hosts',
+    ]
+    popen = PopenLogger()
+    handler = app.EditHandler(popen, env)
+    handler.run(['.edit'], application)
     # Ensure our editor was called with some arbitrary temp filename.
-    command_run = popen_cls.call_args[0][0]
+    command_run = popen.cmd
     assert len(command_run) == 2
     assert command_run[0] == 'my-editor'
+    # Ensure the contents of the temp file are correct
+    expected_contents = 'aws ec2 describe-instances\naws ec2 allocate-hosts'
+    assert popen.contents == expected_contents
 
 
 def test_error_msg_printed_on_error_handler(errstream):
@@ -132,6 +150,25 @@ def test_error_displayed_when_chdir_fails(errstream):
     assert 'FAILED' in errstream.getvalue()
 
 
+def test_history_stored_correctly():
+    mock_prompter = mock.Mock()
+    mock_prompter.buffers = {'clidocs': mock.Mock()}
+    # Simulate the user entering various commands
+    quit_document = mock.Mock()
+    quit_document.text = '.quit'
+    command_document = mock.Mock()
+    command_document.text = 'ec2 describe-instances'
+    mock_prompter.run.side_effect = [command_document, quit_document]
+    shell = app.AWSShell(mock.Mock(), mock.Mock(), mock.Mock(),
+            popen_cls=mock.Mock())
+    shell.create_cli_interface = mock.Mock(return_value=mock_prompter)
+    shell.run()
+
+    # two calls should have been made, history should have added aws
+    assert mock_prompter.run.call_count == 2
+    assert list(shell.history) == ['aws ec2 describe-instances']
+
+
 def test_exit_dot_command_exits_shell():
     mock_prompter = mock.Mock()
     # Simulate the user entering '.quit'"
11;rbgirshick;py-faster-rcnn;ad1570f7c0651c1b39a561883f5cf066a406a2ee;Update memory requirements;"@@ -64,7 +64,8 @@ If you find Faster R-CNN useful in your research, please consider citing:
 ### Requirements: hardware
 
 1. For training smaller networks (ZF, VGG_CNN_M_1024) a good GPU (e.g., Titan, K20, K40, ...) with at least 3G of memory suffices
-2. For training with VGG16, you'll need a K40 (~11G of memory)
+2. For training Fast R-CNN with VGG16, you'll need a K40 (~11G of memory)
+3. For training the end-to-end version of Faster R-CNN with VGG16, 3G of GPU memory is sufficient (using CUDNN)
 
 ### Installation (sufficient for the demo)
 "
11;deepmind;pysc2;55ccc8cf4ef9f37aafa79509e70c1726bcb0201c;"Improvements to replay_actions:
- launch the right version.
- don't request RGB, ie lower memory use, etc.
- skip printing the stats if there are no replays.

PiperOrigin-RevId: 267365037";"@@ -37,6 +37,7 @@
 from pysc2.lib import point
 from pysc2.lib import protocol
 from pysc2.lib import remote_controller
+from pysc2.lib import replay
 
 from pysc2.lib import gfile
 from s2clientprotocol import common_pb2 as sc_common
@@ -190,7 +191,8 @@ def run(self):
       self._print(""Starting up a new SC2 instance."")
       self._update_stage(""launch"")
       try:
-        with self.run_config.start() as controller:
+        with self.run_config.start(
+            want_rgb=interface.HasField(""render"")) as controller:
           self._print(""SC2 Started successfully."")
           ping = controller.ping()
           for _ in range(300):
@@ -355,7 +357,6 @@ def main(unused_argv):
 
   stats_queue = multiprocessing.Queue()
   stats_thread = threading.Thread(target=stats_printer, args=(stats_queue,))
-  stats_thread.start()
   try:
     # For some reason buffering everything into a JoinableQueue makes the
     # program not exit, so save it into a list then slowly fill it into the
@@ -364,7 +365,19 @@ def main(unused_argv):
     # The replay_queue.join below succeeds without doing any work, and exits.
     print(""Getting replay list:"", FLAGS.replays)
     replay_list = sorted(run_config.replay_paths(FLAGS.replays))
-    print(len(replay_list), ""replays found.\n"")
+    print(len(replay_list), ""replays found."")
+    if not replay_list:
+      return
+
+    if not FLAGS[""sc2_version""].present:  # ie not set explicitly.
+      version = replay.get_replay_version(
+          run_config.replay_data(replay_list[0]))
+      run_config = run_configs.get(version=version)
+      print(""Assuming version:"", version.game_version)
+
+    print()
+
+    stats_thread.start()
     replay_queue = multiprocessing.JoinableQueue(FLAGS.parallel * 10)
     replay_queue_thread = threading.Thread(target=replay_queue_filler,
                                            args=(replay_queue, replay_list))
@@ -382,7 +395,8 @@ def main(unused_argv):
     print(""Caught KeyboardInterrupt, exiting."")
   finally:
     stats_queue.put(None)  # Tell the stats_thread to print and exit.
-    stats_thread.join()
+    if stats_thread.is_alive():
+      stats_thread.join()
 
 
 if __name__ == ""__main__"":"
12;giampaolo;psutil;f1f299527634a425cb34b621d6201fa9172d3529;"[Linux] Speedup `Process.full_memory_info()` (#2108)

`Process.memory_full_info()` (reporting proecss USS/PSS/Swap memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps`` which makes it 5 times faster.

Without patch:
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
500 loops, best of 5: 518 usec per loop
```

With patch (5 times faster):
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
2000 loops, best of 5: 111 usec per loop
```

----

`make test-memleaks` suite, who heavily rely on `Process.memory_full_info()`, also received a nice speedup:

Before patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.646s

OK (skipped=9)
SUCCESS
```

After patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.195s

OK (skipped=9)
SUCCESS
```";"@@ -12,6 +12,9 @@ XXXX-XX-XX
   ``/proc`` pseudo files line by line. This should help having more consistent
   results.
 - 2057_, [OpenBSD]: add support for `cpu_freq()`_.
+- 2107_ [Linux]: `Process.memory_full_info()`_ (reporting process USS/PSS/Swap
+  memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps``,
+  which makes it 5 times faster.
 
 **Bug fixes**
 "
12;giampaolo;psutil;f1f299527634a425cb34b621d6201fa9172d3529;"[Linux] Speedup `Process.full_memory_info()` (#2108)

`Process.memory_full_info()` (reporting proecss USS/PSS/Swap memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps`` which makes it 5 times faster.

Without patch:
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
500 loops, best of 5: 518 usec per loop
```

With patch (5 times faster):
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
2000 loops, best of 5: 111 usec per loop
```

----

`make test-memleaks` suite, who heavily rely on `Process.memory_full_info()`, also received a nice speedup:

Before patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.646s

OK (skipped=9)
SUCCESS
```

After patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.195s

OK (skipped=9)
SUCCESS
```";"@@ -77,7 +77,8 @@
 
 
 POWER_SUPPLY_PATH = ""/sys/class/power_supply""
-HAS_SMAPS = os.path.exists('/proc/%s/smaps' % os.getpid())
+HAS_PROC_SMAPS = os.path.exists('/proc/%s/smaps' % os.getpid())
+HAS_PROC_SMAPS_ROLLUP = os.path.exists('/proc/%s/smaps_rollup' % os.getpid())
 HAS_PROC_IO_PRIORITY = hasattr(cext, ""proc_ioprio_get"")
 HAS_CPU_AFFINITY = hasattr(cext, ""proc_cpu_affinity_get"")
 
@@ -1875,18 +1876,42 @@ def memory_info(self):
                 [int(x) * PAGESIZE for x in f.readline().split()[:7]]
         return pmem(rss, vms, shared, text, lib, data, dirty)
 
-    # /proc/pid/smaps does not exist on kernels < 2.6.14 or if
-    # CONFIG_MMU kernel configuration option is not enabled.
-    if HAS_SMAPS:
+    if HAS_PROC_SMAPS_ROLLUP or HAS_PROC_SMAPS:
 
         @wrap_exceptions
-        def memory_full_info(
+        def _parse_smaps_rollup(self):
+            # /proc/pid/smaps_rollup was added to Linux in 2017. Faster
+            # than /proc/pid/smaps. It reports higher PSS than */smaps
+            # (from 1k up to 200k higher; tested against all processes).
+            uss = pss = swap = 0
+            try:
+                with open_binary(""{}/{}/smaps_rollup"".format(
+                        self._procfs_path, self.pid)) as f:
+                    for line in f:
+                        if line.startswith(b""Private_""):
+                            # Private_Clean, Private_Dirty, Private_Hugetlb
+                            uss += int(line.split()[1]) * 1024
+                        elif line.startswith(b""Pss:""):
+                            pss = int(line.split()[1]) * 1024
+                        elif line.startswith(b""Swap:""):
+                            swap = int(line.split()[1]) * 1024
+            except ProcessLookupError:  # happens on readline()
+                if not pid_exists(self.pid):
+                    raise NoSuchProcess(self.pid, self._name)
+                else:
+                    raise ZombieProcess(self.pid, self._name, self._ppid)
+            return (uss, pss, swap)
+
+        @wrap_exceptions
+        def _parse_smaps(
                 self,
                 # Gets Private_Clean, Private_Dirty, Private_Hugetlb.
                 _private_re=re.compile(br""\nPrivate.*:\s+(\d+)""),
                 _pss_re=re.compile(br""\nPss\:\s+(\d+)""),
                 _swap_re=re.compile(br""\nSwap\:\s+(\d+)"")):
-            basic_mem = self.memory_info()
+            # /proc/pid/smaps does not exist on kernels < 2.6.14 or if
+            # CONFIG_MMU kernel configuration option is not enabled.
+
             # Note: using 3 regexes is faster than reading the file
             # line by line.
             # XXX: on Python 3 the 2 regexes are 30% slower than on
@@ -1905,12 +1930,20 @@ def memory_full_info(
             uss = sum(map(int, _private_re.findall(smaps_data))) * 1024
             pss = sum(map(int, _pss_re.findall(smaps_data))) * 1024
             swap = sum(map(int, _swap_re.findall(smaps_data))) * 1024
+            return (uss, pss, swap)
+
+        def memory_full_info(self):
+            if HAS_PROC_SMAPS_ROLLUP:  # faster
+                uss, pss, swap = self._parse_smaps_rollup()
+            else:
+                uss, pss, swap = self._parse_smaps()
+            basic_mem = self.memory_info()
             return pfullmem(*basic_mem + (uss, pss, swap))
 
     else:
         memory_full_info = memory_info
 
-    if HAS_SMAPS:
+    if HAS_PROC_SMAPS:
 
         @wrap_exceptions
         def memory_maps(self):"
12;giampaolo;psutil;f1f299527634a425cb34b621d6201fa9172d3529;"[Linux] Speedup `Process.full_memory_info()` (#2108)

`Process.memory_full_info()` (reporting proecss USS/PSS/Swap memory) now reads ``/proc/pid/smaps_rollup`` instead of ``/proc/pids/smaps`` which makes it 5 times faster.

Without patch:
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
500 loops, best of 5: 518 usec per loop
```

With patch (5 times faster):
```
~/svn/psutil {linux-smaps-rollup}$ python3 -m timeit -s ""import psutil; p = psutil.Process()"" ""p.memory_full_info()""
2000 loops, best of 5: 111 usec per loop
```

----

`make test-memleaks` suite, who heavily rely on `Process.memory_full_info()`, also received a nice speedup:

Before patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.646s

OK (skipped=9)
SUCCESS
```

After patch:

```
$ make test-memleaks
----------------------------------------------------------------------
Ran 99 tests in 1.195s

OK (skipped=9)
SUCCESS
```";"@@ -1775,28 +1775,19 @@ def open_mock(name, *args, **kwargs):
 class TestProcess(PsutilTestCase):
 
     @retry_on_failure()
-    def test_memory_full_info(self):
-        testfn = self.get_testfn()
-        src = textwrap.dedent(""""""
-            import time
-            with open(""%s"", ""w"") as f:
-                time.sleep(10)
-            """""" % testfn)
-        sproc = self.pyrun(src)
-        call_until(lambda: os.listdir('.'), ""'%s' not in ret"" % testfn)
-        p = psutil.Process(sproc.pid)
-        time.sleep(.1)
-        mem = p.memory_full_info()
-        maps = p.memory_maps(grouped=False)
+    def test_parse_smaps_vs_memory_maps(self):
+        sproc = self.spawn_testproc()
+        uss, pss, swap = psutil._pslinux.Process(sproc.pid)._parse_smaps()
+        maps = psutil.Process(sproc.pid).memory_maps(grouped=False)
         self.assertAlmostEqual(
-            mem.uss, sum([x.private_dirty + x.private_clean for x in maps]),
+            uss, sum([x.private_dirty + x.private_clean for x in maps]),
             delta=4096)
         self.assertAlmostEqual(
-            mem.pss, sum([x.pss for x in maps]), delta=4096)
+            pss, sum([x.pss for x in maps]), delta=4096)
         self.assertAlmostEqual(
-            mem.swap, sum([x.swap for x in maps]), delta=4096)
+            swap, sum([x.swap for x in maps]), delta=4096)
 
-    def test_memory_full_info_mocked(self):
+    def test_parse_smaps_mocked(self):
         # See: https://github.com/giampaolo/psutil/issues/1222
         with mock_open_content(
             ""/proc/%s/smaps"" % os.getpid(),
@@ -1823,12 +1814,12 @@ def test_memory_full_info_mocked(self):
                 Locked:                19 kB
                 VmFlags: rd ex
                 """""").encode()) as m:
-            p = psutil.Process()
-            mem = p.memory_full_info()
+            p = psutil._pslinux.Process(os.getpid())
+            uss, pss, swap = p._parse_smaps()
             assert m.called
-            self.assertEqual(mem.uss, (6 + 7 + 14) * 1024)
-            self.assertEqual(mem.pss, 3 * 1024)
-            self.assertEqual(mem.swap, 15 * 1024)
+            self.assertEqual(uss, (6 + 7 + 14) * 1024)
+            self.assertEqual(pss, 3 * 1024)
+            self.assertEqual(swap, 15 * 1024)
 
     # On PYPY file descriptors are not closed fast enough.
     @unittest.skipIf(PYPY, ""unreliable on PYPY"")"
12;jopohl;urh;eb9d5317fed60e69be6ae1257f7373e6366fca0f;get magnitudes in cython for less memory consumption (#946);"@@ -9,7 +9,7 @@ import numpy as np
 from libc.stdint cimport uint8_t, uint16_t, uint32_t, uint64_t, int64_t
 from libc.stdlib cimport malloc, calloc, free
 from cython.parallel import prange
-from libc.math cimport log10,pow
+from libc.math cimport log10,pow,sqrt
 from libcpp cimport bool
 
 from cpython cimport array
@@ -124,6 +124,17 @@ cpdef uint64_t crc(uint8_t[:] inpt, uint8_t[:] polynomial, uint8_t[:] start_valu
 
     return crc & crc_mask
 
+
+cpdef np.ndarray[np.double_t, ndim=1] get_magnitudes(IQ arr):
+    cdef uint64_t i, n = len(arr)
+
+    cdef np.ndarray[np.double_t, ndim=1] result = np.zeros(n, dtype = np.double)
+
+    for i in range(0, n):
+        result[i] = sqrt(arr[i][0] * arr[i][0] + arr[i][1] * arr[i][1])
+
+    return result
+
 cpdef np.ndarray[np.uint64_t, ndim=1] calculate_cache(uint8_t[:] polynomial, bool reverse_polynomial=False, uint8_t bits=8):
     cdef uint8_t j, poly_order = len(polynomial)
     cdef uint64_t crc_mask = <uint64_t> pow(2, poly_order - 1) - 1"
12;jopohl;urh;eb9d5317fed60e69be6ae1257f7373e6366fca0f;get magnitudes in cython for less memory consumption (#946);"@@ -5,6 +5,8 @@
 
 import numpy as np
 
+from urh.cythonext.util import get_magnitudes
+
 
 class IQArray(object):
     def __init__(self, data: np.ndarray, dtype=None, n=None, skip_conversion=False):
@@ -75,13 +77,9 @@ def imag(self):
     def imag(self, value):
         self.__data[:, 1] = value
 
-    @property
-    def magnitudes_squared(self):
-        return self.real**2.0 + self.imag**2.0
-
     @property
     def magnitudes(self):
-        return np.sqrt(self.magnitudes_squared)
+        return get_magnitudes(self.__data)
 
     @property
     def magnitudes_normalized(self):"
13;mikf;gallery-dl;535cbcb185fca7a62e0927be96e9c3275a6cbb24;"cache extracted browser cookies

(in memory, for as long as gallery-dl is running)

Extracting encrypted cookies from a chromium-based browser can take a
long time, so repeating this process for each extractor should be
avoided.

Same goes for creating a temporary copy of the entire cookie database.";"@@ -302,6 +302,7 @@ def _init_cookies(self):
         if cookies:
             if isinstance(cookies, dict):
                 self._update_cookies_dict(cookies, self.cookiedomain)
+
             elif isinstance(cookies, str):
                 cookiefile = util.expand_path(cookies)
                 try:
@@ -311,12 +312,27 @@ def _init_cookies(self):
                     self.log.warning(""cookies: %s"", exc)
                 else:
                     self._cookiefile = cookiefile
+
             elif isinstance(cookies, (list, tuple)):
-                from ..cookies import load_cookies
-                try:
-                    load_cookies(self._cookiejar, cookies)
-                except Exception as exc:
-                    self.log.warning(""cookies: %s"", exc)
+                key = tuple(cookies)
+                cookiejar = _browser_cookies.get(key)
+
+                if cookiejar is None:
+                    from ..cookies import load_cookies
+                    cookiejar = self._cookiejar.__class__()
+                    try:
+                        load_cookies(cookiejar, cookies)
+                    except Exception as exc:
+                        self.log.warning(""cookies: %s"", exc)
+                    else:
+                        _browser_cookies[key] = cookiejar
+                else:
+                    self.log.debug(""Using cached cookies from %s"", key)
+
+                setcookie = self._cookiejar.set_cookie
+                for cookie in cookiejar:
+                    setcookie(cookie)
+
             else:
                 self.log.warning(
                     ""Expected 'dict', 'list', or 'str' value for 'cookies' ""
@@ -692,6 +708,7 @@ def _build_requests_adapter(ssl_options, ssl_ciphers, source_address):
 
 
 _adapter_cache = {}
+_browser_cookies = {}
 
 
 HTTP_HEADERS = {"
13;humphd;have-fun-with-machine-learning;caf3ca00970e40a2548f343046541e54658b1906;Fix #17: add note about increasing memory in Docker;"@@ -317,6 +317,11 @@ where higher is better) and what our **Loss** is (the sum of all the mistakes th
 made, where lower is better).  Ideally we want a network that is able to predict with
 high accuracy, and with few errors (small loss).
 
+**NOTE:** some people have [reported hitting errors in DIGITS](https://github.com/humphd/have-fun-with-machine-learning/issues/17)
+doing this training run. For many, the problem related to available memory (the process
+needs a lot of memory to work).  If you're using Docker, you might want to try
+increasing the amount of memory available to DIGITS (in Docker, preferences -> advanced -> memory).
+
 Initially, our networkâ€™s accuracy is a bit below 50%.  This makes sense, because at first itâ€™s
 just â€œguessingâ€ between two categories using randomly assigned weights.  Over time
 itâ€™s able to achieve 87.5% accuracy, with a loss of 0.37.  The entire 30 epoch run"
14;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"@@ -764,8 +764,10 @@ def assemble(self):
         if is_win:
             # Set checksum to appease antiviral software. Also set build timestamp to current time to increase entropy
             # (but honor SOURCE_DATE_EPOCH environment variable for reproducible builds).
+            logger.info(""Fixing EXE headers"")
             build_timestamp = int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))
-            winutils.fixup_exe_headers(build_name, build_timestamp)
+            winutils.set_exe_build_timestamp(build_name, build_timestamp)
+            winutils.update_exe_pe_checksum(build_name)
         elif is_darwin:
             # If the version of macOS SDK used to build bootloader exceeds that of macOS SDK used to built Python
             # library (and, by extension, bundled Tcl/Tk libraries), force the version declared by the frozen executable"
14;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"@@ -148,18 +148,19 @@ def convert_dll_name_to_str(dll_name):
         return dll_name
 
 
-def fixup_exe_headers(exe_path, timestamp=None):
+def set_exe_build_timestamp(exe_path, timestamp):
     """"""
-    Set executable's checksum and build timestamp in its headers.
-
-    This optional checksum is supposed to protect the executable against corruption but some anti-viral software have
-    taken to flagging anything without it set correctly as malware. See issue #5579.
+    Modifies the executable's build timestamp by updating values in the corresponding PE headers.
     """"""
     import pefile
-    pe = pefile.PE(exe_path, fast_load=False)  # full load because we need all headers
-    # Set build timestamp.
-    # See: https://0xc0decafe.com/malware-analyst-guide-to-pe-timestamps
-    if timestamp is not None:
+
+    with pefile.PE(exe_path, fast_load=True) as pe:
+        # Manually perform a full load. We need it to load all headers, but specifying it in the constructor triggers
+        # byte statistics gathering that takes forever with large files. So we try to go around that...
+        pe.full_load()
+
+        # Set build timestamp.
+        # See: https://0xc0decafe.com/malware-analyst-guide-to-pe-timestamps
         timestamp = int(timestamp)
         # Set timestamp field in FILE_HEADER
         pe.FILE_HEADER.TimeDateStamp = timestamp
@@ -169,7 +170,189 @@ def fixup_exe_headers(exe_path, timestamp=None):
         for debug_entry in debug_entries:
             if debug_entry.struct.TimeDateStamp:
                 debug_entry.struct.TimeDateStamp = timestamp
-    # Set PE checksum
-    pe.OPTIONAL_HEADER.CheckSum = pe.generate_checksum()
-    pe.close()
-    pe.write(exe_path)
+
+        # Generate updated EXE data
+        data = pe.write()
+
+    # Rewrite the exe
+    with open(exe_path, 'wb') as fp:
+        fp.write(data)
+
+
+def update_exe_pe_checksum(exe_path):
+    """"""
+    Compute the executable's PE checksum, and write it to PE headers.
+
+    This optional checksum is supposed to protect the executable against corruption but some anti-viral software have
+    taken to flagging anything without it set correctly as malware. See issue #5579.
+    """"""
+    import pefile
+
+    # Compute checksum using our equivalent of the MapFileAndCheckSumW - for large files, it is significantly faster
+    # than pure-pyton pefile.PE.generate_checksum(). However, it requires the file to be on disk (i.e., cannot operate
+    # on a memory buffer).
+    try:
+        checksum = compute_exe_pe_checksum(exe_path)
+    except Exception as e:
+        raise RuntimeError(""Failed to compute PE checksum!"") from e
+
+    # Update the checksum
+    with pefile.PE(exe_path, fast_load=True) as pe:
+        pe.OPTIONAL_HEADER.CheckSum = checksum
+
+        # Generate updated EXE data
+        data = pe.write()
+
+    # Rewrite the exe
+    with open(exe_path, 'wb') as fp:
+        fp.write(data)
+
+
+def compute_exe_pe_checksum(exe_path):
+    """"""
+    This is a replacement for the MapFileAndCheckSumW function. As noted in MSDN documentation, the Microsoft's
+    implementation of MapFileAndCheckSumW internally calls its ASCII variant (MapFileAndCheckSumA), and therefore
+    cannot handle paths that contain characters that are not representable in the current code page.
+    See: https://docs.microsoft.com/en-us/windows/win32/api/imagehlp/nf-imagehlp-mapfileandchecksumw
+
+    This function is based on Wine's implementation of MapFileAndCheckSumW, and due to being based entirely on
+    the pure widechar-API functions, it is not limited by the current code page.
+    """"""
+    # ctypes bindings for relevant win32 API functions
+    import ctypes
+    from ctypes import windll, wintypes
+
+    INVALID_HANDLE = wintypes.HANDLE(-1).value
+
+    GetLastError = ctypes.windll.kernel32.GetLastError
+    GetLastError.argtypes = ()
+    GetLastError.restype = wintypes.DWORD
+
+    CloseHandle = windll.kernel32.CloseHandle
+    CloseHandle.argtypes = (
+        wintypes.HANDLE,  # hObject
+    )
+    CloseHandle.restype = wintypes.BOOL
+
+    CreateFileW = windll.kernel32.CreateFileW
+    CreateFileW.argtypes = (
+        wintypes.LPCWSTR,  # lpFileName
+        wintypes.DWORD,  # dwDesiredAccess
+        wintypes.DWORD,  # dwShareMode
+        wintypes.LPVOID,  # lpSecurityAttributes
+        wintypes.DWORD,  # dwCreationDisposition
+        wintypes.DWORD,  # dwFlagsAndAttributes
+        wintypes.HANDLE,  # hTemplateFile
+    )
+    CreateFileW.restype = wintypes.HANDLE
+
+    CreateFileMappingW = windll.kernel32.CreateFileMappingW
+    CreateFileMappingW.argtypes = (
+        wintypes.HANDLE,  # hFile
+        wintypes.LPVOID,  # lpSecurityAttributes
+        wintypes.DWORD,  # flProtect
+        wintypes.DWORD,  # dwMaximumSizeHigh
+        wintypes.DWORD,  # dwMaximumSizeLow
+        wintypes.LPCWSTR,  # lpName
+    )
+    CreateFileMappingW.restype = wintypes.HANDLE
+
+    MapViewOfFile = windll.kernel32.MapViewOfFile
+    MapViewOfFile.argtypes = (
+        wintypes.HANDLE,  # hFileMappingObject
+        wintypes.DWORD,  # dwDesiredAccess
+        wintypes.DWORD,  # dwFileOffsetHigh
+        wintypes.DWORD,  # dwFileOffsetLow
+        wintypes.DWORD,  # dwNumberOfBytesToMap
+    )
+    MapViewOfFile.restype = wintypes.LPVOID
+
+    UnmapViewOfFile = windll.kernel32.UnmapViewOfFile
+    UnmapViewOfFile.argtypes = (
+        wintypes.LPCVOID,  # lpBaseAddress
+    )
+    UnmapViewOfFile.restype = wintypes.BOOL
+
+    GetFileSizeEx = windll.kernel32.GetFileSizeEx
+    GetFileSizeEx.argtypes = (
+        wintypes.HANDLE,  # hFile
+        wintypes.PLARGE_INTEGER,  # lpFileSize
+    )
+
+    CheckSumMappedFile = windll.imagehlp.CheckSumMappedFile
+    CheckSumMappedFile.argtypes = (
+        wintypes.LPVOID,  # BaseAddress
+        wintypes.DWORD,  # FileLength
+        wintypes.PDWORD,  # HeaderSum
+        wintypes.PDWORD,  # CheckSum
+    )
+    CheckSumMappedFile.restype = wintypes.LPVOID
+
+    # Open file
+    hFile = CreateFileW(
+        ctypes.c_wchar_p(exe_path),
+        0x80000000,  # dwDesiredAccess = GENERIC_READ
+        0x00000001 | 0x00000002,  # dwShareMode = FILE_SHARE_READ | FILE_SHARE_WRITE,
+        None,  # lpSecurityAttributes = NULL
+        3,  # dwCreationDisposition = OPEN_EXISTING
+        0x80,  # dwFlagsAndAttributes = FILE_ATTRIBUTE_NORMAL
+        None  # hTemplateFile = NULL
+    )
+    if hFile == INVALID_HANDLE:
+        err = GetLastError()
+        raise RuntimeError(f""Failed to open file {exe_path}! Error code: {err}"")
+
+    # Query file size
+    fileLength = wintypes.LARGE_INTEGER(0)
+    if GetFileSizeEx(hFile, fileLength) == 0:
+        err = GetLastError()
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to query file size file! Error code: {err}"")
+    fileLength = fileLength.value
+    if fileLength > (2**32 - 1):
+        raise RuntimeError(""Executable size exceeds maximum allowed executable size on Windows (4 GiB)!"")
+
+    # Map the file
+    hMapping = CreateFileMappingW(
+        hFile,
+        None,  # lpFileMappingAttributes = NULL
+        0x02,  # flProtect = PAGE_READONLY
+        0,  # dwMaximumSizeHigh = 0
+        0,  # dwMaximumSizeLow = 0
+        None  # lpName = NULL
+    )
+    if not hMapping:
+        err = GetLastError()
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to map file! Error code: {err}"")
+
+    # Create map view
+    baseAddress = MapViewOfFile(
+        hMapping,
+        4,  # dwDesiredAccess = FILE_MAP_READ
+        0,  # dwFileOffsetHigh = 0
+        0,  # dwFileOffsetLow = 0
+        0  # dwNumberOfBytesToMap = 0
+    )
+    if baseAddress == 0:
+        err = GetLastError()
+        CloseHandle(hMapping)
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to create map view! Error code: {err}"")
+
+    # Finally, compute the checksum
+    headerSum = wintypes.DWORD(0)
+    checkSum = wintypes.DWORD(0)
+    ret = CheckSumMappedFile(baseAddress, fileLength, ctypes.byref(headerSum), ctypes.byref(checkSum))
+    if ret is None:
+        err = GetLastError()
+
+    # Cleanup
+    UnmapViewOfFile(baseAddress)
+    CloseHandle(hMapping)
+    CloseHandle(hFile)
+
+    if ret is None:
+        raise RuntimeError(f""CheckSumMappedFile failed! Error code: {err}"")
+
+    return checkSum.value"
14;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"@@ -0,0 +1,2 @@
+(Windows) Optimize EXE PE headers fix-up process in an attempt to reduce
+the processing time and the memory footprint with large onefile builds."
16;codertimo;BERT-pytorch;7b145dc5eea4a992fefcad8b492ce9b9a8cc9b87;Adding none memory loading;"@@ -8,8 +8,11 @@ class BERTDataset(Dataset):
     def __init__(self, corpus_path, vocab, seq_len, encoding=""utf-8"", corpus_lines=None, on_memory=True):
         self.vocab = vocab
         self.seq_len = seq_len
+
         self.on_memory = on_memory
         self.corpus_lines = corpus_lines
+        self.corpus_path = corpus_path
+        self.encoding = encoding
 
         with open(corpus_path, ""r"", encoding=encoding) as f:
             if self.corpus_lines is None and not on_memory:
@@ -21,6 +24,13 @@ def __init__(self, corpus_path, vocab, seq_len, encoding=""utf-8"", corpus_lines=N
                               for line in tqdm.tqdm(f, desc=""Loading Dataset"", total=corpus_lines)]
                 self.corpus_lines = len(self.lines)
 
+        if not on_memory:
+            self.file = open(corpus_path, ""r"", encoding=encoding)
+            self.random_file = open(corpus_path, ""r"", encoding=encoding)
+
+            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
+                self.random_file.__next__()
+
     def __len__(self):
         return self.corpus_lines
 
@@ -78,8 +88,36 @@ def random_word(self, sentence):
         return tokens, output_label
 
     def random_sent(self, index):
+        t1, t2 = self.get_corpus_line(index)
+
         # output_text, label(isNotNext:0, isNext:1)
         if random.random() > 0.5:
-            return self.datas[index][0], self.datas[index][1], 1
+            return t1, t2, 1
+        else:
+            return t1, self.get_random_line(), 0
+
+    def get_corpus_line(self, item):
+        if self.on_memory:
+            return self.lines[item][0], self.lines[item][1]
         else:
-            return self.datas[index][0], self.datas[random.randrange(len(self.datas))][1], 0
+            line = self.file.__next__()
+            if line is None:
+                self.file.close()
+                self.file = open(self.corpus_path, ""r"", encoding=self.encoding)
+                line = self.file.__next__()
+
+            t1, t2 = line[:-1].split(""\t"")
+            return t1, t2
+
+    def get_random_line(self):
+        if self.on_memory:
+            return self.lines[random.randrange(len(self.lines))][1]
+
+        line = self.file.__next__()
+        if line is None:
+            self.file.close()
+            self.file = open(self.corpus_path, ""r"", encoding=self.encoding)
+            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
+                self.random_file.__next__()
+            line = self.random_file.__next__()
+        return line[:-1].split(""\t"")[1]"
16;codertimo;BERT-pytorch;7b145dc5eea4a992fefcad8b492ce9b9a8cc9b87;Adding none memory loading;"@@ -3,8 +3,6 @@
 from torch.optim import Adam
 from torch.utils.data import DataLoader
 
-from encoding.parallel import DataParallelModel, DataParallelCriterion
-
 from ..model import BERTLM, BERT
 
 import tqdm
@@ -49,7 +47,7 @@ def __init__(self, bert: BERT, vocab_size: int,
         # Distributed GPU training if CUDA can detect more than 1 GPU
         if with_cuda and torch.cuda.device_count() > 1:
             print(""Using %d GPUS for BERT"" % torch.cuda.device_count())
-            self.model = DataParallelModel(self.model, device_ids=cuda_devices)
+            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)
 
         # Setting the train and test data loader
         self.train_data = train_dataloader
@@ -60,8 +58,6 @@ def __init__(self, bert: BERT, vocab_size: int,
 
         # Using Negative Log Likelihood Loss function for predicting the masked_token
         self.criterion = nn.NLLLoss(ignore_index=0)
-        if with_cuda and torch.cuda.device_count() > 0:
-            self.criterion = DataParallelCriterion(self.criterion, device_ids=cuda_devices)
 
         self.log_freq = log_freq
 "
16;codertimo;BERT-pytorch;7b145dc5eea4a992fefcad8b492ce9b9a8cc9b87;Adding none memory loading;"@@ -1,4 +1,3 @@
 tqdm
 numpy
-torch>=0.4.0
-torch-encodin
\ No newline at end of file
+torch>=0.4.0
\ No newline at end of file"
16;zalando;patroni;ad3d95341085755b45768a2efad60660571e53ac;"K8s: reset watchers if PATCH fails with 409 (#2283)

High CPU load on Etcd nodes and K8s API servers created a very strange situation. A few clusters were running without a leader and the pod which is ahead of others was failing to take a leader lock because updates were failing with HTTP response code `409` (`resource_version` mismatch).

Effectively that means that TCP connections to K8s master nodes were alive (in the opposite case tcp keepalives would have resolved it), but no `UPDATE` events were arriving via these connections, resulting in the stale cache of the cluster in memory.

The only good way to prevent this situation is to intercept 409 HTTP responses and terminate existing TCP connections used for watches.

Now a few words about implementation. Unfortunately, watch threads are waiting in the read() call most of the time and there is no good way to interrupt them. But, the `socket.shutdown()` seems to do this job. We already used this trick in the Etcd3 implementation.

This approach will help to mitigate the issue of not having a leader, but at the same time replicas might still end up with the stale cluster state cached and in the worst case will not stream from the leader. Non-streaming replicas are less dangerous and could be covered by monitoring and partially mitigated by correctly configured `archive_command` and `restore_command`.";"@@ -518,6 +518,8 @@ def __init__(self, dcs, func, retry, condition, name=None):
         self._condition = condition
         self._name = name  # name of this pod
         self._is_ready = False
+        self._response = None  # needs to be accessible from the `kill_stream()` method
+        self._response_lock = Lock()  # protect the `self._response` from concurrent access
         self._object_cache = {}
         self._object_cache_lock = Lock()
         self._annotations_map = {self._dcs.leader_path: self._dcs._LEADER, self._dcs.config_path: self._dcs._CONFIG}
@@ -558,63 +560,99 @@ def get(self, name):
         with self._object_cache_lock:
             return self._object_cache.get(name)
 
+    def _process_event(self, event):
+        ev_type = event['type']
+        obj = event['object']
+        name = obj['metadata']['name']
+
+        if ev_type in ('ADDED', 'MODIFIED'):
+            obj = K8sObject(obj)
+            success, old_value = self.set(name, obj)
+            if success:
+                new_value = (obj.metadata.annotations or {}).get(self._annotations_map.get(name))
+        elif ev_type == 'DELETED':
+            success, old_value = self.delete(name, obj['metadata']['resourceVersion'])
+            new_value = None
+        else:
+            return logger.warning('Unexpected event type: %s', ev_type)
+
+        if success and obj.get('kind') != 'Pod':
+            if old_value:
+                old_value = (old_value.metadata.annotations or {}).get(self._annotations_map.get(name))
+
+            value_changed = old_value != new_value and \
+                (name != self._dcs.config_path or old_value is not None and new_value is not None)
+
+            if value_changed:
+                logger.debug('%s changed from %s to %s', name, old_value, new_value)
+
+            # Do not wake up HA loop if we run as leader and received leader object update event
+            if value_changed or name == self._dcs.leader_path and self._name != new_value:
+                self._dcs.event.set()
+
+    @staticmethod
+    def _finish_response(response):
+        try:
+            response.close()
+        finally:
+            response.release_conn()
+
+    def _do_watch(self, resource_version):
+        with self._response_lock:
+            self._response = None
+        response = self._watch(resource_version)
+        with self._response_lock:
+            if self._response is None:
+                self._response = response
+
+        if not self._response:
+            return self._finish_response(response)
+
+        for event in iter_response_objects(response):
+            if event['object'].get('code') == 410:
+                break
+            self._process_event(event)
+
     def _build_cache(self):
         objects = self._list()
-        return_type = 'V1' + objects.kind[:-4]
         with self._object_cache_lock:
             self._object_cache = {item.metadata.name: item for item in objects.items}
         with self._condition:
             self._is_ready = True
             self._condition.notify()
 
-        response = self._watch(objects.metadata.resource_version)
         try:
-            for event in iter_response_objects(response):
-                obj = event['object']
-                if obj.get('code') == 410:
-                    break
-
-                ev_type = event['type']
-                name = obj['metadata']['name']
-
-                if ev_type in ('ADDED', 'MODIFIED'):
-                    obj = K8sObject(obj)
-                    success, old_value = self.set(name, obj)
-                    if success:
-                        new_value = (obj.metadata.annotations or {}).get(self._annotations_map.get(name))
-                elif ev_type == 'DELETED':
-                    success, old_value = self.delete(name, obj['metadata']['resourceVersion'])
-                    new_value = None
-                else:
-                    logger.warning('Unexpected event type: %s', ev_type)
-                    continue
-
-                if success and return_type != 'V1Pod':
-                    if old_value:
-                        old_value = (old_value.metadata.annotations or {}).get(self._annotations_map.get(name))
-
-                    value_changed = old_value != new_value and \
-                        (name != self._dcs.config_path or old_value is not None and new_value is not None)
-
-                    if value_changed:
-                        logger.debug('%s changed from %s to %s', name, old_value, new_value)
-
-                    # Do not wake up HA loop if we run as leader and received leader object update event
-                    if value_changed or name == self._dcs.leader_path and self._name != new_value:
-                        self._dcs.event.set()
+            self._do_watch(objects.metadata.resource_version)
         finally:
             with self._condition:
                 self._is_ready = False
-            response.close()
-            response.release_conn()
+            with self._response_lock:
+                response, self._response = self._response, None
+            if response:
+                self._finish_response(response)
+
+    def kill_stream(self):
+        sock = None
+        with self._response_lock:
+            if self._response:
+                try:
+                    sock = self._response.connection.sock
+                except Exception:
+                    sock = None
+            else:
+                self._response = False
+        if sock:
+            try:
+                sock.shutdown(socket.SHUT_RDWR)
+                sock.close()
+            except Exception as e:
+                logger.debug('Error on socket.shutdown: %r', e)
 
     def run(self):
         while True:
             try:
                 self._build_cache()
             except Exception as e:
-                with self._condition:
-                    self._is_ready = False
                 logger.error('ObjectCache.run %r', e)
 
     def is_ready(self):
@@ -889,7 +927,14 @@ def _patch_or_create(self, name, annotations, resource_version=None, patch=False
     def patch_or_create(self, name, annotations, resource_version=None, patch=False, retry=True, ips=None):
         if retry is True:
             retry = self.retry
-        return self._patch_or_create(name, annotations, resource_version, patch, retry, ips)
+        try:
+            return self._patch_or_create(name, annotations, resource_version, patch, retry, ips)
+        except k8s_client.rest.ApiException as e:
+            if e.status == 409 and resource_version:  # Conflict in resource_version
+                # Terminate watchers, it could be a sign that K8s API is in a failed state
+                self._kinds.kill_stream()
+                self._pods.kill_stream()
+            raise e
 
     def patch_or_create_config(self, annotations, resource_version=None, patch=False, retry=True):
         # SCOPE-config endpoint requires corresponding service otherwise it might be ""cleaned"" by k8s master"
16;zalando;patroni;ad3d95341085755b45768a2efad60660571e53ac;"K8s: reset watchers if PATCH fails with 409 (#2283)

High CPU load on Etcd nodes and K8s API servers created a very strange situation. A few clusters were running without a leader and the pod which is ahead of others was failing to take a leader lock because updates were failing with HTTP response code `409` (`resource_version` mismatch).

Effectively that means that TCP connections to K8s master nodes were alive (in the opposite case tcp keepalives would have resolved it), but no `UPDATE` events were arriving via these connections, resulting in the stale cache of the cluster in memory.

The only good way to prevent this situation is to intercept 409 HTTP responses and terminate existing TCP connections used for watches.

Now a few words about implementation. Unfortunately, watch threads are waiting in the read() call most of the time and there is no good way to interrupt them. But, the `socket.shutdown()` seems to do this job. We already used this trick in the Etcd3 implementation.

This approach will help to mitigate the issue of not having a leader, but at the same time replicas might still end up with the stale cluster state cached and in the worst case will not stream from the leader. Non-streaming replicas are less dangerous and could be covered by monitoring and partially mitigated by correctly configured `archive_command` and `restore_command`.";"@@ -4,7 +4,7 @@
 import time
 import unittest
 
-from mock import Mock, mock_open, patch
+from mock import Mock, PropertyMock, mock_open, patch
 from patroni.dcs.kubernetes import k8s_client, k8s_config, K8sConfig, K8sConnectionFailed,\
         K8sException, K8sObject, Kubernetes, KubernetesError, KubernetesRetriableException,\
         Retry, RetryFailedError, SERVICE_HOST_ENV_NAME, SERVICE_PORT_ENV_NAME
@@ -235,7 +235,9 @@ def test_manual_failover(self):
             self.k.manual_failover('foo', 'bar')
 
     def test_set_config_value(self):
-        self.k.set_config_value('{}')
+        with patch.object(k8s_client.CoreV1Api, 'patch_namespaced_config_map',
+                          Mock(side_effect=k8s_client.rest.ApiException(409, '')), create=True):
+            self.k.set_config_value('{}', 1)
 
     @patch.object(k8s_client.CoreV1Api, 'patch_namespaced_pod', create=True)
     def test_touch_member(self, mock_patch_namespaced_pod):
@@ -345,3 +347,18 @@ def test_run(self):
     def test__list(self):
         self.k._pods._func = Mock(side_effect=Exception)
         self.assertRaises(Exception, self.k._pods._list)
+
+    @patch('patroni.dcs.kubernetes.ObjectCache._watch', Mock(return_value=None))
+    def test__do_watch(self):
+        self.assertRaises(AttributeError, self.k._kinds._do_watch, '1')
+
+    @patch.object(k8s_client.CoreV1Api, 'list_namespaced_config_map', mock_list_namespaced_config_map, create=True)
+    @patch('patroni.dcs.kubernetes.ObjectCache._watch')
+    def test_kill_stream(self, mock_watch):
+        self.k._kinds.kill_stream()
+        mock_watch.return_value.read_chunked.return_value = []
+        mock_watch.return_value.connection.sock.close.side_effect = Exception
+        self.k._kinds._do_watch('1')
+        self.k._kinds.kill_stream()
+        type(mock_watch.return_value).connection = PropertyMock(side_effect=Exception)
+        self.k._kinds.kill_stream()"
16;karpathy;arxiv-sanity-preserver;99f55f78f8f025c1999162c44cc3c5e4af3c7cb7;store in float32 instead of float64 works okay and half memory;"@@ -35,7 +35,7 @@ def query_db(query, args=(), one=False):
 meta = pickle.load(open(Config.meta_path, 'rb'))
 out = pickle.load(open(Config.tfidf_path, 'rb'))
 X = out['X']
-X = X.todense()
+X = X.todense().astype(np.float32)
 
 xtoi = { strip_version(x):i for x,i in meta['ptoi'].items() }
 "
17;deepmind;pysc2;55ccc8cf4ef9f37aafa79509e70c1726bcb0201c;"Improvements to replay_actions:
- launch the right version.
- don't request RGB, ie lower memory use, etc.
- skip printing the stats if there are no replays.

PiperOrigin-RevId: 267365037";"@@ -37,6 +37,7 @@
 from pysc2.lib import point
 from pysc2.lib import protocol
 from pysc2.lib import remote_controller
+from pysc2.lib import replay
 
 from pysc2.lib import gfile
 from s2clientprotocol import common_pb2 as sc_common
@@ -190,7 +191,8 @@ def run(self):
       self._print(""Starting up a new SC2 instance."")
       self._update_stage(""launch"")
       try:
-        with self.run_config.start() as controller:
+        with self.run_config.start(
+            want_rgb=interface.HasField(""render"")) as controller:
           self._print(""SC2 Started successfully."")
           ping = controller.ping()
           for _ in range(300):
@@ -355,7 +357,6 @@ def main(unused_argv):
 
   stats_queue = multiprocessing.Queue()
   stats_thread = threading.Thread(target=stats_printer, args=(stats_queue,))
-  stats_thread.start()
   try:
     # For some reason buffering everything into a JoinableQueue makes the
     # program not exit, so save it into a list then slowly fill it into the
@@ -364,7 +365,19 @@ def main(unused_argv):
     # The replay_queue.join below succeeds without doing any work, and exits.
     print(""Getting replay list:"", FLAGS.replays)
     replay_list = sorted(run_config.replay_paths(FLAGS.replays))
-    print(len(replay_list), ""replays found.\n"")
+    print(len(replay_list), ""replays found."")
+    if not replay_list:
+      return
+
+    if not FLAGS[""sc2_version""].present:  # ie not set explicitly.
+      version = replay.get_replay_version(
+          run_config.replay_data(replay_list[0]))
+      run_config = run_configs.get(version=version)
+      print(""Assuming version:"", version.game_version)
+
+    print()
+
+    stats_thread.start()
     replay_queue = multiprocessing.JoinableQueue(FLAGS.parallel * 10)
     replay_queue_thread = threading.Thread(target=replay_queue_filler,
                                            args=(replay_queue, replay_list))
@@ -382,7 +395,8 @@ def main(unused_argv):
     print(""Caught KeyboardInterrupt, exiting."")
   finally:
     stats_queue.put(None)  # Tell the stats_thread to print and exit.
-    stats_thread.join()
+    if stats_thread.is_alive():
+      stats_thread.join()
 
 
 if __name__ == ""__main__"":"
20;locuslab;TCN;b89618ab1d1e2e5e6be558d4d1951124d1a221a7;"Merge pull request #7 from kashif/patch-1

Added missing zero_grad at each iteration to copy memory test - training script";"@@ -106,7 +106,8 @@ def train(ep):
 
         x = train_x[start_ind:end_ind]
         y = train_y[start_ind:end_ind]
-
+        
+        optimizer.zero_grad()
         out = model(x.unsqueeze(1).contiguous())
         loss = criterion(out.view(-1, n_classes), y.view(-1))
         pred = out.view(-1, n_classes).data.max(1, keepdim=True)[1]"
21;sripathikrishnan;redis-rdb-tools;f97b72ffc8d698abbc814435be88e856f852117b;"fix crash in parsing module data types containing floats

* modules save binary float, and not the old string based floats.
* fix crash in the memory profiler when handling rdb files with modules or streams
* improve memory report to contains the total estimated memory, and used_mem aux field, etc.";"@@ -25,9 +25,11 @@ def __init__(self, key_groupings = None):
         self.aggregates = {}
         self.scatters = {}
         self.histograms = {}
+        self.metadata = {}
 
     def next_record(self, record):
         self.add_aggregate('database_memory', record.database, record.bytes)
+        self.add_aggregate('database_memory', 'all', record.bytes)
         self.add_aggregate('type_memory', record.type, record.bytes)
         self.add_aggregate('encoding_memory', record.encoding, record.bytes)
         
@@ -47,7 +49,7 @@ def next_record(self, record):
             self.add_scatter('sortedset_memory_by_length', record.bytes, record.size)
         elif record.type == 'string':
             self.add_scatter('string_memory_by_length', record.bytes, record.size)
-        elif record.type == 'dict':
+        elif record.type in ['dict', 'module', 'stream']:
             pass
         else:
             raise Exception('Invalid data type %s' % record.type)
@@ -74,9 +76,12 @@ def add_scatter(self, heading, x, y):
         if not heading in self.scatters:
             self.scatters[heading] = []
         self.scatters[heading].append([x, y])
+
+    def set_metadata(self, key, val):
+        self.metadata[key] = val
   
     def get_json(self):
-        return json.dumps({""aggregates"":self.aggregates, ""scatters"":self.scatters, ""histograms"":self.histograms})
+        return json.dumps({""aggregates"": self.aggregates, ""scatters"": self.scatters, ""histograms"": self.histograms, ""metadata"": self.metadata})
         
 class PrintAllKeys(object):
     def __init__(self, out, bytes, largest):
@@ -159,7 +164,6 @@ def start_rdb(self):
         pass
 
     def aux_field(self, key, value):
-        #print('aux: %s %s' % (key, value))
         if key == 'used-mem':
             self._aux_used_mem = int(value)
         if key == 'redis-ver':
@@ -179,9 +183,13 @@ def end_database(self, db_number):
             self._stream.end_database(db_number)
 
     def end_rdb(self):
-        #print('internal fragmentation: %s' % self._total_internal_frag)
         if hasattr(self._stream, 'end_rdb'):
             self._stream.end_rdb()
+        if hasattr(self._stream, 'set_metadata'):
+            self._stream.set_metadata('used_mem', self._aux_used_mem)
+            self._stream.set_metadata('redis_ver', self._aux_redis_ver)
+            self._stream.set_metadata('redis_bits', self._aux_redis_bits)
+            self._stream.set_metadata('internal_frag', self._total_internal_frag)
 
     def set(self, key, value, expiry, info):
         self._current_encoding = info['encoding']"
21;sripathikrishnan;redis-rdb-tools;f97b72ffc8d698abbc814435be88e856f852117b;"fix crash in parsing module data types containing floats

* modules save binary float, and not the old string based floats.
* fix crash in the memory profiler when handling rdb files with modules or streams
* improve memory report to contains the total estimated memory, and used_mem aux field, etc.";"@@ -558,7 +558,7 @@ def read_object(self, f, enc_type) :
             self._callback.start_sorted_set(self._key, length, self._expiry, info={'encoding':'skiplist','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 val = self.read_string(f)
-                score = read_double(f) if enc_type == REDIS_RDB_TYPE_ZSET_2 else self.read_float(f)
+                score = read_binary_double(f) if enc_type == REDIS_RDB_TYPE_ZSET_2 else self.read_float(f)
                 self._callback.zadd(self._key, score, val)
             self._callback.end_sorted_set(self._key)
         elif enc_type == REDIS_RDB_TYPE_HASH:
@@ -825,9 +825,9 @@ def skip_module(self, f):
             if opcode == REDIS_RDB_MODULE_OPCODE_SINT or opcode == REDIS_RDB_MODULE_OPCODE_UINT:
                 self.read_length(f)
             elif opcode == REDIS_RDB_MODULE_OPCODE_FLOAT:
-                self.skip_float(f)
+                read_binary_float(f)
             elif opcode == REDIS_RDB_MODULE_OPCODE_DOUBLE:
-                read_double(f)
+                read_binary_double(f)
             elif opcode == REDIS_RDB_MODULE_OPCODE_STRING:
                 self.skip_string(f)
             else:
@@ -851,9 +851,9 @@ def read_module(self, f):
             if opcode == REDIS_RDB_MODULE_OPCODE_SINT or opcode == REDIS_RDB_MODULE_OPCODE_UINT:
                 data = self.read_length(iowrapper)
             elif opcode == REDIS_RDB_MODULE_OPCODE_FLOAT:
-                data = self.read_float(iowrapper)
+                data = read_binary_float(iowrapper)
             elif opcode == REDIS_RDB_MODULE_OPCODE_DOUBLE:
-                data = read_double(iowrapper)
+                data = read_binary_double(iowrapper)
             elif opcode == REDIS_RDB_MODULE_OPCODE_STRING:
                 data = self.read_string(iowrapper)
             else:
@@ -1104,9 +1104,12 @@ def read_milliseconds_time(f) :
 def read_unsigned_long_be(f) :
     return struct.unpack('>Q', f.read(8))[0]
 
-def read_double(f) :
+def read_binary_double(f) :
     return struct.unpack('d', f.read(8))[0]
 
+def read_binary_float(f) :
+    return struct.unpack('f', f.read(4))[0]
+
 def string_as_hexcode(string) :
     for s in string :
         if isinstance(s, int) :"
21;sripathikrishnan;redis-rdb-tools;f97b72ffc8d698abbc814435be88e856f852117b;"fix crash in parsing module data types containing floats

* modules save binary float, and not the old string based floats.
* fix crash in the memory profiler when handling rdb files with modules or streams
* improve memory report to contains the total estimated memory, and used_mem aux field, etc.";"@@ -15,7 +15,10 @@
         google.setOnLoadCallback(draw_charts);
         
         function draw_charts() {
-            //draw_pie_chart('database_memory', chart_data.aggregates.database_memory, 'Database Number', 'Size in Bytes', 'Memory Usage by Database')
+            document.getElementById('database_memory').innerHTML = chart_data.aggregates.database_memory['all']
+            document.getElementById('aux_used_mem').innerHTML = chart_data.metadata.used_mem
+            document.getElementById('aux_redis_ver_and_bits').innerHTML = chart_data.metadata.redis_ver + ' - ' + chart_data.metadata.redis_bits + 'bit'
+            document.getElementById('internal_frag').innerHTML = chart_data.metadata.internal_frag
             
             draw_pie_chart('type_memory', chart_data.aggregates.type_memory, 'Data Type', 'Total Size in Bytes', 'Memory Usage by Data Type')
             draw_column_chart('type_count', chart_data.aggregates.type_count, 'Data Type', 'Keys', 'Number of Keys by Data Type')
@@ -105,14 +108,26 @@
   <body>
     <h1>Redis Memory Distribution for dump.rdb</h1>
     <div class=""container"">
-        <!--
+        <h2>Total database computed memory:</h2>
         <div class=""row"">
             <div class=""span6"" id=""database_memory"">
             </div>
-            <div class=""span6"" id="""">
+        </div>
+        <h2>Total database memory from RDB aux field:</h2>
+        <div class=""row"">
+            <div class=""span6"" id=""aux_used_mem"">
+            </div>
+        </div>
+        <h2>Database redis version and bits:</h2>
+        <div class=""row"">
+            <div class=""span6"" id=""aux_redis_ver_and_bits"">
+            </div>
+        </div>
+        <h2>Total estimated internal fragmentation:</h2>
+        <div class=""row"">
+            <div class=""span6"" id=""internal_frag"">
             </div>
         </div>
-        -->
         <h2>Memory Usage By Data Type and Data Encoding</h2>
         <div class=""row"">
             <div class=""span6"" id=""type_memory"">"
21;sripathikrishnan;redis-rdb-tools;cce34491ceb1ffbb58decfbee5c13656d23e3547;"fix #144 - memprofiler on python 2.x crash on long integers (#145)

besides the above crash, it looks like the memory estimation for numeric
list entries was wrong, v3 redis always saved integer encoded strings
inside the robj, with no extra memory

and for some reason, quicklists used the raw string length rather than
using ziplist_entry_overhead, like in a simple ziplist";"@@ -243,8 +243,8 @@ def end_set(self, key):
     
     def start_list(self, key, expiry, info):
         self._current_length = 0
-        self._list_items_size = 0
-        self._list_items_zipped_size = 0
+        self._list_items_size = 0  # size of all elements in case list ends up using linked list
+        self._list_items_zipped_size = 0  # size of all elements in case of ziplist of quicklist
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
         self._key_expiry = expiry
@@ -267,23 +267,26 @@ def start_list(self, key, expiry, info):
             
     def rpush(self, key, value):
         self._current_length += 1
-        size = self.sizeof_string(value) if type(value) != int else 4
+        # in linked list, when the robj has integer encoding, the value consumes no memory on top of the robj
+        size_in_list = self.sizeof_string(value) if not self.is_integer_type(value) else 0
+        # in ziplist and quicklist, this is the size of the value and the value header
+        size_in_zip = self.ziplist_entry_overhead(value)
 
         if(self.element_length(value) > self._len_largest_element):
             self._len_largest_element = self.element_length(value)
 
         if self._current_encoding == ""ziplist"":
-            self._list_items_zipped_size += self.ziplist_entry_overhead(value)
-            if self._current_length > self._list_max_ziplist_entries or size > self._list_max_ziplist_value:
+            self._list_items_zipped_size += size_in_zip
+            if self._current_length > self._list_max_ziplist_entries or size_in_zip > self._list_max_ziplist_value:
                 self._current_encoding = ""linkedlist""
         elif self._current_encoding == ""quicklist"":
-            if self._cur_zip_size + size > self._list_max_ziplist_size:
-                self._cur_zip_size = size
+            if self._cur_zip_size + size_in_zip > self._list_max_ziplist_size:
+                self._cur_zip_size = size_in_zip
                 self._cur_zips += 1
             else:
-                self._cur_zip_size += size
+                self._cur_zip_size += size_in_zip
             self._list_items_zipped_size += self.ziplist_entry_overhead(value)
-        self._list_items_size += size  # not to be used in case of ziplist or quicklist
+        self._list_items_size += size_in_list  # not to be used in case of ziplist or quicklist
 
     def end_list(self, key, info):
         if self._current_encoding == 'quicklist':
@@ -473,7 +476,7 @@ def ziplist_header_overhead(self):
 
     def ziplist_entry_overhead(self, value):
         # See https://github.com/antirez/redis/blob/unstable/src/ziplist.c
-        if type(value) == int:
+        if self.is_integer_type(value):
             header = 1
             if value < 12:
                 size = 0
@@ -539,12 +542,17 @@ def zset_random_level(self):
         else:
             return ZSKIPLIST_MAXLEVEL
 
+    def is_integer_type(self, ob):
+        if isinstance(ob, int):
+            return True
+        if sys.version_info < (3,):
+            if isinstance(ob, long):
+                return True
+        return False
+
     def element_length(self, element):
-        if isinstance(element, int):
+        if self.is_integer_type(element):
             return self._long_size
-        if sys.version_info < (3,):
-            if isinstance(element, long):
-                return self._long_size
         return len(element)
 
 "
21;sripathikrishnan;redis-rdb-tools;c457a0364c42c2b0edb4f094faff0777355e2859;"Merge pull request #135 from oranagra/int_mem_fix

minor fix to memory calculations of numeric values";"@@ -403,7 +403,7 @@ def sizeof_string(self, string):
             if num < REDIS_SHARED_INTEGERS :
                 return 0
             else :
-                return 8
+                return 0  # the integer is part of the robj, no extra memory
         except ValueError:
             pass
         l = len(string)"
21;sripathikrishnan;redis-rdb-tools;bd80d88e550964e921af211f5e48391ded00611f;minor fix to memory calculations of numeric values;"@@ -403,7 +403,7 @@ def sizeof_string(self, string):
             if num < REDIS_SHARED_INTEGERS :
                 return 0
             else :
-                return 8
+                return 0  # the integer is part of the robj, no extra memory
         except ValueError:
             pass
         l = len(string)"
21;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -64,7 +64,8 @@ def _write_comma(self):
     def set(self, key, value, expiry, info):
         self._start_key(key, 0)
         self._out.write(self.encode_key(key) + b':' + self.encode_value(value))
-    
+        self._end_key(key)
+
     def start_hash(self, key, length, expiry, info):
         self._start_key(key, length)
         self._out.write(self.encode_key(key) + b':{')
@@ -113,7 +114,24 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         self._end_key(key)
         self._out.write(b'}')
-        
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b':{')
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        self._end_key(key)
+        self._out.write(b'}')
+
+    def start_module(self, key, module_name, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b':{')
+        return False
+
+    def end_module(self, key, buffer_size, buffer=None):
+        self._end_key(key)
+        self._out.write(b'}')
+
 
 class KeysOnlyCallback(RdbCallback):
     def __init__(self, out, string_escape=None):
@@ -150,6 +168,12 @@ def start_sorted_set(self, key, length, expiry, info):
     def zadd(self, key, score, member):
         self._keyout(key)
         
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._keyout(key)
+
+    def start_module(self, key, module_name, expiry, info):
+        self._keyout(key)
+        return False
 
 class KeyValsOnlyCallback(RdbCallback):
     def __init__(self, out, string_escape=None):
@@ -226,6 +250,20 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         self._end_key(key)
 
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b' ')
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        self._end_key(key)
+
+    def start_module(self, key, module_name, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b' ')
+        return False
+
+    def end_module(self, key, buffer_size, buffer=None):
+        self._end_key(key)
 
 class DiffCallback(RdbCallback):
     '''Prints the contents of RDB in a format that is unix sort friendly, 
@@ -300,6 +338,15 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         pass
 
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        self._out.write(self.dbstr() + self.encode_key(key) + b' -> stream-items=' + encodehelpers.num2bytes(items))
+        self.newline()
+
+    def start_module(self, key, module_name, expiry, info):
+        self._out.write(self.dbstr() + self.encode_key(key) + b' -> module-name=' + codecs.encode(module_name, 'ascii'))
+        self.newline()
+        return False
+
     def newline(self):
         self._out.write(b'\r\n')
 
@@ -398,6 +445,16 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         self.post_expiry(key)
 
+    # streams and modules, not currently supported
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        # TODO send RESTORE command
+        pass
+
+    def start_module(self, key, module_name, expiry, info):
+        # TODO send RESTORE command
+        return False
+
     # Other misc commands
 
     def select(self, db_number):"
21;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -124,7 +124,7 @@ class MemoryCallback(RdbCallback):
     '''Calculates the memory used if this rdb file were loaded into RAM
         The memory usage is approximate, and based on heuristics.
     '''
-    def __init__(self, stream, architecture, redis_version='3.2', string_escape=None):
+    def __init__(self, stream, architecture, redis_version='5.0', string_escape=None):
         super(MemoryCallback, self).__init__(string_escape)
         self._stream = stream
         self._dbnum = 0
@@ -303,7 +303,7 @@ def end_list(self, key, info):
                          self._len_largest_element, self._key_expiry)
         self.end_key()
 
-    def start_module(self, key, module_id, expiry):
+    def start_module(self, key, module_id, expiry, info):
         self._key_expiry = expiry
         self._current_encoding = module_id
         self._current_size = self.top_level_object_overhead(key, expiry)
@@ -316,6 +316,49 @@ def end_module(self, key, buffer_size, buffer=None):
         self.emit_record(""module"", key, size, self._current_encoding, 1, size, self._key_expiry)
         self.end_key()
 
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._key_expiry = expiry
+        self._current_encoding = info['encoding']
+        self._current_size = self.top_level_object_overhead(key, expiry)
+        self._current_size += self.sizeof_pointer()*2 + 8 + 16  # stream struct
+        self._current_size += self.sizeof_pointer() + 8*2  # rax struct
+        self._listpacks_count = listpacks_count
+
+    def stream_listpack(self, key, entry_id, data):
+        self._current_size += self.malloc_overhead(len(data))
+        if(len(data) > self._len_largest_element):
+            self._len_largest_element = len(data)
+        pass
+
+    def sizeof_stream_radix_tree(self, num_elements):
+        # This is a very rough estimation. The only alternative to doing an estimation,
+        # is to fully build a radix tree of similar design, and count the nodes.
+        # There should be at least as many nodes as there are elements in the radix tree (possibly up to 3 times)
+        num_nodes = int(num_elements * 2.5)
+        # formula for memory estimation copied from Redis's streamRadixTreeMemoryUsage
+        return 16*num_elements + num_nodes*4 + num_nodes*30*self.sizeof_long()
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        # Now after we have some global key+value overheads, and all listpacks sizes,
+        # we need to add some estimations for radix tree and consumer groups.
+        # The logic for the memory estimation copied from Redis's MEMORY command.
+        radix_tree_size = self.sizeof_stream_radix_tree(self._listpacks_count)
+        cgroups_size = 0
+        for cg in cgroups:
+            cgroups_size += self.sizeof_pointer() * 2 + 16  # streamCG
+            pending = len(cg['pending'])
+            cgroups_size += self.sizeof_stream_radix_tree(pending)
+            cgroups_size += pending*(self.sizeof_pointer()+8+8)  # streamNACK
+            for c in cg['consumers']:
+                cgroups_size += self.sizeof_pointer()*2 + 8  # streamConsumer
+                cgroups_size += self.sizeof_string(c['name'])
+                pending = len(c['pending'])
+                cgroups_size += self.sizeof_stream_radix_tree(pending)
+        size = self._current_size + radix_tree_size + cgroups_size
+        self._current_length = items
+        self.emit_record(""stream"", key, size, self._current_encoding, 1, self._len_largest_element, self._key_expiry)
+        self.end_key()
+
     def start_sorted_set(self, key, length, expiry, info):
         self._current_length = length
         self._current_encoding = info['encoding']"
21;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -27,6 +27,9 @@
 REDIS_RDB_64BITLEN = 0x81
 REDIS_RDB_ENCVAL = 3
 
+REDIS_RDB_OPCODE_MODULE_AUX = 247
+REDIS_RDB_OPCODE_IDLE = 248
+REDIS_RDB_OPCODE_FREQ = 249
 REDIS_RDB_OPCODE_AUX = 250
 REDIS_RDB_OPCODE_RESIZEDB = 251
 REDIS_RDB_OPCODE_EXPIRETIME_MS = 252
@@ -48,6 +51,7 @@
 REDIS_RDB_TYPE_ZSET_ZIPLIST = 12
 REDIS_RDB_TYPE_HASH_ZIPLIST = 13
 REDIS_RDB_TYPE_LIST_QUICKLIST = 14
+REDIS_RDB_TYPE_STREAM_LISTPACKS = 15
 
 REDIS_RDB_ENC_INT8 = 0
 REDIS_RDB_ENC_INT16 = 1
@@ -63,7 +67,7 @@
 
 DATA_TYPE_MAPPING = {
     0 : ""string"", 1 : ""list"", 2 : ""set"", 3 : ""sortedset"", 4 : ""hash"", 5 : ""sortedset"", 6 : ""module"", 7: ""module"",
-    9 : ""hash"", 10 : ""list"", 11 : ""set"", 12 : ""sortedset"", 13 : ""hash"", 14 : ""list""}
+    9 : ""hash"", 10 : ""list"", 11 : ""set"", 12 : ""sortedset"", 13 : ""hash"", 14 : ""list"", 15 : ""stream""}
 
 class RdbCallback(object):
     """"""
@@ -117,12 +121,13 @@ def start_database(self, db_number):
         """"""
         pass
 
-    def start_module(self, key, module_name, expiry):
+    def start_module(self, key, module_name, expiry, info):
         """"""
         Called to indicate start of a module key
-        :param key: string
+        :param key: string. if key is None, this is module AUX data
         :param module_name: string
         :param expiry:
+        :param info: is a dictionary containing additional information about this object.
         :return: boolean to indicate whatever to record the full buffer or not
         """"""
         return False
@@ -296,7 +301,44 @@ def end_sorted_set(self, key):
         
         """"""
         pass
-    
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        """"""Callback to handle the start of a stream
+
+        `key` is the redis key
+        `listpacks_count` is the number of listpacks in this stream.
+        `expiry` is a `datetime` object. None means the object does not expire
+        `info` is a dictionary containing additional information about this object.
+
+        After `start_stream`, the method `stream_listpack` will be called with this `key` exactly `listpacks_count` times.
+        After that, the `end_stream` method will be called.
+
+        """"""
+        pass
+
+    def stream_listpack(self, key, entry_id, data):
+        """"""
+        Callback to insert a listpack into a stream
+
+        `key` is the redis key for this stream
+        `entry_id` is binary (bigendian)
+        `data` the bytes of the listpack
+
+        """"""
+        pass
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        """"""
+        Called when there is no more data in the stream
+
+        `key` is the redis key for the stream
+        `items` is the total number of items in the stream
+        `last_entry_id` is in ""<millisecondsTime>-<sequenceNumber>"" format
+        `cgroups` is an array of consumer group metadata
+
+        """"""
+        pass
+
     def end_database(self, db_number):
         """"""
         Called when the current database ends
@@ -339,6 +381,8 @@ def __init__(self, callback, filters = None) :
         self._callback = callback
         self._key = None
         self._expiry = None
+        self._idle = None
+        self._freq = None
         self.init_filter(filters)
         self._rdb_version = 0
 
@@ -359,15 +403,25 @@ def parse_fd(self, fd):
             db_number = 0
             while True :
                 self._expiry = None
+                self._idle = None
+                self._freq = None
                 data_type = read_unsigned_char(f)
 
                 if data_type == REDIS_RDB_OPCODE_EXPIRETIME_MS :
-                    self._expiry = to_datetime(read_unsigned_long(f) * 1000)
+                    self._expiry = read_milliseconds_time(f)
                     data_type = read_unsigned_char(f)
                 elif data_type == REDIS_RDB_OPCODE_EXPIRETIME :
                     self._expiry = to_datetime(read_unsigned_int(f) * 1000000)
                     data_type = read_unsigned_char(f)
 
+                if data_type == REDIS_RDB_OPCODE_IDLE:
+                    self._idle = self.read_length(f)
+                    data_type = read_unsigned_char(f)
+
+                if data_type == REDIS_RDB_OPCODE_FREQ:
+                    self._freq = read_unsigned_char(f)
+                    data_type = read_unsigned_char(f)
+
                 if data_type == REDIS_RDB_OPCODE_SELECTDB :
                     if not is_first_database :
                         self._callback.end_database(db_number)
@@ -390,6 +444,10 @@ def parse_fd(self, fd):
                     self._callback.db_size(db_size, expire_size)
                     continue
 
+                if data_type == REDIS_RDB_OPCODE_MODULE_AUX:
+                    self.read_module(f)
+                    continue
+
                 if data_type == REDIS_RDB_OPCODE_EOF:
                     self._callback.end_database(db_number)
                     self._callback.end_rdb()
@@ -403,6 +461,7 @@ def parse_fd(self, fd):
                         self.read_object(f, data_type)
                     else:
                         self.skip_object(f, data_type)
+                    self._key = None
                 else :
                     self.skip_key_and_object(f, data_type)
 
@@ -472,14 +531,14 @@ def read_float(self, f):
     def read_object(self, f, enc_type) :
         if enc_type == REDIS_RDB_TYPE_STRING :
             val = self.read_string(f)
-            self._callback.set(self._key, val, self._expiry, info={'encoding':'string'})
+            self._callback.set(self._key, val, self._expiry, info={'encoding':'string','idle':self._idle,'freq':self._freq})
         elif enc_type == REDIS_RDB_TYPE_LIST :
             # A redis list is just a sequence of strings
             # We successively read strings from the stream and create a list from it
             # The lists are in order i.e. the first string is the head, 
             # and the last string is the tail of the list
             length = self.read_length(f)
-            self._callback.start_list(self._key, self._expiry, info={'encoding':'linkedlist' })
+            self._callback.start_list(self._key, self._expiry, info={'encoding':'linkedlist','idle':self._idle,'freq':self._freq})
             for count in range(0, length) :
                 val = self.read_string(f)
                 self._callback.rpush(self._key, val)
@@ -489,22 +548,22 @@ def read_object(self, f, enc_type) :
             # We successively read strings from the stream and create a set from it
             # Note that the order of strings is non-deterministic
             length = self.read_length(f)
-            self._callback.start_set(self._key, length, self._expiry, info={'encoding':'hashtable'})
+            self._callback.start_set(self._key, length, self._expiry, info={'encoding':'hashtable','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 val = self.read_string(f)
                 self._callback.sadd(self._key, val)
             self._callback.end_set(self._key)
         elif enc_type == REDIS_RDB_TYPE_ZSET or enc_type == REDIS_RDB_TYPE_ZSET_2 :
             length = self.read_length(f)
-            self._callback.start_sorted_set(self._key, length, self._expiry, info={'encoding':'skiplist'})
+            self._callback.start_sorted_set(self._key, length, self._expiry, info={'encoding':'skiplist','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 val = self.read_string(f)
                 score = read_double(f) if enc_type == REDIS_RDB_TYPE_ZSET_2 else self.read_float(f)
                 self._callback.zadd(self._key, score, val)
             self._callback.end_sorted_set(self._key)
         elif enc_type == REDIS_RDB_TYPE_HASH:
             length = self.read_length(f)
-            self._callback.start_hash(self._key, length, self._expiry, info={'encoding':'hashtable'})
+            self._callback.start_hash(self._key, length, self._expiry, info={'encoding':'hashtable','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 field = self.read_string(f)
                 value = self.read_string(f)
@@ -526,6 +585,8 @@ def read_object(self, f, enc_type) :
             raise Exception('read_object', 'Unable to read Redis Modules RDB objects (key %s)' % self._key)
         elif enc_type == REDIS_RDB_TYPE_MODULE_2:
             self.read_module(f)
+        elif enc_type == REDIS_RDB_TYPE_STREAM_LISTPACKS:
+            self.read_stream(f)
         else:
             raise Exception('read_object', 'Invalid object type %d for key %s' % (enc_type, self._key))
 
@@ -592,7 +653,9 @@ def skip_object(self, f, enc_type):
         elif enc_type == REDIS_RDB_TYPE_MODULE:
             raise Exception('skip_object', 'Unable to skip Redis Modules RDB objects (key %s)' % self._key)
         elif enc_type == REDIS_RDB_TYPE_MODULE_2:
-            self.read_module(f)
+            self.skip_module(f)
+        elif enc_type == REDIS_RDB_TYPE_STREAM_LISTPACKS:
+            self.skip_stream(f)
         else:
             raise Exception('skip_object', 'Invalid object type %d for key %s' % (enc_type, self._key))
         for x in range(0, skip_strings):
@@ -604,7 +667,7 @@ def read_intset(self, f) :
         buff = BytesIO(raw_string)
         encoding = read_unsigned_int(buff)
         num_entries = read_unsigned_int(buff)
-        self._callback.start_set(self._key, num_entries, self._expiry, info={'encoding':'intset', 'sizeof_value':len(raw_string)})
+        self._callback.start_set(self._key, num_entries, self._expiry, info={'encoding':'intset', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             if encoding == 8 :
                 entry = read_signed_long(buff)
@@ -623,7 +686,7 @@ def read_ziplist(self, f) :
         zlbytes = read_unsigned_int(buff)
         tail_offset = read_unsigned_int(buff)
         num_entries = read_unsigned_short(buff)
-        self._callback.start_list(self._key, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string)})
+        self._callback.start_list(self._key, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             val = self.read_ziplist_entry(buff)
             self._callback.rpush(self._key, val)
@@ -635,7 +698,7 @@ def read_ziplist(self, f) :
     def read_list_from_quicklist(self, f):
         count = self.read_length(f)
         total_size = 0
-        self._callback.start_list(self._key, self._expiry, info={'encoding': 'quicklist', 'zips': count})
+        self._callback.start_list(self._key, self._expiry, info={'encoding': 'quicklist', 'zips': count,'idle':self._idle,'freq':self._freq})
         for i in range(0, count):
             raw_string = self.read_string(f)
             total_size += len(raw_string)
@@ -659,7 +722,7 @@ def read_zset_from_ziplist(self, f) :
         if (num_entries % 2) :
             raise Exception('read_zset_from_ziplist', ""Expected even number of elements, but found %d for key %s"" % (num_entries, self._key))
         num_entries = num_entries // 2
-        self._callback.start_sorted_set(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string)})
+        self._callback.start_sorted_set(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             member = self.read_ziplist_entry(buff)
             score = self.read_ziplist_entry(buff)
@@ -680,7 +743,7 @@ def read_hash_from_ziplist(self, f) :
         if (num_entries % 2) :
             raise Exception('read_hash_from_ziplist', ""Expected even number of elements, but found %d for key %s"" % (num_entries, self._key))
         num_entries = num_entries // 2
-        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string)})
+        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             field = self.read_ziplist_entry(buff)
             value = self.read_ziplist_entry(buff)
@@ -727,7 +790,7 @@ def read_zipmap(self, f) :
         raw_string = self.read_string(f)
         buff = io.BytesIO(bytearray(raw_string))
         num_entries = read_unsigned_char(buff)
-        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'zipmap', 'sizeof_value':len(raw_string)})
+        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'zipmap', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         while True :
             next_length = self.read_zipmap_next_length(buff)
             if next_length is None :
@@ -756,13 +819,29 @@ def read_zipmap_next_length(self, f) :
         else:
             return None
 
+    def skip_module(self, f):
+        opcode = self.read_length(f)
+        while opcode != REDIS_RDB_MODULE_OPCODE_EOF:
+            if opcode == REDIS_RDB_MODULE_OPCODE_SINT or opcode == REDIS_RDB_MODULE_OPCODE_UINT:
+                self.read_length(f)
+            elif opcode == REDIS_RDB_MODULE_OPCODE_FLOAT:
+                self.skip_float(f)
+            elif opcode == REDIS_RDB_MODULE_OPCODE_DOUBLE:
+                read_double(f)
+            elif opcode == REDIS_RDB_MODULE_OPCODE_STRING:
+                self.skip_string(f)
+            else:
+                raise Exception(""Unknown module opcode %s"" % opcode)
+            # read the next item in the module data type
+            opcode = self.read_length(f)
+
     def read_module(self, f):
         # this method is based on the actual implementation in redis (src/rdb.c:rdbLoadObject)
         iowrapper = IOWrapper(f)
         iowrapper.start_recording_size()
         iowrapper.start_recording()
         length, encoding = self.read_length_with_encoding(iowrapper)
-        record_buffer = self._callback.start_module(self._key, self._decode_module_id(length), self._expiry)
+        record_buffer = self._callback.start_module(self._key, self._decode_module_id(length), self._expiry, info={'idle':self._idle, 'freq':self._freq})
 
         if not record_buffer:
             iowrapper.stop_recording()
@@ -790,6 +869,72 @@ def read_module(self, f):
             iowrapper.stop_recording()
         self._callback.end_module(self._key, buffer_size=iowrapper.get_recorded_size(), buffer=buffer)
 
+    def skip_stream(self, f):
+        listpacks = self.read_length(f)
+        for _lp in range(listpacks):
+            self.skip_string(f)
+            self.skip_string(f)
+        self.read_length(f)
+        self.read_length(f)
+        self.read_length(f)
+        cgroups = self.read_length(f)
+        for _cg in range(cgroups):
+            self.skip_string(f)
+            self.read_length(f)
+            self.read_length(f)
+            pending = self.read_length(f)
+            for _pel in range(pending):
+                f.read(16)
+                f.read(8)
+                self.read_length(f)
+            consumers = self.read_length(f)
+            for _c in range(consumers):
+                self.skip_string(f)
+                f.read(8)
+                pending = self.read_length(f)
+                f.read(pending*16)
+
+    def read_stream(self, f):
+        listpacks = self.read_length(f)
+        self._callback.start_stream(self._key, listpacks, self._expiry,
+                                    info={'encoding': 'listpack', 'idle': self._idle, 'freq': self._freq})
+        for _lp in range(listpacks):
+            self._callback.stream_listpack(self._key, self.read_string(f), self.read_string(f))
+        items = self.read_length(f)
+        last_entry_id = ""%s-%s"" % (self.read_length(f), self.read_length(f))
+        cgroups = self.read_length(f)
+        cgroups_data = []
+        for _cg in range(cgroups):
+            cgname = self.read_string(f)
+            last_cg_entry_id = ""%s-%s"" % (self.read_length(f), self.read_length(f))
+            pending = self.read_length(f)
+            group_pending_entries = []
+            for _pel in range(pending):
+                eid = f.read(16)
+                delivery_time = read_milliseconds_time(f)
+                delivery_count = self.read_length(f)
+                group_pending_entries.append({'id': eid,
+                                              'delivery_time': delivery_time,
+                                              'delivery_count': delivery_count})
+            consumers = self.read_length(f)
+            consumers_data = []
+            for _c in range(consumers):
+                cname = self.read_string(f)
+                seen_time = read_milliseconds_time(f)
+                pending = self.read_length(f)
+                consumer_pending_entries = []
+                for _pel in range( pending):
+                    eid = f.read(16)
+                    consumer_pending_entries.append({'id': eid})
+                consumers_data.append({'name': cname,
+                                       'seen_time': seen_time,
+                                       'pending': consumer_pending_entries})
+            cgroups_data.append({'name': cgname,
+                                 'last_entry_id': last_cg_entry_id,
+                                 'pending': group_pending_entries,
+                                 'consumers': consumers_data})
+        self._callback.end_stream(self._key, items, last_entry_id, cgroups_data)
+
     charset = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_'
 
     def _decode_module_id(self, module_id):
@@ -812,7 +957,7 @@ def verify_magic_string(self, magic_string) :
 
     def verify_version(self, version_str) :
         version = int(version_str)
-        if version < 1 or version > 8: 
+        if version < 1 or version > 9:
             raise Exception('verify_version', 'Invalid RDB version number %d' % version)
         self._rdb_version = version
 
@@ -840,14 +985,13 @@ def init_filter(self, filters):
         else:
             self._filters['not_keys'] = str2regexp(filters['not_keys'])
 
-        if not 'types' in filters:
-            self._filters['types'] = ('set', 'hash', 'sortedset', 'module', 'string', 'list')
-        elif isinstance(filters['types'], bytes):
-            self._filters['types'] = (filters['types'], )
-        elif isinstance(filters['types'], list):
-            self._filters['types'] = [str(x) for x in filters['types']]
-        else:
-            raise Exception('init_filter', 'invalid value for types in filter %s' %filters['types'])
+        if 'types' in filters:
+            if isinstance(filters['types'], bytes):
+                self._filters['types'] = (filters['types'], )
+            elif isinstance(filters['types'], list):
+                self._filters['types'] = [str(x) for x in filters['types']]
+            else:
+                raise Exception('init_filter', 'invalid value for types in filter %s' %filters['types'])
         
     def matches_filter(self, db_number, key=None, data_type=None):
 
@@ -865,7 +1009,7 @@ def matches_filter(self, db_number, key=None, data_type=None):
         if key and (not self._filters['keys'].match(key_to_match)):
             return False
 
-        if data_type is not None and (not self.get_logical_type(data_type) in self._filters['types']):
+        if data_type is not None and 'types' in self._filters and (not self.get_logical_type(data_type) in self._filters['types']):
             return False
         return True
     
@@ -954,6 +1098,9 @@ def read_signed_long(f) :
 def read_unsigned_long(f) :
     return struct.unpack('Q', f.read(8))[0]
     
+def read_milliseconds_time(f) :
+    return to_datetime(read_unsigned_long(f) * 1000)
+
 def read_unsigned_long_be(f) :
     return struct.unpack('>Q', f.read(8))[0]
 "
21;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -4,6 +4,7 @@
 import random
 import sys
 from io import BytesIO
+import traceback
 
 from rdbtools import RdbParser
 from rdbtools import encodehelpers
@@ -79,8 +80,8 @@ def test_all_dumps(self):
             try:
                 parser.parse(dump_name)
             except Exception as err:
-                raise self.failureException(""%s on %s - %s: %s"" % (
-                    self._callback_class.__name__, os.path.basename(dump_name), type(err).__name__, str(err)))
+                raise self.failureException(""%s on %s\n%s"" % (
+                    self._callback_class.__name__, os.path.basename(dump_name), traceback.format_exc()))
             self._out.seek(0)
             self._out.truncate()
 "
21;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -89,3 +89,12 @@ def test_rdb_with_module(self):
                                        bytes=101, encoding='ReJSON-RL', size=1,
                                        len_largest_element=101, expiry=None)
         self.assertEquals(stats['foo'], expected_record)
+
+    def test_rdb_with_stream(self):
+        stats = get_stats('redis_50_with_streams.rdb')
+
+        self.assertTrue('mystream' in stats)
+        expected_record = MemoryRecord(database=0, type='stream', key='mystream',
+                                       bytes=1976, encoding='listpack', size=1,
+                                       len_largest_element=184, expiry=None)
+        self.assertEquals(stats['mystream'], expected_record)"
21;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -204,6 +204,16 @@ def test_multiple_databases_stream(self):
         self.assertEquals(r.databases[0][b""key_in_zeroth_database""], b""zero"")
         self.assertEquals(r.databases[2][b""key_in_second_database""], b""second"")
 
+    def test_rdb_version_8_with_module(self):
+        r = load_rdb('redis_40_with_module.rdb')
+        self.assertEquals(r.databases[0][b'foo']['module_name'], 'ReJSON-RL')
+
+    def test_rdb_version_9_with_stream(self):
+        r = load_rdb('redis_50_with_streams.rdb')
+        self.assertEquals(r.lengths[0][b""mystream""], 4)
+        self.assertEquals(len(r.databases[0][b'mystream']), 1)
+
+
 def floateq(f1, f2) :
     return math.fabs(f1 - f2) < 0.00001
 
@@ -339,6 +349,41 @@ def end_sorted_set(self, key):
             raise Exception('Lengths mismatch on sortedset %s, expected length = %d, actual = %d'
                                  % (key, self.lengths[self.dbnum][key], len(self.currentdb()[key])))
 
+    def start_module(self, key, module_name, expiry, info):
+        if key in self.currentdb() :
+            raise Exception('start_module called with key %s that already exists' % key)
+        else :
+            self.currentdb()[key] = {'module_name': module_name}
+        if expiry :
+            self.store_expiry(key, expiry)
+        return False
+
+    def end_module(self, key, buffer_size, buffer=None):
+        if not key in self.currentdb() :
+            raise Exception('start_module not called for key = %s', key)
+        self.store_length(key, buffer_size)
+        pass
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        if key in self.currentdb() :
+            raise Exception('start_stream called with key %s that already exists' % key)
+        else :
+            self.currentdb()[key] = {}
+        if expiry :
+            self.store_expiry(key, expiry)
+        pass
+
+    def stream_listpack(self, key, entry_id, data):
+        if not key in self.currentdb() :
+            raise Exception('start_hash not called for key = %s', key)
+        self.currentdb()[key][entry_id] = data
+        pass
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        if not key in self.currentdb() :
+            raise Exception('start_stream not called for key = %s', key)
+        self.store_length(key, items)
+
     def end_database(self, dbnum):
         if self.dbnum != dbnum :
             raise Exception('start_database called with %d, but end_database called %d instead' % (self.dbnum, dbnum))"
21;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -9,4 +9,7 @@ tests/dumps/dump_dealers_vins.rdb
 tests/dumps/dump_random_lists.rdb
 tests/dumps/dump_sorted_sets.rdb
 
-.idea/*
\ No newline at end of file
+.idea/*
+venv2.6/
+venv2.7/
+venv3/
\ No newline at end of file"
21;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -18,7 +18,7 @@
 ZSKIPLIST_P=0.25
 REDIS_SHARED_INTEGERS = 10000
 
-MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element'])
+MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element', 'expiry'])
 
 class StatsAggregator(object):
     def __init__(self, key_groupings = None):
@@ -83,8 +83,8 @@ def __init__(self, out, bytes, largest):
         self._bytes = bytes
         self._largest = largest
         self._out = out
-        headers = ""%s,%s,%s,%s,%s,%s,%s\n"" % (
-            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"")
+        headers = ""%s,%s,%s,%s,%s,%s,%s,%s\n"" % (
+            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"", ""expiry"")
         self._out.write(codecs.encode(headers, 'latin-1'))
 
         if self._largest is not None:
@@ -95,9 +95,10 @@ def next_record(self, record) :
             return  # some records are not keys (e.g. dict)
         if self._largest is None:
             if self._bytes is None or record.bytes >= int(self._bytes):
-                rec_str = ""%d,%s,%s,%d,%s,%d,%d\n"" % (
+                rec_str = ""%d,%s,%s,%d,%s,%d,%d,%s\n"" % (
                     record.database, record.type, record.key, record.bytes, record.encoding, record.size,
-                    record.len_largest_element)
+                    record.len_largest_element,
+                    record.expiry.isoformat() if record.expiry else '')
                 self._out.write(codecs.encode(rec_str, 'latin-1'))
         else:
             heappush(self._heap, (record.bytes, record))
@@ -131,6 +132,7 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
         self._current_encoding = None
         self._current_length = 0
         self._len_largest_element = 0
+        self._key_expiry = None
         self._db_keys = 0
         self._db_expires = 0
         self._aux_used_mem = None
@@ -147,10 +149,10 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
             self._long_size = 4
             self._architecture = 32
 
-    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el):
+    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el, expiry):
         if key is not None:
             key = bytes_to_unicode(key, self._escape, skip_printable=True)
-        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el)
+        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el, expiry)
         self._stream.next_record(record)
 
     def start_rdb(self):
@@ -171,8 +173,8 @@ def start_database(self, db_number):
         self._db_expires = 0
 
     def end_database(self, db_number):
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None)
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None, None)
         if hasattr(self._stream, 'end_database'):
             self._stream.end_database(db_number)
 
@@ -184,14 +186,14 @@ def end_rdb(self):
     def set(self, key, value, expiry, info):
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry) + self.sizeof_string(value)
-        
         length = self.element_length(value)
-        self.emit_record(""string"", key, size, self._current_encoding, length, length)
+        self.emit_record(""string"", key, size, self._current_encoding, length, length, expiry)
         self.end_key()
     
     def start_hash(self, key, length, expiry, info):
         self._current_encoding = info['encoding']
-        self._current_length = length        
+        self._current_length = length
+        self._key_expiry = expiry
         size = self.top_level_object_overhead(key, expiry)
         
         if 'sizeof_value' in info:
@@ -217,7 +219,7 @@ def hset(self, key, field, value):
     
     def end_hash(self, key):
         self.emit_record(""hash"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_set(self, key, cardinality, expiry, info):
@@ -236,7 +238,7 @@ def sadd(self, key, member):
     
     def end_set(self, key):
         self.emit_record(""set"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_list(self, key, expiry, info):
@@ -245,6 +247,7 @@ def start_list(self, key, expiry, info):
         self._list_items_zipped_size = 0
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
+        self._key_expiry = expiry
 
         # ignore the encoding in the rdb, and predict the encoding that will be used at the target redis version
         if self._redis_version >= StrictVersion('3.2'):
@@ -297,10 +300,11 @@ def end_list(self, key, info):
                 self._current_size += self.robj_overhead() * self._current_length
             self._current_size += self._list_items_size
         self.emit_record(""list"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
 
     def start_module(self, key, module_id, expiry):
+        self._key_expiry = expiry
         self._current_encoding = module_id
         self._current_size = self.top_level_object_overhead(key, expiry)
         self._current_size += 8 + 1  # add the module id length and EOF byte
@@ -309,14 +313,15 @@ def start_module(self, key, module_id, expiry):
 
     def end_module(self, key, buffer_size, buffer=None):
         size = self._current_size + buffer_size
-        self.emit_record(""module"", key, size, self._current_encoding, 1, size)
+        self.emit_record(""module"", key, size, self._current_encoding, 1, size, self._key_expiry)
         self.end_key()
 
     def start_sorted_set(self, key, length, expiry, info):
         self._current_length = length
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
-        
+        self._key_expiry = expiry
+
         if 'sizeof_value' in info:
             size += info['sizeof_value']
         elif 'encoding' in info and info['encoding'] == 'skiplist':
@@ -338,14 +343,15 @@ def zadd(self, key, score, member):
     
     def end_sorted_set(self, key):
         self.emit_record(""sortedset"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
         
     def end_key(self):
         self._db_keys += 1
         self._current_encoding = None
         self._current_size = 0
         self._len_largest_element = 0
+        self._key_expiry = None
     
     def sizeof_string(self, string):
         # https://github.com/antirez/redis/blob/unstable/src/sds.h"
21;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -1026,5 +1026,3 @@ def end_database(self, db_number):
     
     def end_rdb(self):
         print(']')
-
-"
21;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -1,11 +1,27 @@
+import sys
+import os
+from io import BytesIO
+
 import unittest
 
 from rdbtools import RdbParser
 from rdbtools import MemoryCallback
-import os
 
-from rdbtools.memprofiler import MemoryRecord
 
+from rdbtools.memprofiler import MemoryRecord, PrintAllKeys
+
+CSV_WITH_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,string,expires_ms_precision,128,string,27,27,2022-12-25T10:11:12.573000
+""""""
+
+CSV_WITHOUT_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,list,ziplist_compresses_easily,301,quicklist,6,36,
+""""""
+
+CSV_WITH_MODULE = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,string,simplekey,72,string,7,7,
+0,module,foo,101,ReJSON-RL,1,101,
+""""""
 
 class Stats(object):
     def __init__(self):
@@ -22,11 +38,43 @@ def get_stats(file_name):
     parser.parse(os.path.join(os.path.dirname(__file__), 'dumps', file_name))
     return stats.records
 
+def get_csv(dump_file_name):
+    buff = BytesIO()
+    callback = MemoryCallback(PrintAllKeys(buff, None, None), 64)
+    parser = RdbParser(callback)
+    parser.parse(os.path.join(os.path.dirname(__file__), 
+                    'dumps', dump_file_name))
+    csv = buff.getvalue().decode()
+    return csv
 
 class MemoryCallbackTestCase(unittest.TestCase):
     def setUp(self):
         pass
 
+    def test_csv_with_expiry(self):
+        csv = get_csv('keys_with_expiry.rdb')
+        self.assertEquals(csv, CSV_WITH_EXPIRY)
+
+    def test_csv_without_expiry(self):
+        csv = get_csv('ziplist_that_compresses_easily.rdb')
+        self.assertEquals(csv, CSV_WITHOUT_EXPIRY)
+
+    def test_csv_with_module(self):
+        csv = get_csv('redis_40_with_module.rdb')
+        self.assertEquals(csv, CSV_WITH_MODULE)
+
+    def test_expiry(self):
+        stats = get_stats('keys_with_expiry.rdb')
+
+        expiry = stats['expires_ms_precision'].expiry
+        self.assertEquals(expiry.year, 2022)
+        self.assertEquals(expiry.month, 12)
+        self.assertEquals(expiry.day, 25)
+        self.assertEquals(expiry.hour, 10)
+        self.assertEquals(expiry.minute, 11)
+        self.assertEquals(expiry.second, 12)
+        self.assertEquals(expiry.microsecond, 573000)        
+
     def test_len_largest_element(self):
         stats = get_stats('ziplist_that_compresses_easily.rdb')
 
@@ -39,5 +87,5 @@ def test_rdb_with_module(self):
         self.assertTrue('foo' in stats)
         expected_record = MemoryRecord(database=0, type='module', key='foo',
                                        bytes=101, encoding='ReJSON-RL', size=1,
-                                       len_largest_element=101)
+                                       len_largest_element=101, expiry=None)
         self.assertEquals(stats['foo'], expected_record)"
21;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -9,4 +9,7 @@ tests/dumps/dump_dealers_vins.rdb
 tests/dumps/dump_random_lists.rdb
 tests/dumps/dump_sorted_sets.rdb
 
-.idea/*
\ No newline at end of file
+.idea/*
+venv2.6/
+venv2.7/
+venv3/
\ No newline at end of file"
21;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -18,7 +18,7 @@
 ZSKIPLIST_P=0.25
 REDIS_SHARED_INTEGERS = 10000
 
-MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element'])
+MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element', 'expiry'])
 
 class StatsAggregator(object):
     def __init__(self, key_groupings = None):
@@ -83,8 +83,8 @@ def __init__(self, out, bytes, largest):
         self._bytes = bytes
         self._largest = largest
         self._out = out
-        headers = ""%s,%s,%s,%s,%s,%s,%s\n"" % (
-            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"")
+        headers = ""%s,%s,%s,%s,%s,%s,%s,%s\n"" % (
+            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"", ""expiry"")
         self._out.write(codecs.encode(headers, 'latin-1'))
 
         if self._largest is not None:
@@ -95,9 +95,10 @@ def next_record(self, record) :
             return  # some records are not keys (e.g. dict)
         if self._largest is None:
             if self._bytes is None or record.bytes >= int(self._bytes):
-                rec_str = ""%d,%s,%s,%d,%s,%d,%d\n"" % (
+                rec_str = ""%d,%s,%s,%d,%s,%d,%d,%s\n"" % (
                     record.database, record.type, record.key, record.bytes, record.encoding, record.size,
-                    record.len_largest_element)
+                    record.len_largest_element,
+                    record.expiry.isoformat() if record.expiry else '')
                 self._out.write(codecs.encode(rec_str, 'latin-1'))
         else:
             heappush(self._heap, (record.bytes, record))
@@ -131,6 +132,7 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
         self._current_encoding = None
         self._current_length = 0
         self._len_largest_element = 0
+        self._key_expiry = None
         self._db_keys = 0
         self._db_expires = 0
         self._aux_used_mem = None
@@ -147,10 +149,10 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
             self._long_size = 4
             self._architecture = 32
 
-    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el):
+    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el, expiry):
         if key is not None:
             key = bytes_to_unicode(key, self._escape, skip_printable=True)
-        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el)
+        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el, expiry)
         self._stream.next_record(record)
 
     def start_rdb(self):
@@ -171,8 +173,8 @@ def start_database(self, db_number):
         self._db_expires = 0
 
     def end_database(self, db_number):
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None)
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None, None)
         if hasattr(self._stream, 'end_database'):
             self._stream.end_database(db_number)
 
@@ -184,14 +186,14 @@ def end_rdb(self):
     def set(self, key, value, expiry, info):
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry) + self.sizeof_string(value)
-        
         length = self.element_length(value)
-        self.emit_record(""string"", key, size, self._current_encoding, length, length)
+        self.emit_record(""string"", key, size, self._current_encoding, length, length, expiry)
         self.end_key()
     
     def start_hash(self, key, length, expiry, info):
         self._current_encoding = info['encoding']
-        self._current_length = length        
+        self._current_length = length
+        self._key_expiry = expiry
         size = self.top_level_object_overhead(key, expiry)
         
         if 'sizeof_value' in info:
@@ -217,7 +219,7 @@ def hset(self, key, field, value):
     
     def end_hash(self, key):
         self.emit_record(""hash"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_set(self, key, cardinality, expiry, info):
@@ -236,7 +238,7 @@ def sadd(self, key, member):
     
     def end_set(self, key):
         self.emit_record(""set"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_list(self, key, expiry, info):
@@ -245,6 +247,7 @@ def start_list(self, key, expiry, info):
         self._list_items_zipped_size = 0
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
+        self._key_expiry = expiry
 
         # ignore the encoding in the rdb, and predict the encoding that will be used at the target redis version
         if self._redis_version >= StrictVersion('3.2'):
@@ -297,7 +300,7 @@ def end_list(self, key, info):
                 self._current_size += self.robj_overhead() * self._current_length
             self._current_size += self._list_items_size
         self.emit_record(""list"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
 
     def start_module(self, key, module_id, expiry):
@@ -309,14 +312,15 @@ def start_module(self, key, module_id, expiry):
 
     def end_module(self, key, buffer_size, buffer=None):
         size = self._current_size + buffer_size
-        self.emit_record(""module"", key, size, self._current_encoding, 1, size)
+        self.emit_record(""module"", key, size, self._current_encoding, 1, size, None)
         self.end_key()
 
     def start_sorted_set(self, key, length, expiry, info):
         self._current_length = length
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
-        
+        self._key_expiry = expiry
+
         if 'sizeof_value' in info:
             size += info['sizeof_value']
         elif 'encoding' in info and info['encoding'] == 'skiplist':
@@ -338,14 +342,15 @@ def zadd(self, key, score, member):
     
     def end_sorted_set(self, key):
         self.emit_record(""sortedset"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
         
     def end_key(self):
         self._db_keys += 1
         self._current_encoding = None
         self._current_size = 0
         self._len_largest_element = 0
+        self._key_expiry = None
     
     def sizeof_string(self, string):
         # https://github.com/antirez/redis/blob/unstable/src/sds.h"
21;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -1026,5 +1026,3 @@ def end_database(self, db_number):
     
     def end_rdb(self):
         print(']')
-
-"
21;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -1,10 +1,22 @@
+import sys
+import os
+from io import BytesIO
+
 import unittest
 
 from rdbtools import RdbParser
 from rdbtools import MemoryCallback
-import os
 
-from rdbtools.memprofiler import MemoryRecord
+
+from rdbtools.memprofiler import MemoryRecord, PrintAllKeys
+
+CSV_WITH_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,string,expires_ms_precision,128,string,27,27,2022-12-25T10:11:12.573000
+""""""
+
+CSV_WITHOUT_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,list,ziplist_compresses_easily,301,quicklist,6,36,
+""""""
 
 
 class Stats(object):
@@ -22,11 +34,39 @@ def get_stats(file_name):
     parser.parse(os.path.join(os.path.dirname(__file__), 'dumps', file_name))
     return stats.records
 
+def get_csv(dump_file_name):
+    buff = BytesIO()
+    callback = MemoryCallback(PrintAllKeys(buff, None, None), 64)
+    parser = RdbParser(callback)
+    parser.parse(os.path.join(os.path.dirname(__file__), 
+                    'dumps', dump_file_name))
+    csv = buff.getvalue().decode()
+    return csv
 
 class MemoryCallbackTestCase(unittest.TestCase):
     def setUp(self):
         pass
 
+    def test_csv_with_expiry(self):
+        csv = get_csv('keys_with_expiry.rdb')
+        self.assertEquals(csv, CSV_WITH_EXPIRY)
+
+    def test_csv_without_expiry(self):
+        csv = get_csv('ziplist_that_compresses_easily.rdb')
+        self.assertEquals(csv, CSV_WITHOUT_EXPIRY)
+
+    def test_expiry(self):
+        stats = get_stats('keys_with_expiry.rdb')
+
+        expiry = stats['expires_ms_precision'].expiry
+        self.assertEquals(expiry.year, 2022)
+        self.assertEquals(expiry.month, 12)
+        self.assertEquals(expiry.day, 25)
+        self.assertEquals(expiry.hour, 10)
+        self.assertEquals(expiry.minute, 11)
+        self.assertEquals(expiry.second, 12)
+        self.assertEquals(expiry.microsecond, 573000)        
+
     def test_len_largest_element(self):
         stats = get_stats('ziplist_that_compresses_easily.rdb')
 
@@ -39,5 +79,5 @@ def test_rdb_with_module(self):
         self.assertTrue('foo' in stats)
         expected_record = MemoryRecord(database=0, type='module', key='foo',
                                        bytes=101, encoding='ReJSON-RL', size=1,
-                                       len_largest_element=101)
+                                       len_largest_element=101, expiry=None)
         self.assertEquals(stats['foo'], expected_record)"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -26,6 +26,6 @@ jobs:
     - name: Install dependencies
       run: |
         python -m pip install --upgrade pip
-        pip install -e "".[test]""
+        pip install -e "".[dev]""
     - name: Run Checking Mechanisms
       run: make check"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -14,7 +14,8 @@ BERTopic is a topic modeling technique that leverages ðŸ¤— transformers and c-TF
 allowing for easily interpretable topics whilst keeping important words in the topic descriptions. It even supports 
 visualizations similar to LDAvis! 
 
-Corresponding medium post can be found [here](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6?source=friends_link&sk=0b5a470c006d1842ad4c8a3057063a99).
+Corresponding medium post can be found [here](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6?source=friends_link&sk=0b5a470c006d1842ad4c8a3057063a99) 
+and [here](https://towardsdatascience.com/interactive-topic-modeling-with-bertopic-1ea55e7d73d8?sk=03c2168e9e74b6bda2a1f3ed953427e4).
 
 ## Installation
 
@@ -30,12 +31,10 @@ To use the visualization options, install BERTopic as follows:
 pip install bertopic[visualization]
 ```
 
-<details>
-<summary>Installation Errors</summary>
-
-PyTorch 1.4.0 or higher is recommended. If the install gives an
-error, please install pytorch first [here](https://pytorch.org/get-started/locally/). 
-</details>  
+To use Flair embeddings, install BERTopic as follows:
+```bash
+pip install bertopic[flair]
+```
 
 ## Getting Started
 For an in-depth overview of the features of `BERTopic` 
@@ -51,14 +50,14 @@ from sklearn.datasets import fetch_20newsgroups
  
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
 
-model = BERTopic(language=""english"")
-topics, probabilities = model.fit_transform(docs)
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs)
 ```
 
 After generating topics and their probabilities, we can access the frequent topics that were generated:
 
 ```python
->>> model.get_topic_freq().head()
+>>> topic_model.get_topic_freq().head()
 Topic	Count
 -1	7288
 49	3992
@@ -71,7 +70,7 @@ Topic	Count
 frequent topic that was generated, `topic 49`:
 
 ```python
->>> model.get_topic(49)
+>>> topic_model.get_topic(49)
 [('windows', 0.006152228076250982),
  ('drive', 0.004982897610645755),
  ('dos', 0.004845038866360651),
@@ -84,30 +83,7 @@ frequent topic that was generated, `topic 49`:
  ('pc', 0.003047105930670237)]
 ```  
 
-<details>
-<summary>Supported Languages</summary>
-
-<br>
-Use <b>""multilingual""</b> to select a model that supports 50+ languages. 
-<br><br>
-Moreover, the following <b>languages</b> are supported: <br>
-Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese,
-Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian,
-Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified),
-Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto,
-Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek,
-Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian,
-Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean,
-Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian,
-Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian,
-Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian,
-Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak,
-Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil,
-Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian,
-Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian,
-Xhosa, Yiddish
-<br>
-</details>  
+**NOTE**: Use `BERTopic(language=""multilingual"")` to select a model that supports 50+ languages. 
 
 ### Visualize Topics
 After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good 
@@ -116,59 +92,75 @@ Instead, we can visualize the topics that were generated in a way very similar t
 [LDAvis](https://github.com/cpsievert/LDAvis):
 
 ```python
-model.visualize_topics()
+topic_model.visualize_topics()
 ``` 
 
 <img src=""images/topic_visualization.gif"" width=""60%"" height=""60%"" align=""center"" />
 
-### Visualize Topic Probabilities
 
-The variable `probabilities` that is returned from `transform()` or `fit_transform()` can 
-be used to understand how confident BERTopic is that certain topics can be found in a document. 
+### Embedding Models
+The parameter `embedding_model` takes in a string pointing to a sentence-transformers model, 
+a SentenceTransformer, or a Flair DocumentEmbedding model. 
+
+**Sentence-Transformers**  
+You can select any model from `sentence-transformers` [here](https://www.sbert.net/docs/pretrained_models.html) 
+and pass it through BERTopic with `embedding_model`:
 
-To visualize the distributions, we simply call:
 ```python
-# Make sure to input the probabilities of a single document!
-model.visualize_distribution(probabilities[0])
+from bertopic import BERTopic
+topic_model = BERTopic(embedding_model=""xlm-r-bert-base-nli-stsb-mean-tokens"")
 ```
 
-<img src=""images/probabilities.png"" width=""75%"" height=""75%""/>
+Or select a SentenceTransformer model with your own parameters:
 
-### Embedding Models
-You can select any model from `sentence-transformers` and pass it through 
-BERTopic with `embedding_model`:
+```python
+from bertopic import BERTopic
+from sentence_transformers import SentenceTransformer
+
+sentence_model = SentenceTransformer(""distilbert-base-nli-mean-tokens"", device=""cpu"")
+topic_model = BERTopic(embedding_model=sentence_model)
+```
+
+**Flair**  
+[Flair](https://github.com/flairNLP/flair) allows you to choose almost any embedding model that 
+is publicly available. Flair can be used as follows:
 
 ```python
 from bertopic import BERTopic
-model = BERTopic(embedding_model=""xlm-r-bert-base-nli-stsb-mean-tokens"")
+from flair.embeddings import TransformerDocumentEmbeddings
+
+roberta = TransformerDocumentEmbeddings('roberta-base')
+topic_model = BERTopic(embedding_model=roberta)
 ```
 
+You can select any ðŸ¤— transformers model [here](https://huggingface.co/models).
+
+**Custom Embeddings**    
 You can also use previously generated embeddings by passing it through `fit_transform()`:
 
 ```python
-model = BERTopic()
-topics, probabilities = model.fit_transform(docs, embeddings)
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs, embeddings)
 ```
 
-Click [here](https://www.sbert.net/docs/pretrained_models.html) for a list of supported sentence transformers models.  
-
 ### Overview
 
 | Methods | Code  | 
 |-----------------------|---|
-| Fit the model    |  `model.fit(docs])` |
-| Fit the model and predict documents    |  `model.fit_transform(docs])` |
-| Predict new documents    |  `model.transform([new_doc])` |
-| Access single topic   | `model.get_topic(12)`  |   
-| Access all topics     |  `model.get_topics()` |
-| Get topic freq    |  `model.get_topic_freq()` |
-| Visualize Topics    |  `model.visualize_topics()` |
-| Visualize Topic Probability Distribution    |  `model.visualize_distribution(probabilities[0])` |
-| Update topic representation | `model.update_topics(docs, topics, n_gram_range=(1, 3))` |
-| Reduce nr of topics | `model.reduce_topics(docs, topics, probabilities, nr_topics=30)` |
-| Find topics | `model.find_topics(""vehicle"")` |
-| Save model    |  `model.save(""my_model"")` |
+| Fit the model    |  `topic_model.fit(docs])` |
+| Fit the model and predict documents    |  `topic_model.fit_transform(docs])` |
+| Predict new documents    |  `topic_model.transform([new_doc])` |
+| Access single topic   | `topic_model.get_topic(12)`  |   
+| Access all topics     |  `topic_model.get_topics()` |
+| Get topic freq    |  `topic_model.get_topic_freq()` |
+| Visualize Topics    |  `topic_model.visualize_topics()` |
+| Visualize Topic Probability Distribution    |  `topic_model.visualize_distribution(probabilities[0])` |
+| Update topic representation | `topic_model.update_topics(docs, topics, n_gram_range=(1, 3))` |
+| Reduce nr of topics | `topic_model.reduce_topics(docs, topics, nr_topics=30)` |
+| Find topics | `topic_model.find_topics(""vehicle"")` |
+| Save model    |  `topic_model.save(""my_model"")` |
 | Load model    |  `BERTopic.load(""my_model"")` |
+| Get parameters |  `topic_model.get_params()` |
  
 ### Citation
 To cite BERTopic in your work, please use the following bibtex reference:
@@ -179,7 +171,7 @@ To cite BERTopic in your work, please use the following bibtex reference:
   title        = {BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.},
   year         = 2020,
   publisher    = {Zenodo},
-  version      = {v0.4.2},
+  version      = {v0.5.0},
   doi          = {10.5281/zenodo.4430182},
   url          = {https://doi.org/10.5281/zenodo.4430182}
 }"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -2,7 +2,7 @@
 from bertopic._ctfidf import ClassTFIDF
 from bertopic._embeddings import languages
 
-__version__ = ""0.4.3""
+__version__ = ""0.5.0""
 
 __all__ = [
     ""BERTopic"","
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -3,10 +3,12 @@
 
 import re
 import joblib
+import inspect
 import numpy as np
 import pandas as pd
+from tqdm import tqdm
 from scipy.sparse.csr import csr_matrix
-from typing import List, Tuple, Dict, Union
+from typing import List, Tuple, Union, Mapping, Any
 
 # Models
 import umap
@@ -22,14 +24,23 @@
 from ._embeddings import languages
 from ._mmr import mmr
 
-# Additional dependencies
+# Visualization
 try:
     import matplotlib.pyplot as plt
     import plotly.express as px
     _HAS_VIZ = True
 except ModuleNotFoundError as e:
     _HAS_VIZ = False
 
+# Flair
+try:
+    from flair.embeddings import DocumentEmbeddings, TokenEmbeddings, DocumentPoolEmbeddings
+    from flair.data import Sentence
+    _HAS_FLAIR = True
+except ModuleNotFoundError as e:
+    DocumentEmbeddings, TokenEmbeddings, DocumentPoolEmbeddings = None, None, None
+    _HAS_FLAIR = False
+
 logger = MyLogger(""WARNING"")
 
 
@@ -46,25 +57,21 @@ class BERTopic:
 
     docs = fetch_20newsgroups(subset='all')['data']
 
-    model = BERTopic(""distilbert-base-nli-mean-tokens"", verbose=True)
+    model = BERTopic(verbose=True)
     topics = model.fit_transform(docs)
     ```
 
-    If you want to use your own embeddings, use it as follows:
+    If you want to use your own embedding model, use it as follows:
 
     ```python
     from bertopic import BERTopic
     from sklearn.datasets import fetch_20newsgroups
     from sentence_transformers import SentenceTransformer
 
-    # Create embeddings
     docs = fetch_20newsgroups(subset='all')['data']
     sentence_model = SentenceTransformer(""distilbert-base-nli-mean-tokens"")
-    embeddings = sentence_model.encode(docs, show_progress_bar=True)
-
-    # Create topic model
-    model = BERTopic(verbose=True)
-    topics = model.fit_transform(docs, embeddings)
+    model = BERTopic(verbose=True, embedding_model=sentence_model)
+    topics = model.fit_transform(docs)
     ```
 
     Due to the stochastisch nature of UMAP, the results from BERTopic might differ
@@ -74,96 +81,91 @@ class BERTopic:
     """"""
     def __init__(self,
                  language: str = ""english"",
-                 embedding_model: str = None,
                  top_n_words: int = 10,
-                 nr_topics: Union[int, str] = None,
                  n_gram_range: Tuple[int, int] = (1, 1),
                  min_topic_size: int = 10,
-                 n_neighbors: int = 15,
-                 n_components: int = 5,
-                 stop_words: Union[str, List[str]] = None,
+                 nr_topics: Union[int, str] = None,
+                 low_memory: bool = False,
+                 calculate_probabilities: bool = False,
+                 embedding_model: Union[str,
+                                        SentenceTransformer,
+                                        DocumentEmbeddings,
+                                        TokenEmbeddings] = None,
+                 umap_model: umap.UMAP = None,
+                 hdbscan_model: hdbscan.HDBSCAN = None,
+                 vectorizer_model: CountVectorizer = None,
                  verbose: bool = False,
-                 vectorizer: CountVectorizer = None,
-                 calculate_probabilities: bool = True,
-                 allow_st_model: bool = True):
+                 ):
         """"""BERTopic initialization
 
-        Args:
-            language: The main language used in your documents. For a full overview of supported languages
-                      see bertopic.embeddings.languages. Select ""multilingual"" to load in a model that
-                      support 50+ languages.
-            embedding_model: Model to use. Overview of options can be found here
-                            https://www.sbert.net/docs/pretrained_models.html
+        Arguments:
+            language: The main language used in your documents. For a full overview of
+                      supported languages see bertopic.embeddings.languages. Select
+                      ""multilingual"" to load in a model that support 50+ languages.
             top_n_words: The number of words per topic to extract
-            nr_topics: Specifying the number of topics will reduce the initial
-                       number of topics to the value specified. This reduction can take
-                       a while as each reduction in topics (-1) activates a c-TF-IDF calculation.
-                       IF this is set to None, no reduction is applied. Use ""auto"" to automatically
-                       reduce topics that have a similarity of at least 0.9, do not maps all others.
             n_gram_range: The n-gram range for the CountVectorizer.
                           Advised to keep high values between 1 and 3.
                           More would likely lead to memory issues.
-                          Note that this will not be used if you pass in your own CountVectorizer.
-            min_topic_size: The minimum size of the topic.
-            n_neighbors: The size of local neighborhood (in terms of number of neighboring sample points) used
-                         for manifold approximation (UMAP).
-            n_components: The dimension of the space to embed into when reducing dimensionality with UMAP.
-            stop_words: Stopwords that can be used as either a list of strings, or the name of the
-                        language as a string. For example: 'english' or ['the', 'and', 'I'].
-                        Note that this will not be used if you pass in your own CountVectorizer.
+                          NOTE: This param will not be used if you pass in your own
+                          CountVectorizer.
+            min_topic_size: The minimum size of the topic. Increasing this value will lead
+                            to a lower number of clusters/topics.
+            nr_topics: Specifying the number of topics will reduce the initial
+                       number of topics to the value specified. This reduction can take
+                       a while as each reduction in topics (-1) activates a c-TF-IDF
+                       calculation. If this is set to None, no reduction is applied. Use
+                       ""auto"" to automatically reduce topics that have a similarity of at
+                       least 0.9, do not maps all others.
+            low_memory: Sets UMAP low memory to True to make sure less memory is used.
+            calculate_probabilities: Whether to calculate the topic probabilities. This could
+                                     slow down the extraction of topics if you have many
+                                     documents (> 100_000). Set this only to True if you
+                                     have a low amount of documents or if you do not mind
+                                     more computation time.
+                                     NOTE: since probabilities are not calculated, you cannot
+                                     use the corresponding visualization `visualize_probabilities`.
             verbose: Changes the verbosity of the model, Set to True if you want
                      to track the stages of the model.
-            vectorizer: Pass in your own CountVectorizer from scikit-learn
-            calculate_probabilities: Whether to calculate the topic probabilities. This could slow down
-                                     extraction of topics if you have many documents (>100_000). If so,
-                                     set this to False to increase speed.
-            allow_st_model: This allows BERTopic to use a multi-lingual version of SentenceTransformer
-                            to be used to fine-tune the topic words extracted from the c-TF-IDF representation.
-                            Moreover, it will allow you to search for topics based on search queries.
-
-        Usage:
-
-        ```python
-        from bertopic import BERTopic
-        model = BERTopic(language = ""english"",
-                         embedding_model = None,
-                         top_n_words = 10,
-                         nr_topics = 30,
-                         n_gram_range = (1, 1),
-                         min_topic_size = 10,
-                         n_neighbors = 15,
-                         n_components = 5,
-                         stop_words = None,
-                         verbose = True,
-                         vectorizer = None,
-                         allow_st_model = True)
-        ```
+            embedding_model: Use a custom embedding model. You can pass in a string related
+                             to one of the following models:
+                             https://www.sbert.net/docs/pretrained_models.html
+                             You can also pass in a SentenceTransformer() model or a Flair
+                             DocumentEmbedding model.
+            umap_model: Pass in a umap.UMAP model to be used instead of the default
+            hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default
+            vectorizer_model: Pass in a CountVectorizer instead of the default
         """"""
-
-        # Embedding model
-        self.language = language
-        self.embedding_model = embedding_model
-        self.allow_st_model = allow_st_model
-
         # Topic-based parameters
         if top_n_words > 30:
             raise ValueError(""top_n_words should be lower or equal to 30. The preferred value is 10."")
         self.top_n_words = top_n_words
-        self.nr_topics = nr_topics
         self.min_topic_size = min_topic_size
+        self.nr_topics = nr_topics
+        self.low_memory = low_memory
         self.calculate_probabilities = calculate_probabilities
+        self.verbose = verbose
 
-        # Umap parameters
-        self.n_neighbors = n_neighbors
-        self.n_components = n_components
+        # Embedding model
+        self.language = language if not embedding_model else None
+        self.embedding_model = embedding_model
 
-        # Vectorizer parameters
-        self.stop_words = stop_words
+        # Vectorizer
         self.n_gram_range = n_gram_range
-        self.vectorizer = vectorizer or CountVectorizer(ngram_range=self.n_gram_range, stop_words=self.stop_words)
+        self.vectorizer_model = vectorizer_model or CountVectorizer(ngram_range=self.n_gram_range)
+
+        # UMAP
+        self.umap_model = umap_model or umap.UMAP(n_neighbors=15,
+                                                  n_components=5,
+                                                  min_dist=0.0,
+                                                  metric='cosine',
+                                                  low_memory=self.low_memory)
+
+        # HDBSCAN
+        self.hdbscan_model = hdbscan_model or hdbscan.HDBSCAN(min_cluster_size=self.min_topic_size,
+                                                              metric='euclidean',
+                                                              cluster_selection_method='eom',
+                                                              prediction_data=True)
 
-        self.umap_model = None
-        self.cluster_model = None
         self.topics = None
         self.topic_sizes = None
         self.reduced_topics_mapped = None
@@ -227,7 +229,10 @@ def fit_transform(self,
 
         Returns:
             predictions: Topic predictions for each documents
-            probabilities: The topic probability distribution
+            probabilities: The topic probability distribution which is returned by default.
+                           If `low_memory` in BERTopic is set to False, then the
+                           probabilities are not calculated to speed up computation and
+                           decrease memory usage.
 
         Usage:
 
@@ -267,7 +272,9 @@ def fit_transform(self,
 
         # Extract embeddings
         if not any([isinstance(embeddings, np.ndarray), isinstance(embeddings, csr_matrix)]):
-            embeddings = self._extract_embeddings(documents.Document)
+            self.embedding_model = self._select_embedding_model()
+            embeddings = self._extract_embeddings(documents.Document, verbose=self.verbose)
+            logger.info(""Transformed documents to Embeddings"")
         else:
             self.custom_embeddings = True
 
@@ -300,7 +307,10 @@ def transform(self,
 
         Returns:
             predictions: Topic predictions for each documents
-            probabilities: The topic probability distribution
+            probabilities: The topic probability distribution which is returned by default.
+                           If `low_memory` in BERTopic is set to False, then the
+                           probabilities are not calculated to speed up computation and
+                           decrease memory usage.
 
         Usage:
 
@@ -337,13 +347,14 @@ def transform(self,
             documents = [documents]
 
         if not isinstance(embeddings, np.ndarray):
-            embeddings = self._extract_embeddings(documents)
+            self.embedding_model = self._select_embedding_model()
+            embeddings = self._extract_embeddings(documents, verbose=self.verbose)
 
         umap_embeddings = self.umap_model.transform(embeddings)
-        predictions, _ = hdbscan.approximate_predict(self.cluster_model, umap_embeddings)
+        predictions, _ = hdbscan.approximate_predict(self.hdbscan_model, umap_embeddings)
 
         if self.calculate_probabilities:
-            probabilities = hdbscan.membership_vector(self.cluster_model, umap_embeddings)
+            probabilities = hdbscan.membership_vector(self.hdbscan_model, umap_embeddings)
             if len(documents) == 1:
                 probabilities = probabilities.flatten()
         else:
@@ -368,7 +379,7 @@ def find_topics(self,
         with the topic representation it is advised to keep it
         below 5 words.
 
-        Args:
+        Arguments:
             search_term: the term you want to use to search for topics
             top_n: the number of topics to return
 
@@ -377,15 +388,14 @@ def find_topics(self,
             similarity: the similarity scores from high to low
 
         """"""
-        if self.custom_embeddings and not self.allow_st_model:
-            raise Exception(""This method can only be used if you set `allow_st_model` to True when ""
-                            ""using custom embeddings."")
+        if self.custom_embeddings:
+            raise Exception(""This method can only be used if you did not use custom embeddings."")
 
         topic_list = list(self.topics.keys())
         topic_list.sort()
 
         # Extract search_term embeddings and compare with topic embeddings
-        search_embedding = self._extract_embeddings([search_term]).flatten()
+        search_embedding = self._extract_embeddings([search_term], verbose=False).flatten()
         sims = cosine_similarity(search_embedding.reshape(1, -1), self.topic_embeddings).flatten()
 
         # Extract topics most similar to search_term
@@ -399,8 +409,7 @@ def update_topics(self,
                       docs: List[str],
                       topics: List[int],
                       n_gram_range: Tuple[int, int] = None,
-                      stop_words: str = None,
-                      vectorizer: CountVectorizer = None):
+                      vectorizer_model: CountVectorizer = None):
         """""" Updates the topic representation by recalculating c-TF-IDF with the new
         parameters as defined in this function.
 
@@ -409,14 +418,11 @@ def update_topics(self,
         stop_words or you want to try out a different n_gram_range. This function allows you
         to update the topic representation after they have been formed.
 
-        Args:
-            docs: The docs you used when calling either `fit` or `fit_transform`
+        Arguments:
+            docs: The documents you used when calling either `fit` or `fit_transform`
             topics: The topics that were returned when calling either `fit` or `fit_transform`
             n_gram_range: The n-gram range for the CountVectorizer.
-            stop_words: Stopwords that can be used as either a list of strings, or the name of the
-                        language as a string. For example: 'english' or ['the', 'and', 'I'].
-                        Note that this will not be used if you pass in your own CountVectorizer.
-            vectorizer: Pass in your own CountVectorizer from scikit-learn
+            vectorizer_model: Pass in your own CountVectorizer from scikit-learn
 
         Usage:
         ```python
@@ -425,25 +431,23 @@ def update_topics(self,
 
         # Create topics
         docs = fetch_20newsgroups(subset='train')['data']
-        model = BERTopic(n_gram_range=(1, 1), stop_words=None)
+        model = BERTopic(n_gram_range=(1, 1))
         topics, probs = model.fit_transform(docs)
 
         # Update topic representation
-        model.update_topics(docs, topics, n_gram_range=(2, 3), stop_words=""english"")
+        model.update_topics(docs, topics, n_gram_range=(2, 3))
         ```
         """"""
         check_is_fitted(self)
         if not n_gram_range:
             n_gram_range = self.n_gram_range
 
-        if not stop_words:
-            stop_words = self.stop_words
+        self.vectorizer_model = vectorizer_model or CountVectorizer(ngram_range=n_gram_range)
 
-        self.vectorizer = vectorizer or CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words)
         documents = pd.DataFrame({""Document"": docs, ""Topic"": topics})
         self._extract_topics(documents)
 
-    def get_topics(self) -> Dict[str, Tuple[str, float]]:
+    def get_topics(self) -> Mapping[str, Tuple[str, float]]:
         """""" Return topics with top n words and their c-TF-IDF score
 
         Usage:
@@ -455,7 +459,7 @@ def get_topics(self) -> Dict[str, Tuple[str, float]]:
         check_is_fitted(self)
         return self.topics
 
-    def get_topic(self, topic: int) -> Union[Dict[str, Tuple[str, float]], bool]:
+    def get_topic(self, topic: int) -> Union[Mapping[str, Tuple[str, float]], bool]:
         """""" Return top n words for a specific topic and their c-TF-IDF scores
 
         Usage:
@@ -514,8 +518,8 @@ def reduce_topics(self,
         Arguments:
             docs: The docs you used when calling either `fit` or `fit_transform`
             topics: The topics that were returned when calling either `fit` or `fit_transform`
-            nr_topics: The number of topics you want reduced to
             probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`
+            nr_topics: The number of topics you want reduced to
 
         Returns:
             new_topics: Updated topics
@@ -571,7 +575,7 @@ def visualize_topics(self):
         # Visualize with plotly
         df = pd.DataFrame({""x"": embeddings[1:, 0], ""y"": embeddings[1:, 1],
                            ""Topic"": topic_list[1:], ""Words"": words[1:], ""Size"": frequencies[1:]})
-        self._plotly_topic_visualization(df, topic_list)
+        return self._plotly_topic_visualization(df, topic_list)
 
     def visualize_distribution(self,
                                probabilities: np.ndarray,
@@ -626,7 +630,6 @@ def visualize_distribution(self,
                             "" "".join(label))
                 labels.append(label)
             else:
-                print(idx, probabilities[idx])
                 vals.remove(probabilities[idx])
         pos = range(len(vals))
 
@@ -655,56 +658,131 @@ def visualize_distribution(self,
         if save:
             fig.savefig(""probability.png"", dpi=300, bbox_inches='tight')
 
-    def save(self, path: str) -> None:
+    def save(self,
+             path: str,
+             save_embedding_model: bool = True) -> None:
         """""" Saves the model to the specified path
 
         Arguments:
             path: the location and name of the file you want to save
+            save_embedding_model: Whether to save the embedding model in this class
+                                  as you might have selected a local model or one that
+                                  is downloaded automatically from the cloud.
 
         Usage:
 
         ```python
         model.save(""my_model"")
         ```
+
+        or if you do not want the embedding_model to be saved locally:
+
+        ```python
+        model.save(""my_model"", save_embedding_model=False)
+        ```
         """"""
         with open(path, 'wb') as file:
-            joblib.dump(self, file)
+            if not save_embedding_model:
+                embedding_model = self.embedding_model
+                self.embedding_model = None
+                joblib.dump(self, file)
+                self.embedding_model = embedding_model
+            else:
+                joblib.dump(self, file)
 
     @classmethod
-    def load(cls, path: str):
+    def load(cls,
+             path: str,
+             embedding_model: Union[str, SentenceTransformer, DocumentEmbeddings, TokenEmbeddings] = None):
         """""" Loads the model from the specified path
 
         Arguments:
             path: the location and name of the BERTopic file you want to load
+            embedding_model: If the embedding_model was not saved to save space or to load
+                             it in from the cloud, you can load it in by specifying it here.
 
         Usage:
 
         ```python
         BERTopic.load(""my_model"")
         ```
+
+        or if you did not save the embedding model:
+
+        ```python
+        BERTopic.load(""my_model"", embedding_model=""xlm-r-bert-base-nli-stsb-mean-tokens"")
+        ```
         """"""
         with open(path, 'rb') as file:
-            return joblib.load(file)
+            if embedding_model:
+                topic_model = joblib.load(file)
+                topic_model.embedding_model = embedding_model
+            else:
+                topic_model = joblib.load(file)
+            return topic_model
+
+    def get_params(self, deep: bool = False) -> Mapping[str, Any]:
+        """""" Get parameters for this estimator.
+
+        Adapted from:
+            https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178
 
-    def _extract_embeddings(self, documents: List[str]) -> np.ndarray:
+        Arguments:
+            deep: bool, default=True
+                  If True, will return the parameters for this estimator and
+                  contained subobjects that are estimators.
+
+        Returns:
+            out: Parameter names mapped to their values.
+        """"""
+        out = dict()
+        for key in self._get_param_names():
+            value = getattr(self, key)
+            if deep and hasattr(value, 'get_params'):
+                deep_items = value.get_params().items()
+                out.update((key + '__' + k, val) for k, val in deep_items)
+            out[key] = value
+        return out
+
+    def _extract_embeddings(self, documents: Union[List[str], str], verbose: bool = None) -> np.ndarray:
         """""" Extract sentence/document embeddings through pre-trained embeddings
         For an overview of pre-trained models: https://www.sbert.net/docs/pretrained_models.html
 
         Arguments:
             documents: Dataframe with documents and their corresponding IDs
+            verbose: Whether to show a progressbar demonstrating the time to extract embeddings
 
         Returns:
             embeddings: The extracted embeddings using the sentence transformer
                         module. Typically uses pre-trained huggingface models.
         """"""
-        model = self._select_embedding_model()
-        logger.info(""Loaded embedding model"")
-        embeddings = model.encode(documents, show_progress_bar=False)
-        logger.info(""Transformed documents to Embeddings"")
+        if isinstance(documents, str):
+            documents = [documents]
+
+        # Infer embeddings with SentenceTransformer
+        if isinstance(self.embedding_model, SentenceTransformer):
+            embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)
+
+        # Infer embeddings with Flair
+        elif isinstance(self.embedding_model, DocumentEmbeddings):
+            embeddings = []
+            for index, document in tqdm(enumerate(documents), disable=not verbose):
+                try:
+                    sentence = Sentence(document) if document else Sentence(""an empty document"")
+                    self.embedding_model.embed(sentence)
+                except RuntimeError:
+                    sentence = Sentence(""an empty document"")
+                    self.embedding_model.embed(sentence)
+                embedding = sentence.embedding.detach().cpu().numpy()
+                embeddings.append(embedding)
+            embeddings = np.asarray(embeddings)
+
+        else:
+            raise ValueError(""An incorrect embedding model type was selected."")
 
         return embeddings
 
-    def _map_predictions(self, predictions):
+    def _map_predictions(self, predictions: List[int]) -> List[int]:
         """""" Map predictions to the correct topics if topics were reduced """"""
         mapped_predictions = []
         for prediction in predictions:
@@ -723,17 +801,15 @@ def _reduce_dimensionality(self, embeddings: Union[np.ndarray, csr_matrix]) -> n
             umap_embeddings: The reduced embeddings
         """"""
         if isinstance(embeddings, csr_matrix):
-            self.umap_model = umap.UMAP(n_neighbors=self.n_neighbors,
-                                        n_components=self.n_components,
-                                        metric='hellinger').fit(embeddings)
+            self.umap_model = umap.UMAP(n_neighbors=15,
+                                        n_components=5,
+                                        metric='hellinger',
+                                        low_memory=self.low_memory).fit(embeddings)
         else:
-            self.umap_model = umap.UMAP(n_neighbors=self.n_neighbors,
-                                        n_components=self.n_components,
-                                        min_dist=0.0,
-                                        metric='cosine').fit(embeddings)
+            self.umap_model.fit(embeddings)
         umap_embeddings = self.umap_model.transform(embeddings)
         logger.info(""Reduced dimensionality with UMAP"")
-        return umap_embeddings
+        return np.nan_to_num(umap_embeddings)
 
     def _cluster_embeddings(self,
                             umap_embeddings: np.ndarray,
@@ -750,14 +826,11 @@ def _cluster_embeddings(self,
                        and newly added Topics
             probabilities: The distribution of probabilities
         """"""
-        self.cluster_model = hdbscan.HDBSCAN(min_cluster_size=self.min_topic_size,
-                                             metric='euclidean',
-                                             cluster_selection_method='eom',
-                                             prediction_data=True).fit(umap_embeddings)
-        documents['Topic'] = self.cluster_model.labels_
+        self.hdbscan_model.fit(umap_embeddings)
+        documents['Topic'] = self.hdbscan_model.labels_
 
         if self.calculate_probabilities:
-            probabilities = hdbscan.all_points_membership_vectors(self.cluster_model)
+            probabilities = hdbscan.all_points_membership_vectors(self.hdbscan_model)
         else:
             probabilities = None
 
@@ -791,15 +864,15 @@ def _create_topic_vectors(self):
         a sentence-transformer model to be used or there are custom embeddings but it is allowed
         to use a different multi-lingual sentence-transformer model
         """"""
-        if not self.custom_embeddings or all([self.custom_embeddings and self.allow_st_model]):
+        if not self.custom_embeddings:
             topic_list = list(self.topics.keys())
             topic_list.sort()
             n = self.top_n_words
 
             # Extract embeddings for all words in all topics
             topic_words = [self.get_topic(topic) for topic in topic_list]
             topic_words = [word[0] for topic in topic_words for word in topic]
-            embeddings = self._extract_embeddings(topic_words)
+            embeddings = self._extract_embeddings(topic_words, verbose=False)
 
             # Take the weighted average of word embeddings in a topic based on their c-TF-IDF value
             # The embeddings var is a single numpy matrix and therefore slicing is necessary to
@@ -827,7 +900,7 @@ def _c_tf_idf(self, documents_per_topic: pd.DataFrame, m: int) -> Tuple[csr_matr
             words: The names of the words to which values were given
         """"""
         documents = self._preprocess_text(documents_per_topic.Document.values)
-        count = self.vectorizer.fit(documents)
+        count = self.vectorizer_model.fit(documents)
         words = count.get_feature_names()
         X = count.transform(documents)
         transformer = ClassTFIDF().fit(X, n_samples=m)
@@ -836,7 +909,7 @@ def _c_tf_idf(self, documents_per_topic: pd.DataFrame, m: int) -> Tuple[csr_matr
 
         return c_tf_idf, words
 
-    def _update_topic_size(self, documents: pd.DataFrame) -> None:
+    def _update_topic_size(self, documents: pd.DataFrame):
         """""" Calculate the topic sizes
 
         Arguments:
@@ -849,7 +922,7 @@ def _extract_words_per_topic(self, words: List[str]):
         """""" Based on tf_idf scores per topic, extract the top n words per topic
 
         Arguments:
-        words: List of all words (sorted according to tf_idf matrix position)
+            words: List of all words (sorted according to tf_idf matrix position)
         """"""
 
         # Get top 30 words per topic based on c-TF-IDF score
@@ -862,30 +935,43 @@ def _extract_words_per_topic(self, words: List[str]):
 
         # Extract word embeddings for the top 30 words per topic and compare it
         # with the topic embedding to keep only the words most similar to the topic embedding
-        if not self.custom_embeddings or all([self.custom_embeddings and self.allow_st_model]):
-            model = self._select_embedding_model()
+        if not self.custom_embeddings:
 
             for topic, topic_words in self.topics.items():
                 words = [word[0] for word in topic_words]
-                word_embeddings = model.encode(words)
-                topic_embedding = model.encode("" "".join(words)).reshape(1, -1)
+                word_embeddings = self._extract_embeddings(words, verbose=False)
+                topic_embedding = self._extract_embeddings("" "".join(words), verbose=False).reshape(1, -1)
+
                 topic_words = mmr(topic_embedding, word_embeddings, words, top_n=self.top_n_words, diversity=0)
                 self.topics[topic] = [(word, value) for word, value in self.topics[topic] if word in topic_words]
 
-    def _select_embedding_model(self) -> SentenceTransformer:
+    def _select_embedding_model(self) -> Union[SentenceTransformer, DocumentEmbeddings]:
         """""" Select an embedding model based on language or a specific sentence transformer models.
         When selecting a language, we choose distilbert-base-nli-stsb-mean-tokens for English and
         xlm-r-bert-base-nli-stsb-mean-tokens for all other languages as it support 100+ languages.
+
+        Returns:
+            model: Either a Sentence-Transformer or Flair model
         """"""
 
-        # Used for fine-tuning the topic representation
-        # If a custom embeddings are used, we use the multi-langual model
-        # to extract word embeddings
-        if self.custom_embeddings and self.allow_st_model:
-            return SentenceTransformer(""xlm-r-bert-base-nli-stsb-mean-tokens"")
+        # Sentence Transformer embeddings
+        if isinstance(self.embedding_model, SentenceTransformer):
+            return self.embedding_model
+
+        # Flair word embeddings
+        elif _HAS_FLAIR and isinstance(self.embedding_model, TokenEmbeddings):
+            return DocumentPoolEmbeddings([self.embedding_model])
+
+        # Flair document embeddings + disable fine tune to prevent CUDA OOM
+        # https://github.com/flairNLP/flair/issues/1719
+        elif _HAS_FLAIR and isinstance(self.embedding_model, DocumentEmbeddings):
+            if ""fine_tune"" in self.embedding_model.__dict__:
+                self.embedding_model.fine_tune = False
+            return self.embedding_model
 
         # Select embedding model based on specific sentence transformer model
-        elif self.embedding_model:
+        elif isinstance(self.embedding_model, str):
+            self.sentence_pointer = self.embedding_model
             return SentenceTransformer(self.embedding_model)
 
         # Select embedding model based on language
@@ -905,6 +991,9 @@ def _select_embedding_model(self) -> SentenceTransformer:
                                  ""Else, please select a language from the following list:\n""
                                  f""{languages}"")
 
+        elif self.custom_embeddings:
+            return None
+
         return SentenceTransformer(""xlm-r-bert-base-nli-stsb-mean-tokens"")
 
     def _reduce_topics(self, documents: pd.DataFrame) -> pd.DataFrame:
@@ -925,7 +1014,7 @@ def _reduce_topics(self, documents: pd.DataFrame) -> pd.DataFrame:
 
         return documents
 
-    def _reduce_to_n_topics(self, documents):
+    def _reduce_to_n_topics(self, documents: pd.DataFrame) -> pd.DataFrame:
         """""" Reduce topics to self.nr_topics
 
         Arguments:
@@ -964,8 +1053,8 @@ def _reduce_to_n_topics(self, documents):
 
         return documents
 
-    def _auto_reduce_topics(self, documents):
-        """""" Reduce the number of topics as long as it exceeds a minimum similarity of 0.9
+    def _auto_reduce_topics(self, documents: pd.DataFrame) -> pd.DataFrame:
+        """""" Reduce the number of topics as long as it exceeds a minimum similarity of 0.915
 
         Arguments:
             documents: Dataframe with documents and their corresponding IDs and Topics
@@ -1002,7 +1091,7 @@ def _auto_reduce_topics(self, documents):
                 self._update_topic_size(documents)
                 has_mapped.append(topic_to_merge)
 
-        _ = self._extract_topics(documents)
+        self._extract_topics(documents)
 
         logger.info(f""Reduced number of topics from {initial_nr_topics} to {len(self.get_topic_freq())}"")
 
@@ -1094,13 +1183,13 @@ def get_color(topic_selected):
                       x0=sum(x_range) / 2, y0=y_range[0], x1=sum(x_range) / 2, y1=y_range[1],
                       line=dict(color=""#CFD8DC"", width=2))
         fig.add_shape(type=""line"",
-                      x0=x_range[0], y0=sum(y_range) / 2, x1=y_range[1], y1=sum(y_range) / 2,
+                      x0=x_range[0], y0=sum(y_range) / 2, x1=x_range[1], y1=sum(y_range) / 2,
                       line=dict(color=""#9E9E9E"", width=2))
         fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=""D1"", showarrow=False, yshift=10)
         fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=""D2"", showarrow=False, xshift=10)
         fig.data = fig.data[::-1]
 
-        fig.show()
+        return fig
 
     def _preprocess_text(self, documents: np.ndarray) -> List[str]:
         """""" Basic preprocessing of text
@@ -1117,3 +1206,15 @@ def _preprocess_text(self, documents: np.ndarray) -> List[str]:
             cleaned_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned_documents]
         cleaned_documents = [doc if doc != """" else ""emptydoc"" for doc in cleaned_documents]
         return cleaned_documents
+
+    @classmethod
+    def _get_param_names(cls):
+        """"""Get parameter names for the estimator
+
+        Adapted from:
+            https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178
+        """"""
+        init_signature = inspect.signature(cls.__init__)
+        parameters = sorted([p.name for p in init_signature.parameters.values()
+                             if p.name != 'self' and p.kind != p.VAR_KEYWORD])
+        return parameters"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,3 @@
+# `Maximal Marginal Relevance`
+
+::: bertopic._mmr.mmr"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -1,3 +1,21 @@
+## **Version 0.5.0**
+*Release date:  8 Februari, 2021*
+
+**Highlights**:
+  
+* Add `Flair` to allow for more (custom) token/document embeddings, including ðŸ¤— transformers 
+* Option to use custom UMAP, HDBSCAN, and CountVectorizer
+* Added `low_memory` parameter to reduce memory during computation
+* Improved verbosity (shows progress bar)
+* Return the figure of `visualize_topics()`
+* Expose all parameters with a single function: `get_params()`
+
+**Fixes**:
+    
+* To simplify the API, the parameters stop_words and n_neighbors were removed. These can still be used when a custom UMAP or CountVectorizer is used.
+* Set `calculate_probabilities` to False as a default. Calculating probabilities with HDBSCAN significantly increases computation time and memory usage. Better to remove calculating probabilities or only allow it by manually turning this on.
+* Use the newest version of `sentence-transformers` as it speeds ups encoding significantly
+
 ## **Version 0.4.2**
 *Release date:  10 Januari, 2021*
 "
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,90 @@
+## **Why are the results not consistent between runs?**
+Due to the stochastisch nature of UMAP, the results from BERTopic might differ even if you run the same code
+multiple times. Using your own embeddings allows you to try out BERTopic several times until you find the 
+topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times
+with different parameters. 
+
+## **Which embedding model works best for which language?**
+Unfortunately, there is not a definitive list on the best models for each language, this highly depends 
+on your data, the model, and your specific use-case. However, the default model in BERTopic 
+(`""distilbert-base-nli-stsb-mean-tokens""`) works great for **English** documents. In contrast, for **multi-lingual** 
+documents or any other language, `""xlm-r-bert-base-nli-stsb-mean-tokens""""` has shown great performance.  
+
+**SentenceTransformers**  
+[SentenceTransformers](https://www.sbert.net/docs/pretrained_models.html) work typically quite well 
+and are the preferred models to use. They are great in generating document embeddings and have several 
+multi-lingual versions available.  
+
+**ðŸ¤— transformers**  
+BERTopic allows you to use any ðŸ¤— transformers model. These models  are typically embeddings created on 
+a word/sentence level but can easily be pooled using Flair (see Guides/Embeddings). If you have a 
+specific language for which you want to generate embeddings, you can choose the model [here](https://huggingface.co/models).
+
+## **How can I speed up BERTopic?**
+You can speed up BERTopic by either generating your embeddings beforehand, which is not advised, or by 
+setting `calculate_probabilities` to False. Calculating the probabilities is quite expensive and can 
+significantly increase the computation time. Thus, only use it if you do not mind waiting a bit before 
+the model is done running or if you have less than 50_000 documents. 
+
+## **I am facing memory issues. Help!**
+To prevent any memory issues, it is advised to set `low_memory` to True. This will result in UMAP being 
+a bit slower, but consuming significantly less memory. Moreover, calculating the probabilities of topics 
+is quite computationally consuming and might impact memory. Setting `calculate_probabilities` to False 
+could similarly help. 
+
+If the problem still persists, then this could be an issue related to your available memory. Processing 
+millions of documents is quite computationally expensive and sufficient RAM is necessary.  
+
+## **I have only a few topics, how do I increase them?**
+There are several reasons why your topic model results in only a few topics. 
+
+First, you might only have a few documents (~1000). This makes it very difficult to properly 
+extract topics due to the little amount of data available. Increasing the number of documents 
+might solve your issues. 
+
+Second, `min_topic_size` might be simply too large for your number of documents. If you decrease 
+the minimum size of topics, then you are much more likely to increase the number of topics generated.
+You could also decrease the `n_neighbors` parameter used in `UMAP` if this does not work. 
+
+Third, although this does not happen very often, there simply aren't that many topics to be found 
+in your documents. You can often see this when you have many `-1` topics, which is actually not a topic 
+but a category of outliers.  
+
+## **Why are the probabilities not calculated?**
+Although it is possible to calculate the probabilities, the process of doing so is quite computationally 
+inefficient and might significantly increase the computation time. To prevent this, the probabilities are 
+not calculated as a default. In order to calculate, you will have to set `calculate_probabilities` to True:
+
+```python
+from bertopic import BERTopic
+topic_model = BERTopic(calculate_probabilities=True)
+topics, probs = topic_model.fit_transform(docs) 
+```  
+
+## **Numpy gives me an error when running BERTopic**
+With the release of Numpy 1.20.0, there have been significant issues with using that version (and previous) due 
+to compilation issues and pypi.   
+  
+This is a known issue with the order of install using pypi. You can find more details about this issue 
+[here](https://github.com/lmcinnes/umap/issues/567) and [here](https://github.com/scikit-learn-contrib/hdbscan/issues/457).
+
+I would suggest doing one of the following:
+
+* Install the newest version from BERTopic (>= v0.5).
+* You can install hdbscan with pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation which might resolve the issue
+* Use the above step also with numpy as it is part of the issue
+* Install BERTopic in a fresh environment using these steps. 
+
+
+## **Can I use the GPU to speed up the model?**
+Yes and no. The GPU is automatically used when you use a SentenceTransformer or Flair embedding model. Using a CPU 
+would then definitely slow things down. However, UMAP and HDBSCAN are not GPU-accelerated and are likely not so in 
+the near future. For now, a GPU does help tremendously for extracting embeddings but does not speed up all 
+aspects of BERtopic.   
+
+## **Should I preprocess the data?**
+No. By using document embeddings there is typically no need to preprocess the data as all parts of a document 
+are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you 
+have data that contains a lot of noise, for example HTML-tags, then it would be best to remove them. HTML-tags 
+typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply 
+topic modeling to HTML-code to extract topics of code, then it becomes important. 
\ No newline at end of file"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -6,7 +6,8 @@ BERTopic is a topic modeling technique that leverages ðŸ¤— transformers and c-TF
 allowing for easily interpretable topics whilst keeping important words in the topic descriptions. It even supports 
 visualizations similar to LDAvis! 
 
-Corresponding medium post can be found [here](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6?source=friends_link&sk=0b5a470c006d1842ad4c8a3057063a99).
+Corresponding medium post can be found [here](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6?source=friends_link&sk=0b5a470c006d1842ad4c8a3057063a99) 
+and [here](https://towardsdatascience.com/interactive-topic-modeling-with-bertopic-1ea55e7d73d8?sk=03c2168e9e74b6bda2a1f3ed953427e4).
 
 ###  **Installation**
 
@@ -22,6 +23,17 @@ To use the visualization options, install BERTopic as follows:
 pip install bertopic[visualization]
 ```
 
+To use Flair embeddings, install BERTopic as follows:
+```bash
+pip install bertopic[flair]
+```
+
+Finally, to install all versions:
+```bash
+pip install bertopic[all]
+```
+
+
 ###  **Usage**
 
 Below is an example of how to use the model. The example uses the 
@@ -35,14 +47,14 @@ from sklearn.datasets import fetch_20newsgroups
  
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
 
-model = BERTopic()
-topics, probabilities = model.fit_transform(docs)
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs)
 ```
 
 After generating topics and their probabilities, we can access the frequent topics that were generated:
 
 ```python
->>> model.get_topic_freq().head()
+>>> topic_model.get_topic_freq().head()
 Topic	Count
 -1	7288
 49	3992
@@ -55,7 +67,7 @@ Topic	Count
 frequent topic that was generated, `topic 49`:
 
 ```python
->>> model.get_topic(49)
+>>> topic_model.get_topic(49)
 [('windows', 0.006152228076250982),
  ('drive', 0.004982897610645755),
  ('dos', 0.004845038866360651),
@@ -68,46 +80,38 @@ frequent topic that was generated, `topic 49`:
  ('pc', 0.003047105930670237)]
 ```  
 
-<details>
-<summary>Supported Languages</summary>
-
-<br>
-Use <b>""multilingual""</b> to select a model that supports 50+ languages. 
-<br><br>
-Moreover, the following <b>languages</b> are supported: <br>
-Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese,
-Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian,
-Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified),
-Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto,
-Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek,
-Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian,
-Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean,
-Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian,
-Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian,
-Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian,
-Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak,
-Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil,
-Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian,
-Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian,
-Xhosa, Yiddish
-<br>
-</details>  
-
+**NOTE**: Use `BERTopic(language=""multilingual"")` to select a model that supports 50+ languages.
 
 ### Overview
 
 | Methods | Code  | 
 |-----------------------|---|
-| Fit the model    |  `model.fit(docs])` |
-| Fit the model and predict documents    |  `model.fit_transform(docs])` |
-| Predict new documents    |  `model.transform([new_doc])` |
-| Access single topic   | `model.get_topic(12)`  |   
-| Access all topics     |  `model.get_topics()` |
-| Get topic freq    |  `model.get_topic_freq()` |
-| Visualize Topics    |  `model.visualize_topics()` |
-| Visualize Topic Probability Distribution    |  `model.visualize_distribution(probabilities[0])` |
-| Update topic representation | `model.update_topics(docs, topics, n_gram_range=(1, 3))` |
-| Reduce nr of topics | `model.reduce_topics(docs, topics, probabilities, nr_topics=30)` |
-| Find topics | `model.find_topics(""vehicle"")` |
-| Save model    |  `model.save(""my_model"")` |
+| Fit the model    |  `topic_model.fit(docs])` |
+| Fit the model and predict documents    |  `topic_model.fit_transform(docs])` |
+| Predict new documents    |  `topic_model.transform([new_doc])` |
+| Access single topic   | `topic_model.get_topic(12)`  |   
+| Access all topics     |  `topic_model.get_topics()` |
+| Get topic freq    |  `topic_model.get_topic_freq()` |
+| Visualize Topics    |  `topic_model.visualize_topics()` |
+| Visualize Topic Probability Distribution    |  `topic_model.visualize_distribution(probabilities[0])` |
+| Update topic representation | `topic_model.update_topics(docs, topics, n_gram_range=(1, 3))` |
+| Reduce nr of topics | `topic_model.reduce_topics(docs, topics, nr_topics=30)` |
+| Find topics | `topic_model.find_topics(""vehicle"")` |
+| Save model    |  `topic_model.save(""my_model"")` |
 | Load model    |  `BERTopic.load(""my_model"")` |
+| Get parameters |  `topic_model.get_params()` |
+ 
+### Citation
+To cite BERTopic in your work, please use the following bibtex reference:
+
+```bibtex
+@misc{grootendorst2020bertopic,
+  author       = {Maarten Grootendorst},
+  title        = {BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.},
+  year         = 2020,
+  publisher    = {Zenodo},
+  version      = {v0.5.0},
+  doi          = {10.5281/zenodo.4430182},
+  url          = {https://doi.org/10.5281/zenodo.4430182}
+}
+```
\ No newline at end of file"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -1,9 +1,10 @@
 
-The algorithm contains, roughly, 3 stages:   
+The algorithm contains, roughly, 4 stages:   
    
 - Extract document embeddings with **`Sentence Transformers`** or **`TF-IDF`**  
 - Cluster document embeddings to create groups of similar documents with **`UMAP`** and **`HDBSCAN`**  
-- Extract and reduce topics with **`c-TF-IDF`**  
+- Extract and reduce topics with **`c-TF-IDF`**
+- Improve coherence of words with Maximal Marginal Relevance  
 
 ##  **Create Document Embeddings**
 We start by creating document embeddings from a set of documents using "
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -1,4 +1,51 @@
-## **Transformer Models**
+## **Sentence Transformers**
+You can select any model from sentence-transformers [here](https://www.sbert.net/docs/pretrained_models.html) 
+and pass it through BERTopic with `embedding_model`:
+
+```python
+from bertopic import BERTopic
+topic_model = BERTopic(embedding_model=""xlm-r-bert-base-nli-stsb-mean-tokens"")
+```
+
+Or select a SentenceTransformer model with your own parameters:
+
+```python
+from bertopic import BERTopic
+from sentence_transformers import SentenceTransformer
+
+sentence_model = SentenceTransformer(""distilbert-base-nli-mean-tokens"", device=""cuda"")
+topic_model = BERTopic(embedding_model=sentence_model)
+```
+
+## **Flair**
+[Flair](https://github.com/flairNLP/flair) allows you to choose almost any embedding model that 
+is publicly available. Flair can be used as follows:
+
+```python
+from bertopic import BERTopic
+from flair.embeddings import TransformerDocumentEmbeddings
+
+roberta = TransformerDocumentEmbeddings('roberta-base')
+topic_model = BERTopic(embedding_model=roberta)
+```
+
+You can select any ðŸ¤— transformers model [here](https://huggingface.co/models).
+
+Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. 
+Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily 
+pass it to BERTopic in order to use those word embeddings as document embeddings: 
+
+```python
+from bertopic import BERTopic
+from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings
+
+glove_embedding = WordEmbeddings('crawl')
+document_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])
+
+topic_model = BERTopic(embedding_model=document_glove_embeddings)
+```
+
+## **Custom Embeddings**
 The base models in BERTopic are both BERT-based models that work well with document similarity tasks. You documents, 
 however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding 
 model in BERTopic in order to create document features.   
@@ -15,19 +62,13 @@ sentence_model = SentenceTransformer(""distilbert-base-nli-mean-tokens"")
 embeddings = sentence_model.encode(docs, show_progress_bar=False)
 
 # Create topic model
-model = BERTopic()
-topics, probabilities = model.fit_transform(docs, embeddings)
+topic_model = BERTopic()
+topics, probabilities = topic_model.fit_transform(docs, embeddings)
 ```
 
 As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used 
 `ðŸ¤— transformers`, `Doc2Vec`, or any other embedding method. 
 
-Due to the stochastisch nature of UMAP, the results from BERTopic might differ even if you run the same code
-multiple times. Using your own embeddings allows you to try out BERTopic several times until you find the 
-topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times
-with different parameters. 
-
-
 ## **TF-IDF**
 As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is 
 `cosine` which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix "
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,28 @@
+## **Custom Models**
+There are three models underpinning BERTopic that are most important in creating the topics, 
+namely UMAP, HDBSCAN, and CountVectorizer. The parameters of these models have been carefully 
+selected to give the best results. However, there is no one-size-fits-all solution using these 
+default parameters.
+
+Therefore, BERTopic allows you to pass in any custom UMAP, HDBSCAN, and/or CountVectorizer 
+with the parameters that best suit your use-case. For example, you might want to change the 
+minimum document frequency in CountVectorizer or use a different distance metric in HDBSCAN or UMAP. 
+
+To do this, simply create the instances of these models and initialize BERTopic with them:
+
+```python
+from bertopic import BERTopic
+from umap import UMAP
+from hdbscan import HDBSCAN
+from sklearn.feature_extraction.text import CountVectorizer
+
+# Prepare custom models
+hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)
+umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine')
+vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=""english"")
+
+# Pass the custom models to BERTopic
+topic_model = BERTopic(umap_model=umap_model, 
+                       hdbscan_model=hdbscan_model, 
+                       vectorizer_model=vectorizer_model)
+```"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -1,112 +1,132 @@
 ## **Installation**
 
-**[PyTorch 1.2.0](https://pytorch.org/get-started/locally/)** or higher is recommended. If the install below gives an
-error, please install pytorch first [here](https://pytorch.org/get-started/locally/). 
-
 Installation can be done using [pypi](https://pypi.org/project/bertopic/):
 
 ```bash
 pip install bertopic
 ```
 
+To use the visualization options, install BERTopic as follows:
+
+```bash
+pip install bertopic[visualization]
+```
+
+To use Flair embeddings, install BERTopic as follows:
+```bash
+pip install bertopic[flair]
+```
+
+To install all additional dependencies:
+
+```bash
+pip install bertopic[all]
+```
+
 ## **Quick Start**
-Below is an example of how to use the model. The example uses the 
-[20 newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset.  
+We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of english documents:
+
 
 ```python
 from bertopic import BERTopic
 from sklearn.datasets import fetch_20newsgroups
  
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
 
-model = BERTopic()
-topics, probabilities = model.fit_transform(docs)
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs)
 ```
 
-The resulting topics can be accessed through `model.get_topic(topic)`:
+After generating topics and their probabilities, we can access the frequent topics that were generated:
 
 ```python
->>> model.get_topic(9)
-[('game', 0.005251396890032802),
- ('team', 0.00482651185323754),
- ('hockey', 0.004335032060690186),
- ('players', 0.0034782716706978963),
- ('games', 0.0032873248432630227),
- ('season', 0.003218987432255393),
- ('play', 0.0031855141725669637),
- ('year', 0.002962343114817677),
- ('nhl', 0.0029577648449943144),
- ('baseball', 0.0029245163154193524)]
-```  
+>>> topic_model.get_topic_freq().head()
+Topic	Count
+-1	7288
+49	3992
+30	701
+27	684
+11	568
+```
 
-**NOTE**: If you get less than 10 topics, it is advised to decrease the `min_topic_size` in `BERTopic`. This 
-will allow clusters to be created more easily and will typically result in more clusters.   
+-1 refers to all outliers and should typically be ignored. Next, let's take a look at the most 
+frequent topic that was generated, `topic 49`:
 
+```python
+>>> topic_model.get_topic(49)
+[('windows', 0.006152228076250982),
+ ('drive', 0.004982897610645755),
+ ('dos', 0.004845038866360651),
+ ('file', 0.004140142872194834),
+ ('disk', 0.004131678774810884),
+ ('mac', 0.003624848635985097),
+ ('memory', 0.0034840976976789903),
+ ('software', 0.0034415334250699077),
+ ('email', 0.0034239554442333257),
+ ('pc', 0.003047105930670237)]
+```  
 
-### **Languages**
-BERTopic is set to `english` but supports essentially any language for which a document embedding model exists. 
-You can choose the language by simply setting the `language` parameter in BERTopic. 
+**NOTE**: Use `BERTopic(language=""multilingual"")` to select a model that supports 50+ languages.
+
+## **Visualize Topics**
+After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good 
+understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. 
+Instead, we can visualize the topics that were generated in a way very similar to 
+[LDAvis](https://github.com/cpsievert/LDAvis):
 
 ```python
-from bertopic import BERTopic
-model = BERTopic(language=""Dutch"")
-```
+topic_model.visualize_topics()
+``` 
+
+<iframe src=""viz.html"" style=""width:1000px; height: 680px; border: 0px;""""></iframe>
 
-For a list of supported languages, please select the link below. 
-
-<details>
-<summary>Supported Languages</summary>
-
-The following languages are supported:
-Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese,
-Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian,
-Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified),
-Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto,
-Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek,
-Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian,
-Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean,
-Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian,
-Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian,
-Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian,
-Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak,
-Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil,
-Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian,
-Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian,
-Xhosa, Yiddish
-</details>  
-
-
-### **Embedding model**
-If you want to select any model from `sentence-transformers` you can simply select that model and pass it through 
-BERTopic with `embedding_model`:
+## **Embedding Models**
+The parameter `embedding_model` takes in a string pointing to a sentence-transformers model, 
+a SentenceTransformer, or a Flair DocumentEmbedding model. 
+
+### **Sentence-Transformers**  
+You can select any model from `sentence-transformers` [here](https://www.sbert.net/docs/pretrained_models.html) 
+and pass it through BERTopic with `embedding_model`:
 
 ```python
 from bertopic import BERTopic
-model = BERTopic(embedding_model=""xlm-r-bert-base-nli-stsb-mean-tokens"")
+topic_model = BERTopic(embedding_model=""xlm-r-bert-base-nli-stsb-mean-tokens"")
 ```
 
-Click [here](https://www.sbert.net/docs/pretrained_models.html) for a list of supported sentence transformers models.  
+Or select a SentenceTransformer model with your own parameters:
 
+```python
+from bertopic import BERTopic
+from sentence_transformers import SentenceTransformer
 
-### **Visualize Topic Probabilities**
+sentence_model = SentenceTransformer(""distilbert-base-nli-mean-tokens"", device=""cpu"")
+topic_model = BERTopic(embedding_model=sentence_model)
+```
 
-The variable `probabilities` that is returned from `transform()` or `fit_transform()` can 
-be used to understand how confident BERTopic is that certain topics can be found in a document. 
+### **Flair**
+[Flair](https://github.com/flairNLP/flair) allows you to choose almost any embedding model that 
+is publicly available. Flair can be used as follows:
 
-To visualize the distributions, we simply call:
 ```python
-# Make sure to input the probabilities of a single document!
-model.visualize_distribution(probabilities[0])
+from bertopic import BERTopic
+from flair.embeddings import TransformerDocumentEmbeddings
+
+roberta = TransformerDocumentEmbeddings('roberta-base')
+topic_model = BERTopic(embedding_model=roberta)
 ```
 
-<img src=""probabilities.png"" width=""75%"" height=""75%""/>
+You can select any ðŸ¤— transformers model [here](https://huggingface.co/models).
 
+### **Custom Embeddings**    
+You can also use previously generated embeddings by passing it through `fit_transform()`:
 
-**NOTE**: The distribution of the probabilities does not give an indication to 
-the distribution of the frequencies of topics across a document. It merely shows
-how confident BERTopic is that certain topics can be found in a document.
+```python
+from bertopic import BERTopic
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs, embeddings)
+```
 
-### **Save/Load BERTopic model**
+## **Save/Load BERTopic model**
 We can easily save a trained BERTopic model by calling `save`:
 ```python
 from bertopic import BERTopic
@@ -118,3 +138,7 @@ Then, we can load the model in one line:
 ```python
 loaded_model = BERTopic.load(""my_model"")
 ```
+
+If you do not want to save the embedding model because it is loaded from the cloud, simply run 
+`model.save(""my_model"", save_embedding_model=False)` instead. Then, you can load in the model 
+with `BERTopic.load(""my_model"", embedding_model=""whatever_model_you_used"")`. 
\ No newline at end of file"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -11,17 +11,17 @@ from sklearn.datasets import fetch_20newsgroups
 
 # Create topics
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
-model = BERTopic()
-topics, probs = model.fit_transform(docs)
+topic_model = BERTopic()
+topics, probs = topic_model.fit_transform(docs)
 ```
 
 After having trained our model, we can use `find_topics` to search for topics that are similar 
 to an input search_term. Here, we are going to be searching for topics that closely relate the 
 search term ""motor"". Then, we extract the most similar topic and check the results: 
 
 ```python
->>> similar_topics, similarity = model.find_topics(""motor"", top_n=5)
->>> model.get_topic(similar_topics[0])
+>>> similar_topics, similarity = topic_model.find_topics(""motor"", top_n=5)
+>>> topic_model.get_topic(similar_topics[0])
 [('bike', 0.02275997701645559),
  ('motorcycle', 0.011391202866080292),
  ('bikes', 0.00981187573649205),"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -12,37 +12,11 @@ clusters.
 
 ```python
 from bertopic import BERTopic
-model = BERTopic(min_topic_size=10)
+topic_model = BERTopic(min_topic_size=10)
 ```
 
 You can increase this value if you have more data available or if you expect clusters to be quite large. 
 
-#### **Local Neighborhood**
-The `n_neighbors` parameter is used in `UMAP` when reducing the dimensionality. It is the size of the local 
-neighborhood used for manifold approximation. If we set this relatively high, we get a more global view of 
-the data which might reduce the number of clusters. Smaller values result in more local data being preseverd which 
-could result in more clusters.
-
-```python
-from bertopic import BERTopic
-model = BERTopic(n_neighbors=15)
-```
-
-If you have more data, you can increase the value of this parameter as you are more likely to have more neighbors. 
- 
-#### **Dimensionality**
-The `n_components` refers to the dimension size we reduce the document embeddings to. This is necessary for HDBSCAN 
-to properly find clusters. A higher value will preserve more local structure but makes clustering more complicated 
-for HDBSCAN which can result in fewer clusters if set to high. A small value will preserve less of the local structure 
-but makes clustering easier for HDBSCAN. Similarly, this can result in fewer clusters if set to low.
-
-```python
-from bertopic import BERTopic
-model = BERTopic(n_components=5)
-```
- 
-I would recommend a value between 3 and 10 dimensions.  
-
 ## **Hierarchical Topic Reduction**
 It is not possible for HDBSCAN to specify the number of clusters you would want. To a certain extent, 
 this is actually an advantage, as we can trust HDBSCAN to be better in finding the number of clusters than we are.
@@ -57,18 +31,18 @@ of topics quite easily. We do this until we reach the value of `nr_topics`:
 
 ```python
 from bertopic import BERTopic
-model = BERTopic(nr_topics=20)
+topic_model = BERTopic(nr_topics=20)
 ```
 
 ### **Automatic Topic Reduction**
 One issue with the approach above is that it will merge topics regardless of whether they are actually very similar. They 
 are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. 
 It will reduce the number of topics, starting from the least frequent topic, as long as it exceeds a minimum 
-similarity of 0.9. To use this option, we simply set `nr_topics` to `""auto""`:
+similarity of 0.915. To use this option, we simply set `nr_topics` to `""auto""`:
 
 ```python
 from bertopic import BERTopic
-model = BERTopic(nr_topics=""auto"")
+topic_model = BERTopic(nr_topics=""auto"")
 ```
 
 ### **Topic Reduction after Training**
@@ -83,14 +57,14 @@ from sklearn.datasets import fetch_20newsgroups
  
 # Create topics -> Typically over 50 topics
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
-model = BERTopic()
-topics, probs = model.fit_transform(docs)
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs)
 
 # Further reduce topics
-new_topics, new_probs = model.reduce_topics(docs, topics, probs, nr_topics=30)
+new_topics, new_probs = topic_model.reduce_topics(docs, topics, nr_topics=30)
 ```
 
-The reasoning for putting `docs`, `topics`, and `probs` as parameters is that these values are not saved within 
-BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic 
+The reasoning for putting `docs` and `topics` (and optionally `probabilities`) as parameters is that these values are not saved within 
+BERTopic on purpose. If you were to have a million documents, it is very inefficient to save those in BERTopic 
 instead of a dedicated database.  
 "
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -14,14 +14,14 @@ from sklearn.datasets import fetch_20newsgroups
 
 # Create topics
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
-model = BERTopic(n_gram_range=(2, 3), stop_words=""english"")
-topics, probs = model.fit_transform(docs)
+topic_model = BERTopic(n_gram_range=(2, 3))
+topics, _ = topic_model.fit_transform(docs)
 ```
 
 From the model created above, one of the most frequent topics is the following:
 
 ```python
->>> model.get_topic(31)[:10]
+>>> topic_model.get_topic(31)[:10]
 [('clipper chip', 0.007240771542316232),
  ('key escrow', 0.004601603973377443),
  ('law enforcement', 0.004277247929596332),
@@ -39,8 +39,8 @@ what the topic is about. Instead, let's simplify the topic representation by set
 also allow for single words.
 
 ```python
->>> model.update_topics(docs, topics, n_gram_range=(1, 3), stop_words=""english"")
->>> model.get_topic(31)[:10]
+>>> topic_model.update_topics(docs, topics, n_gram_range=(1, 3))
+>>> topic_model.get_topic(31)[:10]
 [('encryption', 0.008021846079148017),
  ('clipper', 0.00789642647602742),
  ('chip', 0.00637127942464045),
@@ -54,4 +54,5 @@ also allow for single words.
 ```
 
 To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play 
-around with `n_gram_range` and `stop_words` or use your own custom `sklearn.feature_extraction.text.CountVectorizer`. 
\ No newline at end of file
+around with `n_gram_range` or use your own custom `sklearn.feature_extraction.text.CountVectorizer` and pass that  
+instead. 
\ No newline at end of file"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -13,13 +13,12 @@ First, we need to train our model:
 from bertopic import BERTopic
 from sklearn.datasets import fetch_20newsgroups
 
-# Create topics
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
-model = BERTopic()
-topics, probs = model.fit_transform(docs)
+topic_model = BERTopic()
+topics, _ = topic_model.fit_transform(docs)
 ```
 
-Then, we simply call `model.visualize_topics()` in order to visualize our topics. The resulting graph is a 
+Then, we simply call `topic_model.visualize_topics()` in order to visualize our topics. The resulting graph is a 
 plotly interactive graph which can be converted to html. 
 
 Thus, you can play around with the results below:
@@ -30,23 +29,29 @@ You can use the slider to select the topic which then lights up red. If you hove
 information is given about the topic, including size of the topic and its corresponding words.
 
 ## **Visualize Probablities**
-The variable `probabilities` that is returned from `transform()` or `fit_transform()` can be used to understand how 
-confident BERTopic is that certain topics can be found in a document.
+We can also calculate the probabilities of topics found in a document. In order to do so, we have to 
+set `calculate_probabilities` to True as calculating them can be quite computationally expensive. 
+Then, we use the variable `probabilities` that is returned from `transform()` or `fit_transform()` 
+to understand how confident BERTopic is that certain topics can be found in a document:
 
 ```python
 from bertopic import BERTopic
 from sklearn.datasets import fetch_20newsgroups
 
-# Create topics
 docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
-model = BERTopic()
-topics, probs = model.fit_transform(docs)
+topic_model = BERTopic(calculate_probabilities=True)
+topics, probabilities = topic_model.fit_transform(docs)
 ```
 
 To visualize the distributions, we simply call:
 
 ```python
-model.visualize_distribution(probabilities[0])
+topic_model.visualize_distribution(probabilities[0])
 ```
 
-<img src=""probabilities.png"" width=""75%"" height=""75%""/>
\ No newline at end of file
+<img src=""probabilities.png"" width=""75%"" height=""75%""/>
+
+
+**NOTE**: The distribution of the probabilities does not give an indication to 
+the distribution of the frequencies of topics across a document. It merely shows
+how confident BERTopic is that certain topics can be found in a document.
\ No newline at end of file"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -7,22 +7,23 @@ site_author: Maarten P. Grootendorst
 use_directory_urls: false
 nav:
   - Home: index.md
+  - The Algorithm: tutorial/algorithm/algorithm.md
   - Guides:
       - Getting Started: tutorial/quickstart/quickstart.md
-      - Custom Embeddings: tutorial/embeddings/embeddings.md
+      - Embeddings: tutorial/embeddings/embeddings.md
       - Topic Visualization: tutorial/visualization/visualization.md
       - Topic Reduction: tutorial/topicreduction/topicreduction.md
       - Topic Representation: tutorial/topicrepresentation/topicrepresentation.md
       - Search Topics: tutorial/search/search.md
-  - The Algorithm: tutorial/algorithm/algorithm.md
+      - Custom Models: tutorial/models/models.md
   - API:
       - BERTopic: api/bertopic.md
       - cTFIDF: api/ctfidf.md
+      - MMR: api/mmr.md
+  - FAQ: faq.md
   - Changelog: changelog.md
 plugins:
   - mkdocstrings:
-      setup_commands:
-        - import bertopic
       watch:
         - bertopic
   - search
@@ -36,15 +37,17 @@ theme:
     text: Ubuntu
     code: Ubuntu Mono
   favicon: icon.png
-  logo: icon.png
+  logo: icon_white.png
   feature:
     tabs: true
   palette:
-    primary: white
-    accent: blue
+    primary: black
+    accent: grey
 markdown_extensions:
   - admonition
   - codehilite
+  - markdown.extensions.codehilite:
+      guess_lang: false
   - pymdownx.inlinehilite
   - pymdownx.details
   - pymdownx.tabbed"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -11,26 +11,29 @@
     ""mkdocstrings>=0.8.0"",
 ]
 
-
 base_packages = [
-    ""tqdm>=4.41.1"",
-    ""numpy>=1.18.5"",
-    ""umap-learn>=0.4.6"",
-    ""hdbscan>=0.8.26"",
+    ""numpy>=1.19.2"",
+    ""hdbscan>=0.8.27"",
+    ""umap-learn>=0.5.0"",
     ""pandas>=1.1.5"",
-    ""scipy>=1.3.1"",
     ""scikit-learn>=0.22.2.post1"",
-    ""joblib==0.17.0"",
+    ""tqdm>=4.41.1"",
     ""torch>=1.4.0"",
-    ""sentence-transformers>=0.3.9""
+    ""sentence-transformers>=0.4.1""
 ]
 
 visualization_packages = [
     ""matplotlib>=3.2.2"",
     ""plotly>=4.7.0,<4.14.3""
 ]
 
-dev_packages = docs_packages + test_packages + visualization_packages
+flair_packages = [
+    ""flair==0.7""
+]
+
+extra_packages = visualization_packages + flair_packages
+
+dev_packages = docs_packages + test_packages + extra_packages
 
 
 with open(""README.md"", ""r"") as fh:
@@ -39,7 +42,7 @@
 setuptools.setup(
     name=""bertopic"",
     packages=[""bertopic""],
-    version=""0.4.3"",
+    version=""0.5.0"",
     author=""Maarten Grootendorst"",
     author_email=""maartengrootendorst@gmail.com"",
     description=""BERTopic performs topic Modeling with state-of-the-art transformer models."",
@@ -67,7 +70,9 @@
         ""test"": test_packages,
         ""docs"": docs_packages,
         ""dev"": dev_packages,
-        ""visualization"": visualization_packages
+        ""visualization"": visualization_packages,
+        ""flair"": flair_packages,
+        ""all"": extra_packages
     },
     python_requires='>=3.6',
 )"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,10 @@
+from bertopic import BERTopic
+import pytest
+
+
+@pytest.fixture(scope=""module"")
+def base_bertopic():
+    model = BERTopic(language=""english"",
+                     verbose=True,
+                     min_topic_size=5)
+    return model"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,68 @@
+""""""
+Test fitted BERTopic
+
+These test relate to a typical order of BERTopic usage. From training the model
+and predicting new instances, to further reducing topics and visualizing the results.
+
+TO DO:
+    * Add Evaluation measures to check for quality
+""""""
+
+
+from sklearn.datasets import fetch_20newsgroups
+
+newsgroup_docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data'][:1000]
+
+
+def test_full_model(base_bertopic):
+    """""" Tests the entire pipeline in one go. This serves as a sanity check to see if the default
+    settings result in a good separation of topics.
+
+    NOTE: This does not cover all cases but merely combines it all together
+    """"""
+    # Test fit
+    base_bertopic.calculate_probabilities = True
+    topics, probs = base_bertopic.fit_transform(newsgroup_docs)
+
+    for topic in set(topics):
+        words = base_bertopic.get_topic(topic)[:10]
+        assert len(words) == 10
+
+    for topic in base_bertopic.get_topic_freq().Topic:
+        words = base_bertopic.get_topic(topic)[:10]
+        assert len(words) == 10
+
+    assert len(base_bertopic.get_topic_freq()) > 2
+    assert probs.shape == (1000, len(base_bertopic.get_topic_freq())-1)
+    assert len(base_bertopic.get_topics()) == len(base_bertopic.get_topic_freq())
+
+    # Test transform
+    doc = ""This is a new document to predict.""
+    topics_test, probs_test = base_bertopic.transform([doc])
+
+    assert len(probs_test) == len(base_bertopic.get_topic_freq())-1
+    assert len(topics_test) == 1
+
+    # Test find topic
+    similar_topics, similarity = base_bertopic.find_topics(""query"", top_n=2)
+    assert len(similar_topics) == 2
+    assert len(similarity) == 2
+    assert max(similarity) <= 1
+
+    # Test update topics
+    topic = base_bertopic.get_topic(1)[:10]
+    base_bertopic.update_topics(newsgroup_docs, topics, n_gram_range=(2, 2))
+    updated_topic = base_bertopic.get_topic(1)[:10]
+    base_bertopic.update_topics(newsgroup_docs, topics)
+    original_topic = base_bertopic.get_topic(1)[:10]
+
+    assert topic != updated_topic
+    assert topic == original_topic
+
+    # Test topic reduction
+    nr_topics = 2
+    new_topics, new_probs = base_bertopic.reduce_topics(newsgroup_docs, topics, probs, nr_topics=nr_topics)
+
+    assert len(base_bertopic.get_topic_freq()) == nr_topics + 1
+    assert len(new_topics) == len(topics)
+    assert len(new_probs) == len(probs)"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -1,238 +0,0 @@
-import pytest
-import numpy as np
-import pandas as pd
-from unittest import mock
-from sklearn.datasets import fetch_20newsgroups, make_blobs
-from sklearn.feature_extraction.text import CountVectorizer
-from sentence_transformers import SentenceTransformer
-
-from bertopic import BERTopic
-
-newsgroup_docs = fetch_20newsgroups(subset='all')['data'][:1000]
-embedding_model = SentenceTransformer(""distilbert-base-nli-stsb-mean-tokens"")
-
-
-def create_embeddings(docs):
-    """""" For mocking the _extract_embeddings function """"""
-    if len(docs) > 1:
-        blobs, _ = make_blobs(n_samples=len(docs), centers=5, n_features=768, random_state=42)
-    else:
-        blobs, _ = make_blobs(n_samples=len(docs), centers=1, n_features=768, random_state=42)
-    return blobs
-
-
-def test_full():
-    model = BERTopic(language=""english"", verbose=True, n_neighbors=5, min_topic_size=5)
-
-    # Test fit
-    topics, probs = model.fit_transform(newsgroup_docs)
-
-    for topic in set(topics):
-        words = model.get_topic(topic)[:10]
-        assert len(words) == 10
-
-    for topic in model.get_topic_freq().Topic:
-        words = model.get_topic(topic)[:10]
-        assert len(words) == 10
-
-    assert len(model.get_topic_freq()) > 2
-    assert probs.shape == (1000, len(model.get_topic_freq())-1)
-    assert len(model.get_topics()) == len(model.get_topic_freq())
-
-    # Test transform
-    doc = ""This is a new document to predict.""
-    topics_test, probs_test = model.transform([doc])
-
-    assert len(probs_test) == len(model.get_topic_freq())-1
-    assert len(topics_test) == 1
-
-    # Test find topic
-    similar_topics, similarity = model.find_topics(""query"", top_n=2)
-    assert len(similar_topics) == 2
-    assert len(similarity) == 2
-    assert max(similarity) <= 1
-
-    # Test update topics
-    topic = model.get_topic(1)[:10]
-    model.update_topics(newsgroup_docs, topics, n_gram_range=(2, 2), stop_words=""english"")
-    updated_topic = model.get_topic(1)[:10]
-    model.update_topics(newsgroup_docs, topics)
-    original_topic = model.get_topic(1)[:10]
-
-    assert topic != updated_topic
-    assert topic == original_topic
-
-    # Test topic reduction
-    nr_topics = 2
-    new_topics, new_probs = model.reduce_topics(newsgroup_docs, topics, probs, nr_topics=nr_topics)
-
-    assert len(model.get_topic_freq()) == nr_topics + 1
-    assert len(new_topics) == len(topics)
-    assert len(new_probs) == len(probs)
-
-
-def test_load_model():
-    """""" Check if the model is correctly saved """"""
-    model = BERTopic(language=""Dutch"", embedding_model=None, n_components=12)
-    model.save(""test"")
-    loaded_model = BERTopic.load(""test"")
-    assert type(model) == type(loaded_model)
-    assert model.language == loaded_model.language
-    assert model.embedding_model == loaded_model.embedding_model
-    assert model.top_n_words == loaded_model.top_n_words
-    assert model.n_neighbors == loaded_model.n_neighbors
-    assert model.n_components == loaded_model.n_components
-
-
-def test_extract_incorrect_embeddings():
-    """""" Test if errors are raised when loading incorrect model """"""
-    with pytest.raises(ValueError):
-        model = BERTopic(language=""Unknown language"")
-        model._extract_embeddings([""Some document""])
-
-
-def test_extract_embeddings():
-    """""" Test if correct model is loaded and embeddings match the sentence-transformers version """"""
-    docs = [""some document""]
-    model = BERTopic(embedding_model=""distilbert-base-nli-stsb-mean-tokens"")
-    bertopic_embeddings = model._extract_embeddings(docs)
-
-    assert isinstance(bertopic_embeddings, np.ndarray)
-    assert bertopic_embeddings.shape == (1, 768)
-
-    sentence_embeddings = embedding_model.encode(docs, show_progress_bar=False)
-    assert np.array_equal(bertopic_embeddings, sentence_embeddings)
-
-
-@pytest.mark.parametrize(""embeddings,shape"", [(np.random.rand(100, 68), 100),
-                                              (np.random.rand(10, 768), 10),
-                                              (np.random.rand(1000, 5), 1000)])
-def test_reduce_dimensionality(embeddings, shape):
-    """""" Testing whether the dimensionality is reduced to the correct shape """"""
-    model = BERTopic()
-    umap_embeddings = model._reduce_dimensionality(embeddings)
-    assert umap_embeddings.shape == (shape, 5)
-
-
-@pytest.mark.parametrize(""samples,features,centers"",
-                         [(200, 500, 1),
-                          (500, 200, 1),
-                          (200, 500, 2),
-                          (500, 200, 2),
-                          (200, 500, 4),
-                          (500, 200, 4)])
-def test_cluster_embeddings(samples, features, centers):
-    """""" Testing whether the clusters are correctly created and if the old and new dataframes
-    are the exact same aside from the Topic column """"""
-    embeddings, _ = make_blobs(n_samples=samples, centers=centers, n_features=features, random_state=42)
-    documents = [str(i + 1) for i in range(embeddings.shape[0])]
-    old_df = pd.DataFrame({""Document"": documents,
-                           ""ID"": range(len(documents)),
-                           ""Topic"": None})
-    model = BERTopic()
-    new_df, _ = model._cluster_embeddings(embeddings, old_df)
-
-    assert len(new_df.Topic.unique()) == centers
-    assert ""Topic"" in new_df.columns
-    pd.testing.assert_frame_equal(old_df.drop(""Topic"", 1), new_df.drop(""Topic"", 1))
-
-
-def test_extract_topics():
-    """""" Test whether the topics are correctly extracted using c-TF-IDF """"""
-    nr_topics = 5
-    documents = pd.DataFrame({""Document"": newsgroup_docs,
-                              ""ID"": range(len(newsgroup_docs)),
-                              ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
-    model = BERTopic()
-    model._update_topic_size(documents)
-    model._extract_topics(documents)
-    freq = model.get_topic_freq()
-
-    assert model.c_tf_idf.shape[0] == 5
-    assert model.c_tf_idf.shape[1] > 100
-    assert isinstance(freq, pd.DataFrame)
-    assert nr_topics == len(freq.Topic.unique())
-    assert freq.Count.sum() == len(documents)
-    assert len(freq.Topic.unique()) == len(freq)
-
-
-def test_extract_topics_custom_cv():
-    """""" Test whether the topics are correctly extracted using c-TF-IDF
-    with custom CountVectorizer
-    """"""
-    nr_topics = 5
-    documents = pd.DataFrame({""Document"": newsgroup_docs,
-                              ""ID"": range(len(newsgroup_docs)),
-                              ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
-
-    cv = CountVectorizer(ngram_range=(1, 2))
-    model = BERTopic(vectorizer=cv)
-    model._update_topic_size(documents)
-    model._extract_topics(documents)
-    freq = model.get_topic_freq()
-
-    assert model.c_tf_idf.shape[0] == 5
-    assert model.c_tf_idf.shape[1] > 100
-    assert isinstance(freq, pd.DataFrame)
-    assert nr_topics == len(freq.Topic.unique())
-    assert freq.Count.sum() == len(documents)
-    assert len(freq.Topic.unique()) == len(freq)
-
-
-@pytest.mark.parametrize(""reduced_topics"", [5, 10, 20, 40])
-def test_topic_reduction(reduced_topics):
-    """""" Test whether the topics are correctly reduced """"""
-    model = BERTopic()
-    nr_topics = reduced_topics + 2
-    model.nr_topics = reduced_topics
-    old_documents = pd.DataFrame({""Document"": newsgroup_docs,
-                                  ""ID"": range(len(newsgroup_docs)),
-                                  ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
-    model._update_topic_size(old_documents)
-    model._extract_topics(old_documents.copy())
-    old_freq = model.get_topic_freq()
-
-    new_documents = model._reduce_topics(old_documents.copy())
-    new_freq = model.get_topic_freq()
-
-    assert old_freq.Count.sum() == new_freq.Count.sum()
-    assert len(old_freq.Topic.unique()) == len(old_freq)
-    assert len(new_freq.Topic.unique()) == len(new_freq)
-    assert isinstance(model.mapped_topics, dict)
-    assert not set(model.get_topic_freq().Topic).difference(set(new_documents.Topic))
-    assert model.mapped_topics
-
-
-def test_topic_reduction_edge_cases():
-    """""" Test whether the topics are not reduced if the reduced number
-    of topics exceeds the actual number of topics found """"""
-    model = BERTopic()
-    nr_topics = 5
-    model.nr_topics = 100
-    old_documents = pd.DataFrame({""Document"": newsgroup_docs,
-                                  ""ID"": range(len(newsgroup_docs)),
-                                  ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
-    model._update_topic_size(old_documents)
-    model._extract_topics(old_documents)
-    old_freq = model.get_topic_freq()
-
-    new_documents = model._reduce_topics(old_documents)
-    new_freq = model.get_topic_freq()
-
-    assert not set(old_documents.Topic).difference(set(new_documents.Topic))
-    pd.testing.assert_frame_equal(old_documents, new_documents)
-    pd.testing.assert_frame_equal(old_freq, new_freq)
-
-
-@mock.patch(""bertopic._bertopic.BERTopic._extract_embeddings"")
-def test_fit_transform(embeddings):
-    """""" Test whether predictions are correctly made """"""
-    blobs, _ = make_blobs(n_samples=len(newsgroup_docs), centers=5, n_features=768, random_state=42)
-    embeddings.return_value = blobs
-    model = BERTopic()
-    predictions, probabilities = model.fit_transform(newsgroup_docs)
-
-    assert isinstance(predictions, list)
-    assert len(predictions) == len(newsgroup_docs)
-    assert not set(predictions).difference(set(model.get_topics().keys()))
-    assert probabilities.shape[0] == len(newsgroup_docs)"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,270 @@
+""""""
+Unit tests for the models underpinning BERTopic:
+    * SentenceTransformers
+    * UMAP
+    * HDBSCAN
+    * class-based TF-IDF
+    * MMR
+
+For each model, several common cases are tested to check whether they, isolated,
+work as intended. This does not include whether the values in itself are correct.
+For example, if embeddings are extracted from SentenceTransformer, we assume that
+the embeddings themselves are of quality. However, sanity checks will be executed.
+""""""
+
+
+import pytest
+import numpy as np
+import pandas as pd
+
+from scipy.sparse.csr import csr_matrix
+from sklearn.feature_extraction.text import CountVectorizer
+from sklearn.datasets import fetch_20newsgroups, make_blobs
+from sentence_transformers import SentenceTransformer
+from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings
+
+from umap import UMAP
+from hdbscan import HDBSCAN
+from bertopic import BERTopic
+from bertopic._mmr import mmr
+from bertopic._ctfidf import ClassTFIDF
+
+
+newsgroup_docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data'][:1000]
+embedding_model = SentenceTransformer(""distilbert-base-nli-stsb-mean-tokens"")
+
+
+def test_extract_embeddings(base_bertopic):
+    """""" Test SentenceTransformer
+
+    Check whether the embeddings are correctly generated
+    for both a single string or a list of strings. This means that
+    the correct shape should be outputted. The embeddings by itself
+    should not exceed certain values as a sanity check.
+    """"""
+    base_bertopic.embedding_model = base_bertopic._select_embedding_model()
+    single_embedding = base_bertopic._extract_embeddings(""a document"")
+    multiple_embeddings = base_bertopic._extract_embeddings([""a document"", ""another document""])
+
+    assert single_embedding.shape[0] == 1
+    assert single_embedding.shape[1] == 768
+    assert np.min(single_embedding) > -5
+    assert np.max(single_embedding) < 5
+
+    assert multiple_embeddings.shape[0] == 2
+    assert multiple_embeddings.shape[1] == 768
+    assert np.min(multiple_embeddings) > -5
+    assert np.max(multiple_embeddings) < 5
+
+
+def test_extract_embeddings_compare():
+    """""" Test SentenceTransformer with BERTopic
+
+    Test if the correct embedding model is loaded in BERTopic and
+    whether BERTopic embeddings match the sentence-transformers embeddings.
+    """"""
+    docs = [""some document""]
+    model = BERTopic(embedding_model=""distilbert-base-nli-stsb-mean-tokens"")
+    model.embedding_model = model._select_embedding_model()
+    bertopic_embeddings = model._extract_embeddings(docs)
+
+    assert isinstance(bertopic_embeddings, np.ndarray)
+    assert bertopic_embeddings.shape == (1, 768)
+
+    sentence_embeddings = embedding_model.encode(docs, show_progress_bar=False)
+    assert np.array_equal(bertopic_embeddings, sentence_embeddings)
+
+
+def test_extract_incorrect_embeddings():
+    """""" Test if errors are raised when loading incorrect model """"""
+    with pytest.raises(ValueError):
+        model = BERTopic(language=""Unknown language"")
+        model.embedding_model = model._select_embedding_model()
+        model._extract_embeddings([""Some document""])
+
+
+@pytest.mark.parametrize(""embeddings,shape"", [(np.random.rand(100, 68), 100),
+                                              (np.random.rand(10, 768), 10),
+                                              (np.random.rand(1000, 5), 1000)])
+def test_umap_reduce_dimensionality(embeddings, shape):
+    """""" Test UMAP
+
+    Testing whether the dimensionality across different shapes is
+    reduced to the correct shape. For now, testing the shape is sufficient
+    as the main goal here is to reduce the dimensionality, the quality is
+    tested in the full pipeline.
+    """"""
+    model = BERTopic()
+    umap_embeddings = model._reduce_dimensionality(embeddings)
+    assert umap_embeddings.shape == (shape, 5)
+
+
+@pytest.mark.parametrize(""embeddings,shape,n_components"", [(np.random.rand(100, 68), 100, 2),
+                                                           (np.random.rand(10, 768), 10, 5),
+                                                           (np.random.rand(1000, 5), 1000, 10)])
+def test_custom_umap_reduce_dimensionality(embeddings, shape, n_components):
+    """""" Test Custom UMAP
+
+    Testing whether the dimensionality is reduced to the correct shape with
+    a custom UMAP model. The custom UMAP model differs in the resulting
+    dimensionality and is tested across different embeddings.
+    """"""
+    model = BERTopic(umap_model=UMAP(n_components=n_components))
+    umap_embeddings = model._reduce_dimensionality(embeddings)
+    assert umap_embeddings.shape == (shape, n_components)
+
+
+@pytest.mark.parametrize(""samples,features,centers"",
+                         [(200, 500, 1),
+                          (500, 200, 1),
+                          (200, 500, 2),
+                          (500, 200, 2),
+                          (200, 500, 4),
+                          (500, 200, 4)])
+def test_hdbscan_cluster_embeddings(samples, features, centers):
+    """""" Test HDBSCAN
+
+    Testing whether the clusters are correctly created and if the old and new dataframes
+    are the exact same aside from the Topic column.
+    """"""
+    embeddings, _ = make_blobs(n_samples=samples, centers=centers, n_features=features, random_state=42)
+    documents = [str(i + 1) for i in range(embeddings.shape[0])]
+    old_df = pd.DataFrame({""Document"": documents, ""ID"": range(len(documents)), ""Topic"": None})
+    model = BERTopic()
+    new_df, _ = model._cluster_embeddings(embeddings, old_df)
+
+    assert len(new_df.Topic.unique()) == centers
+    assert ""Topic"" in new_df.columns
+    pd.testing.assert_frame_equal(old_df.drop(""Topic"", 1), new_df.drop(""Topic"", 1))
+
+
+@pytest.mark.parametrize(""samples,features,centers"",
+                         [(200, 500, 1),
+                          (500, 200, 1),
+                          (200, 500, 2),
+                          (500, 200, 2),
+                          (200, 500, 4),
+                          (500, 200, 4)])
+def test_custom_hdbscan_cluster_embeddings(samples, features, centers):
+    """""" Test Custom HDBSCAN
+
+    Testing whether the clusters are correctly created using a custom HDBSCAN instance
+    and if the old and new dataframes are the exact same aside from the Topic column.
+    """"""
+    embeddings, _ = make_blobs(n_samples=samples, centers=centers, n_features=features, random_state=42)
+    documents = [str(i + 1) for i in range(embeddings.shape[0])]
+    old_df = pd.DataFrame({""Document"": documents, ""ID"": range(len(documents)), ""Topic"": None})
+    hdbscan_model = HDBSCAN(min_cluster_size=10, metric=""euclidean"", cluster_selection_method=""eom"", prediction_data=True)
+    model = BERTopic(hdbscan_model=hdbscan_model)
+    new_df, _ = model._cluster_embeddings(embeddings, old_df)
+
+    assert len(new_df.Topic.unique()) == centers
+    assert ""Topic"" in new_df.columns
+    pd.testing.assert_frame_equal(old_df.drop(""Topic"", 1), new_df.drop(""Topic"", 1))
+    assert model.hdbscan_model.metric == ""euclidean""
+
+
+def test_ctfidf(base_bertopic):
+    """""" Test c-TF-IDF
+
+    Test whether the c-TF-IDF matrix is correctly calculated.
+    This includes the general shape of the matrix as well as the
+    possible values that could occupy the matrix.
+    """"""
+    nr_topics = 10
+    documents = pd.DataFrame({""Document"": newsgroup_docs,
+                              ""ID"": range(len(newsgroup_docs)),
+                              ""Topic"": np.random.randint(-1, nr_topics, len(newsgroup_docs))})
+    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})
+    documents = base_bertopic._preprocess_text(documents_per_topic.Document.values)
+    count = base_bertopic.vectorizer_model.fit(documents)
+    words = count.get_feature_names()
+    X = count.transform(documents)
+    transformer = ClassTFIDF().fit(X, n_samples=len(newsgroup_docs))
+    c_tf_idf = transformer.transform(X)
+
+    assert len(words) > 1000
+    assert all([isinstance(x, str) for x in words])
+
+    assert isinstance(X, csr_matrix)
+    assert isinstance(c_tf_idf, csr_matrix)
+
+    assert X.shape[0] == nr_topics + 1
+    assert X.shape[1] == len(words)
+
+    assert c_tf_idf.shape[0] == nr_topics + 1
+    assert c_tf_idf.shape[1] == len(words)
+
+    assert np.min(c_tf_idf) > -1
+    assert np.max(c_tf_idf) < 1
+
+    assert np.min(X) == 0
+
+
+def test_ctfidf_custom_cv():
+    """""" Test c-TF-IDF with custom CountVectorizer
+
+    Test whether the c-TF-IDF matrix is correctly calculated
+    with a custom countvectorizer. By increasing the ngram_range, a larger
+    matrix should be generated.
+    """"""
+    cv = CountVectorizer(ngram_range=(1, 3), stop_words=""english"")
+    model = BERTopic(vectorizer_model=cv)
+
+    nr_topics = 10
+    documents = pd.DataFrame({""Document"": newsgroup_docs,
+                              ""ID"": range(len(newsgroup_docs)),
+                              ""Topic"": np.random.randint(-1, nr_topics, len(newsgroup_docs))})
+    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})
+    documents = model._preprocess_text(documents_per_topic.Document.values)
+    count = model.vectorizer_model.fit(documents)
+    words = count.get_feature_names()
+    X = count.transform(documents)
+    transformer = ClassTFIDF().fit(X, n_samples=len(newsgroup_docs))
+    c_tf_idf = transformer.transform(X)
+
+    assert len(words) > 1000
+    assert all([isinstance(x, str) for x in words])
+
+    assert isinstance(X, csr_matrix)
+    assert isinstance(c_tf_idf, csr_matrix)
+
+    assert X.shape[0] == nr_topics + 1
+    assert X.shape[1] == len(words)
+
+    assert c_tf_idf.shape[0] == nr_topics + 1
+    assert c_tf_idf.shape[1] == len(words)
+
+    assert np.min(c_tf_idf) > -1
+    assert np.max(c_tf_idf) < 1
+
+    assert np.min(X) == 0
+
+
+@pytest.mark.parametrize(""words,diversity"",
+                         [(['stars', 'star', 'starry', 'astronaut', 'astronauts'], 0),
+                          (['stars', 'spaceship', 'nasa', 'skies', 'sky'], 1)])
+def test_mmr(words, diversity):
+    """""" Test MMR
+
+    Testing both low and high diversity when selecing candidates.
+    In the parameters, you can see that low diversity leads to very
+    similar words/vectors to be selected, whereas a high diversity
+    leads to a selection of candidates that, albeit similar to the input
+    document, are less similar to each other.
+    """"""
+    candidates = mmr(doc_embedding=np.array([5, 5, 5, 5]).reshape(1, -1),
+                     word_embeddings=np.array([[1, 1, 2, 2],
+                                               [1, 2, 4, 7],
+                                               [4, 4, 4, 4],
+                                               [4, 4, 4, 4],
+                                               [4, 4, 4, 4],
+                                               [1, 1, 9, 3],
+                                               [5, 3, 5, 8],
+                                               [6, 6, 6, 6],
+                                               [6, 6, 6, 6],
+                                               [5, 8, 7, 2]]),
+                     words=['space', 'nasa', 'stars', 'star', 'starry', 'spaceship',
+                            'sky', 'astronaut', 'astronauts', 'skies'],
+                     diversity=diversity)
+    assert candidates == words"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,36 @@
+""""""
+Unit Tests of uncategorized functions/features
+
+These tests are those that could not easily be categorized
+into one of the other test_XXX.py files.
+
+
+""""""
+
+from sklearn.datasets import fetch_20newsgroups
+from bertopic import BERTopic
+
+newsgroup_docs = fetch_20newsgroups(subset='all')['data'][:1000]
+
+
+def test_load_save_model():
+    """""" Check if the model is correctly saved """"""
+    model = BERTopic(language=""Dutch"", embedding_model=None)
+    model.save(""test"")
+    loaded_model = BERTopic.load(""test"")
+    assert type(model) == type(loaded_model)
+    assert model.language == loaded_model.language
+    assert model.embedding_model == loaded_model.embedding_model
+    assert model.top_n_words == loaded_model.top_n_words
+
+
+def test_get_params():
+    """""" Test if parameters could be extracted """"""
+    model = BERTopic()
+    params = model.get_params()
+    assert not params[""embedding_model""]
+    assert not params[""low_memory""]
+    assert not params[""nr_topics""]
+    assert params[""n_gram_range""] == (1, 1)
+    assert params[""min_topic_size""] == 10
+    assert params[""language""] == 'english'"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -0,0 +1,124 @@
+""""""
+Unit tests for topic representation
+
+This includes the following features:
+    * Extracting Topics
+    * Updating topics after extraction
+    * Topic reduction
+""""""
+
+import pytest
+import numpy as np
+import pandas as pd
+from sklearn.datasets import fetch_20newsgroups
+from sklearn.feature_extraction.text import CountVectorizer
+
+from bertopic import BERTopic
+
+newsgroup_docs = fetch_20newsgroups(subset='all')['data'][:1000]
+
+
+def test_extract_topics():
+    """""" Test Topic Extraction
+
+    Test whether topics could be extracted using c-TF-IDF.
+    Checks are related to the existence of topic representation,
+    not so much whether they make sense semantically.
+    """"""
+    nr_topics = 5
+    documents = pd.DataFrame({""Document"": newsgroup_docs,
+                              ""ID"": range(len(newsgroup_docs)),
+                              ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
+    model = BERTopic()
+    model.embedding_model = model._select_embedding_model()
+    model._update_topic_size(documents)
+    model._extract_topics(documents)
+    freq = model.get_topic_freq()
+
+    assert model.c_tf_idf.shape[0] == 5
+    assert model.c_tf_idf.shape[1] > 100
+    assert isinstance(freq, pd.DataFrame)
+    assert nr_topics == len(freq.Topic.unique())
+    assert freq.Count.sum() == len(documents)
+    assert len(freq.Topic.unique()) == len(freq)
+
+
+def test_extract_topics_custom_cv():
+    """""" Test Topic Extraction with custom Countvectorizer
+
+    Test whether topics could be extracted using c-TF-IDF.
+    Checks are related to the existence of topic representation,
+    not so much whether they make sense semantically.
+    """"""
+    nr_topics = 5
+    documents = pd.DataFrame({""Document"": newsgroup_docs,
+                              ""ID"": range(len(newsgroup_docs)),
+                              ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
+
+    cv = CountVectorizer(ngram_range=(1, 2))
+    model = BERTopic(vectorizer_model=cv)
+    model.embedding_model = model._select_embedding_model()
+    model._update_topic_size(documents)
+    model._extract_topics(documents)
+    freq = model.get_topic_freq()
+
+    assert model.c_tf_idf.shape[0] == 5
+    assert model.c_tf_idf.shape[1] > 100
+    assert isinstance(freq, pd.DataFrame)
+    assert nr_topics == len(freq.Topic.unique())
+    assert freq.Count.sum() == len(documents)
+    assert len(freq.Topic.unique()) == len(freq)
+
+
+@pytest.mark.parametrize(""reduced_topics"", [1, 2, 4, 10])
+def test_topic_reduction(reduced_topics):
+    """""" Test Topic Reduction
+
+    The the reduction of topics after having generated
+    topics. This generation of the initial topics is done
+    manually as the training takes quite a while.
+    """"""
+    nr_topics = reduced_topics + 2
+    model = BERTopic(nr_topics=reduced_topics)
+    model.embedding_model = model._select_embedding_model()
+    old_documents = pd.DataFrame({""Document"": newsgroup_docs,
+                                  ""ID"": range(len(newsgroup_docs)),
+                                  ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
+    model._update_topic_size(old_documents)
+    model._extract_topics(old_documents.copy())
+    old_freq = model.get_topic_freq()
+
+    new_documents = model._reduce_topics(old_documents.copy())
+    new_freq = model.get_topic_freq()
+
+    assert old_freq.Count.sum() == new_freq.Count.sum()
+    assert len(old_freq.Topic.unique()) == len(old_freq)
+    assert len(new_freq.Topic.unique()) == len(new_freq)
+    assert isinstance(model.mapped_topics, dict)
+    assert not set(model.get_topic_freq().Topic).difference(set(new_documents.Topic))
+    assert model.mapped_topics
+
+
+def test_topic_reduction_edge_cases():
+    """""" Test Topic Reduction Large Nr Topics
+
+    Test whether the topics are not reduced if the reduced number
+    of topics exceeds the actual number of topics found
+    """"""
+    model = BERTopic()
+    model.embedding_model = model._select_embedding_model()
+    nr_topics = 5
+    model.nr_topics = 100
+    old_documents = pd.DataFrame({""Document"": newsgroup_docs,
+                                  ""ID"": range(len(newsgroup_docs)),
+                                  ""Topic"": np.random.randint(-1, nr_topics-1, len(newsgroup_docs))})
+    model._update_topic_size(old_documents)
+    model._extract_topics(old_documents)
+    old_freq = model.get_topic_freq()
+
+    new_documents = model._reduce_topics(old_documents)
+    new_freq = model.get_topic_freq()
+
+    assert not set(old_documents.Topic).difference(set(new_documents.Topic))
+    pd.testing.assert_frame_equal(old_documents, new_documents)
+    pd.testing.assert_frame_equal(old_freq, new_freq)"
22;MaartenGr;BERTopic;e84d7d16924b71dc2f0f289594d12088d326f81d;"v0.5 (#46)

* Add Flair to allow for more (custom) token/document embeddings
* Option to use custom UMAP, HDBSCAN, and CountVectorizer
* Added low_memory parameter to reduce memory during computation
* Improved verbosity (shows progress bar)
* Improved testing
* Use the newest version of sentence-transformers as it speeds ups encoding significantly
* Return the figure of visualize_topics()
* Expose all parameters with a single function: get_params()
* Option to disable the saving of embedding_model, should reduce BERTopic size significantly
* Add FAQ page";"@@ -1,10 +1,15 @@
+""""""
+Unit tests for utility functions
+""""""
+
 import pytest
 import logging
 import numpy as np
 from bertopic._utils import check_documents_type, check_embeddings_shape, MyLogger
 
 
 def test_logger():
+    """""" Test whether the logger could correctly be instantiated """"""
     logger = MyLogger(""DEBUG"")
     assert isinstance(logger.logger, logging.Logger)
     assert logger.logger.level == 10
@@ -23,12 +28,20 @@ def test_logger():
     ],
 )
 def test_check_documents_type(docs):
+    """""" Making sure the checks for document type succeed """"""
     with pytest.raises(TypeError):
         check_documents_type(docs)
 
 
 def test_check_embeddings_shape():
+    """""" Testing correct embeddings shape
+
+    Checking for embeddings is typically done when custom embeddings
+    are used instead of the default SentenceTransformer models. This means
+    that the embeddings should have the correct shape, which is tested for
+    in check_embeddings_shape.
+    """"""
     docs = [""doc_one"", ""doc_two""]
     embeddings = np.array([[1, 2, 3],
                            [2, 3, 4]])
-    check_embeddings_shape(embeddings, docs)
\ No newline at end of file
+    check_embeddings_shape(embeddings, docs)"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -4,7 +4,7 @@ channels:
   - defaults
 dependencies:
   # required
-  - pip
+  - pip<22.0
   - audioread==2.1.5
   - numpy==1.17.0
   - scipy==1.2.0"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -6,7 +6,8 @@ jobs:
     steps:
       - uses: actions/checkout@v2
       - uses: actions/setup-python@v2
-      - run: pip install bandit codespell flake8
+      - run: pip install bandit codespell flake8 velin
       - run: bandit --recursive --skip B101,B110 .
       - run: codespell --ignore-words-list=""ba,trough,ue"" --skip=""*demo.ipynb""
       - run: flake8 librosa --count --select=E9,F63,F7,F82 --show-source --statistics
+      - run: python -m velin --check librosa"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -77,7 +77,10 @@ Contributors
 * N. Dorukhan Sergin <https://github.com/dorukhansergin>
 * Paul Biberstein <https://github.com/P-bibs>
 * Myungchul Keum <https://github.com/dofuuz>
-
+* Daniel Faronbi <https://github.com/dafaronbi>
+* Iran Roman <https://github.com/iranroman>
+* philstem <https://github.com/philstem>
+* Alex Malins <https://github.com/alexmalins>
 
 Some feature extraction code was based on <https://github.com/ronw/frontend> by Ron Weiss.
 "
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -2,6 +2,91 @@
 Changelog
 *********
 
+v0.9
+====
+
+v0.9.0
+------
+
+2022-02-??
+
+The main feature of this release is (nearly) full support for arbitrary multi-channel processing, along with several speed and stability enhancements.
+A detailed list of changes is provided below.
+
+New Features
+    - `#1130`_ Nearly full support for multi-channel processing. *Brian McFee, Daniel Faronbi, Iran Roman*
+    - `#1331`_ Option to disable unicode characters in display functions. *Brian McFee*
+    - `#1441`_ Significantly expanded the library of example audio clips. *Brian McFee*
+
+API changes
+    - `#1114`_ Most functions now require keyword arguments. *Brian McFee*
+    - `#1382`_ The default padding mode for most functions (including STFT) is now zero-padding. *Brian McFee*
+    - `#1418`_ `librosa.load` and `librosa.stream` can now operate directly on open `soundfile` objects. *Brian McFee*
+    - `#1414`_ `librosa.display.specshow` now uses centered coordinate grids. *Brian McFee*
+    - `#1398`_ `librosa.iirt` now exposes API control over resampling modes. *Brian McFee*
+    - `#1416`_ Removed deprecated functions `librosa.display.waveplot` and `librosa.util.example_audio_file`. *Brian McFee*
+
+Bug fixes
+    - `#1387`_ Fixed errors in support of odd frame lengths in various functions. *Brian McFee*
+    - `#1273`_ Minor corrections to constants in perceptual weighting functions. *Brian McFee*
+    - `#1350`_ Removed uses of deprecated numpy numerical types. *Brian McFee*
+    - `#1361`_ Maximum frequency is now correctly inferred as Nyquist in onset strength calculation. *Brian McFee*
+    - `#1362`_ `librosa.effects.deemphasis` no longer modifies the input signal in-place. *Brian McFee*
+    - `#1375`_ `librosa.util.frame` now correctly works for arbitrary memory layouts and numbers of axes. *Brian McFee*
+    - `#1425`_ Fixed an off-by-one error in `librosa.yin` and `librosa.pyin`. *@philstem, Brian McFee*
+    - `#1430`_ Removed unnecessary `__all__` specifications to better support static analysis. *Fabian Keller*
+    - `#1407`_ Corrected a normalization error in inverse CQT. *Brian McFee, Vincent Lostanlen*
+
+Documentation
+    - `#1328`_ Retired the `examples/` folder and expanded the `Advanced Examples` gallery. *Brian McFee*
+    - `#1427`_ Fixed docstring for `librosa.reassigned_spectrogram`. *Fabian Keller*
+
+Other changes
+    - `#418`_ `librosa.cqt` now supports arbitrary hop lengths. *Brian McFee*
+    - `#1405`_ Improvements and generalizations to constant-Q/variable-Q basis construction. *Brian McFee, Vincent Lostanlen*
+    - `#1324`_ Added a run-time check for minimally supported matplotlib versions. *Brian McFee*
+    - `#1325`_ Enhanced continuous integration testing for oldest and newest environments. *Brian McFee*
+    - `#1358`_ Harmonic interpolation now preemptively detects repeated values that produce unstable estimates. *Brian McFee*
+    - `#1432`_ Specify stack levels in warnings to provide more helpful warning messages. *Brian McFee*
+    - `#1404`_, `#1406`_ Improved packaging configurations. *Alex Malins*
+    - `#1384`_ Fixed package configuration error for documentation builds. *Adam Weiss*
+
+Deprecations
+    - `#1389`_ The `mono` parameter of `librosa.util.valid_audio` is deprecated and the default is now set to `False`. *Brian McFee*
+    - `#1405`_ CQT filter-bank constructors `librosa.filters.constant_q` are now deprecated in favor of `librosa.filters.wavelet`. *Brian McFee, Vincent Lostanlen*
+
+
+.. _#418: https://github.com/librosa/librosa/issues/418
+.. _#1114: https://github.com/librosa/librosa/issues/1114
+.. _#1130: https://github.com/librosa/librosa/issues/1130
+.. _#1273: https://github.com/librosa/librosa/issues/1273
+.. _#1324: https://github.com/librosa/librosa/issues/1324
+.. _#1325: https://github.com/librosa/librosa/issues/1325
+.. _#1328: https://github.com/librosa/librosa/issues/1328
+.. _#1331: https://github.com/librosa/librosa/issues/1331
+.. _#1350: https://github.com/librosa/librosa/issues/1350
+.. _#1358: https://github.com/librosa/librosa/issues/1358
+.. _#1361: https://github.com/librosa/librosa/issues/1361
+.. _#1362: https://github.com/librosa/librosa/issues/1362
+.. _#1375: https://github.com/librosa/librosa/issues/1375
+.. _#1382: https://github.com/librosa/librosa/issues/1382
+.. _#1384: https://github.com/librosa/librosa/issues/1384
+.. _#1387: https://github.com/librosa/librosa/issues/1387
+.. _#1389: https://github.com/librosa/librosa/issues/1389
+.. _#1398: https://github.com/librosa/librosa/issues/1398
+.. _#1404: https://github.com/librosa/librosa/issues/1404
+.. _#1405: https://github.com/librosa/librosa/issues/1405
+.. _#1406: https://github.com/librosa/librosa/issues/1406
+.. _#1407: https://github.com/librosa/librosa/issues/1407
+.. _#1414: https://github.com/librosa/librosa/issues/1414
+.. _#1416: https://github.com/librosa/librosa/issues/1416
+.. _#1418: https://github.com/librosa/librosa/issues/1418
+.. _#1425: https://github.com/librosa/librosa/issues/1425
+.. _#1427: https://github.com/librosa/librosa/issues/1427
+.. _#1430: https://github.com/librosa/librosa/issues/1430
+.. _#1432: https://github.com/librosa/librosa/issues/1432
+.. _#1441: https://github.com/librosa/librosa/issues/1441
+
 v0.8
 ====
 "
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -390,7 +390,7 @@ def reset_mpl(gallery_conf, fname):
 autodoc_member_order = ""bysource""
 
 smv_branch_whitelist = r""^(main)$""  # build main branch, and anything relating to documentation
-smv_tag_whitelist = r""^((0\.7\.2)|(0\.[89]\.\d+))$""  # use this for final builds
+smv_tag_whitelist = r""^((0\.7\.2)|(0\.[89]\.\d+)|(0\.9\.0rc0))$""  # use this for final builds
 smv_released_pattern = r'.*tags.*'
 smv_remote_whitelist = None
 smv_greatest_tag = True"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -51,59 +51,43 @@ def beat_track(
            Journal of New Music Research 36.1 (2007): 51-60.
            http://labrosa.ee.columbia.edu/projects/beattrack/
 
-
     Parameters
     ----------
-
     y : np.ndarray [shape=(n,)] or None
         audio time series
-
     sr : number > 0 [scalar]
         sampling rate of ``y``
-
     onset_envelope : np.ndarray [shape=(n,)] or None
         (optional) pre-computed onset strength envelope.
-
     hop_length : int > 0 [scalar]
         number of audio samples between successive ``onset_envelope`` values
-
-    start_bpm  : float > 0 [scalar]
+    start_bpm : float > 0 [scalar]
         initial guess for the tempo estimator (in beats per minute)
-
-    tightness  : float [scalar]
+    tightness : float [scalar]
         tightness of beat distribution around tempo
-
-    trim       : bool [scalar]
+    trim : bool [scalar]
         trim leading/trailing beats with weak onsets
-
-    bpm        : float [scalar]
+    bpm : float [scalar]
         (optional) If provided, use ``bpm`` as the tempo instead of
         estimating it from ``onsets``.
-
-    prior      : scipy.stats.rv_continuous [optional]
+    prior : scipy.stats.rv_continuous [optional]
         An optional prior distribution over tempo.
         If provided, ``start_bpm`` will be ignored.
-
     units : {'frames', 'samples', 'time'}
         The units to encode detected beat events in.
         By default, 'frames' are used.
 
-
     Returns
     -------
-
     tempo : float [scalar, non-negative]
         estimated global tempo (in beats per minute)
-
     beats : np.ndarray [shape=(m,)]
         estimated beat event locations in the specified units
         (default is frame indices)
-
     .. note::
         If no onset strength could be detected, beat_tracker estimates 0 BPM
         and returns an empty list.
 
-
     Raises
     ------
     ParameterError
@@ -114,7 +98,6 @@ def beat_track(
     --------
     librosa.onset.onset_strength
 
-
     Examples
     --------
     Track beats using time series input
@@ -125,14 +108,12 @@ def beat_track(
     >>> tempo
     135.99917763157896
 
-
     Print the frames corresponding to beats
 
     >>> beats
     array([  3,  21,  40,  59,  78,  96, 116, 135, 154, 173, 192, 211,
            230, 249, 268, 287, 306, 325, 344, 363])
 
-
     Or print them as timestamps
 
     >>> librosa.frames_to_time(beats, sr=sr)
@@ -152,7 +133,6 @@ def beat_track(
     array([  3,  21,  40,  59,  78,  96, 116, 135, 154, 173, 192, 211,
            230, 249, 268, 287, 306, 325, 344, 363])
 
-
     Plot the beat events against the onset strength envelope
 
     >>> import matplotlib.pyplot as plt
@@ -230,32 +210,23 @@ def tempo(
     ----------
     y : np.ndarray [shape=(..., n)] or None
         audio time series. Multi-channel is supported.
-
     sr : number > 0 [scalar]
         sampling rate of the time series
-
-    onset_envelope    : np.ndarray [shape=(..., n)]
+    onset_envelope : np.ndarray [shape=(..., n)]
         pre-computed onset strength envelope
-
     hop_length : int > 0 [scalar]
         hop length of the time series
-
     start_bpm : float [scalar]
         initial guess of the BPM
-
     std_bpm : float > 0 [scalar]
         standard deviation of tempo distribution
-
     ac_size : float > 0 [scalar]
         length (in seconds) of the auto-correlation window
-
     max_tempo : float > 0 [scalar, optional]
         If provided, only estimate tempo below this threshold
-
     aggregate : callable [optional]
         Aggregation function for estimating global tempo.
         If `None`, then tempo is estimated independently for each frame.
-
     prior : scipy.stats.rv_continuous [optional]
         A prior distribution over tempo (in beats per minute).
         By default, a pseudo-log-normal prior is used.
@@ -408,7 +379,6 @@ def plp(
     since `plp` does not require the entire signal to make predictions, it may be
     preferable when beat-tracking long recordings in a streaming setting.
 
-
     .. [#] Grosche, P., & Muller, M. (2011).
         ""Extracting predominant local pulse information from music recordings.""
         IEEE Transactions on Audio, Speech, and Language Processing, 19(6), 1688-1701.
@@ -495,7 +465,6 @@ def plp(
     >>> ax[2].set(title='Log-normal tempo prior, mean=120', xlim=[5, 20])
     >>> ax[2].legend()
 
-
     PLP local maxima can be used as estimates of beat positions.
 
     >>> tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env)
@@ -584,16 +553,12 @@ def __beat_tracker(onset_envelope, bpm, fft_res, tightness, trim):
     ----------
     onset_envelope : np.ndarray [shape=(n,)]
         onset strength envelope
-
     bpm : float [scalar]
         tempo estimate
-
-    fft_res  : float [scalar]
+    fft_res : float [scalar]
         resolution of the fft (sr / hop_length)
-
-    tightness: float [scalar]
+    tightness : float [scalar]
         how closely do we adhere to bpm?
-
     trim : bool [scalar]
         trim leading/trailing beats with weak onsets?
 "
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -76,7 +76,7 @@ def load(
         On the contrary, if the codec is not supported by `soundfile`
         (for example, MP3), then `path` must be a file path (string or `pathlib.Path`).
 
-    sr   : number > 0 [scalar]
+    sr : number > 0 [scalar]
         target sampling rate
 
         'None' uses the native sampling rate
@@ -106,16 +106,13 @@ def load(
 
            See :ref:`ioformats` for alternate loading methods.
 
-
     Returns
     -------
-    y    : np.ndarray [shape=(n,) or (..., n)]
+    y : np.ndarray [shape=(n,) or (..., n)]
         audio time series. Multi-channel is supported.
-
-    sr   : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-
     Examples
     --------
     >>> # Load an ogg vorbis file
@@ -171,8 +168,7 @@ def load(
     except RuntimeError as exc:
         # If soundfile failed, try audioread instead
         if isinstance(path, (str, pathlib.PurePath)):
-            warnings.warn(""PySoundFile failed. Trying audioread instead."",
-                          stacklevel=2)
+            warnings.warn(""PySoundFile failed. Trying audioread instead."", stacklevel=2)
             y, sr_native = __audioread_load(path, offset, duration, dtype)
         else:
             raise (exc)
@@ -535,7 +531,7 @@ def resample(
         Scale the resampled signal so that ``y`` and ``y_hat`` have approximately
         equal total energy.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         If ``fix==True``, additional keyword arguments to pass to
         `librosa.util.fix_length`.
 
@@ -668,13 +664,13 @@ def get_duration(
         up to the frame resolution. If high precision is required,
         it is better to use the audio time series directly.
 
-    n_fft       : int > 0 [scalar]
+    n_fft : int > 0 [scalar]
         FFT window size for ``S``
 
-    hop_length  : int > 0 [ scalar]
+    hop_length : int > 0 [ scalar]
         number of audio samples between columns of ``S``
 
-    center  : boolean
+    center : boolean
         - If ``True``, ``S[:, t]`` is centered at ``y[t * hop_length]``
         - If ``False``, then ``S[:, t]`` begins at ``y[t * hop_length]``
 
@@ -775,11 +771,9 @@ def autocorrelate(y, *, max_size=None, axis=-1):
     ----------
     y : np.ndarray
         array to autocorrelate
-
-    max_size  : int > 0 or None
+    max_size : int > 0 or None
         maximum correlation lag.
         If unspecified, defaults to ``y.shape[axis]`` (unbounded)
-
     axis : int
         The axis along which to autocorrelate.
         By default, the last axis (-1) is taken.
@@ -860,10 +854,8 @@ def lpc(y, *, order, axis=-1):
     ----------
     y : np.ndarray [shape=(..., n)]
         Time series to fit. Multi-channel is supported..
-
     order : int > 0
         Order of the linear filter
-
     axis : int
         Axis along which to compute the coefficients
 
@@ -881,7 +873,7 @@ def lpc(y, *, order, axis=-1):
     FloatingPointError
         - If ``y`` is ill-conditioned
 
-    See also
+    See Also
     --------
     scipy.signal.lfilter
 
@@ -1034,7 +1026,6 @@ def zero_crossings(
     If ``y`` is multi-dimensional, then zero-crossings are computed along
     the specified ``axis``.
 
-
     Parameters
     ----------
     y : np.ndarray
@@ -1174,44 +1165,34 @@ def clicks(
     ----------
     times : np.ndarray or None
         times to place clicks, in seconds
-
     frames : np.ndarray or None
         frame indices to place clicks
-
     sr : number > 0
         desired sampling rate of the output signal
-
     hop_length : int > 0
         if positions are specified by ``frames``, the number of samples between frames.
-
     click_freq : float > 0
         frequency (in Hz) of the default click signal.  Default is 1KHz.
-
     click_duration : float > 0
         duration (in seconds) of the default click signal.  Default is 100ms.
-
     click : np.ndarray or None
         (optional) click signal sample to use instead of the default click.
         Multi-channel is supported.
-
     length : int > 0
         desired number of samples in the output signal
 
-
     Returns
     -------
     click_signal : np.ndarray
         Synthesized click signal.
         This will be monophonic by default, or match the number of channels to a provided ``click`` signal.
 
-
     Raises
     ------
     ParameterError
         - If neither ``times`` nor ``frames`` are provided.
         - If any of ``click_freq``, ``click_duration``, or ``length`` are out of range.
 
-
     Examples
     --------
     >>> # Sonify detected beat events
@@ -1308,37 +1289,30 @@ def tone(frequency, *, sr=22050, length=None, duration=None, phi=None):
     ----------
     frequency : float > 0
         frequency
-
     sr : number > 0
         desired sampling rate of the output signal
-
     length : int > 0
         desired number of samples in the output signal.
         When both ``duration`` and ``length`` are defined,
         ``length`` takes priority.
-
     duration : float > 0
         desired duration in seconds.
         When both ``duration`` and ``length`` are defined,
         ``length`` takes priority.
-
     phi : float or None
         phase offset, in radians. If unspecified, defaults to ``-np.pi * 0.5``.
 
-
     Returns
     -------
     tone_signal : np.ndarray [shape=(length,), dtype=float64]
         Synthesized pure sine tone signal
 
-
     Raises
     ------
     ParameterError
         - If ``frequency`` is not provided.
         - If neither ``length`` nor ``duration`` are provided.
 
-
     Examples
     --------
     Generate a pure sine tone A4
@@ -1409,25 +1383,21 @@ def chirp(*, fmin, fmax, sr=22050, length=None, duration=None, linear=False, phi
         phase offset, in radians.
         If unspecified, defaults to ``-np.pi * 0.5``.
 
-
     Returns
     -------
     chirp_signal : np.ndarray [shape=(length,), dtype=float64]
         Synthesized chirp signal
 
-
     Raises
     ------
     ParameterError
         - If either ``fmin`` or ``fmax`` are not provided.
         - If neither ``length`` nor ``duration`` are provided.
 
-
     See Also
     --------
     scipy.signal.chirp
 
-
     Examples
     --------
     Generate a exponential chirp from A2 to A8
@@ -1490,7 +1460,6 @@ def mu_compress(x, *, mu=255, quantize=True):
 
         sign(x) * ln(1 + mu * abs(x)) /  ln(1 + mu)
 
-
     Parameters
     ----------
     x : np.ndarray with values in [-1, +1]
@@ -1590,11 +1559,9 @@ def mu_expand(x, *, mu=255.0, quantize=True):
     x : np.ndarray
         The compressed signal.
         If ``quantize=True``, values must be in the range [-1, +1].
-
     mu : positive number
         The compression parameter.  Values of the form ``2**n - 1``
         (e.g., 15, 31, 63, etc.) are most common.
-
     quantize : boolean
         If ``True``, the input is assumed to be quantized to
         ``1 + mu`` distinct integer values."
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -249,6 +249,10 @@ def hybrid_cqt(
     filter_scale : float > 0
         Filter filter_scale factor. Larger values use longer windows.
 
+    norm : {inf, -inf, 0, float > 0}
+        Type of norm to use for basis function normalization.
+        See `librosa.util.normalize`.
+
     sparsity : float in [0, 1)
         Sparsify the CQT basis by discarding up to ``sparsity``
         fraction of the energy in each basis.
@@ -259,6 +263,13 @@ def hybrid_cqt(
         Window specification for the basis filters.
         See `filters.get_window` for details.
 
+    scale : bool
+        If ``True``, scale the CQT response by square-root the length of
+        each channel's filter.  This is analogous to ``norm='ortho'`` in FFT.
+
+        If ``False``, do not scale the CQT. This is analogous to
+        ``norm=None`` in FFT.
+
     pad_mode : string
         Padding mode for centered frame analysis.
 
@@ -422,6 +433,10 @@ def pseudo_cqt(
     filter_scale : float > 0
         Filter filter_scale factor. Larger values use longer windows.
 
+    norm : {inf, -inf, 0, float > 0}
+        Type of norm to use for basis function normalization.
+        See `librosa.util.normalize`.
+
     sparsity : float in [0, 1)
         Sparsify the CQT basis by discarding up to ``sparsity``
         fraction of the energy in each basis.
@@ -432,6 +447,13 @@ def pseudo_cqt(
         Window specification for the basis filters.
         See `filters.get_window` for details.
 
+    scale : bool
+        If ``True``, scale the CQT response by square-root the length of
+        each channel's filter.  This is analogous to ``norm='ortho'`` in FFT.
+
+        If ``False``, do not scale the CQT. This is analogous to
+        ``norm=None`` in FFT.
+
     pad_mode : string
         Padding mode for centered frame analysis.
 
@@ -533,18 +555,23 @@ def icqt(
     Given a constant-Q transform representation ``C`` of an audio signal ``y``,
     this function produces an approximation ``y_hat``.
 
-
     Parameters
     ----------
     C : np.ndarray, [shape=(..., n_bins, n_frames)]
         Constant-Q representation as produced by `cqt`
 
+    sr : number > 0 [scalar]
+        sampling rate of the signal
+
     hop_length : int > 0 [scalar]
         number of samples between successive frames
 
     fmin : float > 0 [scalar]
         Minimum frequency. Defaults to `C1 ~= 32.70 Hz`
 
+    bins_per_octave : int > 0 [scalar]
+        Number of bins per octave
+
     tuning : float [scalar]
         Tuning offset in fractions of a bin.
 
@@ -711,7 +738,12 @@ def icqt(
         y_oct = istft(D_oct, window=""ones"", hop_length=my_hop, dtype=dtype)
 
         y_oct = audio.resample(
-            y_oct, orig_sr=1, target_sr=sr // my_sr, res_type=res_type, scale=False, fix=False
+            y_oct,
+            orig_sr=1,
+            target_sr=sr // my_sr,
+            res_type=res_type,
+            scale=False,
+            fix=False,
         )
 
         if y is None:
@@ -1004,7 +1036,9 @@ def vqt(
         if my_hop % 2 == 0:
             my_hop //= 2
             my_sr /= 2.0
-            my_y = audio.resample(my_y, orig_sr=2, target_sr=1, res_type=res_type, scale=True)
+            my_y = audio.resample(
+                my_y, orig_sr=2, target_sr=1, res_type=res_type, scale=True
+            )
 
     V = __trim_stack(vqt_resp, n_bins, dtype)
 
@@ -1163,7 +1197,9 @@ def __early_downsample(
             )
 
         new_sr = sr / float(downsample_factor)
-        y = audio.resample(y, orig_sr=sr, target_sr=new_sr, res_type=res_type, scale=True)
+        y = audio.resample(
+            y, orig_sr=sr, target_sr=new_sr, res_type=res_type, scale=True
+        )
 
         # If we're not going to length-scale after CQT, we
         # need to compensate for the downsampling factor here
@@ -1328,13 +1364,11 @@ def griffinlim_cqt(
 
         If ``None``, defaults to the current `np.random` object.
 
-
     Returns
     -------
     y : np.ndarray [shape=(..., n)]
         time-domain signal reconstructed from ``C``
 
-
     See Also
     --------
     cqt
@@ -1382,7 +1416,7 @@ def griffinlim_cqt(
         warnings.warn(
             ""Griffin-Lim with momentum={} > 1 can be unstable. ""
             ""Proceed with caution!"".format(momentum),
-            stacklevel=2
+            stacklevel=2,
         )
     elif momentum < 0:
         raise ParameterError("
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -57,12 +57,10 @@ def frames_to_samples(frames, *, hop_length=512, n_fft=None):
 
     Parameters
     ----------
-    frames     : number or np.ndarray [shape=(n,)]
+    frames : number or np.ndarray [shape=(n,)]
         frame index or vector of frame indices
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
@@ -150,15 +148,12 @@ def frames_to_time(frames, *, sr=22050, hop_length=512, n_fft=None):
 
     Parameters
     ----------
-    frames     : np.ndarray [shape=(n,)]
+    frames : np.ndarray [shape=(n,)]
         frame index or vector of frame indices
-
-    sr         : number > 0 [scalar]
+    sr : number > 0 [scalar]
         audio sampling rate
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
@@ -243,7 +238,6 @@ def time_to_samples(times, *, sr=22050):
     ----------
     times : number or np.ndarray
         Time value or array of time values (in seconds)
-
     sr : number > 0
         Sampling rate
 
@@ -275,7 +269,6 @@ def samples_to_time(samples, *, sr=22050):
     ----------
     samples : np.ndarray
         Sample index or array of sample indices
-
     sr : number > 0
         Sampling rate
 
@@ -313,7 +306,6 @@ def blocks_to_frames(blocks, *, block_length):
     ----------
     blocks : np.ndarray
         Block index or array of block indices
-
     block_length : int > 0
         The number of frames per block
 
@@ -350,10 +342,8 @@ def blocks_to_samples(blocks, *, block_length, hop_length):
     ----------
     blocks : np.ndarray
         Block index or array of block indices
-
     block_length : int > 0
         The number of frames per block
-
     hop_length : int > 0
         The number of samples to advance between frames
 
@@ -395,13 +385,10 @@ def blocks_to_time(blocks, *, block_length, hop_length, sr):
     ----------
     blocks : np.ndarray
         Block index or array of block indices
-
     block_length : int > 0
         The number of frames per block
-
     hop_length : int > 0
         The number of samples to advance between frames
-
     sr : int > 0
         The sampling rate (samples per second)
 
@@ -457,8 +444,7 @@ def note_to_hz(note, **kwargs):
     ----------
     note : str or iterable of str
         One or more note names to convert
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters to `note_to_midi`
 
     Returns
@@ -488,7 +474,6 @@ def note_to_midi(note, *, round_midi=True):
     ----------
     note : str or iterable of str
         One or more note names.
-
     round_midi : bool
         - If ``True``, allow for fractional midi notes
         - Otherwise, round cent deviations to the nearest note
@@ -619,23 +604,22 @@ def midi_to_note(midi, *, octave=True, cents=False, key=""C:maj"", unicode=True):
     >>> librosa.midi_to_note(range(12, 24), key='F:min')
     ['C0', 'Dâ™­0', 'D0', 'Eâ™­0', 'E0', 'F0', 'Gâ™­0', 'G0', 'Aâ™­0', 'A0', 'Bâ™­0', 'B0']
 
-
     Parameters
     ----------
     midi : int or iterable of int
         Midi numbers to convert.
 
-    octave: bool
+    octave : bool
         If True, include the octave number
 
-    cents: bool
+    cents : bool
         If true, cent markers will be appended for fractional notes.
         Eg, ``midi_to_note(69.3, cents=True) == 'A4+03'``
 
     key : str
         A key signature to use when resolving enharmonic equivalences.
 
-    unicode: bool
+    unicode : bool
         If ``True`` (default), accidentals will use Unicode notation: â™­ or â™¯
 
         If ``False``, accidentals will use ASCII-compatible notation: b or #
@@ -697,12 +681,12 @@ def midi_to_hz(notes):
 
     Parameters
     ----------
-    notes       : int or np.ndarray [shape=(n,), dtype=int]
+    notes : int or np.ndarray [shape=(n,), dtype=int]
         midi number(s) of the note(s)
 
     Returns
     -------
-    frequency   : number or np.ndarray [shape=(n,), dtype=float]
+    frequency : number or np.ndarray [shape=(n,), dtype=float]
         frequency (frequencies) of ``notes`` in Hz
 
     See Also
@@ -726,12 +710,12 @@ def hz_to_midi(frequencies):
 
     Parameters
     ----------
-    frequencies   : float or np.ndarray [shape=(n,), dtype=float]
+    frequencies : float or np.ndarray [shape=(n,), dtype=float]
         frequencies to convert
 
     Returns
     -------
-    note_nums     : number or np.ndarray [shape=(n,), dtype=float]
+    note_nums : number or np.ndarray [shape=(n,), dtype=float]
         MIDI notes to ``frequencies``
 
     See Also
@@ -751,25 +735,21 @@ def hz_to_note(frequencies, **kwargs):
     ----------
     frequencies : float or iterable of float
         Input frequencies, specified in Hz
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments passed through to `midi_to_note`
 
-
     Returns
     -------
     notes : list of str
         ``notes[i]`` is the closest note name to ``frequency[i]``
         (or ``frequency`` if the input is scalar)
 
-
     See Also
     --------
     hz_to_midi
     midi_to_note
     note_to_hz
 
-
     Examples
     --------
     Get a single note name for a frequency
@@ -804,14 +784,14 @@ def hz_to_mel(frequencies, *, htk=False):
 
     Parameters
     ----------
-    frequencies   : number or np.ndarray [shape=(n,)] , float
+    frequencies : number or np.ndarray [shape=(n,)] , float
         scalar or array of frequencies
-    htk           : bool
+    htk : bool
         use HTK formula instead of Slaney
 
     Returns
     -------
-    mels        : number or np.ndarray [shape=(n,)]
+    mels : number or np.ndarray [shape=(n,)]
         input frequencies in Mels
 
     See Also
@@ -860,14 +840,14 @@ def mel_to_hz(mels, *, htk=False):
 
     Parameters
     ----------
-    mels          : np.ndarray [shape=(n,)], float
+    mels : np.ndarray [shape=(n,)], float
         mel bins to convert
-    htk           : bool
+    htk : bool
         use HTK formula instead of Slaney
 
     Returns
     -------
-    frequencies   : np.ndarray [shape=(n,)]
+    frequencies : np.ndarray [shape=(n,)]
         input mels in Hz
 
     See Also
@@ -913,18 +893,16 @@ def hz_to_octs(frequencies, *, tuning=0.0, bins_per_octave=12):
 
     Parameters
     ----------
-    frequencies   : number >0 or np.ndarray [shape=(n,)] or float
+    frequencies : number >0 or np.ndarray [shape=(n,)] or float
         scalar or vector of frequencies
-
-    tuning        : float
+    tuning : float
         Tuning deviation from A440 in (fractional) bins per octave.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    octaves       : number or np.ndarray [shape=(n,)]
+    octaves : number or np.ndarray [shape=(n,)]
         octave number for each frequency
 
     See Also
@@ -951,18 +929,16 @@ def octs_to_hz(octs, *, tuning=0.0, bins_per_octave=12):
 
     Parameters
     ----------
-    octaves       : np.ndarray [shape=(n,)] or float
+    octs : np.ndarray [shape=(n,)] or float
         octave number for each frequency
-
     tuning : float
         Tuning deviation from A440 in (fractional) bins per octave.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    frequencies   : number or np.ndarray [shape=(n,)]
+    frequencies : number or np.ndarray [shape=(n,)]
         scalar or vector of frequencies
 
     See Also
@@ -1006,15 +982,14 @@ def A4_to_tuning(A4, *, bins_per_octave=12):
 
     Parameters
     ----------
-    A4: float or np.ndarray [shape=(n,), dtype=float]
+    A4 : float or np.ndarray [shape=(n,), dtype=float]
         Reference frequency(s) corresponding to A4.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    tuning   : float or np.ndarray [shape=(n,), dtype=float]
+    tuning : float or np.ndarray [shape=(n,), dtype=float]
         Tuning deviation from A440 in (fractional) bins per octave.
 
     See Also
@@ -1059,13 +1034,12 @@ def tuning_to_A4(tuning, *, bins_per_octave=12):
     ----------
     tuning : float or np.ndarray [shape=(n,), dtype=float]
         Tuning deviation from A440 in fractional bins per octave.
-
     bins_per_octave : int > 0
         Number of bins per octave.
 
     Returns
     -------
-    A4  : float or np.ndarray [shape=(n,), dtype=float]
+    A4 : float or np.ndarray [shape=(n,), dtype=float]
         Reference frequency corresponding to A4.
 
     See Also
@@ -1082,17 +1056,14 @@ def fft_frequencies(*, sr=22050, n_fft=2048):
     ----------
     sr : number > 0 [scalar]
         Audio sampling rate
-
     n_fft : int > 0 [scalar]
         FFT window size
 
-
     Returns
     -------
     freqs : np.ndarray [shape=(1 + n_fft/2,)]
         Frequencies ``(0, sr/n_fft, 2*sr/n_fft, ..., sr/2)``
 
-
     Examples
     --------
     >>> librosa.fft_frequencies(sr=22050, n_fft=16)
@@ -1118,15 +1089,12 @@ def cqt_frequencies(n_bins, *, fmin, bins_per_octave=12, tuning=0.0):
 
     Parameters
     ----------
-    n_bins  : int > 0 [scalar]
+    n_bins : int > 0 [scalar]
         Number of constant-Q bins
-
-    fmin    : float > 0 [scalar]
+    fmin : float > 0 [scalar]
         Minimum frequency
-
     bins_per_octave : int > 0 [scalar]
         Number of bins per octave
-
     tuning : float
         Deviation from A440 tuning in fractional bins
 
@@ -1174,27 +1142,22 @@ def mel_frequencies(n_mels=128, *, fmin=0.0, fmax=11025.0, htk=False):
         Moore, G., Odell, J., Ollason, D., Povey, D., Valtchev, V., & Woodland, P.
         The HTK book, version 3.4. Cambridge University, March 2009.
 
-
     See Also
     --------
     hz_to_mel
     mel_to_hz
     librosa.feature.melspectrogram
     librosa.feature.mfcc
 
-
     Parameters
     ----------
-    n_mels    : int > 0 [scalar]
+    n_mels : int > 0 [scalar]
         Number of mel bins.
-
-    fmin      : float >= 0 [scalar]
+    fmin : float >= 0 [scalar]
         Minimum frequency (Hz).
-
-    fmax      : float >= 0 [scalar]
+    fmax : float >= 0 [scalar]
         Maximum frequency (Hz).
-
-    htk       : bool
+    htk : bool
         If True, use HTK formula to convert Hz to mel.
         Otherwise (False), use Slaney's Auditory Toolbox.
 
@@ -1237,10 +1200,8 @@ def tempo_frequencies(n_bins, *, hop_length=512, sr=22050):
     ----------
     n_bins : int > 0
         The number of lag bins
-
     hop_length : int > 0
         The number of samples between each bin
-
     sr : number > 0
         The audio sampling rate
 
@@ -1276,10 +1237,8 @@ def fourier_tempo_frequencies(*, sr=22050, win_length=384, hop_length=512):
     ----------
     sr : number > 0
         The audio sampling rate
-
     win_length : int > 0
         The number of frames per analysis window
-
     hop_length : int > 0
         The number of samples between each bin
 
@@ -1310,7 +1269,6 @@ def A_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1329,10 +1287,8 @@ def A_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the A-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1366,7 +1322,6 @@ def B_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1385,10 +1340,8 @@ def B_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the B-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1421,7 +1374,6 @@ def C_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1440,10 +1392,8 @@ def C_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     B_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the C-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1474,7 +1424,6 @@ def D_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     min_db : float [scalar] or None
         Clip weights below this threshold.
         If `None`, no clipping is performed.
@@ -1493,10 +1442,8 @@ def D_weighting(frequencies, *, min_db=-80.0):  # pylint: disable=invalid-name
     B_weighting
     C_weighting
 
-
     Examples
     --------
-
     Get the D-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1540,20 +1487,17 @@ def Z_weighting(frequencies, *, min_db=None):  # pylint: disable=invalid-name
 }
 
 
-def frequency_weighting(frequencies, *, kind=""A"", **kw):
+def frequency_weighting(frequencies, *, kind=""A"", **kwargs):
     """"""Compute the weighting of a set of frequencies.
 
     Parameters
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     kind : str in
         The weighting kind. e.g. `'A'`, `'B'`, `'C'`, `'D'`, `'Z'`
-
-    min_db : float [scalar] or None
-        Clip weights below this threshold.
-        If `None`, no clipping is performed.
+    **kwargs
+        Additional keyword arguments to A_weighting, B_weighting, etc.
 
     Returns
     -------
@@ -1569,10 +1513,8 @@ def frequency_weighting(frequencies, *, kind=""A"", **kw):
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the A-weighting for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1585,21 +1527,19 @@ def frequency_weighting(frequencies, *, kind=""A"", **kw):
     """"""
     if isinstance(kind, str):
         kind = kind.upper()
-    return WEIGHTING_FUNCTIONS[kind](frequencies, **kw)
+    return WEIGHTING_FUNCTIONS[kind](frequencies, **kwargs)
 
 
-def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kw):
+def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kwargs):
     """"""Compute multiple weightings of a set of frequencies.
 
     Parameters
     ----------
     frequencies : scalar or np.ndarray [shape=(n,)]
         One or more frequencies (in Hz)
-
     kinds : list or tuple or str
         An iterable of weighting kinds. e.g. `('Z', 'B')`, `'ZAD'`, `'C'`
-
-    **kw : keywords to pass to the weighting function.
+    **kwargs : keywords to pass to the weighting function.
 
     Returns
     -------
@@ -1616,10 +1556,8 @@ def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kw):
     C_weighting
     D_weighting
 
-
     Examples
     --------
-
     Get the A, B, C, D, and Z weightings for CQT frequencies
 
     >>> import matplotlib.pyplot as plt
@@ -1634,7 +1572,7 @@ def multi_frequency_weighting(frequencies, *, kinds=""ZAC"", **kw):
     >>> ax.legend()
     """"""
     return np.stack(
-        [frequency_weighting(frequencies, kind=k, **kw) for k in kinds], axis=0
+        [frequency_weighting(frequencies, kind=k, **kwargs) for k in kinds], axis=0
     )
 
 
@@ -1646,18 +1584,14 @@ def times_like(X, *, sr=22050, hop_length=512, n_fft=None, axis=-1):
     X : np.ndarray or scalar
         - If ndarray, X is a feature matrix, e.g. STFT, chromagram, or mel spectrogram.
         - If scalar, X represents the number of frames.
-
     sr : number > 0 [scalar]
         audio sampling rate
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
         to counteract windowing effects when using a non-centered STFT.
-
     axis : int [scalar]
         The axis representing the time axis of X.
         By default, the last axis (-1) is taken.
@@ -1669,7 +1603,8 @@ def times_like(X, *, sr=22050, hop_length=512, n_fft=None, axis=-1):
 
     See Also
     --------
-    samples_like : Return an array of sample indices to match the time axis from a feature matrix.
+    samples_like :
+        Return an array of sample indices to match the time axis from a feature matrix.
 
     Examples
     --------
@@ -1701,15 +1636,12 @@ def samples_like(X, *, hop_length=512, n_fft=None, axis=-1):
     X : np.ndarray or scalar
         - If ndarray, X is a feature matrix, e.g. STFT, chromagram, or mel spectrogram.
         - If scalar, X represents the number of frames.
-
     hop_length : int > 0 [scalar]
         number of samples between successive frames
-
     n_fft : None or int > 0 [scalar]
         Optional: length of the FFT window.
         If given, time conversion will include an offset of ``n_fft // 2``
         to counteract windowing effects when using a non-centered STFT.
-
     axis : int [scalar]
         The axis representing the time axis of ``X``.
         By default, the last axis (-1) is taken.
@@ -1721,7 +1653,8 @@ def samples_like(X, *, hop_length=512, n_fft=None, axis=-1):
 
     See Also
     --------
-    times_like : Return an array of time values to match the time axis from a feature matrix.
+    times_like :
+        Return an array of time values to match the time axis from a feature matrix.
 
     Examples
     --------"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -141,23 +141,18 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
     ----------
     x : np.ndarray
         The input energy
-
     freqs : np.ndarray, shape=(X.shape[axis])
         The frequency values corresponding to X's elements along the
         chosen axis.
-
     harmonics : list-like, non-negative
         Harmonics to compute as ``harmonics[i] * freqs``.
         The first harmonic (1) corresponds to ``freqs``.
         Values less than one (e.g., 1/2) correspond to sub-harmonics.
-
     kind : str
         Interpolation type.  See `scipy.interpolate.interp1d`.
-
     fill_value : float
         The value to fill when extrapolating beyond the observed
         frequency range.
-
     axis : int
         The axis along which to compute harmonics
 
@@ -173,7 +168,6 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
     --------
     scipy.interpolate.interp1d
 
-
     Examples
     --------
     Estimate the harmonics of a time-averaged tempogram
@@ -227,7 +221,7 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
         if not is_unique(freqs, axis=0):
             warnings.warn(
                 ""Frequencies are not unique. This may produce incorrect harmonic interpolations."",
-                stacklevel=2
+                stacklevel=2,
             )
 
         f_interp = scipy.interpolate.interp1d(
@@ -250,7 +244,7 @@ def interp_harmonics(x, *, freqs, harmonics, kind=""linear"", fill_value=0, axis=-
         if not np.all(is_unique(freqs, axis=axis)):
             warnings.warn(
                 ""Frequencies are not unique. This may produce incorrect harmonic interpolations."",
-                stacklevel=2
+                stacklevel=2,
             )
 
         # If we have time-varying frequencies, then it must match exactly the shape of the input"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -483,7 +483,7 @@ def key_to_notes(key, *, unicode=True):
 
         Examples: ``C:maj, Db:min, Aâ™­:min``.
 
-    unicode: bool
+    unicode : bool
         If ``True`` (default), use Unicode symbols (â™¯ð„ªâ™­ð„«)for accidentals.
 
         If ``False``, Unicode symbols will be mapped to low-order ASCII representations::"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -31,26 +31,20 @@ def estimate_tuning(
 
     Parameters
     ----------
-    y: np.ndarray [shape=(..., n)] or None
+    y : np.ndarray [shape=(..., n)] or None
         audio signal. Multi-channel is supported..
-
     sr : number > 0 [scalar]
         audio sampling rate of ``y``
-
-    S: np.ndarray [shape=(..., d, t)] or None
+    S : np.ndarray [shape=(..., d, t)] or None
         magnitude or power spectrogram
-
     n_fft : int > 0 [scalar] or None
         number of FFT bins to use, if ``y`` is provided.
-
     resolution : float in `(0, 1)`
         Resolution of the tuning as a fraction of a bin.
         0.01 corresponds to measurements in cents.
-
     bins_per_octave : int > 0 [scalar]
         How many frequency bins per octave
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments passed to `piptrack`
 
     Returns
@@ -63,8 +57,7 @@ def estimate_tuning(
 
     See Also
     --------
-    piptrack
-        Pitch tracking by parabolic interpolation
+    piptrack : Pitch tracking by parabolic interpolation
 
     Examples
     --------
@@ -118,11 +111,9 @@ def pitch_tuning(frequencies, *, resolution=0.01, bins_per_octave=12):
     frequencies : array-like, float
         A collection of frequencies detected in the signal.
         See `piptrack`
-
     resolution : float in `(0, 1)`
         Resolution of the tuning as a fraction of a bin.
         0.01 corresponds to cents.
-
     bins_per_octave : int > 0 [scalar]
         How many frequency bins per octave
 
@@ -133,8 +124,7 @@ def pitch_tuning(frequencies, *, resolution=0.01, bins_per_octave=12):
 
     See Also
     --------
-    estimate_tuning
-        Estimating tuning from time-series or spectrogram input
+    estimate_tuning : Estimating tuning from time-series or spectrogram input
 
     Examples
     --------
@@ -160,7 +150,9 @@ def pitch_tuning(frequencies, *, resolution=0.01, bins_per_octave=12):
     frequencies = frequencies[frequencies > 0]
 
     if not np.any(frequencies):
-        warnings.warn(""Trying to estimate tuning from empty frequency set."", stacklevel=2)
+        warnings.warn(
+            ""Trying to estimate tuning from empty frequency set."", stacklevel=2
+        )
         return 0.0
 
     # Compute the residual relative to the number of bins
@@ -204,13 +196,13 @@ def piptrack(
 
     Parameters
     ----------
-    y: np.ndarray [shape=(..., n)] or None
+    y : np.ndarray [shape=(..., n)] or None
         audio signal. Multi-channel is supported..
 
     sr : number > 0 [scalar]
         audio sampling rate of ``y``
 
-    S: np.ndarray [shape=(..., d, t)] or None
+    S : np.ndarray [shape=(..., d, t)] or None
         magnitude or power spectrogram
 
     n_fft : int > 0 [scalar] or None
@@ -387,16 +379,12 @@ def _cumulative_mean_normalized_difference(
     ----------
     y_frames : np.ndarray [shape=(frame_length, n_frames)]
         framed audio time series.
-
     frame_length : int > 0 [scalar]
-         length of the frames in samples.
-
+        length of the frames in samples.
     win_length : int > 0 [scalar]
         length of the window for calculating autocorrelation in samples.
-
     min_period : int > 0 [scalar]
         minimum period.
-
     max_period : int > 0 [scalar]
         maximum period.
 
@@ -492,43 +480,34 @@ def yin(
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported..
-
-    fmin: number > 0 [scalar]
+    fmin : number > 0 [scalar]
         minimum frequency in Hertz.
         The recommended minimum is ``librosa.note_to_hz('C2')`` (~65 Hz)
         though lower values may be feasible.
-
-    fmax: number > 0 [scalar]
+    fmax : number > 0 [scalar]
         maximum frequency in Hertz.
         The recommended maximum is ``librosa.note_to_hz('C7')`` (~2093 Hz)
         though higher values may be feasible.
-
     sr : number > 0 [scalar]
         sampling rate of ``y`` in Hertz.
-
     frame_length : int > 0 [scalar]
-         length of the frames in samples.
-         By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
-         a sampling rate of 22050 Hz.
-
+        length of the frames in samples.
+        By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
+        a sampling rate of 22050 Hz.
     win_length : None or int > 0 [scalar]
         length of the window for calculating autocorrelation in samples.
         If ``None``, defaults to ``frame_length // 2``
-
     hop_length : None or int > 0 [scalar]
-         number of audio samples between adjacent YIN predictions.
-         If ``None``, defaults to ``frame_length // 4``.
-
-    trough_threshold: number > 0 [scalar]
+        number of audio samples between adjacent YIN predictions.
+        If ``None``, defaults to ``frame_length // 4``.
+    trough_threshold : number > 0 [scalar]
         absolute threshold for peak estimation.
-
     center : boolean
         If ``True``, the signal `y` is padded so that frame
         ``D[:, t]`` is centered at `y[t * hop_length]`.
         If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.
         Defaults to ``True``,  which simplifies the alignment of ``D`` onto a
         time grid by means of ``librosa.core.frames_to_samples``.
-
     pad_mode : string or function
         If ``center=True``, this argument is passed to ``np.pad`` for padding
         the edges of the signal ``y``. By default (``pad_mode=""constant""``),
@@ -545,7 +524,7 @@ def yin(
 
     See Also
     --------
-    librosa.pyin
+    librosa.pyin :
         Fundamental frequency (F0) estimation using probabilistic YIN (pYIN).
 
     Examples
@@ -670,72 +649,55 @@ def pyin(
         ""YIN, a fundamental frequency estimator for speech and music.""
         The Journal of the Acoustical Society of America 111.4 (2002): 1917-1930.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    fmin: number > 0 [scalar]
+    fmin : number > 0 [scalar]
         minimum frequency in Hertz.
         The recommended minimum is ``librosa.note_to_hz('C2')`` (~65 Hz)
         though lower values may be feasible.
-
-    fmax: number > 0 [scalar]
+    fmax : number > 0 [scalar]
         maximum frequency in Hertz.
         The recommended maximum is ``librosa.note_to_hz('C7')`` (~2093 Hz)
         though higher values may be feasible.
-
     sr : number > 0 [scalar]
         sampling rate of ``y`` in Hertz.
-
     frame_length : int > 0 [scalar]
-         length of the frames in samples.
-         By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
-         a sampling rate of 22050 Hz.
-
+        length of the frames in samples.
+        By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
+        a sampling rate of 22050 Hz.
     win_length : None or int > 0 [scalar]
         length of the window for calculating autocorrelation in samples.
         If ``None``, defaults to ``frame_length // 2``
-
     hop_length : None or int > 0 [scalar]
         number of audio samples between adjacent pYIN predictions.
         If ``None``, defaults to ``frame_length // 4``.
-
     n_thresholds : int > 0 [scalar]
         number of thresholds for peak estimation.
-
     beta_parameters : tuple
         shape parameters for the beta distribution prior over thresholds.
-
-    boltzmann_parameter: number > 0 [scalar]
+    boltzmann_parameter : number > 0 [scalar]
         shape parameter for the Boltzmann distribution prior over troughs.
         Larger values will assign more mass to smaller periods.
-
     resolution : float in `(0, 1)`
         Resolution of the pitch bins.
         0.01 corresponds to cents.
-
     max_transition_rate : float > 0
         maximum pitch transition rate in octaves per second.
-
     switch_prob : float in ``(0, 1)``
         probability of switching from voiced to unvoiced or vice versa.
-
     no_trough_prob : float in ``(0, 1)``
         maximum probability to add to global minimum if no trough is below threshold.
-
     fill_na : None, float, or ``np.nan``
         default value for unvoiced frames of ``f0``.
         If ``None``, the unvoiced frames will contain a best guess value.
-
     center : boolean
         If ``True``, the signal ``y`` is padded so that frame
         ``D[:, t]`` is centered at ``y[t * hop_length]``.
         If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.
         Defaults to ``True``,  which simplifies the alignment of ``D`` onto a
         time grid by means of ``librosa.core.frames_to_samples``.
-
     pad_mode : string or function
         If ``center=True``, this argument is passed to ``np.pad`` for padding
         the edges of the signal ``y``. By default (``pad_mode=""constant""``),
@@ -747,18 +709,15 @@ def pyin(
     -------
     f0: np.ndarray [shape=(..., n_frames)]
         time series of fundamental frequencies in Hertz.
-
     voiced_flag: np.ndarray [shape=(..., n_frames)]
         time series containing boolean flags indicating whether a frame is voiced or not.
-
     voiced_prob: np.ndarray [shape=(..., n_frames)]
         time series containing the probability that a frame is voiced.
-
     .. note:: If multi-channel input is provided, f0 and voicing are estimated separately for each channel.
 
     See Also
     --------
-    librosa.yin
+    librosa.yin :
         Fundamental frequency (F0) estimation using the YIN algorithm.
 
     Examples
@@ -771,7 +730,6 @@ def pyin(
     ...                                              fmax=librosa.note_to_hz('C7'))
     >>> times = librosa.times_like(f0)
 
-
     Overlay F0 over a spectrogram
 
     >>> import matplotlib.pyplot as plt"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -67,7 +67,6 @@ def stft(
     The integers ``t`` and ``f`` can be converted to physical units by means
     of the utility functions `frames_to_sample` and `fft_frequencies`.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)], real-valued
@@ -143,29 +142,23 @@ def stft(
 
         .. see also:: `numpy.pad`
 
-
     Returns
     -------
     D : np.ndarray [shape=(..., 1 + n_fft/2, n_frames), dtype=dtype]
         Complex-valued matrix of short-term Fourier transform
         coefficients.
 
-
     See Also
     --------
     istft : Inverse STFT
-
     reassigned_spectrogram : Time-frequency reassigned spectrogram
 
-
     Notes
     -----
     This function caches at level 20.
 
-
     Examples
     --------
-
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
     >>> S = np.abs(librosa.stft(y))
     >>> S
@@ -180,12 +173,10 @@ def stft(
 
     >>> S_left = librosa.stft(y, center=False)
 
-
     Use a shorter hop length
 
     >>> D_short = librosa.stft(y, hop_length=64)
 
-
     Display a spectrogram
 
     >>> import matplotlib.pyplot as plt
@@ -223,7 +214,7 @@ def stft(
                 ""n_fft={} is too small for input signal of length={}"".format(
                     n_fft, y.shape[-1]
                 ),
-                stacklevel=2
+                stacklevel=2,
             )
 
         padding = [(0, 0) for _ in range(y.ndim)]
@@ -552,12 +543,11 @@ def __reassign_frequencies(
     freqs : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=real]
         Instantaneous frequencies:
         ``freqs[f, t]`` is the frequency for bin ``f``, frame ``t``.
-
     S : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=complex]
         Short-time Fourier transform
 
     Warns
-    --------
+    -----
     RuntimeWarning
         Frequencies with zero support will produce a divide-by-zero warning and
         will be returned as `np.nan`.
@@ -716,12 +706,11 @@ def __reassign_times(
     times : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=real]
         Reassigned times:
         ``times[f, t]`` is the time for bin ``f``, frame ``t``.
-
     S : np.ndarray [shape=(..., 1 + n_fft/2, t), dtype=complex]
         Short-time Fourier transform
 
     Warns
-    --------
+    -----
     RuntimeWarning
         Time estimates with zero support will produce a divide-by-zero warning
         and will be returned as `np.nan`.
@@ -964,7 +953,7 @@ def reassigned_spectrogram(
             ``mags[..., f, t]`` is the magnitude for bin ``f``, frame ``t``.
 
     Warns
-    --------
+    -----
     RuntimeWarning
         Frequency or time estimates with zero support will produce a
         divide-by-zero warning, and will be returned as `np.nan` unless
@@ -1123,26 +1112,21 @@ def magphase(D, *, power=1):
     """"""Separate a complex-valued spectrogram D into its magnitude (S)
     and phase (P) components, so that ``D = S * P``.
 
-
     Parameters
     ----------
     D : np.ndarray [shape=(..., d, t), dtype=complex]
         complex-valued spectrogram
-
     power : float > 0
         Exponent for the magnitude spectrogram,
         e.g., 1 for energy, 2 for power, etc.
 
-
     Returns
     -------
     D_mag : np.ndarray [shape=(..., d, t), dtype=real]
         magnitude of ``D``, raised to ``power``
-
     D_phase : np.ndarray [shape=(..., d, t), dtype=complex]
         ``exp(1.j * phi)`` where ``phi`` is the phase of ``D``
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -1202,7 +1186,6 @@ def phase_vocoder(D, *, rate, hop_length=None, n_fft=None):
 
     .. [#] https://breakfastquay.com/rubberband/
 
-
     Examples
     --------
     >>> # Play at double speed
@@ -1222,7 +1205,7 @@ def phase_vocoder(D, *, rate, hop_length=None, n_fft=None):
     D : np.ndarray [shape=(..., d, t), dtype=complex]
         STFT matrix
 
-    rate :  float > 0 [scalar]
+    rate : float > 0 [scalar]
         Speed-up factor: ``rate > 1`` is faster, ``rate < 1`` is slower.
 
     hop_length : int > 0 [scalar] or None
@@ -1332,44 +1315,34 @@ def iirt(
            ""Information Retrieval for Music and Motion.""
            Springer Verlag. 2007.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
     sr : number > 0 [scalar]
         sampling rate of ``y``
-
     win_length : int > 0, <= n_fft
         Window length.
-
     hop_length : int > 0 [scalar]
         Hop length, number samples between subsequent frames.
         If not supplied, defaults to ``win_length // 4``.
-
     center : boolean
         - If ``True``, the signal ``y`` is padded so that frame
           ``D[..., :, t]`` is centered at ``y[t * hop_length]``.
         - If ``False``, then `D[..., :, t]`` begins at ``y[t * hop_length]``
-
     tuning : float [scalar]
         Tuning deviation from A440 in fractions of a bin.
-
     pad_mode : string
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, this function uses zero padding.
-
     flayout : string
         - If `sos` (default), a series of second-order filters is used for filtering with `scipy.signal.sosfiltfilt`.
           Minimizes numerical precision errors for high-order filters, but is slower.
         - If `ba`, the standard difference equation is used for filtering with `scipy.signal.filtfilt`.
           Can be unstable for high-order filters.
-
     res_type : string
         The resampling mode.  See `librosa.resample` for details.
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments for `librosa.filters.semitone_filterbank`
         (e.g., could be used to provide another set of ``center_freqs`` and ``sample_rates``).
 
@@ -1540,7 +1513,6 @@ def power_to_db(S, *, ref=1.0, amin=1e-10, top_db=80.0):
     -----
     This function caches at level 30.
 
-
     Examples
     --------
     Get a power spectrogram from a waveform ``y``
@@ -1572,7 +1544,6 @@ def power_to_db(S, *, ref=1.0, amin=1e-10, top_db=80.0):
            [16.578, 16.578, ..., 16.578, 16.578],
            [16.578, 16.578, ..., 16.578, 16.578]], dtype=float32)
 
-
     And plot the results
 
     >>> import matplotlib.pyplot as plt
@@ -1633,7 +1604,6 @@ def db_to_power(S_db, *, ref=1.0):
     ----------
     S_db : np.ndarray
         dB-scaled spectrogram
-
     ref : number > 0
         Reference power: output will be scaled by this value
 
@@ -1674,7 +1644,6 @@ def amplitude_to_db(S, *, ref=1.0, amin=1e-5, top_db=80.0):
         threshold the output at ``top_db`` below the peak:
         ``max(20 * log10(S)) - top_db``
 
-
     Returns
     -------
     S_db : np.ndarray
@@ -1696,7 +1665,7 @@ def amplitude_to_db(S, *, ref=1.0, amin=1e-5, top_db=80.0):
             ""amplitude_to_db was called on complex input so phase ""
             ""information will be discarded. To suppress this warning, ""
             ""call amplitude_to_db(np.abs(S)) instead."",
-            stacklevel=2
+            stacklevel=2,
         )
 
     magnitude = np.abs(S)
@@ -1724,8 +1693,7 @@ def db_to_amplitude(S_db, *, ref=1.0):
     ----------
     S_db : np.ndarray
         dB-scaled spectrogram
-
-    ref: number > 0
+    ref : number > 0
         Optional reference power.
 
     Returns
@@ -1750,15 +1718,12 @@ def perceptual_weighting(S, frequencies, *, kind=""A"", **kwargs):
     ----------
     S : np.ndarray [shape=(..., d, t)]
         Power spectrogram
-
     frequencies : np.ndarray [shape=(d,)]
         Center frequency for each row of` `S``
-
     kind : str
         The frequency weighting curve to use.
         e.g. `'A'`, `'B'`, `'C'`, `'D'`, `None or 'Z'`
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional keyword arguments to `power_to_db`.
 
     Returns
@@ -1774,7 +1739,6 @@ def perceptual_weighting(S, frequencies, *, kind=""A"", **kwargs):
     -----
     This function caches at level 30.
 
-
     Examples
     --------
     Re-weight a CQT power spectrum, using peak power as reference
@@ -1827,7 +1791,6 @@ def fmt(y, *, t_min=0.5, n_fmt=None, kind=""cubic"", beta=0.5, over_sample=1, axis
     to scaling of the domain (e.g., time stretching or compression).  This is analogous
     to the magnitude of the Fourier transform being invariant to shifts in the input domain.
 
-
     .. [#] De Sena, Antonio, and Davide Rocchesso.
         ""A fast Mellin and scale transform.""
         EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.
@@ -1885,7 +1848,6 @@ def fmt(y, *, t_min=0.5, n_fmt=None, kind=""cubic"", beta=0.5, over_sample=1, axis
     -----
     This function caches at level 30.
 
-
     Examples
     --------
     >>> # Generate a signal and time-stretch it (with energy normalization)
@@ -2079,7 +2041,6 @@ def pcen(
        Kelling, S., and Bello, J. P. Per-Channel Energy Normalization: Why and How.
        IEEE Signal Processing Letters, 26(1), 39-43.
 
-
     Parameters
     ----------
     S : np.ndarray (non-negative)
@@ -2143,12 +2104,10 @@ def pcen(
 
         If ``False`` (default) only the PCEN values ``P`` are returned.
 
-
     Returns
     -------
     P : np.ndarray, non-negative [shape=(n, m)]
         The per-channel energy normalized version of ``S``.
-
     zf : np.ndarray (optional)
         The final filter delay values.  Only returned if ``return_zf=True``.
 
@@ -2159,7 +2118,6 @@ def pcen(
 
     Examples
     --------
-
     Compare PCEN to log amplitude (dB) scaling on Mel spectra
 
     >>> import matplotlib.pyplot as plt
@@ -2229,7 +2187,7 @@ def pcen(
             ""pcen was called on complex input so phase ""
             ""information will be discarded. To suppress this warning, ""
             ""call pcen(np.abs(D)) instead."",
-            stacklevel=2
+            stacklevel=2,
         )
         S = np.abs(S)
 
@@ -2380,7 +2338,6 @@ def griffinlim(
 
         If `None`, defaults to the current `np.random` object.
 
-
     Returns
     -------
     y : np.ndarray [shape=(..., n)]
@@ -2430,7 +2387,7 @@ def griffinlim(
         warnings.warn(
             ""Griffin-Lim with momentum={} > 1 can be unstable. ""
             ""Proceed with caution!"".format(momentum),
-            stacklevel=2
+            stacklevel=2,
         )
     elif momentum < 0:
         raise ParameterError(
@@ -2518,7 +2475,6 @@ def _spectrogram(
     This is primarily used in feature extraction functions that can operate on
     either audio time-series or spectrogram input.
 
-
     Parameters
     ----------
     y : None or np.ndarray
@@ -2561,13 +2517,11 @@ def _spectrogram(
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, STFT uses zero padding.
 
-
     Returns
     -------
     S_out : np.ndarray [dtype=np.float]
         - If ``S`` is provided as input, then ``S_out == S``
         - Else, ``S_out = |stft(y, ...)|**power``
-
     n_fft : int > 0
         - If ``S`` is provided, then ``n_fft`` is inferred from ``S``
         - Else, copied from input"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -38,7 +38,6 @@ def decompose(
     By default, this is done with with non-negative matrix factorization (NMF),
     but any `sklearn.decomposition`-type object will work.
 
-
     Parameters
     ----------
     S : np.ndarray [shape=(..., n_features, n_samples), dtype=float]
@@ -92,32 +91,27 @@ def decompose(
         If `False`, components are assumed to be pre-computed and stored
         in ``transformer``, and are not changed.
 
-    kwargs : Additional keyword arguments to the default transformer
+    **kwargs : Additional keyword arguments to the default transformer
         `sklearn.decomposition.NMF`
 
-
     Returns
     -------
     components: np.ndarray [shape=(..., n_features, n_components)]
         matrix of components (basis elements).
-
     activations: np.ndarray [shape=(n_components, n_samples)]
         transformed matrix/activation matrix
 
-
     Raises
     ------
     ParameterError
         if ``fit`` is False and no ``transformer`` object is provided.
 
         if the input array is multi-channel and ``sort=True`` is specified.
 
-
     See Also
     --------
     sklearn.decomposition : SciKit-Learn matrix decomposition modules
 
-
     Examples
     --------
     Decompose a magnitude spectrogram into 16 components with NMF
@@ -131,7 +125,6 @@ def decompose(
     >>> comps, acts = librosa.decompose.decompose(S, n_components=16,
     ...                                           sort=True)
 
-
     Or with sparse dictionary learning
 
     >>> import sklearn.decomposition
@@ -254,7 +247,6 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
         Components can be recovered by multiplying ``S * mask_H``
         or ``S * mask_P``.
 
-
     margin : float or tuple (margin_harmonic, margin_percussive)
         margin size(s) for the masks (as described in [2]_)
 
@@ -267,11 +259,9 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
     -------
     harmonic : np.ndarray [shape=(..., d, n)]
         harmonic component (or mask)
-
     percussive : np.ndarray [shape=(..., d, n)]
         percussive component (or mask)
 
-
     See Also
     --------
     librosa.util.softmask
@@ -306,7 +296,6 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
     >>> ax[2].set(title='Percussive power spectrogram')
     >>> fig.colorbar(img, ax=ax, format='%+2.0f dB')
 
-
     Or with a narrower horizontal filter
 
     >>> H, P = librosa.decompose.hpss(D, kernel_size=(13, 31))
@@ -337,7 +326,6 @@ def hpss(S, *, kernel_size=31, power=2.0, mask=False, margin=1.0):
     >>> y_perc = librosa.istft(P)
     >>> y_resi = librosa.istft(R)
 
-
     Get a more isolated percussive component by widening its margin
 
     >>> H, P = librosa.decompose.hpss(D, margin=(1.0,5.0))
@@ -441,11 +429,10 @@ def nn_filter(S, *, rec=None, aggregate=None, axis=-1, **kwargs):
         For all other aggregation functions, all neighbors
         are treated equally.
 
-
     axis : int
         The axis along which to filter (by default, columns)
 
-    kwargs
+    **kwargs
         Additional keyword arguments provided to
         `librosa.segment.recurrence_matrix` if ``rec`` is not provided
 
@@ -459,21 +446,18 @@ def nn_filter(S, *, rec=None, aggregate=None, axis=-1, **kwargs):
     ParameterError
         if ``rec`` is provided and its shape is incompatible with ``S``.
 
-    See also
+    See Also
     --------
     decompose
     hpss
     librosa.segment.recurrence_matrix
 
-
     Notes
     -----
     This function caches at level 30.
 
-
     Examples
     --------
-
     De-noise a chromagram by non-local median filtering.
     By default this would use euclidean distance to select neighbors,
     but this can be overridden directly by setting the ``metric`` parameter.
@@ -549,14 +533,11 @@ def __nn_filter_helper(R_data, R_indices, R_ptr, S, aggregate):
     ----------
     R_data, R_indices, R_ptr : np.ndarrays
         The ``data``, ``indices``, and ``indptr`` of a scipy.sparse matrix
-
     S : np.ndarray
         The observation data to filter
-
     aggregate : callable
         The aggregation operator
 
-
     Returns
     -------
     S_out : np.ndarray like S"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -566,17 +566,13 @@ def cmap(
     ----------
     data : np.ndarray
         Input data
-
     robust : bool
         If True, discard the top and bottom 2% of data when calculating
         range.
-
     cmap_seq : str
         The sequential colormap name
-
     cmap_bool : str
         The boolean colormap name
-
     cmap_div : str
         The diverging colormap name
 
@@ -731,7 +727,6 @@ def specshow(
             using `feature.fourier_tempogram`.
 
     x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]
-
         Optional positioning coordinates of the input data.
         These can be use to explicitly set the location of each
         element ``data[i, j]``, e.g., for displaying beat-synchronous
@@ -801,7 +796,7 @@ def specshow(
     ax : matplotlib.axes.Axes or None
         Axes to plot on instead of the default `plt.gca()`.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments passed through to `matplotlib.pyplot.pcolormesh`.
 
         By default, the following options are set:
@@ -815,14 +810,11 @@ def specshow(
     colormesh : `matplotlib.collections.QuadMesh`
         The color mesh object produced by `matplotlib.pyplot.pcolormesh`
 
-
     See Also
     --------
     cmap : Automatic colormap detection
-
     matplotlib.pyplot.pcolormesh
 
-
     Examples
     --------
     Visualize an STFT power spectrum using default parameters
@@ -836,7 +828,6 @@ def specshow(
     >>> ax[0].set(title='Linear-frequency power spectrogram')
     >>> ax[0].label_outer()
 
-
     Or on a logarithmic scale, and using a larger hop
 
     >>> hop_length = 1024
@@ -852,7 +843,7 @@ def specshow(
     if np.issubdtype(data.dtype, np.complexfloating):
         warnings.warn(
             ""Trying to display complex-valued input. "" ""Showing magnitude instead."",
-            stacklevel=2
+            stacklevel=2,
         )
         data = np.abs(data)
 
@@ -1326,7 +1317,6 @@ def waveshow(
         If you want to visualize both channels at the sample level, it is recommended to
         plot each signal independently.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(n,) or (2,n)]
@@ -1362,7 +1352,6 @@ def waveshow(
 
         - `None`, 'none', or 'off': ticks and tick markers are hidden.
 
-
     ax : matplotlib.axes.Axes or None
         Axes to plot on instead of the default `plt.gca()`.
 
@@ -1386,7 +1375,7 @@ def waveshow(
         The label string applied to this plot.
         Note that the label
 
-    kwargs
+    **kwargs
         Additional keyword arguments to `matplotlib.pyplot.fill_between` and
         `matplotlib.pyplot.step`.
 
@@ -1398,14 +1387,13 @@ def waveshow(
     librosa.display.AdaptiveWaveplot
         An object of type `librosa.display.AdaptiveWaveplot`
 
-    See also
+    See Also
     --------
     AdaptiveWaveplot
     matplotlib.pyplot.step
     matplotlib.pyplot.fill_between
     matplotlib.markers
 
-
     Examples
     --------
     Plot a monophonic waveform with an envelope view"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -60,21 +60,17 @@ def hpss(y, **kwargs):
     This function automates the STFT->HPSS->ISTFT pipeline, and ensures that
     the output waveforms have equal length to the input waveform ``y``.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.hpss` for details.
 
-
     Returns
     -------
     y_harmonic : np.ndarray [shape=(..., n)]
         audio time series of the harmonic elements
-
     y_percussive : np.ndarray [shape=(..., n)]
         audio time series of the percussive elements
 
@@ -84,7 +80,6 @@ def hpss(y, **kwargs):
     percussive : Extract only the percussive component
     librosa.decompose.hpss : HPSS on spectrograms
 
-
     Examples
     --------
     >>> # Extract harmonic and percussive components
@@ -116,8 +111,7 @@ def harmonic(y, **kwargs):
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.hpss` for details.
 
     Returns
@@ -161,8 +155,7 @@ def percussive(y, **kwargs):
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.hpss` for details.
 
     Returns
@@ -202,17 +195,14 @@ def percussive(y, **kwargs):
 def time_stretch(y, *, rate, **kwargs):
     """"""Time-stretch an audio series by a fixed rate.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
         audio time series. Multi-channel is supported.
-
     rate : float > 0 [scalar]
         Stretch factor.  If ``rate > 1``, then the signal is sped up.
         If ``rate < 1``, then the signal is slowed down.
-
-    kwargs : additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.stft` for details.
 
     Returns
@@ -222,9 +212,12 @@ def time_stretch(y, *, rate, **kwargs):
 
     See Also
     --------
-    pitch_shift : pitch shifting
-    librosa.phase_vocoder : spectrogram phase vocoder
-    pyrubberband.pyrb.time_stretch : high-quality time stretching using RubberBand
+    pitch_shift :
+        pitch shifting
+    librosa.phase_vocoder :
+        spectrogram phase vocoder
+    pyrubberband.pyrb.time_stretch :
+        high-quality time stretching using RubberBand
 
     Examples
     --------
@@ -288,20 +281,22 @@ def pitch_shift(
 
         See `librosa.resample` for more information.
 
-    kwargs: additional keyword arguments.
+    **kwargs : additional keyword arguments.
         See `librosa.decompose.stft` for details.
 
     Returns
     -------
     y_shift : np.ndarray [shape=(..., n)]
         The pitch-shifted audio time-series
 
-
     See Also
     --------
-    time_stretch : time stretching
-    librosa.phase_vocoder : spectrogram phase vocoder
-    pyrubberband.pyrb.pitch_shift : high-quality pitch shifting using RubberBand
+    time_stretch :
+        time stretching
+    librosa.phase_vocoder :
+        spectrogram phase vocoder
+    pyrubberband.pyrb.pitch_shift :
+        high-quality pitch shifting using RubberBand
 
     Examples
     --------
@@ -327,7 +322,10 @@ def pitch_shift(
 
     # Stretch in time, then resample
     y_shift = core.resample(
-        time_stretch(y, rate=rate, **kwargs), orig_sr=float(sr) / rate, target_sr=sr, res_type=res_type
+        time_stretch(y, rate=rate, **kwargs),
+        orig_sr=float(sr) / rate,
+        target_sr=sr,
+        res_type=res_type,
     )
 
     # Crop to the same dimension as the input
@@ -337,53 +335,44 @@ def pitch_shift(
 def remix(y, intervals, *, align_zeros=True):
     """"""Remix an audio signal by re-ordering time intervals.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., t)]
         Audio time series. Multi-channel is supported.
-
     intervals : iterable of tuples (start, end)
         An iterable (list-like or generator) where the ``i``th item
         ``intervals[i]`` indicates the start and end (in samples)
         of a slice of ``y``.
-
     align_zeros : boolean
         If ``True``, interval boundaries are mapped to the closest
         zero-crossing in ``y``.  If ``y`` is stereo, zero-crossings
         are computed after converting to mono.
 
-
     Returns
     -------
     y_remix : np.ndarray [shape=(..., d)]
         ``y`` remixed in the order specified by ``intervals``
 
-
     Examples
     --------
     Load in the example track and reverse the beats
 
     >>> y, sr = librosa.load(librosa.ex('choice'))
 
-
     Compute beats
 
     >>> _, beat_frames = librosa.beat.beat_track(y=y, sr=sr,
     ...                                          hop_length=512)
 
-
     Convert from frames to sample indices
 
     >>> beat_samples = librosa.frames_to_samples(beat_frames)
 
-
     Generate intervals from consecutive events
 
     >>> intervals = librosa.util.frame(beat_samples, frame_length=2,
     ...                                hop_length=1).T
 
-
     Reverse the beat intervals
 
     >>> y_out = librosa.effects.remix(y, intervals[::-1])
@@ -465,35 +454,28 @@ def trim(
     ----------
     y : np.ndarray, shape=(..., n)
         Audio signal. Multi-channel is supported.
-
     top_db : number > 0
         The threshold (in decibels) below reference to consider as
         silence
-
     ref : number or callable
         The reference power.  By default, it uses `np.max` and compares
         to the peak power in the signal.
-
     frame_length : int > 0
         The number of samples per analysis frame
-
     hop_length : int > 0
         The number of samples between analysis frames
-
     aggregate : callable [default: np.max]
         Function to aggregate across channels (if y.ndim > 1)
 
     Returns
     -------
     y_trimmed : np.ndarray, shape=(..., m)
         The trimmed signal
-
     index : np.ndarray, shape=(2,)
         the interval of ``y`` corresponding to the non-silent region:
         ``y_trimmed = y[index[0]:index[1]]`` (for mono) or
         ``y_trimmed = y[:, index[0]:index[1]]`` (for stereo).
 
-
     Examples
     --------
     >>> # Load some audio
@@ -544,22 +526,17 @@ def split(
     ----------
     y : np.ndarray, shape=(..., n)
         An audio signal. Multi-channel is supported.
-
     top_db : number > 0
         The threshold (in decibels) below reference to consider as
         silence
-
     ref : number or callable
         The reference power.  By default, it uses `np.max` and compares
         to the peak power in the signal.
-
     frame_length : int > 0
         The number of samples per analysis frame
-
     hop_length : int > 0
         The number of samples between analysis frames
-
-    aggregate callable [default: np.max]
+    aggregate : callable [default: np.max]
         Function to aggregate across channels (if y.ndim > 1)
 
     Returns
@@ -610,7 +587,6 @@ def preemphasis(y, *, coef=0.97, zi=None, return_zf=False):
 
         y[n] -> y[n] - coef * y[n-1]
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)]
@@ -643,7 +619,6 @@ def preemphasis(y, *, coef=0.97, zi=None, return_zf=False):
     -------
     y_out : np.ndarray
         pre-emphasized signal
-
     zf : number
         if ``return_zf=True``, the final filter state is also returned
 
@@ -731,12 +706,10 @@ def deemphasis(y, *, coef=0.97, zi=None, return_zf=False):
         If ``True``, return the final filter state.
         If ``False``, only return the pre-emphasized signal.
 
-
     Returns
     -------
     y_out : np.ndarray
         de-emphasized signal
-
     zf : number
         if ``return_zf=True``, the final filter state is also returned
 "
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -24,35 +24,28 @@ def mel_to_stft(M, *, sr=22050, n_fft=2048, power=2.0, **kwargs):
     ----------
     M : np.ndarray [shape=(..., n_mels, n), non-negative]
         The spectrogram as produced by `feature.melspectrogram`
-
     sr : number > 0 [scalar]
         sampling rate of the underlying signal
-
     n_fft : int > 0 [scalar]
         number of FFT components in the resulting STFT
-
     power : float > 0 [scalar]
         Exponent for the magnitude melspectrogram
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Mel filter bank parameters.
         See `librosa.filters.mel` for details
 
-
     Returns
     -------
     S : np.ndarray [shape=(..., n_fft, t), non-negative]
         An approximate linear magnitude spectrogram
 
-
     See Also
     --------
     librosa.feature.melspectrogram
     librosa.stft
     librosa.filters.mel
     librosa.util.nnls
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -118,47 +111,34 @@ def mel_to_audio(
     ----------
     M : np.ndarray [shape=(..., n_mels, n), non-negative]
         The spectrogram as produced by `feature.melspectrogram`
-
     sr : number > 0 [scalar]
         sampling rate of the underlying signal
-
     n_fft : int > 0 [scalar]
         number of FFT components in the resulting STFT
-
     hop_length : None or int > 0
         The hop length of the STFT.  If not provided, it will default to ``n_fft // 4``
-
     win_length : None or int > 0
         The window length of the STFT.  By default, it will equal ``n_fft``
-
     window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]
         A window specification as supported by `stft` or `istft`
-
     center : boolean
         If `True`, the STFT is assumed to use centered frames.
         If `False`, the STFT is assumed to use left-aligned frames.
-
     pad_mode : string
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, STFT uses zero padding.
-
     power : float > 0 [scalar]
         Exponent for the magnitude melspectrogram
-
     n_iter : int > 0
         The number of iterations for Griffin-Lim
-
     length : None or int > 0
         If provided, the output ``y`` is zero-padded or clipped to exactly ``length``
         samples.
-
     dtype : np.dtype
         Real numeric type for the time-domain signal.  Default is 32-bit float.
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Mel filter bank parameters
 
-
     Returns
     -------
     y : np.ndarray [shape(..., n,)]
@@ -197,7 +177,6 @@ def mfcc_to_mel(mfcc, *, n_mels=128, dct_type=2, norm=""ortho"", ref=1.0, lifter=0
         1. The inverse DCT is applied to the MFCCs
         2. `librosa.db_to_power` is applied to map the dB-scaled result to a power spectrogram
 
-
     Parameters
     ----------
     mfcc : np.ndarray [shape=(..., n_mfcc, n)]
@@ -230,7 +209,7 @@ def mfcc_to_mel(mfcc, *, n_mels=128, dct_type=2, norm=""ortho"", ref=1.0, lifter=0
         An approximate Mel power spectrum recovered from ``mfcc``
 
     Warns
-    --------
+    -----
     UserWarning
         due to critical values in lifter array that invokes underflow.
 
@@ -274,7 +253,6 @@ def mfcc_to_audio(
         1. Convert mfcc to Mel power spectrum (`mfcc_to_mel`)
         2. Convert Mel power spectrum to time-domain audio (`mel_to_audio`)
 
-
     Parameters
     ----------
     mfcc : np.ndarray [shape=(..., n_mfcc, n)]
@@ -301,7 +279,7 @@ def mfcc_to_audio(
 
             M[n, :] <- M[n, :] / (1 + sin(pi * (n + 1) / lifter)) * lifter / 2
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Parameters to pass through to `mel_to_audio`
 
     Returns"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -87,7 +87,6 @@ def tempogram(
     librosa.util.normalize
     librosa.stft
 
-
     Examples
     --------
     >>> # Compute local onset autocorrelation
@@ -194,26 +193,20 @@ def fourier_tempogram(
     ----------
     y : np.ndarray [shape=(..., n)] or None
         Audio time series.  Multi-channel is supported.
-
     sr : number > 0 [scalar]
         sampling rate of ``y``
-
     onset_envelope : np.ndarray [shape=(..., n)] or None
         Optional pre-computed onset strength envelope as provided by
         ``librosa.onset.onset_strength``.
         Multi-channel is supported.
-
     hop_length : int > 0
         number of audio samples between successive onset measurements
-
     win_length : int > 0
         length of the onset window (in frames/onset measurements)
         The default settings (384) corresponds to ``384 * hop_length / sr ~= 8.9s``.
-
     center : bool
         If `True`, onset windows are centered.
         If `False`, windows are left-aligned.
-
     window : string, function, number, tuple, or np.ndarray [shape=(win_length,)]
         A window specification as in `stft`.
 
@@ -236,7 +229,6 @@ def fourier_tempogram(
     librosa.util.normalize
     librosa.stft
 
-
     Examples
     --------
     >>> # Compute local onset autocorrelation"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -116,19 +116,15 @@ def spectral_centroid(
         If ``center=True``, the padding mode to use at the edges of the signal.
         By default, STFT uses zero padding.
 
-
     Returns
     -------
     centroid : np.ndarray [shape=(..., 1, t)]
         centroid frequencies
 
     See Also
     --------
-    librosa.stft
-        Short-time Fourier Transform
-
-    librosa.reassigned_spectrogram
-        Time-frequency reassigned spectrogram
+    librosa.stft : Short-time Fourier Transform
+    librosa.reassigned_spectrogram : Time-frequency reassigned spectrogram
 
     Examples
     --------
@@ -282,13 +278,11 @@ def spectral_bandwidth(
     p : float > 0
         Power to raise deviation from spectral centroid.
 
-
     Returns
     -------
     bandwidth : np.ndarray [shape=(..., 1, t)]
         frequency bandwidth for each frame
 
-
     Examples
     --------
     From time-series input
@@ -471,14 +465,12 @@ def spectral_contrast(
         If `False`, return the logarithmic difference:
         ``log(peaks) - log(valleys)``.
 
-
     Returns
     -------
     contrast : np.ndarray [shape=(..., n_bands + 1, t)]
         each row of spectral contrast values corresponds to a given
         octave-based frequency
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -653,7 +645,6 @@ def spectral_rolloff(
     rolloff : np.ndarray [shape=(..., 1, t)]
         roll-off frequency for each frame
 
-
     Examples
     --------
     From time-series input
@@ -814,7 +805,6 @@ def spectral_flatness(
         spectral flatness for each frame.
         The returned value is in [0, 1] and often converted to dB scale.
 
-
     Examples
     --------
     From time-series input
@@ -885,7 +875,6 @@ def rms(
     representation of energy over time because its frames can be windowed,
     thus prefer using ``S`` if it's already available.
 
-
     Parameters
     ----------
     y : np.ndarray [shape=(..., n)] or None
@@ -916,7 +905,6 @@ def rms(
     rms : np.ndarray [shape=(..., 1, t)]
         RMS value for each frame
 
-
     Examples
     --------
     >>> y, sr = librosa.load(librosa.ex('trumpet'))
@@ -1159,7 +1147,7 @@ def zero_crossing_rate(y, *, frame_length=2048, hop_length=512, center=True, **k
         This is similar to the padding in `librosa.stft`,
         but uses edge-value copies instead of zero-padding.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         See `librosa.zero_crossings`
 
         .. note:: By default, the ``pad`` parameter is set to `False`, which
@@ -1173,8 +1161,7 @@ def zero_crossing_rate(y, *, frame_length=2048, hop_length=512, center=True, **k
 
     See Also
     --------
-    librosa.zero_crossings
-        Compute zero-crossings in a time-series
+    librosa.zero_crossings : Compute zero-crossings in a time-series
 
     Examples
     --------
@@ -1281,7 +1268,7 @@ def chroma_stft(
     n_chroma : int > 0 [scalar]
         Number of chroma bins to produce (12 by default).
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to parameterize chroma filters.
         See `librosa.filters.chroma` for details.
 
@@ -1292,11 +1279,8 @@ def chroma_stft(
 
     See Also
     --------
-    librosa.filters.chroma
-        Chroma filter bank construction
-
-    librosa.util.normalize
-        Vector normalization
+    librosa.filters.chroma : Chroma filter bank construction
+    librosa.util.normalize : Vector normalization
 
     Examples
     --------
@@ -1430,7 +1414,6 @@ def chroma_cqt(
 
         If `None`, it will match ``n_chroma``.
 
-
     cqt_mode : ['full', 'hybrid']
         Constant-Q transform mode
 
@@ -1450,7 +1433,6 @@ def chroma_cqt(
     --------
     Compare a long-window STFT chromagram to the CQT chromagram
 
-
     >>> y, sr = librosa.load(librosa.ex('nutcracker'), duration=15)
     >>> chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr,
     ...                                           n_chroma=12, n_fft=4096)
@@ -1601,20 +1583,14 @@ def chroma_cens(
 
     See Also
     --------
-    chroma_cqt
-        Compute a chromagram from a constant-Q transform.
-
-    chroma_stft
-        Compute a chromagram from an STFT spectrogram or waveform.
-
-    librosa.filters.get_window
-        Compute a window function.
+    chroma_cqt : Compute a chromagram from a constant-Q transform.
+    chroma_stft : Compute a chromagram from an STFT spectrogram or waveform.
+    librosa.filters.get_window : Compute a window function.
 
     Examples
     --------
     Compare standard cqt chroma to CENS.
 
-
     >>> y, sr = librosa.load(librosa.ex('nutcracker'), duration=15)
     >>> chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
     >>> chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)
@@ -1707,7 +1683,7 @@ def tonnetz(*, y=None, sr=22050, chroma=None, **kwargs):
 
         If `None`, a cqt chromagram is performed.
 
-    kwargs
+    **kwargs
         Additional keyword arguments to `chroma_cqt`, if ``chroma`` is not
         pre-computed.
 
@@ -1726,11 +1702,8 @@ def tonnetz(*, y=None, sr=22050, chroma=None, **kwargs):
 
     See Also
     --------
-    chroma_cqt
-        Compute a chromagram from a constant-Q transform.
-
-    chroma_stft
-        Compute a chromagram from an STFT spectrogram or waveform.
+    chroma_cqt : Compute a chromagram from a constant-Q transform.
+    chroma_stft : Compute a chromagram from an STFT spectrogram or waveform.
 
     Examples
     --------
@@ -1812,7 +1785,7 @@ def mfcc(
     S : np.ndarray [shape=(..., d, t)] or None
         log-power Mel spectrogram
 
-    n_mfcc: int > 0 [scalar]
+    n_mfcc : int > 0 [scalar]
         number of MFCCs to return
 
     dct_type : {1, 2, 3}
@@ -1833,7 +1806,7 @@ def mfcc(
         Setting ``lifter >= 2 * n_mfcc`` emphasizes the higher-order coefficients.
         As ``lifter`` increases, the coefficient weighting becomes approximately linear.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to `melspectrogram`, if operating
         on time series input
 
@@ -2004,7 +1977,7 @@ def melspectrogram(
         Exponent for the magnitude melspectrogram.
         e.g., 1 for energy, 2 for power, etc.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Mel filter bank parameters.
 
         See `librosa.filters.mel` for details.
@@ -2016,12 +1989,8 @@ def melspectrogram(
 
     See Also
     --------
-    librosa.filters.mel
-        Mel filter bank construction
-
-    librosa.stft
-        Short-time Fourier Transform
-
+    librosa.filters.mel : Mel filter bank construction
+    librosa.stft : Short-time Fourier Transform
 
     Examples
     --------"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -21,32 +21,32 @@ def delta(data, *, width=9, order=1, axis=-1, mode=""interp"", **kwargs):
 
     Parameters
     ----------
-    data      : np.ndarray
+    data : np.ndarray
         the input data matrix (eg, spectrogram)
 
-    width     : int, positive, odd [scalar]
+    width : int, positive, odd [scalar]
         Number of frames over which to compute the delta features.
         Cannot exceed the length of ``data`` along the specified axis.
 
         If ``mode='interp'``, then ``width`` must be at least ``data.shape[axis]``.
 
-    order     : int > 0 [scalar]
+    order : int > 0 [scalar]
         the order of the difference operator.
         1 for first derivative, 2 for second, etc.
 
-    axis      : int [scalar]
+    axis : int [scalar]
         the axis along which to compute deltas.
         Default is -1 (columns).
 
     mode : str, {'interp', 'nearest', 'mirror', 'constant', 'wrap'}
         Padding mode for estimating differences at the boundaries.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         See `scipy.signal.savgol_filter`
 
     Returns
     -------
-    delta_data   : np.ndarray [shape=(..., t)]
+    delta_data : np.ndarray [shape=(..., t)]
         delta matrix of ``data`` at specified order
 
     Notes
@@ -133,7 +133,6 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
     overridden by supplying additional keyword arguments which are passed
     to `np.pad()`.
 
-
     Parameters
     ----------
     data : np.ndarray [shape=(..., d, t)]
@@ -150,8 +149,8 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
 
         Negative values embed from the future (subsequent columns).
 
-    kwargs : additional keyword arguments
-      Additional arguments to pass to `numpy.pad`
+    **kwargs : additional keyword arguments
+        Additional arguments to pass to `numpy.pad`
 
     Returns
     -------
@@ -163,7 +162,6 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
     -----
     This function caches at level 40.
 
-
     Examples
     --------
     Keep two steps (current and previous)
@@ -214,8 +212,6 @@ def stack_memory(data, *, n_steps=2, delay=1, **kwargs):
     >>> ax.text(1.0, 1/6, ""Lag=0"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
     >>> ax.text(1.0, 3/6, ""Lag=1"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
     >>> ax.text(1.0, 5/6, ""Lag=2"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
-    >>> ax.axline((0, 1/3), (1, 1/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
-    >>> ax.axline((0, 2/3), (1, 2/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
     >>> ax.set(title='Time-lagged chroma', ylabel="""")
     """"""
 
@@ -270,11 +266,8 @@ def __stack(history, data, n_steps, delay):
     Parameters
     ----------
     history : output array (2-dimensional)
-
     data : pre-padded input array (2-dimensional)
-
     n_steps : int > 0, the number of steps to stack
-
     delay : int != 0, the amount of delay between steps
 
     Returns"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -140,23 +140,23 @@ def mel(
 
     Parameters
     ----------
-    sr        : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of the incoming signal
 
-    n_fft     : int > 0 [scalar]
+    n_fft : int > 0 [scalar]
         number of FFT components
 
-    n_mels    : int > 0 [scalar]
+    n_mels : int > 0 [scalar]
         number of Mel bands to generate
 
-    fmin      : float >= 0 [scalar]
+    fmin : float >= 0 [scalar]
         lowest frequency (in Hz)
 
-    fmax      : float >= 0 [scalar]
+    fmax : float >= 0 [scalar]
         highest frequency (in Hz).
         If `None`, use ``fmax = sr / 2.0``
 
-    htk       : bool [scalar]
+    htk : bool [scalar]
         use HTK formula instead of Slaney
 
     norm : {None, 'slaney', or number} [scalar]
@@ -175,10 +175,10 @@ def mel(
 
     Returns
     -------
-    M         : np.ndarray [shape=(n_mels, 1 + n_fft/2)]
+    M : np.ndarray [shape=(n_mels, 1 + n_fft/2)]
         Mel transform matrix
 
-    See also
+    See Also
     --------
     librosa.util.normalize
 
@@ -196,7 +196,6 @@ def mel(
            [ 0.   ,  0.   , ...,  0.   ,  0.   ],
            [ 0.   ,  0.   , ...,  0.   ,  0.   ]])
 
-
     Clip the maximum frequency to 8KHz
 
     >>> librosa.filters.mel(sr=22050, n_fft=2048, fmax=8000)
@@ -206,7 +205,6 @@ def mel(
            [ 0.  ,  0.  , ...,  0.  ,  0.  ],
            [ 0.  ,  0.  , ...,  0.  ,  0.  ]])
 
-
     >>> import matplotlib.pyplot as plt
     >>> fig, ax = plt.subplots()
     >>> img = librosa.display.specshow(melfb, x_axis='linear', ax=ax)
@@ -277,24 +275,23 @@ def chroma(
     This creates a linear transformation matrix to project
     FFT bins onto chroma bins (i.e. pitch classes).
 
-
     Parameters
     ----------
-    sr        : number > 0 [scalar]
+    sr : number > 0 [scalar]
         audio sampling rate
 
-    n_fft     : int > 0 [scalar]
+    n_fft : int > 0 [scalar]
         number of FFT bins
 
-    n_chroma  : int > 0 [scalar]
+    n_chroma : int > 0 [scalar]
         number of chroma bins
 
     tuning : float
         Tuning deviation from A440 in fractions of a chroma bin.
 
-    ctroct    : float > 0 [scalar]
+    ctroct : float > 0 [scalar]
 
-    octwidth  : float > 0 or None [scalar]
+    octwidth : float > 0 or None [scalar]
         ``ctroct`` and ``octwidth`` specify a dominance window:
         a Gaussian weighting centered on ``ctroct`` (in octs, A0 = 27.5Hz)
         and with a gaussian half-width of ``octwidth``.
@@ -346,7 +343,6 @@ def chroma(
            [  1.162e-05,   2.372e-04, ...,   6.417e-38,   9.923e-38],
            [  1.180e-05,   2.260e-04, ...,   4.697e-50,   7.772e-50]])
 
-
     Equally weight all octaves
 
     >>> librosa.filters.chroma(sr=22050, n_fft=4096, octwidth=None)
@@ -461,6 +457,9 @@ def constant_q(
     Frequencies are spaced geometrically, increasing by a factor of
     ``(2**(1./bins_per_octave))`` at each successive band.
 
+    .. warning:: This function is deprecated as of v0.9 and will be removed in 1.0.
+        See `librosa.filters.wavelet`.
+
     Parameters
     ----------
     sr : number > 0 [scalar]
@@ -500,14 +499,13 @@ def constant_q(
         The data type of the output basis.
         By default, uses 64-bit (single precision) complex floating point.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to `np.pad()` when ``pad==True``.
 
     Returns
     -------
     filters : np.ndarray, ``len(filters) == n_bins``
         ``filters[i]`` is ``i``\ th time-domain CQT basis filter
-
     lengths : np.ndarray, ``len(lengths) == n_bins``
         The (fractional) length of each filter
 
@@ -523,7 +521,6 @@ def constant_q(
     librosa.vqt
     librosa.util.normalize
 
-
     Examples
     --------
     Use a shorter window for each filter
@@ -605,25 +602,26 @@ def constant_q_lengths(
 ):
     r""""""Return length of each filter in a constant-Q basis.
 
+    .. warning:: This function is deprecated as of v0.9 and will be removed in 1.0.
+        See `librosa.filters.wavelet_lengths`.
+
     Parameters
     ----------
     sr : number > 0 [scalar]
         Audio sampling rate
-
     fmin : float > 0 [scalar]
         Minimum frequency bin.
-
     n_bins : int > 0 [scalar]
         Number of frequencies.  Defaults to 7 octaves (84 bins).
-
     bins_per_octave : int > 0 [scalar]
         Number of bins per octave
-
     window : str or callable
         Window function to use on filters
-
     filter_scale : float > 0 [scalar]
         Resolution of filter windows. Larger values use longer windows.
+    gamma : number >= 0
+        Bandwidth offset for variable-Q transforms.
+        ``gamma=0`` produces a constant-Q filterbank.
 
     Returns
     -------
@@ -738,7 +736,6 @@ def wavelet_lengths(
     -------
     lengths : np.ndarray
         The length of each filter.
-
     f_cutoff : float
         The lowest frequency at which all filters' main lobes have decayed by
         at least 3dB.
@@ -828,7 +825,6 @@ def wavelet(
 
     Parameters
     ----------
-
     freqs : np.ndarray (positive)
         Center frequencies of the filters (in Hz).
         Must be in ascending order.
@@ -867,14 +863,13 @@ def wavelet(
 
         If two or more frequencies are provided, this parameter is ignored.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Arguments to `np.pad()` when ``pad==True``.
 
     Returns
     -------
     filters : np.ndarray, ``len(filters) == n_bins``
         each ``filters[i]`` is a (complex) time-domain filter
-
     lengths : np.ndarray, ``len(lengths) == n_bins``
         The (fractional) length of each filter in samples
 
@@ -889,7 +884,6 @@ def wavelet(
     librosa.vqt
     librosa.util.normalize
 
-
     Examples
     --------
     Create a constant-Q basis
@@ -973,35 +967,27 @@ def cq_to_chroma(
     """"""Construct a linear transformation matrix to map Constant-Q bins
     onto chroma bins (i.e., pitch classes).
 
-
     Parameters
     ----------
     n_input : int > 0 [scalar]
         Number of input components (CQT bins)
-
     bins_per_octave : int > 0 [scalar]
         How many bins per octave in the CQT
-
     n_chroma : int > 0 [scalar]
         Number of output bins (per octave) in the chroma
-
     fmin : None or float > 0
         Center frequency of the first constant-Q channel.
         Default: 'C1' ~= 32.7 Hz
-
     window : None or np.ndarray
         If provided, the cq_to_chroma filter bank will be
         convolved with ``window``.
-
     base_c : bool
         If True, the first chroma bin will start at 'C'
         If False, the first chroma bin will start at 'A'
-
     dtype : np.dtype
         The data type of the output basis.
         By default, uses 32-bit (single-precision) floating point.
 
-
     Returns
     -------
     cq_to_chroma : np.ndarray [shape=(n_chroma, n_input)]
@@ -1097,15 +1083,13 @@ def cq_to_chroma(
 def window_bandwidth(window, n=1000):
     """"""Get the equivalent noise bandwidth of a window function.
 
-
     Parameters
     ----------
     window : callable or string
         A window function, or the name of a window function.
         Examples:
         - scipy.signal.hann
         - 'boxcar'
-
     n : int > 0
         The number of coefficients to use in estimating the
         window bandwidth
@@ -1222,7 +1206,6 @@ def _multirate_fb(
 
      This implementation uses `scipy.signal.iirdesign` to design the filters.
 
-
     Parameters
     ----------
     center_freqs : np.ndarray [shape=(n,), dtype=float]
@@ -1260,12 +1243,10 @@ def _multirate_fb(
 
         - If `zpk`, returns zeros, poles, and system gains of the transfer functions.
 
-
     Returns
     -------
     filterbank : list [shape=(n,), dtype=float]
         Each list entry comprises the filter coefficients for a single filter.
-
     sample_rates : np.ndarray [shape=(n,), dtype=float]
         Samplerate for each filter.
 
@@ -1340,7 +1321,6 @@ def mr_frequencies(tuning):
            ""Information Retrieval for Music and Motion.""
            Springer Verlag. 2007.
 
-
     Parameters
     ----------
     tuning : float [scalar]
@@ -1352,15 +1332,13 @@ def mr_frequencies(tuning):
     center_freqs : np.ndarray [shape=(n,), dtype=float]
         Center frequencies of the filter kernels.
         Also defines the number of filters in the filterbank.
-
     sample_rates : np.ndarray [shape=(n,), dtype=float]
         Sample rate for each filter, used for multirate filterbank.
 
     Notes
     -----
     This function caches at level 10.
 
-
     See Also
     --------
     librosa.filters.semitone_filterbank
@@ -1409,34 +1387,28 @@ def semitone_filterbank(
            ""Information Retrieval for Music and Motion.""
            Springer Verlag. 2007.
 
-
     Parameters
     ----------
     center_freqs : np.ndarray [shape=(n,), dtype=float]
         Center frequencies of the filter kernels.
         Also defines the number of filters in the filterbank.
-
     tuning : float [scalar]
         Tuning deviation from A440 as a fraction of a semitone (1/12 of an octave
         in equal temperament).
-
     sample_rates : np.ndarray [shape=(n,), dtype=float]
         Sample rates of each filter in the multirate filterbank.
-
     flayout : string
         - If `ba`, the standard difference equation is used for filtering with `scipy.signal.filtfilt`.
           Can be unstable for high-order filters.
         - If `sos`, a series of second-order filters is used for filtering with `scipy.signal.sosfiltfilt`.
           Minimizes numerical precision errors for high-order filters, but is slower.
-
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments to the private function `_multirate_fb()`.
 
     Returns
     -------
     filterbank : list [shape=(n,), dtype=float]
         Each list entry contains the filter coefficients for a single filter.
-
     fb_sample_rates : np.ndarray [shape=(n,), dtype=float]
         Sample rate for each filter.
 
@@ -1501,21 +1473,19 @@ def window_sumsquare(
     ----------
     window : string, tuple, number, callable, or list-like
         Window specification, as in `get_window`
-
     n_frames : int > 0
         The number of analysis frames
-
     hop_length : int > 0
         The number of samples to advance between frames
-
     win_length : [optional]
         The length of the window function.  By default, this matches ``n_fft``.
-
     n_fft : int > 0
         The length of each analysis frame.
-
     dtype : np.dtype
         The data type of the output
+    norm : {np.inf, -np.inf, 0, float > 0, None}
+        Normalization mode used in window construction.
+        Note that this does not affect the squaring operation.
 
     Returns
     -------
@@ -1591,13 +1561,11 @@ def diagonal_filter(window, n, *, slope=1.0, angle=None, zero_mean=False):
         This should be enabled if you want to enhance paths and suppress
         blocks.
 
-
     Returns
     -------
     kernel : np.ndarray, shape=[(m, m)]
         The 2-dimensional filter kernel
 
-
     Notes
     -----
     This function caches at level 10."
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -44,16 +44,15 @@ def onset_detect(
 
     .. [#] https://github.com/CPJKU/onset_db
 
-
     Parameters
     ----------
-    y          : np.ndarray [shape=(n,)]
+    y : np.ndarray [shape=(n,)]
         audio time series, must be monophonic
 
-    sr         : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-    onset_envelope     : np.ndarray [shape=(m,)]
+    onset_envelope : np.ndarray [shape=(m,)]
         (optional) pre-computed onset strength envelope
 
     hop_length : int > 0 [scalar]
@@ -80,15 +79,13 @@ def onset_detect(
 
         Otherwise, the onset envelope is left unnormalized.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters for peak picking.
 
         See `librosa.util.peak_pick` for details.
 
-
     Returns
     -------
-
     onsets : np.ndarray [shape=(n_onsets,)]
         estimated positions of detected onsets, in whichever units
         are specified.  By default, frame indices.
@@ -97,7 +94,6 @@ def onset_detect(
             If no onset strength could be detected, onset_detect returns
             an empty list.
 
-
     Raises
     ------
     ParameterError
@@ -111,7 +107,6 @@ def onset_detect(
     onset_backtrack : backtracking onset events
     librosa.util.peak_pick : pick peaks from a time series
 
-
     Examples
     --------
     Get onset times from a signal
@@ -127,7 +122,6 @@ def onset_detect(
     >>> times = librosa.times_like(o_env, sr=sr)
     >>> onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)
 
-
     >>> import matplotlib.pyplot as plt
     >>> D = np.abs(librosa.stft(y))
     >>> fig, ax = plt.subplots(nrows=2, sharex=True)
@@ -224,16 +218,16 @@ def onset_strength(
 
     Parameters
     ----------
-    y        : np.ndarray [shape=(..., n)]
+    y : np.ndarray [shape=(..., n)]
         audio time-series. Multi-channel is supported.
 
-    sr       : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-    S        : np.ndarray [shape=(..., d, m)]
+    S : np.ndarray [shape=(..., d, m)]
         pre-computed (log-power) spectrogram
 
-    lag      : int > 0
+    lag : int > 0
         time lag for computing differences
 
     max_size : int > 0
@@ -263,31 +257,27 @@ def onset_strength(
 
         Default: `np.mean`
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters to ``feature()``, if ``S`` is not provided.
 
-
     Returns
     -------
-    onset_envelope   : np.ndarray [shape=(..., m,)]
+    onset_envelope : np.ndarray [shape=(..., m,)]
         vector containing the onset strength envelope.
         If the input contains multiple channels, then onset envelope is computed for each channel.
 
-
     Raises
     ------
     ParameterError
         if neither ``(y, sr)`` nor ``S`` are provided
 
         or if ``lag`` or ``max_size`` are not positive integers
 
-
     See Also
     --------
     onset_detect
     onset_strength_multi
 
-
     Examples
     --------
     First, load some audio and plot the spectrogram
@@ -308,7 +298,6 @@ def onset_strength(
     >>> ax[1].plot(times, 2 + onset_env / onset_env.max(), alpha=0.8,
     ...            label='Mean (mel)')
 
-
     Median aggregation, and custom mel options
 
     >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,
@@ -317,7 +306,6 @@ def onset_strength(
     >>> ax[1].plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,
     ...            label='Median (custom mel)')
 
-
     Constant-Q spectrogram instead of Mel
 
     >>> C = np.abs(librosa.cqt(y=y, sr=sr))
@@ -370,7 +358,6 @@ def onset_backtrack(events, energy):
     ----------
     events : np.ndarray, dtype=int
         List of onset event frame indices, as computed by `onset_detect`
-
     energy : np.ndarray, shape=(m,)
         An energy function
 
@@ -451,16 +438,15 @@ def onset_strength_multi(
 
         mean_{f in channels[i]} max(0, S[f, t+1] - S[f, t])
 
-
     Parameters
     ----------
-    y        : np.ndarray [shape=(..., n,)]
+    y : np.ndarray [shape=(..., n,)]
         audio time-series. Multi-channel is supported.
 
-    sr       : number > 0 [scalar]
+    sr : number > 0 [scalar]
         sampling rate of ``y``
 
-    S        : np.ndarray [shape=(..., d, m)]
+    S : np.ndarray [shape=(..., d, m)]
         pre-computed (log-power) spectrogram
 
     n_fft : int > 0 [scalar]
@@ -469,7 +455,7 @@ def onset_strength_multi(
     hop_length : int > 0 [scalar]
         hop length for use in ``feature()`` if ``S`` is not provided.
 
-    lag      : int > 0
+    lag : int > 0
         time lag for computing differences
 
     max_size : int > 0
@@ -507,22 +493,19 @@ def onset_strength_multi(
         Array of channel boundaries or slice objects.
         If `None`, then a single channel is generated to span all bands.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional parameters to ``feature()``, if ``S`` is not provided.
 
-
     Returns
     -------
-    onset_envelope   : np.ndarray [shape=(..., n_channels, m)]
+    onset_envelope : np.ndarray [shape=(..., n_channels, m)]
         array containing the onset strength envelope for each specified channel
 
-
     Raises
     ------
     ParameterError
         if neither ``(y, sr)`` nor ``S`` are provided
 
-
     See Also
     --------
     onset_strength"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -70,7 +70,6 @@ def cross_similarity(
     The output is a matrix ``xsim``, where ``xsim[i, j]`` is non-zero
     if ``data_ref[..., i]`` is a k-nearest neighbor of ``data[..., j]``.
 
-
     Parameters
     ----------
     data : np.ndarray [shape=(..., d, n)]
@@ -302,7 +301,6 @@ def recurrence_matrix(
 
     The general term *recurrence matrix* can refer to any of the three forms above.
 
-
     Parameters
     ----------
     data : np.ndarray [shape=(..., d, n)]
@@ -648,7 +646,6 @@ def lag_to_recurrence(lag, *, axis=-1):
     ----------
     lag : np.ndarray or scipy.sparse.spmatrix
         A lag matrix, as produced by ``recurrence_to_lag``
-
     axis : int
         The axis corresponding to the time dimension.
         The alternate axis will be interpreted in lag coordinates.
@@ -735,25 +732,20 @@ def timelag_filter(function, pad=True, index=0):
     ----------
     function : callable
         The filtering function to wrap, e.g., `scipy.ndimage.median_filter`
-
     pad : bool
         Whether to zero-pad the structure feature matrix
-
     index : int >= 0
         If ``function`` accepts input data as a positional argument, it should be
         indexed by ``index``
 
-
     Returns
     -------
     wrapped_function : callable
         A new filter function which applies in time-lag space rather than
         time-time space.
 
-
     Examples
     --------
-
     Apply a 31-bin median filter to the diagonal of a recurrence matrix.
     With default, parameters, this corresponds to a time window of about
     0.72 seconds.
@@ -820,16 +812,13 @@ def subsegment(data, frames, *, n_segments=4, axis=-1):
     ----------
     data : np.ndarray
         Data matrix to use in clustering
-
     frames : np.ndarray [shape=(n_boundaries,)], dtype=int, non-negative]
         Array of beat or segment boundaries, as provided by
         `librosa.beat.beat_track`,
         `librosa.onset.onset_detect`,
         or `agglomerative`.
-
     n_segments : int > 0
         Maximum number of frames to sub-divide each interval.
-
     axis : int
         Axis along which to apply the segmentation.
         By default, the last index (-1) is taken.
@@ -902,16 +891,13 @@ def agglomerative(data, k, *, clusterer=None, axis=-1):
 
     Parameters
     ----------
-    data     : np.ndarray
+    data : np.ndarray
         data to cluster
-
-    k        : int > 0 [scalar]
+    k : int > 0 [scalar]
         number of segments to produce
-
     clusterer : sklearn.cluster.AgglomerativeClustering, optional
         An optional AgglomerativeClustering object.
         If `None`, a constrained Ward object is instantiated.
-
     axis : int
         axis along which to cluster.
         By default, the last axis (-1) is chosen.
@@ -1067,10 +1053,9 @@ def path_enhance(
         If True, the smoothed similarity matrix will be thresholded at 0, and will not contain
         negative entries.
 
-    kwargs : additional keyword arguments
+    **kwargs : additional keyword arguments
         Additional arguments to pass to `scipy.ndimage.convolve`
 
-
     Returns
     -------
     R_smooth : np.ndarray, shape=R.shape
@@ -1081,7 +1066,6 @@ def path_enhance(
     librosa.filters.diagonal_filter
     recurrence_matrix
 
-
     Examples
     --------
     Use a 51-frame diagonal smoothing filter to enhance paths in a recurrence matrix"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -128,12 +128,10 @@ def dtw(
         accumulated cost matrix.
         D[N, M] is the total alignment cost.
         When doing subsequence DTW, D[N,:] indicates a matching function.
-
     wp : np.ndarray [shape=(N, 2)]
         Warping path with index pairs.
         Each row of the array contains an index pair (n, m).
         Only returned when ``backtrack`` is True.
-
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
@@ -363,26 +361,19 @@ def __dtw_calc_accu_cost(
     ----------
     C : np.ndarray [shape=(N, M)]
         pre-computed cost matrix
-
     D : np.ndarray [shape=(N, M)]
         accumulated cost matrix
-
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
-
     step_sizes_sigma : np.ndarray [shape=[n, 2]]
         Specifies allowed step sizes as used by the dtw.
-
     weights_add : np.ndarray [shape=[n, ]]
         Additive weights to penalize certain step sizes.
-
     weights_mul : np.ndarray [shape=[n, ]]
         Multiplicative weights to penalize certain step sizes.
-
     max_0 : int
         maximum number of steps in step_sizes_sigma in dim 0.
-
     max_1 : int
         maximum number of steps in step_sizes_sigma in dim 1.
 
@@ -392,7 +383,6 @@ def __dtw_calc_accu_cost(
         accumulated cost matrix.
         D[N, M] is the total alignment cost.
         When doing subsequence DTW, D[N,:] indicates a matching function.
-
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
@@ -433,19 +423,15 @@ def __dtw_backtracking(steps, step_sizes_sigma, subseq, start=None):  # pragma:
     step to backtrack the index pairs for an optimal
     warping path.
 
-
     Parameters
     ----------
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
-
     step_sizes_sigma : np.ndarray [shape=[n, 2]]
         Specifies allowed step sizes as used by the dtw.
-
     subseq : bool
         Enable subsequence DTW, e.g., for retrieval tasks.
-
     start : int
         Start column index for backtraing (only allowed for ``subseq=True``)
 
@@ -499,19 +485,15 @@ def dtw_backtracking(steps, *, step_sizes_sigma=None, subseq=False, start=None):
     Uses the saved step sizes from the cost accumulation
     step to backtrack the index pairs for a warping path.
 
-
     Parameters
     ----------
     steps : np.ndarray [shape=(N, M)]
         Step matrix, containing the indices of the used steps from the cost
         accumulation step.
-
     step_sizes_sigma : np.ndarray [shape=[n, 2]]
         Specifies allowed step sizes as used by the dtw.
-
     subseq : bool
         Enable subsequence DTW, e.g., for retrieval tasks.
-
     start : int
         Start column index for backtraing (only allowed for ``subseq=True``)
 
@@ -583,7 +565,6 @@ def rqa(sim, *, gap_onset=1, gap_extend=1, knight_moves=True, backtrack=True):
     Note that setting ``gap_onset`` and ``gap_extend`` to `np.inf` recovers the second
     method, and disabling knight moves recovers the first.
 
-
     .. [#] SerrÃ , Joan, Xavier Serra, and Ralph G. Andrzejak.
         ""Cross recurrence quantification for cover song identification.""
         New Journal of Physics 11, no. 9 (2009): 093017.
@@ -592,7 +573,6 @@ def rqa(sim, *, gap_onset=1, gap_extend=1, knight_moves=True, backtrack=True):
         ""Recurrence plots of dynamical systems.""
         World Scientific Series on Nonlinear Science Series A 16 (1995): 441-446.
 
-
     Parameters
     ----------
     sim : np.ndarray [shape=(N, M), non-negative]
@@ -623,7 +603,6 @@ def rqa(sim, *, gap_onset=1, gap_extend=1, knight_moves=True, backtrack=True):
     score : np.ndarray [shape=(N, M)]
         The alignment score matrix.  ``score[n, m]`` is the cumulative value of
         the best alignment sequence ending in frames ``n`` and ``m``.
-
     path : np.ndarray [shape=(k, 2)] (optional)
         If ``backtrack=True``, ``path`` contains a list of pairs of aligned frames
         in the best alignment sequence.
@@ -907,11 +886,9 @@ def _viterbi(log_prob, log_trans, log_p_init):  # pragma: no cover
     log_prob : np.ndarray [shape=(T, m)]
         ``log_prob[t, s]`` is the conditional log-likelihood
         ``log P[X = X(t) | State(t) = s]``
-
     log_trans : np.ndarray [shape=(m, m)]
         The log transition matrix
         ``log_trans[i, j] = log P[State(t+1) = j | State(t) = i]``
-
     log_p_init : np.ndarray [shape=(m,)]
         log of the initial state distribution
 
@@ -979,27 +956,22 @@ def viterbi(prob, transition, *, p_init=None, return_logp=False):
     prob : np.ndarray [shape=(..., n_states, n_steps), non-negative]
         ``prob[..., s, t]`` is the probability of observation at time ``t``
         being generated by state ``s``.
-
     transition : np.ndarray [shape=(n_states, n_states), non-negative]
         ``transition[i, j]`` is the probability of a transition from i->j.
         Each row must sum to 1.
-
     p_init : np.ndarray [shape=(n_states,)]
         Optional: initial state distribution.
         If not provided, a uniform distribution is assumed.
-
     return_logp : bool
         If ``True``, return the log-likelihood of the state sequence.
 
     Returns
     -------
     Either ``states`` or ``(states, logp)``:
-
     states : np.ndarray [shape=(..., n_steps,)]
         The most likely state sequence.
         If ``prob`` contains multiple channels of input, then each channel is
         decoded independently.
-
     logp : scalar [float] or np.ndarray
         If ``return_logp=True``, the log probability of ``states`` given
         the observations.
@@ -1008,7 +980,6 @@ def viterbi(prob, transition, *, p_init=None, return_logp=False):
     --------
     viterbi_discriminative : Viterbi decoding from state likelihoods
 
-
     Examples
     --------
     Example from https://en.wikipedia.org/wiki/Viterbi_algorithm#Example
@@ -1126,40 +1097,36 @@ def viterbi_discriminative(
         ``prob[s, t]`` is the probability of state ``s`` conditional on
         the observation at time ``t``.
         Must be non-negative and sum to 1 along each column.
-
     transition : np.ndarray [shape=(n_states, n_states), non-negative]
         ``transition[i, j]`` is the probability of a transition from i->j.
         Each row must sum to 1.
-
     p_state : np.ndarray [shape=(n_states,)]
         Optional: marginal probability distribution over states,
         must be non-negative and sum to 1.
         If not provided, a uniform distribution is assumed.
-
     p_init : np.ndarray [shape=(n_states,)]
         Optional: initial state distribution.
         If not provided, it is assumed to be uniform.
-
     return_logp : bool
         If ``True``, return the log-likelihood of the state sequence.
 
     Returns
     -------
     Either ``states`` or ``(states, logp)``:
-
     states : np.ndarray [shape=(..., n_steps,)]
         The most likely state sequence.
         If ``prob`` contains multiple input channels,
         then each channel is decoded independently.
-
     logp : scalar [float] or np.ndarray
         If ``return_logp=True``, the log probability of ``states`` given
         the observations.
 
     See Also
     --------
-    viterbi : Viterbi decoding from observation likelihoods
-    viterbi_binary: Viterbi decoding for multi-label, conditional state likelihoods
+    viterbi :
+        Viterbi decoding from observation likelihoods
+    viterbi_binary :
+        Viterbi decoding for multi-label, conditional state likelihoods
 
     Examples
     --------
@@ -1363,18 +1330,18 @@ def viterbi_binary(prob, transition, *, p_state=None, p_init=None, return_logp=F
     Returns
     -------
     Either ``states`` or ``(states, logp)``:
-
     states : np.ndarray [shape=(..., n_states, n_steps)]
         The most likely state sequence.
-
     logp : np.ndarray [shape=(..., n_states,)]
         If ``return_logp=True``, the log probability of each state activation
         sequence ``states``
 
     See Also
     --------
-    viterbi : Viterbi decoding from observation likelihoods
-    viterbi_discriminative : Viterbi decoding for discriminative (mutually exclusive) state predictions
+    viterbi :
+        Viterbi decoding from observation likelihoods
+    viterbi_discriminative :
+        Viterbi decoding for discriminative (mutually exclusive) state predictions
 
     Examples
     --------
@@ -1485,7 +1452,6 @@ def transition_uniform(n_states):
 
     Examples
     --------
-
     >>> librosa.sequence.transition_uniform(3)
     array([[0.333, 0.333, 0.333],
            [0.333, 0.333, 0.333],
@@ -1667,7 +1633,6 @@ def transition_local(n_states, width, *, window=""triangle"", wrap=False):
             so and effectively have ``width-2`` non-zero values.  You may have to expand
             ``width`` to get the desired behavior.
 
-
     wrap : bool
         If ``True``, then state locality ``|i - j|`` is computed modulo ``n_states``.
         If ``False`` (default), then locality is absolute.
@@ -1683,7 +1648,6 @@ def transition_local(n_states, width, *, window=""triangle"", wrap=False):
 
     Examples
     --------
-
     Triangular distributions with and without wrapping
 
     >>> librosa.sequence.transition_local(5, 3, window='triangle', wrap=False)
@@ -1729,7 +1693,9 @@ def transition_local(n_states, width, *, window=""triangle"", wrap=False):
 
     # Fill in the widths.  This is inefficient, but simple
     for i, width_i in enumerate(width):
-        trans_row = pad_center(get_window(window, width_i, fftbins=False), size=n_states)
+        trans_row = pad_center(
+            get_window(window, width_i, fftbins=False), size=n_states
+        )
         trans_row = np.roll(trans_row, n_states // 2 + i + 1)
 
         if not wrap:"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -44,14 +44,11 @@ def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):
     ----------
     A : np.ndarray [shape=(m, d)]
         The basis matrix
-
     B : np.ndarray [shape=(m, N)]
         The regression targets
-
     x_init : np.ndarray [shape=(d, N)]
         An initial guess
-
-    kwargs
+    **kwargs
         Additional keyword arguments to `scipy.optimize.fmin_l_bfgs_b`
 
     Returns
@@ -93,11 +90,9 @@ def nnls(A, B, **kwargs):
     ----------
     A : np.ndarray [shape=(m, n)]
         The basis matrix
-
     B : np.ndarray [shape=(..., m, N)]
         The target array.  Additional leading dimensions are supported.
-
-    kwargs
+    **kwargs
         Additional keyword arguments to `scipy.optimize.fmin_l_bfgs_b`
 
     Returns"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -26,7 +26,7 @@ def __wrapper(func, *args, **kwargs):
                 moved_from, func.__module__, func.__name__, version, version_removed
             ),
             category=DeprecationWarning,
-            stacklevel=3  # Would be 2, but the decorator adds a level
+            stacklevel=3,  # Would be 2, but the decorator adds a level
         )
         return func(*args, **kwargs)
 
@@ -47,7 +47,7 @@ def __wrapper(func, *args, **kwargs):
                 func.__module__, func.__name__, version, version_removed
             ),
             category=DeprecationWarning,
-            stacklevel=3  # Would be 2, but the decorator adds a level
+            stacklevel=3,  # Would be 2, but the decorator adds a level
         )
         return func(*args, **kwargs)
 "
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -23,14 +23,11 @@ def rename_kw(
     old_name : str
     old_value
         The name and value of the old argument
-
     new_name : str
     new_value
         The name and value of the new argument
-
     version_deprecated : str
         The version at which the old name became deprecated
-
     version_removed : str
         The version at which the old name will be removed
 "
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -55,12 +55,10 @@ def example(key, *, hq=False):
     >>> os.environ['LIBROSA_DATA_DIR'] = '/path/to/store/data'
     >>> import librosa
 
-
     Parameters
     ----------
     key : str
         The identifier for the track to load
-
     hq : bool
         If ``True``, return the high-quality version of the recording.
         If ``False``, return the 22KHz mono version of the recording."
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -144,10 +144,8 @@ def match_intervals(intervals_from, intervals_to, strict=True):
         to ``intervals_from[i, 1]``.
         ``intervals_from[0, 0]`` should be 0, ``intervals_from[-1, 1]``
         should be the track duration.
-
     intervals_to : np.ndarray [shape=(m, 2)]
         Analogous to ``intervals_from``.
-
     strict : bool
         If ``True``, intervals can only match if they intersect.
         If ``False``, disjoint intervals can match.
@@ -239,12 +237,10 @@ def match_events(events_from, events_to, left=True, right=True):
     Parameters
     ----------
     events_from : ndarray [shape=(n,)]
-      Array of events (eg, times, sample or frame indices) to match from.
-
+        Array of events (eg, times, sample or frame indices) to match from.
     events_to : ndarray [shape=(m,)]
-      Array of events (eg, times, sample or frame indices) to
-      match against.
-
+        Array of events (eg, times, sample or frame indices) to
+        match against.
     left : bool
     right : bool
         If ``False``, then matched events cannot be to the left (or right)"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -79,26 +79,20 @@ def frame(x, *, frame_length, hop_length, axis=-1, writeable=False, subok=False)
     adding a new ""frame axis"" either before the framing axis (if ``axis < 0``)
     or after the framing axis (if ``axis >= 0``).
 
-
     Parameters
     ----------
     x : np.ndarray
         Array to frame
-
     frame_length : int > 0 [scalar]
         Length of the frame
-
     hop_length : int > 0 [scalar]
         Number of steps to advance between frames
-
     axis : int
         The axis along which to frame.
-
     writeable : bool
         If ``True``, then the framed view of ``x`` is read-only.
         If ``False``, then the framed view is read-write.  Note that writing to the framed view
         will also write to the input array ``x`` in this case.
-
     subok : bool
         If True, sub-classes will be passed-through, otherwise the returned array will be
         forced to be a base-class array (default).
@@ -231,13 +225,13 @@ def valid_audio(y, *, mono=Deprecated()):
     Parameters
     ----------
     y : np.ndarray
-      The input data to validate
+        The input data to validate
 
     mono : bool
-      Whether or not to require monophonic audio
+        Whether or not to require monophonic audio
 
-      .. warning:: The ``mono`` parameter is deprecated in version 0.9 and will be
-        removed in 0.10.
+        .. warning:: The ``mono`` parameter is deprecated in version 0.9 and will be
+          removed in 0.10.
 
     Returns
     -------
@@ -266,7 +260,7 @@ def valid_audio(y, *, mono=Deprecated()):
     >>> librosa.util.valid_audio(y_stereo, mono=False)
     True
 
-    See also
+    See Also
     --------
     numpy.float32
     """"""
@@ -308,7 +302,6 @@ def valid_int(x, *, cast=None):
     ----------
     x : number
         A scalar value to be cast to int
-
     cast : function [optional]
         A function to modify ``x`` before casting.
         Default: `np.floor`
@@ -395,15 +388,12 @@ def pad_center(data, *, size, axis=-1, **kwargs):
     ----------
     data : np.ndarray
         Vector to be padded and centered
-
     size : int >= len(data) [scalar]
         Length to pad ``data``
-
     axis : int
         Axis along which to pad and center the data
-
-    kwargs : additional keyword arguments
-      arguments passed to `np.pad`
+    **kwargs : additional keyword arguments
+        arguments passed to `np.pad`
 
     Returns
     -------
@@ -445,10 +435,8 @@ def expand_to(x, *, ndim, axes):
     ----------
     x : np.ndarray
         The input array
-
     ndim : int
         The number of dimensions to expand to.  Must be at least ``x.ndim``
-
     axes : int or slice
         The target axis or axes to preserve from x.
         All other axes will have length 1.
@@ -532,15 +520,12 @@ def fix_length(data, *, size, axis=-1, **kwargs):
     Parameters
     ----------
     data : np.ndarray
-      array to be length-adjusted
-
+        array to be length-adjusted
     size : int >= 0 [scalar]
-      desired length of the array
-
+        desired length of the array
     axis : int, <= data.ndim
-      axis along which to fix length
-
-    kwargs : additional keyword arguments
+        axis along which to fix length
+    **kwargs : additional keyword arguments
         Parameters to ``np.pad``
 
     Returns
@@ -604,18 +589,14 @@ def fix_frames(frames, *, x_min=0, x_max=None, pad=True):
     array([  0, 200, 233, 266, 299, 332, 365, 398, 431, 464, 497,
            500])
 
-
     Parameters
     ----------
     frames : np.ndarray [shape=(n_frames,)]
         List of non-negative frame indices
-
     x_min : int >= 0 or None
         Minimum allowed frame index
-
     x_max : int >= 0 or None
         Maximum allowed frame index
-
     pad : boolean
         If ``True``, then ``frames`` is expanded to span the full range
         ``[x_min, x_max]``
@@ -723,7 +704,6 @@ def axis_sort(S, *, axis=-1, index=False, value=None):
     -------
     S_sort : np.ndarray [shape=(d, n)]
         ``S`` with the columns or rows permuted in sorting order
-
     idx : np.ndarray (optional) [shape=(d,) or (n,)]
         If ``index == True``, the sorting index used to permute ``S``.
         Length of ``idx`` corresponds to the selected ``axis``.
@@ -774,7 +754,6 @@ def normalize(S, *, norm=np.inf, axis=0, threshold=None, fill=None):
     `scipy.linalg.norm` in two ways: multi-dimensional arrays
     are supported, but matrix-norms are not.
 
-
     Parameters
     ----------
     S : np.ndarray
@@ -1012,15 +991,14 @@ def localmax(x, *, axis=0):
 
     Parameters
     ----------
-    x     : np.ndarray [shape=(d1,d2,...)]
-      input vector or array
-
+    x : np.ndarray [shape=(d1,d2,...)]
+        input vector or array
     axis : int
-      axis along which to compute local maximality
+        axis along which to compute local maximality
 
     Returns
     -------
-    m     : np.ndarray [shape=x.shape, dtype=bool]
+    m : np.ndarray [shape=x.shape, dtype=bool]
         indicator array of local maximality along ``axis``
 
     See Also
@@ -1074,15 +1052,14 @@ def localmin(x, *, axis=0):
 
     Parameters
     ----------
-    x     : np.ndarray [shape=(d1,d2,...)]
-      input vector or array
-
+    x : np.ndarray [shape=(d1,d2,...)]
+        input vector or array
     axis : int
-      axis along which to compute local minimality
+        axis along which to compute local minimality
 
     Returns
     -------
-    m     : np.ndarray [shape=x.shape, dtype=bool]
+    m : np.ndarray [shape=x.shape, dtype=bool]
         indicator array of local minimality along ``axis``
 
     See Also
@@ -1124,33 +1101,26 @@ def peak_pick(x, *, pre_max, post_max, pre_avg, post_avg, delta, wait):
 
     .. [#] https://github.com/CPJKU/onset_detection/blob/master/onset_program.py
 
-
     Parameters
     ----------
-    x         : np.ndarray [shape=(n,)]
+    x : np.ndarray [shape=(n,)]
         input signal to peak picks from
-
-    pre_max   : int >= 0 [scalar]
+    pre_max : int >= 0 [scalar]
         number of samples before ``n`` over which max is computed
-
-    post_max  : int >= 1 [scalar]
+    post_max : int >= 1 [scalar]
         number of samples after ``n`` over which max is computed
-
-    pre_avg   : int >= 0 [scalar]
+    pre_avg : int >= 0 [scalar]
         number of samples before ``n`` over which mean is computed
-
-    post_avg  : int >= 1 [scalar]
+    post_avg : int >= 1 [scalar]
         number of samples after ``n`` over which mean is computed
-
-    delta     : float >= 0 [scalar]
+    delta : float >= 0 [scalar]
         threshold offset for mean
-
-    wait      : int >= 0 [scalar]
+    wait : int >= 0 [scalar]
         number of samples to wait after picking a peak
 
     Returns
     -------
-    peaks     : np.ndarray [shape=(n_peaks,), dtype=int]
+    peaks : np.ndarray [shape=(n_peaks,), dtype=int]
         indices of peaks in ``x``
 
     Raises
@@ -1275,10 +1245,8 @@ def sparsify_rows(x, *, quantile=0.01, dtype=None):
     ----------
     x : np.ndarray [ndim <= 2]
         The input matrix to sparsify.
-
     quantile : float in [0, 1.0)
         Percentage of magnitude to discard in each row of ``x``
-
     dtype : np.dtype, optional
         The dtype of the output array.
         If not provided, then ``x.dtype`` will be used.
@@ -1377,10 +1345,8 @@ def buf_to_float(x, *, n_bytes=2, dtype=np.float32):
     ----------
     x : np.ndarray [dtype=int]
         The integer-valued data buffer
-
     n_bytes : int [1, 2, 4]
         The number of bytes per sample in ``x``
-
     dtype : numeric type
         The target output type (default: 32-bit float)
 
@@ -1407,14 +1373,11 @@ def index_to_slice(idx, *, idx_min=None, idx_max=None, step=None, pad=True):
     ----------
     idx : list-like
         Array of index boundaries
-
     idx_min, idx_max : None or int
         Minimum and maximum allowed indices
-
     step : None or int
         Step size for each slice.  If `None`, then the default
         step of 1 is used.
-
     pad : boolean
         If `True`, pad ``idx`` to span the range ``idx_min:idx_max``.
 
@@ -1468,20 +1431,15 @@ def sync(data, idx, *, aggregate=None, pad=True, axis=-1):
 
     Parameters
     ----------
-    data      : np.ndarray
+    data : np.ndarray
         multi-dimensional array of features
-
     idx : iterable of ints or slices
         Either an ordered array of boundary indices, or
         an iterable collection of slice objects.
-
-
     aggregate : function
         aggregation function (default: `np.mean`)
-
     pad : boolean
         If `True`, ``idx`` is padded to span the full range ``[0, data.shape[axis]]``
-
     axis : int
         The axis along which to aggregate data
 
@@ -1528,7 +1486,6 @@ def sync(data, idx, *, aggregate=None, pad=True, axis=-1):
     >>> sub_beats = librosa.util.fix_frames(sub_beats)
     >>> C_med_sub = librosa.util.sync(C, sub_beats, aggregate=np.median)
 
-
     Plot the results
 
     >>> import matplotlib.pyplot as plt
@@ -1590,7 +1547,6 @@ def softmask(X, X_ref, *, power=1, split_zeros=False):
 
         ``M = X**power / (X**power + X_ref**power)``
 
-
     Parameters
     ----------
     X : np.ndarray
@@ -1606,14 +1562,12 @@ def softmask(X, X_ref, *, power=1, split_zeros=False):
         If infinite, returns a hard (binary) mask equivalent to ``X > X_ref``.
         Note: for hard masks, ties are always broken in favor of ``X_ref`` (``mask=0``).
 
-
     split_zeros : bool
         If `True`, entries where ``X`` and ``X_ref`` are both small (close to 0)
         will receive mask values of 0.5.
 
         Otherwise, the mask is set to 0 for these entries.
 
-
     Returns
     -------
     mask : np.ndarray, shape=X.shape
@@ -1630,7 +1584,6 @@ def softmask(X, X_ref, *, power=1, split_zeros=False):
 
     Examples
     --------
-
     >>> X = 2 * np.ones((3, 3))
     >>> X_ref = np.vander(np.arange(3.0))
     >>> X
@@ -1731,7 +1684,6 @@ def tiny(x):
 
     Examples
     --------
-
     For a standard double-precision floating point number:
 
     >>> librosa.util.tiny(1.0)
@@ -1853,11 +1805,9 @@ def cyclic_gradient(data, *, edge_order=1, axis=-1):
     data : np.ndarray
         The function values observed at uniformly spaced positions on
         a periodic domain
-
-    edge_order: {1, 2}
+    edge_order : {1, 2}
         The order of the difference approximation used for estimating
         the gradient
-
     axis : int
         The axis along which gradients are calculated.
 
@@ -1963,15 +1913,12 @@ def shear(X, *, factor=1, axis=-1):
     to a horizontal.  Shearing with ``factor=1`` converts a horizontal to
     a diagonal.
 
-
     Parameters
     ----------
     X : np.ndarray [ndim=2] or scipy.sparse matrix
         The array to be sheared
-
     factor : integer
         The shear factor: ``X[:, n] -> np.roll(X[:, n], factor * n)``
-
     axis : integer
         The axis along which to shear
 
@@ -2020,12 +1967,10 @@ def stack(arrays, *, axis=0):
     ----------
     arrays : list
         one or more `np.ndarray`
-
     axis : integer
         The target axis along which to stack.  ``axis=0`` creates a new first axis,
         and ``axis=-1`` creates a new last axis.
 
-
     Returns
     -------
     arr_stack : np.ndarray [shape=(len(arrays), array_shape) or shape=(array_shape, len(arrays))]
@@ -2038,7 +1983,6 @@ def stack(arrays, *, axis=0):
     Raises
     ------
     ParameterError
-
         - If ``arrays`` do not all have the same shape
         - If no ``arrays`` are given
 
@@ -2122,13 +2066,11 @@ def dtype_r2c(d, *, default=np.complex64):
     A `float32` (single-precision) type maps to `complex64`,
     while a `float64` (double-precision) maps to `complex128`.
 
-
     Parameters
     ----------
     d : np.dtype
         The real-valued dtype to convert to complex.
         If ``d`` is a complex type already, it will be returned.
-
     default : np.dtype, optional
         The default complex target type, if ``d`` does not match a
         known dtype
@@ -2180,13 +2122,11 @@ def dtype_c2r(d, *, default=np.float32):
     A `complex64` (single-precision) type maps to `float32`,
     while a `complex128` (double-precision) maps to `float64`.
 
-
     Parameters
     ----------
     d : np.dtype
         The complex-valued dtype to convert to real.
         If ``d`` is a real (float) type already, it will be returned.
-
     default : np.dtype, optional
         The default real target type, if ``d`` does not match a
         known dtype
@@ -2250,7 +2190,6 @@ def count_unique(data, *, axis=-1):
     ----------
     data : np.ndarray
         The input array
-
     axis : int
         The target axis to count
 
@@ -2266,7 +2205,6 @@ def count_unique(data, *, axis=-1):
 
     Examples
     --------
-
     >>> x = np.vander(np.arange(5))
     >>> x
     array([[  0,   0,   0,   0,   1],
@@ -2304,7 +2242,6 @@ def is_unique(data, *, axis=-1):
     ----------
     data : np.ndarray
         The input array
-
     axis : int
         The target axis
 
@@ -2321,7 +2258,6 @@ def is_unique(data, *, axis=-1):
 
     Examples
     --------
-
     >>> x = np.vander(np.arange(5))
     >>> x
     array([[  0,   0,   0,   0,   1],"
23;librosa;librosa;a8c4e1b150522007f6d6ae63907be187cc8ec992;"0.9.0rc0 release preparation (#1439)

* bumped version number to 0.9.0rc0

* updated docs to have deprecation warnings

* updated authors list

* fixed some markdown errors in docstrings

* blacked code for audit

* velinated top-level modules

* velinated all modules

* pinning pip version for 3.6 builds

* aggressive velination

* full velination

* fixed some variable naming in frequency weighting

* fixed missing close paren in sphinx-multiversion pattern

* removed axline transforms in stack_memory plot to support historical build requirements";"@@ -5,8 +5,8 @@
 import sys
 import importlib
 
-short_version = ""0.8""
-version = ""0.8.1""
+short_version = ""0.9""
+version = ""0.9.0rc0""
 
 
 def __get_mod_version(modname):"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -18,7 +18,7 @@ dependencies:
   - packaging==20.0
 
   # optional, but required for testing
-  - matplotlib==3.0.1
+  - matplotlib==3.3.0
   - pytest-mpl
   - pytest-cov
   - pytest"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -27,9 +27,9 @@ deposited under `build/html`.  To deploy, we sync the compiled site to the
 
 Because the historical docs include example code that is executed to generate
 figures, the environment for building historical docs can be brittle.
-Presently, the oldest compiled doc is for release 0.6.3.
+Presently, the oldest compiled doc is for release 0.7.2.
 The historical docs work with the following dependency versions:
 
     - numba=0.48 : decorators submodule move in 0.49 gives warnings, and 0.50 breaks old librosa
     - numpy=1.17 : strict dtype requirements in linspace parameters break some of our old examples from 1.18 on
-    - matplotlib=3.2 : log axes API changes cause warnings in 3.3 onward
+    - matplotlib=3.3,<3.5 : log axes API changes cause warnings in 3.3 onward"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -374,7 +374,7 @@
 # beats contains the frame indices of each detected beat
 # for synchronization and visualization, we'll need to expand this
 # to cover the limits of the data.  This can be done as follows:
-beats = librosa.util.fix_frames(beats, x_min=0, x_max=chroma.shape[1])
+beats = librosa.util.fix_frames(beats, x_min=0)
 
 # Now beat-synchronize the chroma features
 chroma_sync = librosa.util.sync(chroma, beats, aggregate=np.median)"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -60,8 +60,7 @@
 # For plotting purposes, we'll need the timing of the beats
 # we fix_frames to include non-beat frames 0 and C.shape[1] (final frame)
 beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats,
-                                                            x_min=0,
-                                                            x_max=C.shape[1]),
+                                                            x_min=0),
                                     sr=sr)
 
 fig, ax = plt.subplots()
@@ -189,9 +188,12 @@
                          y_coords=beat_times, ax=ax[0])
 ax[0].set(title='Structure components')
 
-img = librosa.display.specshow(np.atleast_2d(seg_ids).T, cmap=colors,
-                         y_axis='time', y_coords=beat_times, ax=ax[2])
-ax[2].set(title='Estimated segments')
+img = librosa.display.specshow(np.atleast_2d(seg_ids).T, cmap=colors, 
+                         y_axis='time',
+                         x_coords=[0, 1], y_coords=list(beat_times) + [beat_times[-1]], 
+                         ax=ax[2])
+ax[2].set(title='Estimated labels')
+
 ax[2].label_outer()
 fig.colorbar(img, ax=[ax[2]], ticks=range(k))
 "
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -870,7 +870,7 @@ def specshow(
             tempograms are calculated in the Frequency domain
             using `feature.fourier_tempogram`.
 
-    x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]+1]
+    x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]
 
         Optional positioning coordinates of the input data.
         These can be use to explicitly set the location of each
@@ -947,7 +947,7 @@ def specshow(
         By default, the following options are set:
 
             - ``rasterized=True``
-            - ``shading='flat'``
+            - ``shading='auto'``
             - ``edgecolors='None'``
 
     Returns
@@ -998,7 +998,7 @@ def specshow(
     kwargs.setdefault(""cmap"", cmap(data))
     kwargs.setdefault(""rasterized"", True)
     kwargs.setdefault(""edgecolors"", ""None"")
-    kwargs.setdefault(""shading"", ""flat"")
+    kwargs.setdefault(""shading"", ""auto"")
 
     all_params = dict(
         kwargs=kwargs,
@@ -1021,10 +1021,8 @@ def specshow(
 
     axes = __check_axes(ax)
     out = axes.pcolormesh(x_coords, y_coords, data, **kwargs)
-    __set_current_image(ax, out)
 
-    axes.set_xlim(x_coords.min(), x_coords.max())
-    axes.set_ylim(y_coords.min(), y_coords.max())
+    __set_current_image(ax, out)
 
     # Set up axis scaling
     __scale_axes(axes, x_axis, ""x"")
@@ -1037,6 +1035,7 @@ def specshow(
     # If the plot is a self-similarity/covariance etc. plot, square it
     if __same_axes(x_axis, y_axis, axes.get_xlim(), axes.get_ylim()) and auto_aspect:
         axes.set_aspect(""equal"")
+
     return out
 
 
@@ -1057,9 +1056,9 @@ def __mesh_coords(ax_type, coords, n, **kwargs):
     """"""Compute axis coordinates""""""
 
     if coords is not None:
-        if len(coords) < n:
+        if len(coords) not in (n, n+1):
             raise ParameterError(
-                ""Coordinate shape mismatch: "" ""{}<{}"".format(len(coords), n)
+                f""Coordinate shape mismatch: {len(coords)}!={n} or {n}+1""
             )
         return coords
 
@@ -1171,14 +1170,14 @@ def __decorate_axis(axis, ax_type, key=""C:maj"", Sa=None, mela=None, thaat=None,
 
     if ax_type == ""tonnetz"":
         axis.set_major_formatter(TonnetzFormatter())
-        axis.set_major_locator(FixedLocator(0.5 + np.arange(6)))
+        axis.set_major_locator(FixedLocator(np.arange(6)))
         axis.set_label_text(""Tonnetz"")
 
     elif ax_type == ""chroma"":
         axis.set_major_formatter(ChromaFormatter(key=key, unicode=unicode))
         degrees = core.key_to_degrees(key)
         axis.set_major_locator(
-            FixedLocator(0.5 + np.add.outer(12 * np.arange(10), degrees).ravel())
+            FixedLocator(np.add.outer(12 * np.arange(10), degrees).ravel())
         )
         axis.set_label_text(""Pitch class"")
 
@@ -1194,7 +1193,7 @@ def __decorate_axis(axis, ax_type, key=""C:maj"", Sa=None, mela=None, thaat=None,
         # Rotate degrees relative to Sa
         degrees = np.mod(degrees + Sa, 12)
         axis.set_major_locator(
-            FixedLocator(0.5 + np.add.outer(12 * np.arange(10), degrees).ravel())
+            FixedLocator(np.add.outer(12 * np.arange(10), degrees).ravel())
         )
         axis.set_label_text(""Svara"")
 
@@ -1206,7 +1205,7 @@ def __decorate_axis(axis, ax_type, key=""C:maj"", Sa=None, mela=None, thaat=None,
         # Rotate degrees relative to Sa
         degrees = np.mod(degrees + Sa, 12)
         axis.set_major_locator(
-            FixedLocator(0.5 + np.add.outer(12 * np.arange(10), degrees).ravel())
+            FixedLocator(np.add.outer(12 * np.arange(10), degrees).ravel())
         )
         axis.set_label_text(""Svara"")
 
@@ -1335,9 +1334,6 @@ def __coord_fft_hz(n, sr=22050, n_fft=None, **_kwargs):
     # The following code centers the FFT bins at their frequencies
     # and clips to the non-negative frequency range [0, nyquist]
     basis = core.fft_frequencies(sr=sr, n_fft=n_fft)
-    fmax = basis[-1]
-    basis -= 0.5 * (basis[1] - basis[0])
-    basis = np.append(np.maximum(0, basis), [fmax])
     return basis
 
 
@@ -1350,8 +1346,6 @@ def __coord_mel_hz(n, fmin=0, fmax=None, sr=22050, htk=False, **_kwargs):
         fmax = 0.5 * sr
 
     basis = core.mel_frequencies(n, fmin=fmin, fmax=fmax, htk=htk)
-    basis[1:] -= 0.5 * np.diff(basis)
-    basis = np.append(np.maximum(0, basis), [fmax])
     return basis
 
 
@@ -1365,8 +1359,8 @@ def __coord_cqt_hz(n, fmin=None, bins_per_octave=12, sr=22050, **_kwargs):
 
     # we drop by half a bin so that CQT bins are centered vertically
     freqs = core.cqt_frequencies(
-        n + 1,
-        fmin=fmin / 2.0 ** (0.5 / bins_per_octave),
+        n,
+        fmin=fmin,
         bins_per_octave=bins_per_octave,
     )
 
@@ -1381,14 +1375,13 @@ def __coord_cqt_hz(n, fmin=None, bins_per_octave=12, sr=22050, **_kwargs):
 
 def __coord_chroma(n, bins_per_octave=12, **_kwargs):
     """"""Get chroma bin numbers""""""
-    return np.linspace(0, (12.0 * n) / bins_per_octave, num=n + 1, endpoint=True)
+    return np.linspace(0, (12.0 * n) / bins_per_octave, num=n, endpoint=False)
 
 
 def __coord_tempo(n, sr=22050, hop_length=512, **_kwargs):
     """"""Tempo coordinates""""""
-    basis = core.tempo_frequencies(n + 2, sr=sr, hop_length=hop_length)[1:]
-    edges = np.arange(1, n + 2)
-    return basis * (edges + 0.5) / edges
+    basis = core.tempo_frequencies(n + 1, sr=sr, hop_length=hop_length)[1:]
+    return basis
 
 
 def __coord_fourier_tempo(n, sr=22050, hop_length=512, win_length=None, **_kwargs):
@@ -1400,20 +1393,17 @@ def __coord_fourier_tempo(n, sr=22050, hop_length=512, win_length=None, **_kwarg
     basis = core.fourier_tempo_frequencies(
         sr=sr, hop_length=hop_length, win_length=win_length
     )
-    fmax = basis[-1]
-    basis -= 0.5 * (basis[1] - basis[0])
-    basis = np.append(np.maximum(0, basis), [fmax])
     return basis
 
 
 def __coord_n(n, **_kwargs):
     """"""Get bare positions""""""
-    return np.arange(n + 1)
+    return np.arange(n)
 
 
 def __coord_time(n, sr=22050, hop_length=512, **_kwargs):
     """"""Get time coordinates from frames""""""
-    return core.frames_to_time(np.arange(n + 1), sr=sr, hop_length=hop_length)
+    return core.frames_to_time(np.arange(n), sr=sr, hop_length=hop_length)
 
 
 def __same_axes(x_axis, y_axis, xlim, ylim):"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -321,7 +321,8 @@ def spectral_bandwidth(
     >>> librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
     ...                          y_axis='log', x_axis='time', ax=ax[1])
     >>> ax[1].set(title='log Power spectrogram')
-    >>> ax[1].fill_between(times, centroid[0] - spec_bw[0], centroid[0] + spec_bw[0],
+    >>> ax[1].fill_between(times, np.maximum(0, centroid[0] - spec_bw[0]),
+    ...                 np.minimum(centroid[0] + spec_bw[0], sr/2),
     ...                 alpha=0.5, label='Centroid +- bandwidth')
     >>> ax[1].plot(times, centroid[0], label='Spectral centroid', color='w')
     >>> ax[1].legend(loc='lower right')"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -197,10 +197,10 @@ def stack_memory(data, n_steps=2, delay=1, **kwargs):
 
     Stack time-lagged beat-synchronous chroma edge padding
 
-    >>> y, sr = librosa.load(librosa.ex('choice'))
+    >>> y, sr = librosa.load(librosa.ex('choice'), duration=10)
     >>> chroma = librosa.feature.chroma_stft(y=y, sr=sr)
     >>> tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)
-    >>> beats = librosa.util.fix_frames(beats, x_min=0, x_max=chroma.shape[1])
+    >>> beats = librosa.util.fix_frames(beats, x_min=0)
     >>> chroma_sync = librosa.util.sync(chroma, beats)
     >>> chroma_lag = librosa.feature.stack_memory(chroma_sync, n_steps=3,
     ...                                           mode='edge')
@@ -212,9 +212,12 @@ def stack_memory(data, n_steps=2, delay=1, **kwargs):
     >>> beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=512)
     >>> librosa.display.specshow(chroma_lag, y_axis='chroma', x_axis='time',
     ...                          x_coords=beat_times, ax=ax)
-    >>> ax.set(yticks=[0, 12, 24], yticklabels=['Lag=0', 'Lag=1', 'Lag=2'],
-    ...           title='Time-lagged chroma')
-    >>> ax.hlines([0, 12, 24], beat_times.min(), beat_times.max(), color='w')
+    >>> ax.text(1.0, 1/6, ""Lag=0"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
+    >>> ax.text(1.0, 3/6, ""Lag=1"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
+    >>> ax.text(1.0, 5/6, ""Lag=2"", transform=ax.transAxes, rotation=-90, ha=""left"", va=""center"")
+    >>> ax.axline((0, 1/3), (1, 1/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
+    >>> ax.axline((0, 2/3), (1, 2/3), transform=ax.transAxes, color='w', alpha=0.75, linestyle='--')
+    >>> ax.set(title='Time-lagged chroma', ylabel="""")
     """"""
 
     if n_steps < 1:"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -940,10 +940,14 @@ def agglomerative(data, k, clusterer=None, axis=-1):
     Plot the segmentation over the chromagram
 
     >>> import matplotlib.pyplot as plt
+    >>> import matplotlib.transforms as mpt
     >>> fig, ax = plt.subplots()
+    >>> trans = mpt.blended_transform_factory(
+    ...             ax.transData, ax.transAxes)
     >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=ax)
-    >>> ax.vlines(bound_times, 0, chroma.shape[0], color='linen', linestyle='--',
-    ...           linewidth=2, alpha=0.9, label='Segment boundaries')
+    >>> ax.vlines(bound_times, 0, 1, color='linen', linestyle='--',
+    ...           linewidth=2, alpha=0.9, label='Segment boundaries',
+    ...           transform=trans)
     >>> ax.legend()
     >>> ax.set(title='Power spectrogram')
     """""""
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -1208,11 +1208,11 @@ def viterbi_discriminative(
     >>> fig, ax = plt.subplots()
     >>> librosa.display.specshow(probs, x_axis='time', cmap='gray', ax=ax)
     >>> times = librosa.times_like(chords_vit)
-    >>> ax.scatter(times, chords_ind + 0.75, color='lime', alpha=0.5, marker='+',
+    >>> ax.scatter(times, chords_ind + 0.25, color='lime', alpha=0.5, marker='+',
     ...            s=15, label='Independent')
-    >>> ax.scatter(times, chords_vit + 0.25, color='deeppink', alpha=0.5, marker='o',
+    >>> ax.scatter(times, chords_vit - 0.25, color='deeppink', alpha=0.5, marker='o',
     ...            s=15, label='Viterbi')
-    >>> ax.set(yticks=0.5 + np.unique(chords_vit),
+    >>> ax.set(yticks=np.unique(chords_vit),
     ...        yticklabels=[labels[i] for i in np.unique(chords_vit)])
     >>> ax.legend()
     """""""
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -65,15 +65,15 @@ docs =
     sphinx != 1.3.1
     sphinx_rtd_theme==0.5.*
     numba < 0.50
-    matplotlib >= 2.0.0, < 3.3
+    matplotlib >= 3.3.0
     sphinx-multiversion >= 0.2.3
     sphinx-gallery >= 0.7
     mir_eval >= 0.5
     ipython >= 7.0
     sphinxcontrib-svg2pdfconverter
     presets
 tests =
-    matplotlib >= 3.0.1
+    matplotlib >= 3.3.0
     pytest-mpl
     pytest-cov
     pytest"
23;librosa;librosa;132489f0cc321d9767c8584f42ec94188347c9ed;"specshow coordinate centering (#1414)

* work in progress toward #1357. Autolimiting is a real pain

* revised test configuration for display

* updated display fixtures

* updated minimal matplotlib version

* updated minimal matplotlib version

* updates to docs and examples

* fixed stack-memory example

* fixed segmentation demo

* fixed segment docstrings

* fixed chord model demo

* updated doc build readme";"@@ -18,20 +18,14 @@
 
 matplotlib = pytest.importorskip(""matplotlib"", minversion=""3.4"")
 
-matplotlib.use(""Agg"")
-matplotlib.rcParams.update(matplotlib.rcParamsDefault)
-
-import matplotlib.style
-
-matplotlib.style.use(""seaborn-ticks"")
+STYLE = ""default""
 
 import matplotlib.pyplot as plt
 
 import librosa
 import librosa.display
 import numpy as np
 
-
 # Workaround for old freetype builds with our image fixtures
 FT_VERSION = version.parse(matplotlib.ft2font.__freetype_version__)
 OLD_FT = not (FT_VERSION >= version.parse(""2.10""))
@@ -92,7 +86,7 @@ def tempo(rhythm):
 
 @pytest.fixture
 def beats(rhythm, C):
-    return librosa.util.fix_frames(rhythm[1], x_max=C.shape[1])
+    return librosa.util.fix_frames(rhythm[1])
 
 
 @pytest.fixture
@@ -118,7 +112,7 @@ def test_unknown_time_unit(y):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""complex""], extensions=[""png""], tolerance=6
+    baseline_images=[""complex""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_complex_input(S):
@@ -127,7 +121,7 @@ def test_complex_input(S):
     return plt.gcf()
 
 
-@pytest.mark.mpl_image_compare(baseline_images=[""abs""], extensions=[""png""], tolerance=6)
+@pytest.mark.mpl_image_compare(baseline_images=[""abs""], extensions=[""png""], tolerance=6, style=STYLE)
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_abs_input(S_abs):
     plt.figure()
@@ -136,7 +130,7 @@ def test_abs_input(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""cqt_note""], extensions=[""png""], tolerance=6
+    baseline_images=[""cqt_note""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_cqt_note(C):
@@ -146,7 +140,7 @@ def test_cqt_note(C):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""fft_note""], extensions=[""png""], tolerance=6
+    baseline_images=[""fft_note""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_fft_note(S_abs):
@@ -156,7 +150,7 @@ def test_fft_note(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""cqt_hz""], extensions=[""png""], tolerance=6
+    baseline_images=[""cqt_hz""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_cqt_hz(C):
@@ -166,7 +160,7 @@ def test_cqt_hz(C):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""tempo""], extensions=[""png""], tolerance=6
+    baseline_images=[""tempo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_tempo(y, sr):
@@ -178,7 +172,7 @@ def test_tempo(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""fourier_tempo""], extensions=[""png""], tolerance=6
+    baseline_images=[""fourier_tempo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_fourier_tempo(y, sr):
@@ -190,7 +184,7 @@ def test_fourier_tempo(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""tonnetz""], extensions=[""png""], tolerance=6
+    baseline_images=[""tonnetz""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_tonnetz(C):
@@ -202,7 +196,7 @@ def test_tonnetz(C):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""chroma""], extensions=[""png""], tolerance=6
+    baseline_images=[""chroma""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_chroma(S_abs, sr):
@@ -222,7 +216,7 @@ def test_chroma(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""chroma_svara""], extensions=[""png""], tolerance=6
+    baseline_images=[""chroma_svara""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_chroma_svara(C, sr):
@@ -251,7 +245,7 @@ def test_chroma_svara(C, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""double_chroma""], extensions=[""png""], tolerance=6
+    baseline_images=[""double_chroma""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_double_chroma(S_abs, sr):
@@ -264,7 +258,7 @@ def test_double_chroma(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_mel""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_mel""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_x_mel(S_abs):
@@ -276,7 +270,7 @@ def test_x_mel(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""y_mel""], extensions=[""png""], tolerance=6
+    baseline_images=[""y_mel""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_y_mel(S_abs):
@@ -288,7 +282,7 @@ def test_y_mel(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""y_mel_bounded""], extensions=[""png""], tolerance=6
+    baseline_images=[""y_mel_bounded""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_y_mel_bounded(S_abs):
@@ -301,7 +295,7 @@ def test_y_mel_bounded(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_linear""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_linear""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_linear(S_abs, S_signed, S_bin):
@@ -318,7 +312,7 @@ def test_xaxis_none_yaxis_linear(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""specshow_ext_axes""], extensions=[""png""], tolerance=6
+    baseline_images=[""specshow_ext_axes""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_specshow_ext_axes(S_abs):
@@ -333,7 +327,7 @@ def test_specshow_ext_axes(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_log""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_log""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_log(S_abs, S_signed, S_bin):
@@ -351,7 +345,7 @@ def test_xaxis_none_yaxis_log(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_linear_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_linear_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_linear_yaxis_none(S_abs, S_signed, S_bin):
@@ -369,7 +363,7 @@ def test_xaxis_linear_yaxis_none(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_log_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_log_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_log_yaxis_none(S_abs, S_signed, S_bin):
@@ -388,7 +382,7 @@ def test_xaxis_log_yaxis_none(S_abs, S_signed, S_bin):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_time_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_time_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_time_yaxis_none(S_abs):
@@ -399,7 +393,7 @@ def test_xaxis_time_yaxis_none(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_time""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_time""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_time(S_abs):
@@ -410,7 +404,7 @@ def test_xaxis_none_yaxis_time(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_frames_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_frames_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_frames_yaxis_none(S_abs):
@@ -421,7 +415,7 @@ def test_xaxis_frames_yaxis_none(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_frames""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_frames""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_none_yaxis_frames(S_abs):
@@ -432,7 +426,7 @@ def test_xaxis_none_yaxis_frames(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_lag_y_none""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_lag_y_none""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_lag_yaxis_none(S_abs):
@@ -443,7 +437,7 @@ def test_xaxis_lag_yaxis_none(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""x_none_y_lag""], extensions=[""png""], tolerance=6
+    baseline_images=[""x_none_y_lag""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_xaxis_time_yaxis_lag(S_abs):
@@ -454,7 +448,7 @@ def test_xaxis_time_yaxis_lag(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""time_scales_auto""], extensions=[""png""], tolerance=6
+    baseline_images=[""time_scales_auto""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_time_scales_auto(S_abs, sr):
@@ -483,7 +477,7 @@ def test_time_scales_auto(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""time_unit""], extensions=[""png""], tolerance=6
+    baseline_images=[""time_unit""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_time_unit(S_abs, sr):
@@ -508,7 +502,7 @@ def test_time_unit(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""time_unit_lag""], extensions=[""png""], tolerance=6
+    baseline_images=[""time_unit_lag""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_time_unit_lag(S_abs, sr):
@@ -531,7 +525,7 @@ def test_time_unit_lag(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveplot_mono""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveplot_mono""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveplot_mono(y, sr):
@@ -549,7 +543,7 @@ def test_waveplot_mono(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_mono""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_mono""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_mono(y, sr):
@@ -560,7 +554,7 @@ def test_waveshow_mono(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_mono_zoom""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_mono_zoom""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_mono_zoom(y, sr):
@@ -573,7 +567,7 @@ def test_waveshow_mono_zoom(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_mono_zoom_out""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_mono_zoom_out""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_mono_zoom_out(y, sr):
@@ -588,7 +582,7 @@ def test_waveshow_mono_zoom_out(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveplot_ext_axes""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveplot_ext_axes""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveplot_ext_axes(y):
@@ -603,7 +597,7 @@ def test_waveplot_ext_axes(y):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_ext_axes""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_ext_axes""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_ext_axes(y):
@@ -618,7 +612,7 @@ def test_waveshow_ext_axes(y):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveplot_stereo""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveplot_stereo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveplot_stereo(y, sr):
@@ -631,7 +625,7 @@ def test_waveplot_stereo(y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""waveshow_stereo""], extensions=[""png""], tolerance=6
+    baseline_images=[""waveshow_stereo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_waveshow_stereo(y, sr):
@@ -716,7 +710,7 @@ def test_cmap_robust(data):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""coords""], extensions=[""png""], tolerance=6
+    baseline_images=[""coords""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_coords(Csync, beat_t):
@@ -734,7 +728,7 @@ def test_bad_coords(S_abs):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""sharex_specshow_ms""], extensions=[""png""], tolerance=6
+    baseline_images=[""sharex_specshow_ms""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_sharex_specshow_ms(S_abs, y, sr):
@@ -752,7 +746,7 @@ def test_sharex_specshow_ms(S_abs, y, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""sharex_waveplot_ms""], extensions=[""png""], tolerance=6
+    baseline_images=[""sharex_waveplot_ms""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_sharex_waveplot_ms(y, sr, S_abs):
@@ -788,7 +782,7 @@ def test_axis_bound_warning(format_str):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""cqt_svara""], extensions=[""png""], tolerance=6
+    baseline_images=[""cqt_svara""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_display_cqt_svara(C, sr):
@@ -815,7 +809,7 @@ def test_display_cqt_svara(C, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""fft_svara""], extensions=[""png""], tolerance=6
+    baseline_images=[""fft_svara""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_display_fft_svara(S_abs, sr):
@@ -841,7 +835,7 @@ def test_display_fft_svara(S_abs, sr):
 
 
 @pytest.mark.mpl_image_compare(
-        baseline_images=[""nfft_odd""], extensions=[""png""], tolerance=6
+        baseline_images=[""nfft_odd""], extensions=[""png""], tolerance=6, style=STYLE
 )
 def test_display_fft_odd():
 
@@ -867,7 +861,7 @@ def test_display_fft_odd():
 
 
 @pytest.mark.mpl_image_compare(
-        baseline_images=[""nfft_odd_ftempo""], extensions=[""png""], tolerance=6
+        baseline_images=[""nfft_odd_ftempo""], extensions=[""png""], tolerance=6, style=STYLE
 )
 def test_display_fourier_tempo_odd():
 
@@ -937,7 +931,7 @@ def test_auto_aspect():
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""specshow_unicode_true""], extensions=[""png""], tolerance=6
+    baseline_images=[""specshow_unicode_true""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_specshow_unicode_true(C, sr):
@@ -968,7 +962,7 @@ def test_specshow_unicode_true(C, sr):
 
 
 @pytest.mark.mpl_image_compare(
-    baseline_images=[""specshow_unicode_false""], extensions=[""png""], tolerance=6
+    baseline_images=[""specshow_unicode_false""], extensions=[""png""], tolerance=6, style=STYLE
 )
 @pytest.mark.xfail(OLD_FT, reason=f""freetype version < {FT_VERSION}"", strict=False)
 def test_specshow_unicode_false(C, sr):"
24;karpathy;arxiv-sanity-preserver;99f55f78f8f025c1999162c44cc3c5e4af3c7cb7;store in float32 instead of float64 works okay and half memory;"@@ -35,7 +35,7 @@ def query_db(query, args=(), one=False):
 meta = pickle.load(open(Config.meta_path, 'rb'))
 out = pickle.load(open(Config.tfidf_path, 'rb'))
 X = out['X']
-X = X.todense()
+X = X.todense().astype(np.float32)
 
 xtoi = { strip_version(x):i for x,i in meta['ptoi'].items() }
 "
25;sripathikrishnan;redis-rdb-tools;f97b72ffc8d698abbc814435be88e856f852117b;"fix crash in parsing module data types containing floats

* modules save binary float, and not the old string based floats.
* fix crash in the memory profiler when handling rdb files with modules or streams
* improve memory report to contains the total estimated memory, and used_mem aux field, etc.";"@@ -25,9 +25,11 @@ def __init__(self, key_groupings = None):
         self.aggregates = {}
         self.scatters = {}
         self.histograms = {}
+        self.metadata = {}
 
     def next_record(self, record):
         self.add_aggregate('database_memory', record.database, record.bytes)
+        self.add_aggregate('database_memory', 'all', record.bytes)
         self.add_aggregate('type_memory', record.type, record.bytes)
         self.add_aggregate('encoding_memory', record.encoding, record.bytes)
         
@@ -47,7 +49,7 @@ def next_record(self, record):
             self.add_scatter('sortedset_memory_by_length', record.bytes, record.size)
         elif record.type == 'string':
             self.add_scatter('string_memory_by_length', record.bytes, record.size)
-        elif record.type == 'dict':
+        elif record.type in ['dict', 'module', 'stream']:
             pass
         else:
             raise Exception('Invalid data type %s' % record.type)
@@ -74,9 +76,12 @@ def add_scatter(self, heading, x, y):
         if not heading in self.scatters:
             self.scatters[heading] = []
         self.scatters[heading].append([x, y])
+
+    def set_metadata(self, key, val):
+        self.metadata[key] = val
   
     def get_json(self):
-        return json.dumps({""aggregates"":self.aggregates, ""scatters"":self.scatters, ""histograms"":self.histograms})
+        return json.dumps({""aggregates"": self.aggregates, ""scatters"": self.scatters, ""histograms"": self.histograms, ""metadata"": self.metadata})
         
 class PrintAllKeys(object):
     def __init__(self, out, bytes, largest):
@@ -159,7 +164,6 @@ def start_rdb(self):
         pass
 
     def aux_field(self, key, value):
-        #print('aux: %s %s' % (key, value))
         if key == 'used-mem':
             self._aux_used_mem = int(value)
         if key == 'redis-ver':
@@ -179,9 +183,13 @@ def end_database(self, db_number):
             self._stream.end_database(db_number)
 
     def end_rdb(self):
-        #print('internal fragmentation: %s' % self._total_internal_frag)
         if hasattr(self._stream, 'end_rdb'):
             self._stream.end_rdb()
+        if hasattr(self._stream, 'set_metadata'):
+            self._stream.set_metadata('used_mem', self._aux_used_mem)
+            self._stream.set_metadata('redis_ver', self._aux_redis_ver)
+            self._stream.set_metadata('redis_bits', self._aux_redis_bits)
+            self._stream.set_metadata('internal_frag', self._total_internal_frag)
 
     def set(self, key, value, expiry, info):
         self._current_encoding = info['encoding']"
25;sripathikrishnan;redis-rdb-tools;f97b72ffc8d698abbc814435be88e856f852117b;"fix crash in parsing module data types containing floats

* modules save binary float, and not the old string based floats.
* fix crash in the memory profiler when handling rdb files with modules or streams
* improve memory report to contains the total estimated memory, and used_mem aux field, etc.";"@@ -558,7 +558,7 @@ def read_object(self, f, enc_type) :
             self._callback.start_sorted_set(self._key, length, self._expiry, info={'encoding':'skiplist','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 val = self.read_string(f)
-                score = read_double(f) if enc_type == REDIS_RDB_TYPE_ZSET_2 else self.read_float(f)
+                score = read_binary_double(f) if enc_type == REDIS_RDB_TYPE_ZSET_2 else self.read_float(f)
                 self._callback.zadd(self._key, score, val)
             self._callback.end_sorted_set(self._key)
         elif enc_type == REDIS_RDB_TYPE_HASH:
@@ -825,9 +825,9 @@ def skip_module(self, f):
             if opcode == REDIS_RDB_MODULE_OPCODE_SINT or opcode == REDIS_RDB_MODULE_OPCODE_UINT:
                 self.read_length(f)
             elif opcode == REDIS_RDB_MODULE_OPCODE_FLOAT:
-                self.skip_float(f)
+                read_binary_float(f)
             elif opcode == REDIS_RDB_MODULE_OPCODE_DOUBLE:
-                read_double(f)
+                read_binary_double(f)
             elif opcode == REDIS_RDB_MODULE_OPCODE_STRING:
                 self.skip_string(f)
             else:
@@ -851,9 +851,9 @@ def read_module(self, f):
             if opcode == REDIS_RDB_MODULE_OPCODE_SINT or opcode == REDIS_RDB_MODULE_OPCODE_UINT:
                 data = self.read_length(iowrapper)
             elif opcode == REDIS_RDB_MODULE_OPCODE_FLOAT:
-                data = self.read_float(iowrapper)
+                data = read_binary_float(iowrapper)
             elif opcode == REDIS_RDB_MODULE_OPCODE_DOUBLE:
-                data = read_double(iowrapper)
+                data = read_binary_double(iowrapper)
             elif opcode == REDIS_RDB_MODULE_OPCODE_STRING:
                 data = self.read_string(iowrapper)
             else:
@@ -1104,9 +1104,12 @@ def read_milliseconds_time(f) :
 def read_unsigned_long_be(f) :
     return struct.unpack('>Q', f.read(8))[0]
 
-def read_double(f) :
+def read_binary_double(f) :
     return struct.unpack('d', f.read(8))[0]
 
+def read_binary_float(f) :
+    return struct.unpack('f', f.read(4))[0]
+
 def string_as_hexcode(string) :
     for s in string :
         if isinstance(s, int) :"
25;sripathikrishnan;redis-rdb-tools;f97b72ffc8d698abbc814435be88e856f852117b;"fix crash in parsing module data types containing floats

* modules save binary float, and not the old string based floats.
* fix crash in the memory profiler when handling rdb files with modules or streams
* improve memory report to contains the total estimated memory, and used_mem aux field, etc.";"@@ -15,7 +15,10 @@
         google.setOnLoadCallback(draw_charts);
         
         function draw_charts() {
-            //draw_pie_chart('database_memory', chart_data.aggregates.database_memory, 'Database Number', 'Size in Bytes', 'Memory Usage by Database')
+            document.getElementById('database_memory').innerHTML = chart_data.aggregates.database_memory['all']
+            document.getElementById('aux_used_mem').innerHTML = chart_data.metadata.used_mem
+            document.getElementById('aux_redis_ver_and_bits').innerHTML = chart_data.metadata.redis_ver + ' - ' + chart_data.metadata.redis_bits + 'bit'
+            document.getElementById('internal_frag').innerHTML = chart_data.metadata.internal_frag
             
             draw_pie_chart('type_memory', chart_data.aggregates.type_memory, 'Data Type', 'Total Size in Bytes', 'Memory Usage by Data Type')
             draw_column_chart('type_count', chart_data.aggregates.type_count, 'Data Type', 'Keys', 'Number of Keys by Data Type')
@@ -105,14 +108,26 @@
   <body>
     <h1>Redis Memory Distribution for dump.rdb</h1>
     <div class=""container"">
-        <!--
+        <h2>Total database computed memory:</h2>
         <div class=""row"">
             <div class=""span6"" id=""database_memory"">
             </div>
-            <div class=""span6"" id="""">
+        </div>
+        <h2>Total database memory from RDB aux field:</h2>
+        <div class=""row"">
+            <div class=""span6"" id=""aux_used_mem"">
+            </div>
+        </div>
+        <h2>Database redis version and bits:</h2>
+        <div class=""row"">
+            <div class=""span6"" id=""aux_redis_ver_and_bits"">
+            </div>
+        </div>
+        <h2>Total estimated internal fragmentation:</h2>
+        <div class=""row"">
+            <div class=""span6"" id=""internal_frag"">
             </div>
         </div>
-        -->
         <h2>Memory Usage By Data Type and Data Encoding</h2>
         <div class=""row"">
             <div class=""span6"" id=""type_memory"">"
25;sripathikrishnan;redis-rdb-tools;cce34491ceb1ffbb58decfbee5c13656d23e3547;"fix #144 - memprofiler on python 2.x crash on long integers (#145)

besides the above crash, it looks like the memory estimation for numeric
list entries was wrong, v3 redis always saved integer encoded strings
inside the robj, with no extra memory

and for some reason, quicklists used the raw string length rather than
using ziplist_entry_overhead, like in a simple ziplist";"@@ -243,8 +243,8 @@ def end_set(self, key):
     
     def start_list(self, key, expiry, info):
         self._current_length = 0
-        self._list_items_size = 0
-        self._list_items_zipped_size = 0
+        self._list_items_size = 0  # size of all elements in case list ends up using linked list
+        self._list_items_zipped_size = 0  # size of all elements in case of ziplist of quicklist
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
         self._key_expiry = expiry
@@ -267,23 +267,26 @@ def start_list(self, key, expiry, info):
             
     def rpush(self, key, value):
         self._current_length += 1
-        size = self.sizeof_string(value) if type(value) != int else 4
+        # in linked list, when the robj has integer encoding, the value consumes no memory on top of the robj
+        size_in_list = self.sizeof_string(value) if not self.is_integer_type(value) else 0
+        # in ziplist and quicklist, this is the size of the value and the value header
+        size_in_zip = self.ziplist_entry_overhead(value)
 
         if(self.element_length(value) > self._len_largest_element):
             self._len_largest_element = self.element_length(value)
 
         if self._current_encoding == ""ziplist"":
-            self._list_items_zipped_size += self.ziplist_entry_overhead(value)
-            if self._current_length > self._list_max_ziplist_entries or size > self._list_max_ziplist_value:
+            self._list_items_zipped_size += size_in_zip
+            if self._current_length > self._list_max_ziplist_entries or size_in_zip > self._list_max_ziplist_value:
                 self._current_encoding = ""linkedlist""
         elif self._current_encoding == ""quicklist"":
-            if self._cur_zip_size + size > self._list_max_ziplist_size:
-                self._cur_zip_size = size
+            if self._cur_zip_size + size_in_zip > self._list_max_ziplist_size:
+                self._cur_zip_size = size_in_zip
                 self._cur_zips += 1
             else:
-                self._cur_zip_size += size
+                self._cur_zip_size += size_in_zip
             self._list_items_zipped_size += self.ziplist_entry_overhead(value)
-        self._list_items_size += size  # not to be used in case of ziplist or quicklist
+        self._list_items_size += size_in_list  # not to be used in case of ziplist or quicklist
 
     def end_list(self, key, info):
         if self._current_encoding == 'quicklist':
@@ -473,7 +476,7 @@ def ziplist_header_overhead(self):
 
     def ziplist_entry_overhead(self, value):
         # See https://github.com/antirez/redis/blob/unstable/src/ziplist.c
-        if type(value) == int:
+        if self.is_integer_type(value):
             header = 1
             if value < 12:
                 size = 0
@@ -539,12 +542,17 @@ def zset_random_level(self):
         else:
             return ZSKIPLIST_MAXLEVEL
 
+    def is_integer_type(self, ob):
+        if isinstance(ob, int):
+            return True
+        if sys.version_info < (3,):
+            if isinstance(ob, long):
+                return True
+        return False
+
     def element_length(self, element):
-        if isinstance(element, int):
+        if self.is_integer_type(element):
             return self._long_size
-        if sys.version_info < (3,):
-            if isinstance(element, long):
-                return self._long_size
         return len(element)
 
 "
25;sripathikrishnan;redis-rdb-tools;c457a0364c42c2b0edb4f094faff0777355e2859;"Merge pull request #135 from oranagra/int_mem_fix

minor fix to memory calculations of numeric values";"@@ -403,7 +403,7 @@ def sizeof_string(self, string):
             if num < REDIS_SHARED_INTEGERS :
                 return 0
             else :
-                return 8
+                return 0  # the integer is part of the robj, no extra memory
         except ValueError:
             pass
         l = len(string)"
25;sripathikrishnan;redis-rdb-tools;bd80d88e550964e921af211f5e48391ded00611f;minor fix to memory calculations of numeric values;"@@ -403,7 +403,7 @@ def sizeof_string(self, string):
             if num < REDIS_SHARED_INTEGERS :
                 return 0
             else :
-                return 8
+                return 0  # the integer is part of the robj, no extra memory
         except ValueError:
             pass
         l = len(string)"
25;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -64,7 +64,8 @@ def _write_comma(self):
     def set(self, key, value, expiry, info):
         self._start_key(key, 0)
         self._out.write(self.encode_key(key) + b':' + self.encode_value(value))
-    
+        self._end_key(key)
+
     def start_hash(self, key, length, expiry, info):
         self._start_key(key, length)
         self._out.write(self.encode_key(key) + b':{')
@@ -113,7 +114,24 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         self._end_key(key)
         self._out.write(b'}')
-        
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b':{')
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        self._end_key(key)
+        self._out.write(b'}')
+
+    def start_module(self, key, module_name, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b':{')
+        return False
+
+    def end_module(self, key, buffer_size, buffer=None):
+        self._end_key(key)
+        self._out.write(b'}')
+
 
 class KeysOnlyCallback(RdbCallback):
     def __init__(self, out, string_escape=None):
@@ -150,6 +168,12 @@ def start_sorted_set(self, key, length, expiry, info):
     def zadd(self, key, score, member):
         self._keyout(key)
         
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._keyout(key)
+
+    def start_module(self, key, module_name, expiry, info):
+        self._keyout(key)
+        return False
 
 class KeyValsOnlyCallback(RdbCallback):
     def __init__(self, out, string_escape=None):
@@ -226,6 +250,20 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         self._end_key(key)
 
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b' ')
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        self._end_key(key)
+
+    def start_module(self, key, module_name, expiry, info):
+        self._start_key(key, 0)
+        self._out.write(self.encode_key(key) + b' ')
+        return False
+
+    def end_module(self, key, buffer_size, buffer=None):
+        self._end_key(key)
 
 class DiffCallback(RdbCallback):
     '''Prints the contents of RDB in a format that is unix sort friendly, 
@@ -300,6 +338,15 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         pass
 
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        self._out.write(self.dbstr() + self.encode_key(key) + b' -> stream-items=' + encodehelpers.num2bytes(items))
+        self.newline()
+
+    def start_module(self, key, module_name, expiry, info):
+        self._out.write(self.dbstr() + self.encode_key(key) + b' -> module-name=' + codecs.encode(module_name, 'ascii'))
+        self.newline()
+        return False
+
     def newline(self):
         self._out.write(b'\r\n')
 
@@ -398,6 +445,16 @@ def zadd(self, key, score, member):
     def end_sorted_set(self, key):
         self.post_expiry(key)
 
+    # streams and modules, not currently supported
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        # TODO send RESTORE command
+        pass
+
+    def start_module(self, key, module_name, expiry, info):
+        # TODO send RESTORE command
+        return False
+
     # Other misc commands
 
     def select(self, db_number):"
25;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -124,7 +124,7 @@ class MemoryCallback(RdbCallback):
     '''Calculates the memory used if this rdb file were loaded into RAM
         The memory usage is approximate, and based on heuristics.
     '''
-    def __init__(self, stream, architecture, redis_version='3.2', string_escape=None):
+    def __init__(self, stream, architecture, redis_version='5.0', string_escape=None):
         super(MemoryCallback, self).__init__(string_escape)
         self._stream = stream
         self._dbnum = 0
@@ -303,7 +303,7 @@ def end_list(self, key, info):
                          self._len_largest_element, self._key_expiry)
         self.end_key()
 
-    def start_module(self, key, module_id, expiry):
+    def start_module(self, key, module_id, expiry, info):
         self._key_expiry = expiry
         self._current_encoding = module_id
         self._current_size = self.top_level_object_overhead(key, expiry)
@@ -316,6 +316,49 @@ def end_module(self, key, buffer_size, buffer=None):
         self.emit_record(""module"", key, size, self._current_encoding, 1, size, self._key_expiry)
         self.end_key()
 
+    def start_stream(self, key, listpacks_count, expiry, info):
+        self._key_expiry = expiry
+        self._current_encoding = info['encoding']
+        self._current_size = self.top_level_object_overhead(key, expiry)
+        self._current_size += self.sizeof_pointer()*2 + 8 + 16  # stream struct
+        self._current_size += self.sizeof_pointer() + 8*2  # rax struct
+        self._listpacks_count = listpacks_count
+
+    def stream_listpack(self, key, entry_id, data):
+        self._current_size += self.malloc_overhead(len(data))
+        if(len(data) > self._len_largest_element):
+            self._len_largest_element = len(data)
+        pass
+
+    def sizeof_stream_radix_tree(self, num_elements):
+        # This is a very rough estimation. The only alternative to doing an estimation,
+        # is to fully build a radix tree of similar design, and count the nodes.
+        # There should be at least as many nodes as there are elements in the radix tree (possibly up to 3 times)
+        num_nodes = int(num_elements * 2.5)
+        # formula for memory estimation copied from Redis's streamRadixTreeMemoryUsage
+        return 16*num_elements + num_nodes*4 + num_nodes*30*self.sizeof_long()
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        # Now after we have some global key+value overheads, and all listpacks sizes,
+        # we need to add some estimations for radix tree and consumer groups.
+        # The logic for the memory estimation copied from Redis's MEMORY command.
+        radix_tree_size = self.sizeof_stream_radix_tree(self._listpacks_count)
+        cgroups_size = 0
+        for cg in cgroups:
+            cgroups_size += self.sizeof_pointer() * 2 + 16  # streamCG
+            pending = len(cg['pending'])
+            cgroups_size += self.sizeof_stream_radix_tree(pending)
+            cgroups_size += pending*(self.sizeof_pointer()+8+8)  # streamNACK
+            for c in cg['consumers']:
+                cgroups_size += self.sizeof_pointer()*2 + 8  # streamConsumer
+                cgroups_size += self.sizeof_string(c['name'])
+                pending = len(c['pending'])
+                cgroups_size += self.sizeof_stream_radix_tree(pending)
+        size = self._current_size + radix_tree_size + cgroups_size
+        self._current_length = items
+        self.emit_record(""stream"", key, size, self._current_encoding, 1, self._len_largest_element, self._key_expiry)
+        self.end_key()
+
     def start_sorted_set(self, key, length, expiry, info):
         self._current_length = length
         self._current_encoding = info['encoding']"
25;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -27,6 +27,9 @@
 REDIS_RDB_64BITLEN = 0x81
 REDIS_RDB_ENCVAL = 3
 
+REDIS_RDB_OPCODE_MODULE_AUX = 247
+REDIS_RDB_OPCODE_IDLE = 248
+REDIS_RDB_OPCODE_FREQ = 249
 REDIS_RDB_OPCODE_AUX = 250
 REDIS_RDB_OPCODE_RESIZEDB = 251
 REDIS_RDB_OPCODE_EXPIRETIME_MS = 252
@@ -48,6 +51,7 @@
 REDIS_RDB_TYPE_ZSET_ZIPLIST = 12
 REDIS_RDB_TYPE_HASH_ZIPLIST = 13
 REDIS_RDB_TYPE_LIST_QUICKLIST = 14
+REDIS_RDB_TYPE_STREAM_LISTPACKS = 15
 
 REDIS_RDB_ENC_INT8 = 0
 REDIS_RDB_ENC_INT16 = 1
@@ -63,7 +67,7 @@
 
 DATA_TYPE_MAPPING = {
     0 : ""string"", 1 : ""list"", 2 : ""set"", 3 : ""sortedset"", 4 : ""hash"", 5 : ""sortedset"", 6 : ""module"", 7: ""module"",
-    9 : ""hash"", 10 : ""list"", 11 : ""set"", 12 : ""sortedset"", 13 : ""hash"", 14 : ""list""}
+    9 : ""hash"", 10 : ""list"", 11 : ""set"", 12 : ""sortedset"", 13 : ""hash"", 14 : ""list"", 15 : ""stream""}
 
 class RdbCallback(object):
     """"""
@@ -117,12 +121,13 @@ def start_database(self, db_number):
         """"""
         pass
 
-    def start_module(self, key, module_name, expiry):
+    def start_module(self, key, module_name, expiry, info):
         """"""
         Called to indicate start of a module key
-        :param key: string
+        :param key: string. if key is None, this is module AUX data
         :param module_name: string
         :param expiry:
+        :param info: is a dictionary containing additional information about this object.
         :return: boolean to indicate whatever to record the full buffer or not
         """"""
         return False
@@ -296,7 +301,44 @@ def end_sorted_set(self, key):
         
         """"""
         pass
-    
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        """"""Callback to handle the start of a stream
+
+        `key` is the redis key
+        `listpacks_count` is the number of listpacks in this stream.
+        `expiry` is a `datetime` object. None means the object does not expire
+        `info` is a dictionary containing additional information about this object.
+
+        After `start_stream`, the method `stream_listpack` will be called with this `key` exactly `listpacks_count` times.
+        After that, the `end_stream` method will be called.
+
+        """"""
+        pass
+
+    def stream_listpack(self, key, entry_id, data):
+        """"""
+        Callback to insert a listpack into a stream
+
+        `key` is the redis key for this stream
+        `entry_id` is binary (bigendian)
+        `data` the bytes of the listpack
+
+        """"""
+        pass
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        """"""
+        Called when there is no more data in the stream
+
+        `key` is the redis key for the stream
+        `items` is the total number of items in the stream
+        `last_entry_id` is in ""<millisecondsTime>-<sequenceNumber>"" format
+        `cgroups` is an array of consumer group metadata
+
+        """"""
+        pass
+
     def end_database(self, db_number):
         """"""
         Called when the current database ends
@@ -339,6 +381,8 @@ def __init__(self, callback, filters = None) :
         self._callback = callback
         self._key = None
         self._expiry = None
+        self._idle = None
+        self._freq = None
         self.init_filter(filters)
         self._rdb_version = 0
 
@@ -359,15 +403,25 @@ def parse_fd(self, fd):
             db_number = 0
             while True :
                 self._expiry = None
+                self._idle = None
+                self._freq = None
                 data_type = read_unsigned_char(f)
 
                 if data_type == REDIS_RDB_OPCODE_EXPIRETIME_MS :
-                    self._expiry = to_datetime(read_unsigned_long(f) * 1000)
+                    self._expiry = read_milliseconds_time(f)
                     data_type = read_unsigned_char(f)
                 elif data_type == REDIS_RDB_OPCODE_EXPIRETIME :
                     self._expiry = to_datetime(read_unsigned_int(f) * 1000000)
                     data_type = read_unsigned_char(f)
 
+                if data_type == REDIS_RDB_OPCODE_IDLE:
+                    self._idle = self.read_length(f)
+                    data_type = read_unsigned_char(f)
+
+                if data_type == REDIS_RDB_OPCODE_FREQ:
+                    self._freq = read_unsigned_char(f)
+                    data_type = read_unsigned_char(f)
+
                 if data_type == REDIS_RDB_OPCODE_SELECTDB :
                     if not is_first_database :
                         self._callback.end_database(db_number)
@@ -390,6 +444,10 @@ def parse_fd(self, fd):
                     self._callback.db_size(db_size, expire_size)
                     continue
 
+                if data_type == REDIS_RDB_OPCODE_MODULE_AUX:
+                    self.read_module(f)
+                    continue
+
                 if data_type == REDIS_RDB_OPCODE_EOF:
                     self._callback.end_database(db_number)
                     self._callback.end_rdb()
@@ -403,6 +461,7 @@ def parse_fd(self, fd):
                         self.read_object(f, data_type)
                     else:
                         self.skip_object(f, data_type)
+                    self._key = None
                 else :
                     self.skip_key_and_object(f, data_type)
 
@@ -472,14 +531,14 @@ def read_float(self, f):
     def read_object(self, f, enc_type) :
         if enc_type == REDIS_RDB_TYPE_STRING :
             val = self.read_string(f)
-            self._callback.set(self._key, val, self._expiry, info={'encoding':'string'})
+            self._callback.set(self._key, val, self._expiry, info={'encoding':'string','idle':self._idle,'freq':self._freq})
         elif enc_type == REDIS_RDB_TYPE_LIST :
             # A redis list is just a sequence of strings
             # We successively read strings from the stream and create a list from it
             # The lists are in order i.e. the first string is the head, 
             # and the last string is the tail of the list
             length = self.read_length(f)
-            self._callback.start_list(self._key, self._expiry, info={'encoding':'linkedlist' })
+            self._callback.start_list(self._key, self._expiry, info={'encoding':'linkedlist','idle':self._idle,'freq':self._freq})
             for count in range(0, length) :
                 val = self.read_string(f)
                 self._callback.rpush(self._key, val)
@@ -489,22 +548,22 @@ def read_object(self, f, enc_type) :
             # We successively read strings from the stream and create a set from it
             # Note that the order of strings is non-deterministic
             length = self.read_length(f)
-            self._callback.start_set(self._key, length, self._expiry, info={'encoding':'hashtable'})
+            self._callback.start_set(self._key, length, self._expiry, info={'encoding':'hashtable','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 val = self.read_string(f)
                 self._callback.sadd(self._key, val)
             self._callback.end_set(self._key)
         elif enc_type == REDIS_RDB_TYPE_ZSET or enc_type == REDIS_RDB_TYPE_ZSET_2 :
             length = self.read_length(f)
-            self._callback.start_sorted_set(self._key, length, self._expiry, info={'encoding':'skiplist'})
+            self._callback.start_sorted_set(self._key, length, self._expiry, info={'encoding':'skiplist','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 val = self.read_string(f)
                 score = read_double(f) if enc_type == REDIS_RDB_TYPE_ZSET_2 else self.read_float(f)
                 self._callback.zadd(self._key, score, val)
             self._callback.end_sorted_set(self._key)
         elif enc_type == REDIS_RDB_TYPE_HASH:
             length = self.read_length(f)
-            self._callback.start_hash(self._key, length, self._expiry, info={'encoding':'hashtable'})
+            self._callback.start_hash(self._key, length, self._expiry, info={'encoding':'hashtable','idle':self._idle,'freq':self._freq})
             for count in range(0, length):
                 field = self.read_string(f)
                 value = self.read_string(f)
@@ -526,6 +585,8 @@ def read_object(self, f, enc_type) :
             raise Exception('read_object', 'Unable to read Redis Modules RDB objects (key %s)' % self._key)
         elif enc_type == REDIS_RDB_TYPE_MODULE_2:
             self.read_module(f)
+        elif enc_type == REDIS_RDB_TYPE_STREAM_LISTPACKS:
+            self.read_stream(f)
         else:
             raise Exception('read_object', 'Invalid object type %d for key %s' % (enc_type, self._key))
 
@@ -592,7 +653,9 @@ def skip_object(self, f, enc_type):
         elif enc_type == REDIS_RDB_TYPE_MODULE:
             raise Exception('skip_object', 'Unable to skip Redis Modules RDB objects (key %s)' % self._key)
         elif enc_type == REDIS_RDB_TYPE_MODULE_2:
-            self.read_module(f)
+            self.skip_module(f)
+        elif enc_type == REDIS_RDB_TYPE_STREAM_LISTPACKS:
+            self.skip_stream(f)
         else:
             raise Exception('skip_object', 'Invalid object type %d for key %s' % (enc_type, self._key))
         for x in range(0, skip_strings):
@@ -604,7 +667,7 @@ def read_intset(self, f) :
         buff = BytesIO(raw_string)
         encoding = read_unsigned_int(buff)
         num_entries = read_unsigned_int(buff)
-        self._callback.start_set(self._key, num_entries, self._expiry, info={'encoding':'intset', 'sizeof_value':len(raw_string)})
+        self._callback.start_set(self._key, num_entries, self._expiry, info={'encoding':'intset', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             if encoding == 8 :
                 entry = read_signed_long(buff)
@@ -623,7 +686,7 @@ def read_ziplist(self, f) :
         zlbytes = read_unsigned_int(buff)
         tail_offset = read_unsigned_int(buff)
         num_entries = read_unsigned_short(buff)
-        self._callback.start_list(self._key, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string)})
+        self._callback.start_list(self._key, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             val = self.read_ziplist_entry(buff)
             self._callback.rpush(self._key, val)
@@ -635,7 +698,7 @@ def read_ziplist(self, f) :
     def read_list_from_quicklist(self, f):
         count = self.read_length(f)
         total_size = 0
-        self._callback.start_list(self._key, self._expiry, info={'encoding': 'quicklist', 'zips': count})
+        self._callback.start_list(self._key, self._expiry, info={'encoding': 'quicklist', 'zips': count,'idle':self._idle,'freq':self._freq})
         for i in range(0, count):
             raw_string = self.read_string(f)
             total_size += len(raw_string)
@@ -659,7 +722,7 @@ def read_zset_from_ziplist(self, f) :
         if (num_entries % 2) :
             raise Exception('read_zset_from_ziplist', ""Expected even number of elements, but found %d for key %s"" % (num_entries, self._key))
         num_entries = num_entries // 2
-        self._callback.start_sorted_set(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string)})
+        self._callback.start_sorted_set(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             member = self.read_ziplist_entry(buff)
             score = self.read_ziplist_entry(buff)
@@ -680,7 +743,7 @@ def read_hash_from_ziplist(self, f) :
         if (num_entries % 2) :
             raise Exception('read_hash_from_ziplist', ""Expected even number of elements, but found %d for key %s"" % (num_entries, self._key))
         num_entries = num_entries // 2
-        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string)})
+        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'ziplist', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         for x in range(0, num_entries) :
             field = self.read_ziplist_entry(buff)
             value = self.read_ziplist_entry(buff)
@@ -727,7 +790,7 @@ def read_zipmap(self, f) :
         raw_string = self.read_string(f)
         buff = io.BytesIO(bytearray(raw_string))
         num_entries = read_unsigned_char(buff)
-        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'zipmap', 'sizeof_value':len(raw_string)})
+        self._callback.start_hash(self._key, num_entries, self._expiry, info={'encoding':'zipmap', 'sizeof_value':len(raw_string),'idle':self._idle,'freq':self._freq})
         while True :
             next_length = self.read_zipmap_next_length(buff)
             if next_length is None :
@@ -756,13 +819,29 @@ def read_zipmap_next_length(self, f) :
         else:
             return None
 
+    def skip_module(self, f):
+        opcode = self.read_length(f)
+        while opcode != REDIS_RDB_MODULE_OPCODE_EOF:
+            if opcode == REDIS_RDB_MODULE_OPCODE_SINT or opcode == REDIS_RDB_MODULE_OPCODE_UINT:
+                self.read_length(f)
+            elif opcode == REDIS_RDB_MODULE_OPCODE_FLOAT:
+                self.skip_float(f)
+            elif opcode == REDIS_RDB_MODULE_OPCODE_DOUBLE:
+                read_double(f)
+            elif opcode == REDIS_RDB_MODULE_OPCODE_STRING:
+                self.skip_string(f)
+            else:
+                raise Exception(""Unknown module opcode %s"" % opcode)
+            # read the next item in the module data type
+            opcode = self.read_length(f)
+
     def read_module(self, f):
         # this method is based on the actual implementation in redis (src/rdb.c:rdbLoadObject)
         iowrapper = IOWrapper(f)
         iowrapper.start_recording_size()
         iowrapper.start_recording()
         length, encoding = self.read_length_with_encoding(iowrapper)
-        record_buffer = self._callback.start_module(self._key, self._decode_module_id(length), self._expiry)
+        record_buffer = self._callback.start_module(self._key, self._decode_module_id(length), self._expiry, info={'idle':self._idle, 'freq':self._freq})
 
         if not record_buffer:
             iowrapper.stop_recording()
@@ -790,6 +869,72 @@ def read_module(self, f):
             iowrapper.stop_recording()
         self._callback.end_module(self._key, buffer_size=iowrapper.get_recorded_size(), buffer=buffer)
 
+    def skip_stream(self, f):
+        listpacks = self.read_length(f)
+        for _lp in range(listpacks):
+            self.skip_string(f)
+            self.skip_string(f)
+        self.read_length(f)
+        self.read_length(f)
+        self.read_length(f)
+        cgroups = self.read_length(f)
+        for _cg in range(cgroups):
+            self.skip_string(f)
+            self.read_length(f)
+            self.read_length(f)
+            pending = self.read_length(f)
+            for _pel in range(pending):
+                f.read(16)
+                f.read(8)
+                self.read_length(f)
+            consumers = self.read_length(f)
+            for _c in range(consumers):
+                self.skip_string(f)
+                f.read(8)
+                pending = self.read_length(f)
+                f.read(pending*16)
+
+    def read_stream(self, f):
+        listpacks = self.read_length(f)
+        self._callback.start_stream(self._key, listpacks, self._expiry,
+                                    info={'encoding': 'listpack', 'idle': self._idle, 'freq': self._freq})
+        for _lp in range(listpacks):
+            self._callback.stream_listpack(self._key, self.read_string(f), self.read_string(f))
+        items = self.read_length(f)
+        last_entry_id = ""%s-%s"" % (self.read_length(f), self.read_length(f))
+        cgroups = self.read_length(f)
+        cgroups_data = []
+        for _cg in range(cgroups):
+            cgname = self.read_string(f)
+            last_cg_entry_id = ""%s-%s"" % (self.read_length(f), self.read_length(f))
+            pending = self.read_length(f)
+            group_pending_entries = []
+            for _pel in range(pending):
+                eid = f.read(16)
+                delivery_time = read_milliseconds_time(f)
+                delivery_count = self.read_length(f)
+                group_pending_entries.append({'id': eid,
+                                              'delivery_time': delivery_time,
+                                              'delivery_count': delivery_count})
+            consumers = self.read_length(f)
+            consumers_data = []
+            for _c in range(consumers):
+                cname = self.read_string(f)
+                seen_time = read_milliseconds_time(f)
+                pending = self.read_length(f)
+                consumer_pending_entries = []
+                for _pel in range( pending):
+                    eid = f.read(16)
+                    consumer_pending_entries.append({'id': eid})
+                consumers_data.append({'name': cname,
+                                       'seen_time': seen_time,
+                                       'pending': consumer_pending_entries})
+            cgroups_data.append({'name': cgname,
+                                 'last_entry_id': last_cg_entry_id,
+                                 'pending': group_pending_entries,
+                                 'consumers': consumers_data})
+        self._callback.end_stream(self._key, items, last_entry_id, cgroups_data)
+
     charset = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_'
 
     def _decode_module_id(self, module_id):
@@ -812,7 +957,7 @@ def verify_magic_string(self, magic_string) :
 
     def verify_version(self, version_str) :
         version = int(version_str)
-        if version < 1 or version > 8: 
+        if version < 1 or version > 9:
             raise Exception('verify_version', 'Invalid RDB version number %d' % version)
         self._rdb_version = version
 
@@ -840,14 +985,13 @@ def init_filter(self, filters):
         else:
             self._filters['not_keys'] = str2regexp(filters['not_keys'])
 
-        if not 'types' in filters:
-            self._filters['types'] = ('set', 'hash', 'sortedset', 'module', 'string', 'list')
-        elif isinstance(filters['types'], bytes):
-            self._filters['types'] = (filters['types'], )
-        elif isinstance(filters['types'], list):
-            self._filters['types'] = [str(x) for x in filters['types']]
-        else:
-            raise Exception('init_filter', 'invalid value for types in filter %s' %filters['types'])
+        if 'types' in filters:
+            if isinstance(filters['types'], bytes):
+                self._filters['types'] = (filters['types'], )
+            elif isinstance(filters['types'], list):
+                self._filters['types'] = [str(x) for x in filters['types']]
+            else:
+                raise Exception('init_filter', 'invalid value for types in filter %s' %filters['types'])
         
     def matches_filter(self, db_number, key=None, data_type=None):
 
@@ -865,7 +1009,7 @@ def matches_filter(self, db_number, key=None, data_type=None):
         if key and (not self._filters['keys'].match(key_to_match)):
             return False
 
-        if data_type is not None and (not self.get_logical_type(data_type) in self._filters['types']):
+        if data_type is not None and 'types' in self._filters and (not self.get_logical_type(data_type) in self._filters['types']):
             return False
         return True
     
@@ -954,6 +1098,9 @@ def read_signed_long(f) :
 def read_unsigned_long(f) :
     return struct.unpack('Q', f.read(8))[0]
     
+def read_milliseconds_time(f) :
+    return to_datetime(read_unsigned_long(f) * 1000)
+
 def read_unsigned_long_be(f) :
     return struct.unpack('>Q', f.read(8))[0]
 "
25;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -4,6 +4,7 @@
 import random
 import sys
 from io import BytesIO
+import traceback
 
 from rdbtools import RdbParser
 from rdbtools import encodehelpers
@@ -79,8 +80,8 @@ def test_all_dumps(self):
             try:
                 parser.parse(dump_name)
             except Exception as err:
-                raise self.failureException(""%s on %s - %s: %s"" % (
-                    self._callback_class.__name__, os.path.basename(dump_name), type(err).__name__, str(err)))
+                raise self.failureException(""%s on %s\n%s"" % (
+                    self._callback_class.__name__, os.path.basename(dump_name), traceback.format_exc()))
             self._out.seek(0)
             self._out.truncate()
 "
25;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -89,3 +89,12 @@ def test_rdb_with_module(self):
                                        bytes=101, encoding='ReJSON-RL', size=1,
                                        len_largest_element=101, expiry=None)
         self.assertEquals(stats['foo'], expected_record)
+
+    def test_rdb_with_stream(self):
+        stats = get_stats('redis_50_with_streams.rdb')
+
+        self.assertTrue('mystream' in stats)
+        expected_record = MemoryRecord(database=0, type='stream', key='mystream',
+                                       bytes=1976, encoding='listpack', size=1,
+                                       len_largest_element=184, expiry=None)
+        self.assertEquals(stats['mystream'], expected_record)"
25;sripathikrishnan;redis-rdb-tools;08058f4aa171eeaf245d50ebf82c5acc8c815bee;"add support for rdb v9 (redis 5.0) and memory analysis of streams #128 (#131)

other changes:
* adding both streams and modules to callbacks (with minimal info)
  at least providing an indication that the key exists.
* adding LRU and LFU metadata from RDBv9 if exists.
* start_module callback was missing info dict for metadata.
* supporting module AUX data (out of keyspace module data)
* module filtering was inefficient (called read_module rather than skip_module)
* tests and minor test suite improvement";"@@ -204,6 +204,16 @@ def test_multiple_databases_stream(self):
         self.assertEquals(r.databases[0][b""key_in_zeroth_database""], b""zero"")
         self.assertEquals(r.databases[2][b""key_in_second_database""], b""second"")
 
+    def test_rdb_version_8_with_module(self):
+        r = load_rdb('redis_40_with_module.rdb')
+        self.assertEquals(r.databases[0][b'foo']['module_name'], 'ReJSON-RL')
+
+    def test_rdb_version_9_with_stream(self):
+        r = load_rdb('redis_50_with_streams.rdb')
+        self.assertEquals(r.lengths[0][b""mystream""], 4)
+        self.assertEquals(len(r.databases[0][b'mystream']), 1)
+
+
 def floateq(f1, f2) :
     return math.fabs(f1 - f2) < 0.00001
 
@@ -339,6 +349,41 @@ def end_sorted_set(self, key):
             raise Exception('Lengths mismatch on sortedset %s, expected length = %d, actual = %d'
                                  % (key, self.lengths[self.dbnum][key], len(self.currentdb()[key])))
 
+    def start_module(self, key, module_name, expiry, info):
+        if key in self.currentdb() :
+            raise Exception('start_module called with key %s that already exists' % key)
+        else :
+            self.currentdb()[key] = {'module_name': module_name}
+        if expiry :
+            self.store_expiry(key, expiry)
+        return False
+
+    def end_module(self, key, buffer_size, buffer=None):
+        if not key in self.currentdb() :
+            raise Exception('start_module not called for key = %s', key)
+        self.store_length(key, buffer_size)
+        pass
+
+    def start_stream(self, key, listpacks_count, expiry, info):
+        if key in self.currentdb() :
+            raise Exception('start_stream called with key %s that already exists' % key)
+        else :
+            self.currentdb()[key] = {}
+        if expiry :
+            self.store_expiry(key, expiry)
+        pass
+
+    def stream_listpack(self, key, entry_id, data):
+        if not key in self.currentdb() :
+            raise Exception('start_hash not called for key = %s', key)
+        self.currentdb()[key][entry_id] = data
+        pass
+
+    def end_stream(self, key, items, last_entry_id, cgroups):
+        if not key in self.currentdb() :
+            raise Exception('start_stream not called for key = %s', key)
+        self.store_length(key, items)
+
     def end_database(self, dbnum):
         if self.dbnum != dbnum :
             raise Exception('start_database called with %d, but end_database called %d instead' % (self.dbnum, dbnum))"
25;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -9,4 +9,7 @@ tests/dumps/dump_dealers_vins.rdb
 tests/dumps/dump_random_lists.rdb
 tests/dumps/dump_sorted_sets.rdb
 
-.idea/*
\ No newline at end of file
+.idea/*
+venv2.6/
+venv2.7/
+venv3/
\ No newline at end of file"
25;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -18,7 +18,7 @@
 ZSKIPLIST_P=0.25
 REDIS_SHARED_INTEGERS = 10000
 
-MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element'])
+MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element', 'expiry'])
 
 class StatsAggregator(object):
     def __init__(self, key_groupings = None):
@@ -83,8 +83,8 @@ def __init__(self, out, bytes, largest):
         self._bytes = bytes
         self._largest = largest
         self._out = out
-        headers = ""%s,%s,%s,%s,%s,%s,%s\n"" % (
-            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"")
+        headers = ""%s,%s,%s,%s,%s,%s,%s,%s\n"" % (
+            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"", ""expiry"")
         self._out.write(codecs.encode(headers, 'latin-1'))
 
         if self._largest is not None:
@@ -95,9 +95,10 @@ def next_record(self, record) :
             return  # some records are not keys (e.g. dict)
         if self._largest is None:
             if self._bytes is None or record.bytes >= int(self._bytes):
-                rec_str = ""%d,%s,%s,%d,%s,%d,%d\n"" % (
+                rec_str = ""%d,%s,%s,%d,%s,%d,%d,%s\n"" % (
                     record.database, record.type, record.key, record.bytes, record.encoding, record.size,
-                    record.len_largest_element)
+                    record.len_largest_element,
+                    record.expiry.isoformat() if record.expiry else '')
                 self._out.write(codecs.encode(rec_str, 'latin-1'))
         else:
             heappush(self._heap, (record.bytes, record))
@@ -131,6 +132,7 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
         self._current_encoding = None
         self._current_length = 0
         self._len_largest_element = 0
+        self._key_expiry = None
         self._db_keys = 0
         self._db_expires = 0
         self._aux_used_mem = None
@@ -147,10 +149,10 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
             self._long_size = 4
             self._architecture = 32
 
-    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el):
+    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el, expiry):
         if key is not None:
             key = bytes_to_unicode(key, self._escape, skip_printable=True)
-        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el)
+        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el, expiry)
         self._stream.next_record(record)
 
     def start_rdb(self):
@@ -171,8 +173,8 @@ def start_database(self, db_number):
         self._db_expires = 0
 
     def end_database(self, db_number):
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None)
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None, None)
         if hasattr(self._stream, 'end_database'):
             self._stream.end_database(db_number)
 
@@ -184,14 +186,14 @@ def end_rdb(self):
     def set(self, key, value, expiry, info):
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry) + self.sizeof_string(value)
-        
         length = self.element_length(value)
-        self.emit_record(""string"", key, size, self._current_encoding, length, length)
+        self.emit_record(""string"", key, size, self._current_encoding, length, length, expiry)
         self.end_key()
     
     def start_hash(self, key, length, expiry, info):
         self._current_encoding = info['encoding']
-        self._current_length = length        
+        self._current_length = length
+        self._key_expiry = expiry
         size = self.top_level_object_overhead(key, expiry)
         
         if 'sizeof_value' in info:
@@ -217,7 +219,7 @@ def hset(self, key, field, value):
     
     def end_hash(self, key):
         self.emit_record(""hash"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_set(self, key, cardinality, expiry, info):
@@ -236,7 +238,7 @@ def sadd(self, key, member):
     
     def end_set(self, key):
         self.emit_record(""set"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_list(self, key, expiry, info):
@@ -245,6 +247,7 @@ def start_list(self, key, expiry, info):
         self._list_items_zipped_size = 0
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
+        self._key_expiry = expiry
 
         # ignore the encoding in the rdb, and predict the encoding that will be used at the target redis version
         if self._redis_version >= StrictVersion('3.2'):
@@ -297,10 +300,11 @@ def end_list(self, key, info):
                 self._current_size += self.robj_overhead() * self._current_length
             self._current_size += self._list_items_size
         self.emit_record(""list"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
 
     def start_module(self, key, module_id, expiry):
+        self._key_expiry = expiry
         self._current_encoding = module_id
         self._current_size = self.top_level_object_overhead(key, expiry)
         self._current_size += 8 + 1  # add the module id length and EOF byte
@@ -309,14 +313,15 @@ def start_module(self, key, module_id, expiry):
 
     def end_module(self, key, buffer_size, buffer=None):
         size = self._current_size + buffer_size
-        self.emit_record(""module"", key, size, self._current_encoding, 1, size)
+        self.emit_record(""module"", key, size, self._current_encoding, 1, size, self._key_expiry)
         self.end_key()
 
     def start_sorted_set(self, key, length, expiry, info):
         self._current_length = length
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
-        
+        self._key_expiry = expiry
+
         if 'sizeof_value' in info:
             size += info['sizeof_value']
         elif 'encoding' in info and info['encoding'] == 'skiplist':
@@ -338,14 +343,15 @@ def zadd(self, key, score, member):
     
     def end_sorted_set(self, key):
         self.emit_record(""sortedset"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
         
     def end_key(self):
         self._db_keys += 1
         self._current_encoding = None
         self._current_size = 0
         self._len_largest_element = 0
+        self._key_expiry = None
     
     def sizeof_string(self, string):
         # https://github.com/antirez/redis/blob/unstable/src/sds.h"
25;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -1026,5 +1026,3 @@ def end_database(self, db_number):
     
     def end_rdb(self):
         print(']')
-
-"
25;sripathikrishnan;redis-rdb-tools;c02b41041979268ee08131f0a239b1d9f89e9040;"Merge pull request #109 from hashedin/expiry_in_csv

Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC. For keys that don't have an expiry, this column is blank.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing. With this new column, it is now possible to aggregate keys on basis of expiry. For example, you can now identify that 50% of memory used is resident/never expires, 30% is set to expire in the next hour and so on.";"@@ -1,11 +1,27 @@
+import sys
+import os
+from io import BytesIO
+
 import unittest
 
 from rdbtools import RdbParser
 from rdbtools import MemoryCallback
-import os
 
-from rdbtools.memprofiler import MemoryRecord
 
+from rdbtools.memprofiler import MemoryRecord, PrintAllKeys
+
+CSV_WITH_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,string,expires_ms_precision,128,string,27,27,2022-12-25T10:11:12.573000
+""""""
+
+CSV_WITHOUT_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,list,ziplist_compresses_easily,301,quicklist,6,36,
+""""""
+
+CSV_WITH_MODULE = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,string,simplekey,72,string,7,7,
+0,module,foo,101,ReJSON-RL,1,101,
+""""""
 
 class Stats(object):
     def __init__(self):
@@ -22,11 +38,43 @@ def get_stats(file_name):
     parser.parse(os.path.join(os.path.dirname(__file__), 'dumps', file_name))
     return stats.records
 
+def get_csv(dump_file_name):
+    buff = BytesIO()
+    callback = MemoryCallback(PrintAllKeys(buff, None, None), 64)
+    parser = RdbParser(callback)
+    parser.parse(os.path.join(os.path.dirname(__file__), 
+                    'dumps', dump_file_name))
+    csv = buff.getvalue().decode()
+    return csv
 
 class MemoryCallbackTestCase(unittest.TestCase):
     def setUp(self):
         pass
 
+    def test_csv_with_expiry(self):
+        csv = get_csv('keys_with_expiry.rdb')
+        self.assertEquals(csv, CSV_WITH_EXPIRY)
+
+    def test_csv_without_expiry(self):
+        csv = get_csv('ziplist_that_compresses_easily.rdb')
+        self.assertEquals(csv, CSV_WITHOUT_EXPIRY)
+
+    def test_csv_with_module(self):
+        csv = get_csv('redis_40_with_module.rdb')
+        self.assertEquals(csv, CSV_WITH_MODULE)
+
+    def test_expiry(self):
+        stats = get_stats('keys_with_expiry.rdb')
+
+        expiry = stats['expires_ms_precision'].expiry
+        self.assertEquals(expiry.year, 2022)
+        self.assertEquals(expiry.month, 12)
+        self.assertEquals(expiry.day, 25)
+        self.assertEquals(expiry.hour, 10)
+        self.assertEquals(expiry.minute, 11)
+        self.assertEquals(expiry.second, 12)
+        self.assertEquals(expiry.microsecond, 573000)        
+
     def test_len_largest_element(self):
         stats = get_stats('ziplist_that_compresses_easily.rdb')
 
@@ -39,5 +87,5 @@ def test_rdb_with_module(self):
         self.assertTrue('foo' in stats)
         expected_record = MemoryRecord(database=0, type='module', key='foo',
                                        bytes=101, encoding='ReJSON-RL', size=1,
-                                       len_largest_element=101)
+                                       len_largest_element=101, expiry=None)
         self.assertEquals(stats['foo'], expected_record)"
25;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -9,4 +9,7 @@ tests/dumps/dump_dealers_vins.rdb
 tests/dumps/dump_random_lists.rdb
 tests/dumps/dump_sorted_sets.rdb
 
-.idea/*
\ No newline at end of file
+.idea/*
+venv2.6/
+venv2.7/
+venv3/
\ No newline at end of file"
25;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -18,7 +18,7 @@
 ZSKIPLIST_P=0.25
 REDIS_SHARED_INTEGERS = 10000
 
-MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element'])
+MemoryRecord = namedtuple('MemoryRecord', ['database', 'type', 'key', 'bytes', 'encoding','size', 'len_largest_element', 'expiry'])
 
 class StatsAggregator(object):
     def __init__(self, key_groupings = None):
@@ -83,8 +83,8 @@ def __init__(self, out, bytes, largest):
         self._bytes = bytes
         self._largest = largest
         self._out = out
-        headers = ""%s,%s,%s,%s,%s,%s,%s\n"" % (
-            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"")
+        headers = ""%s,%s,%s,%s,%s,%s,%s,%s\n"" % (
+            ""database"", ""type"", ""key"", ""size_in_bytes"", ""encoding"", ""num_elements"", ""len_largest_element"", ""expiry"")
         self._out.write(codecs.encode(headers, 'latin-1'))
 
         if self._largest is not None:
@@ -95,9 +95,10 @@ def next_record(self, record) :
             return  # some records are not keys (e.g. dict)
         if self._largest is None:
             if self._bytes is None or record.bytes >= int(self._bytes):
-                rec_str = ""%d,%s,%s,%d,%s,%d,%d\n"" % (
+                rec_str = ""%d,%s,%s,%d,%s,%d,%d,%s\n"" % (
                     record.database, record.type, record.key, record.bytes, record.encoding, record.size,
-                    record.len_largest_element)
+                    record.len_largest_element,
+                    record.expiry.isoformat() if record.expiry else '')
                 self._out.write(codecs.encode(rec_str, 'latin-1'))
         else:
             heappush(self._heap, (record.bytes, record))
@@ -131,6 +132,7 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
         self._current_encoding = None
         self._current_length = 0
         self._len_largest_element = 0
+        self._key_expiry = None
         self._db_keys = 0
         self._db_expires = 0
         self._aux_used_mem = None
@@ -147,10 +149,10 @@ def __init__(self, stream, architecture, redis_version='3.2', string_escape=None
             self._long_size = 4
             self._architecture = 32
 
-    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el):
+    def emit_record(self, record_type, key, byte_count, encoding, size, largest_el, expiry):
         if key is not None:
             key = bytes_to_unicode(key, self._escape, skip_printable=True)
-        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el)
+        record = MemoryRecord(self._dbnum, record_type, key, byte_count, encoding, size, largest_el, expiry)
         self._stream.next_record(record)
 
     def start_rdb(self):
@@ -171,8 +173,8 @@ def start_database(self, db_number):
         self._db_expires = 0
 
     def end_database(self, db_number):
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None)
-        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_keys), None, None, None, None)
+        self.emit_record(""dict"", None, self.hashtable_overhead(self._db_expires), None, None, None, None)
         if hasattr(self._stream, 'end_database'):
             self._stream.end_database(db_number)
 
@@ -184,14 +186,14 @@ def end_rdb(self):
     def set(self, key, value, expiry, info):
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry) + self.sizeof_string(value)
-        
         length = self.element_length(value)
-        self.emit_record(""string"", key, size, self._current_encoding, length, length)
+        self.emit_record(""string"", key, size, self._current_encoding, length, length, expiry)
         self.end_key()
     
     def start_hash(self, key, length, expiry, info):
         self._current_encoding = info['encoding']
-        self._current_length = length        
+        self._current_length = length
+        self._key_expiry = expiry
         size = self.top_level_object_overhead(key, expiry)
         
         if 'sizeof_value' in info:
@@ -217,7 +219,7 @@ def hset(self, key, field, value):
     
     def end_hash(self, key):
         self.emit_record(""hash"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_set(self, key, cardinality, expiry, info):
@@ -236,7 +238,7 @@ def sadd(self, key, member):
     
     def end_set(self, key):
         self.emit_record(""set"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
     
     def start_list(self, key, expiry, info):
@@ -245,6 +247,7 @@ def start_list(self, key, expiry, info):
         self._list_items_zipped_size = 0
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
+        self._key_expiry = expiry
 
         # ignore the encoding in the rdb, and predict the encoding that will be used at the target redis version
         if self._redis_version >= StrictVersion('3.2'):
@@ -297,7 +300,7 @@ def end_list(self, key, info):
                 self._current_size += self.robj_overhead() * self._current_length
             self._current_size += self._list_items_size
         self.emit_record(""list"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
 
     def start_module(self, key, module_id, expiry):
@@ -309,14 +312,15 @@ def start_module(self, key, module_id, expiry):
 
     def end_module(self, key, buffer_size, buffer=None):
         size = self._current_size + buffer_size
-        self.emit_record(""module"", key, size, self._current_encoding, 1, size)
+        self.emit_record(""module"", key, size, self._current_encoding, 1, size, None)
         self.end_key()
 
     def start_sorted_set(self, key, length, expiry, info):
         self._current_length = length
         self._current_encoding = info['encoding']
         size = self.top_level_object_overhead(key, expiry)
-        
+        self._key_expiry = expiry
+
         if 'sizeof_value' in info:
             size += info['sizeof_value']
         elif 'encoding' in info and info['encoding'] == 'skiplist':
@@ -338,14 +342,15 @@ def zadd(self, key, score, member):
     
     def end_sorted_set(self, key):
         self.emit_record(""sortedset"", key, self._current_size, self._current_encoding, self._current_length,
-                         self._len_largest_element)
+                         self._len_largest_element, self._key_expiry)
         self.end_key()
         
     def end_key(self):
         self._db_keys += 1
         self._current_encoding = None
         self._current_size = 0
         self._len_largest_element = 0
+        self._key_expiry = None
     
     def sizeof_string(self, string):
         # https://github.com/antirez/redis/blob/unstable/src/sds.h"
25;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -1026,5 +1026,3 @@ def end_database(self, db_number):
     
     def end_rdb(self):
         print(']')
-
-"
25;sripathikrishnan;redis-rdb-tools;363aeec375a66e6ab6ac1e88751564dae56ba7e5;"Adding expiry to memory csv

This adds a new column called expiry to the memory csv file. This column contains the key expiry timestamp in ISO format. The timestamp is in UTC.

When analysing memory, it is useful to know if the key is ephemeral or permanent. Up until now, this information was missing.";"@@ -1,10 +1,22 @@
+import sys
+import os
+from io import BytesIO
+
 import unittest
 
 from rdbtools import RdbParser
 from rdbtools import MemoryCallback
-import os
 
-from rdbtools.memprofiler import MemoryRecord
+
+from rdbtools.memprofiler import MemoryRecord, PrintAllKeys
+
+CSV_WITH_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,string,expires_ms_precision,128,string,27,27,2022-12-25T10:11:12.573000
+""""""
+
+CSV_WITHOUT_EXPIRY = """"""database,type,key,size_in_bytes,encoding,num_elements,len_largest_element,expiry
+0,list,ziplist_compresses_easily,301,quicklist,6,36,
+""""""
 
 
 class Stats(object):
@@ -22,11 +34,39 @@ def get_stats(file_name):
     parser.parse(os.path.join(os.path.dirname(__file__), 'dumps', file_name))
     return stats.records
 
+def get_csv(dump_file_name):
+    buff = BytesIO()
+    callback = MemoryCallback(PrintAllKeys(buff, None, None), 64)
+    parser = RdbParser(callback)
+    parser.parse(os.path.join(os.path.dirname(__file__), 
+                    'dumps', dump_file_name))
+    csv = buff.getvalue().decode()
+    return csv
 
 class MemoryCallbackTestCase(unittest.TestCase):
     def setUp(self):
         pass
 
+    def test_csv_with_expiry(self):
+        csv = get_csv('keys_with_expiry.rdb')
+        self.assertEquals(csv, CSV_WITH_EXPIRY)
+
+    def test_csv_without_expiry(self):
+        csv = get_csv('ziplist_that_compresses_easily.rdb')
+        self.assertEquals(csv, CSV_WITHOUT_EXPIRY)
+
+    def test_expiry(self):
+        stats = get_stats('keys_with_expiry.rdb')
+
+        expiry = stats['expires_ms_precision'].expiry
+        self.assertEquals(expiry.year, 2022)
+        self.assertEquals(expiry.month, 12)
+        self.assertEquals(expiry.day, 25)
+        self.assertEquals(expiry.hour, 10)
+        self.assertEquals(expiry.minute, 11)
+        self.assertEquals(expiry.second, 12)
+        self.assertEquals(expiry.microsecond, 573000)        
+
     def test_len_largest_element(self):
         stats = get_stats('ziplist_that_compresses_easily.rdb')
 
@@ -39,5 +79,5 @@ def test_rdb_with_module(self):
         self.assertTrue('foo' in stats)
         expected_record = MemoryRecord(database=0, type='module', key='foo',
                                        bytes=101, encoding='ReJSON-RL', size=1,
-                                       len_largest_element=101)
+                                       len_largest_element=101, expiry=None)
         self.assertEquals(stats['foo'], expected_record)"
26;dwisiswant0;apkleaks;0d4993b8b3aaf038799cc04e5407839d714ed0ee;Fix unclosed file output that caused PermissionError (and memory leak potentially);"@@ -141,5 +141,6 @@ def cleanup(self):
 			self.fileout.close()
 			print(""%s\n** Results saved into '%s%s%s%s'%s."" % (col.HEADER, col.ENDC, col.OKGREEN, self.output, col.HEADER, col.ENDC))
 		else:
+			self.fileout.close()
 			os.remove(self.output)
 			util.writeln(""\n** Done with nothing. Â¯\\_(ãƒ„)_/Â¯"", col.WARNING)"
26;facebookresearch;MUSE;03af0aeb82ef43858be1dd35ef50beb45cac9a9b;more memory efficient CSLS;"@@ -130,7 +130,8 @@ def get_word_translation_accuracy(lang1, word2id1, emb1, lang2, word2id2, emb2,
         query = emb1[dico[:, 0]]
         scores = query.mm(emb2.transpose(0, 1))
         scores.mul_(2)
-        scores.sub_(average_dist1[dico[:, 0]][:, None] + average_dist2[None, :])
+        scores.sub_(average_dist1[dico[:, 0]][:, None])
+        scores.sub_(average_dist2[None, :])
 
     else:
         raise Exception('Unknown method: ""%s""' % method)"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -1531,28 +1531,6 @@ def activate_hw_preset(self, name, run_benchmarks=False):
     def enable_talkback(value: bool):
         enable_sentry_logger(value)
 
-    @rpc_utils.expose('net.peer.block')
-    def block_node(
-            self,
-            node_id: Union[str, list],
-            timeout_seconds: int = -1,
-    ) -> Tuple[bool, Optional[str]]:
-        if not self.task_server:
-            return False, 'Client is not ready'
-
-        try:
-            if isinstance(node_id, str):
-                node_id = [node_id]
-
-            for item in node_id:
-                self.task_server.disallow_node(item,
-                                               timeout_seconds=timeout_seconds,
-                                               persist=True)
-
-            return True, None
-        except Exception as e:  # pylint: disable=broad-except
-            return False, str(e)
-
 
 class DoWorkService(LoopingCallService):
     def __init__(self, client: Client) -> None:"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -70,12 +70,12 @@ def is_allowed(self, node_id: str) -> Tuple[bool, Optional[DenyReason]]:
         raise NotImplementedError
 
     @abc.abstractmethod
-    def disallow(self, node_id: str, timeout_seconds: int, persist: bool) \
-            -> None:
+    def disallow(self, node_id: str, timeout_seconds: int) \
+            -> bool:
         raise NotImplementedError
 
     @abc.abstractmethod
-    def allow(self, node_id: str, persist: bool) -> None:
+    def allow(self, node_id: str, persist: bool) -> bool:
         raise NotImplementedError
 
     @abc.abstractmethod
@@ -147,41 +147,43 @@ def is_allowed(self, node_id: str) -> Tuple[bool, Optional[DenyReason]]:
         return True, None
 
     def disallow(self, node_id: str,
-                 timeout_seconds: int = -1,
-                 persist: bool = False) -> None:
+                 timeout_seconds: int = -1) -> bool:
+        persist = timeout_seconds < 0
         logger.info(
             'Banned node. node_id=%s, timeout=%ds, persist=%s',
             common.short_node_id(node_id),
             timeout_seconds,
-            persist,
+            persist
         )
-
-        if timeout_seconds < 0:
+        if persist:
             self._deny_deadlines[node_id] = self._always
         else:
             if node_id not in self._deny_deadlines:
                 self._deny_deadlines[node_id] = SortedList(key=operator.neg)
             node_deadlines = self._deny_deadlines[node_id]
 
             if node_deadlines is self._always:
-                return
+                return False
 
             assert isinstance(node_deadlines, SortedList)
             node_deadlines.add(self._deadline(timeout_seconds))
 
-        if persist and timeout_seconds == -1:
+        if persist:
             try:
-                existNode = ACLDeniedNodes.get(node_id=node_id)
+                ACLDeniedNodes.get(node_id=node_id)
+                return False
             except ACLDeniedNodes.DoesNotExist:
-                existNode = None
-            if not existNode:
                 peers = self._client.p2pservice.incoming_peers or dict()
-                node = peers[node_id]
+                if node_id in peers:
+                    node = peers[node_id]
+                else:
+                    node = dict(node_name=""Unknown"")
                 node_db = ACLDeniedNodes(
                     node_id=node_id, node_name=node['node_name'])
                 node_db.save()
+        return True
 
-    def allow(self, node_id: str, persist: bool = False) -> None:
+    def allow(self, node_id: str, persist: bool = False) -> bool:
         logger.info(
             'Whitelist node. node_id=%s, persist=%s',
             common.short_node_id(node_id),
@@ -192,15 +194,15 @@ def allow(self, node_id: str, persist: bool = False) -> None:
 
         if persist:
             try:
-                existNode = ACLDeniedNodes.get(node_id=node_id)
+                ACLDeniedNodes.get(node_id=node_id)
             except ACLDeniedNodes.DoesNotExist:
-                existNode = None
-
-            if existNode:
+                return False
+            finally:
                 ACLDeniedNodes \
                     .delete() \
                     .where(ACLDeniedNodes.node_id == node_id) \
                     .execute()
+        return True
 
     def status(self) -> AclStatus:
         _always = self._always
@@ -273,30 +275,28 @@ def is_allowed(self, node_id: str) -> Tuple[bool, Optional[DenyReason]]:
         return False, DenyReason.not_whitelisted
 
     def disallow(self, node_id: str,
-                 timeout_seconds: int = 0,
-                 persist: bool = False) -> None:
-
+                 timeout_seconds: int = -1) -> bool:
+        persist = timeout_seconds < 0
         if persist:
             logger.info(
-                'Banned node. node_id=%s, timeout=%ds, persist=%s',
+                'Removed node. node_id=%s, timeout=%ds, persist=True',
                 common.short_node_id(node_id),
-                timeout_seconds,
-                persist,
+                timeout_seconds
             )
             self._allow_list = [node for node in self._allow_list if not (
                 node_id == node.node_id)]
             try:
-                existNode = ACLAllowedNodes.get(node_id=node_id)
+                ACLAllowedNodes.get(node_id=node_id)
             except ACLAllowedNodes.DoesNotExist:
-                existNode = None
-
-            if existNode:
+                return False
+            finally:
                 ACLAllowedNodes \
                     .delete() \
                     .where(ACLAllowedNodes.node_id == node_id) \
                     .execute()
+        return True
 
-    def allow(self, node_id: str, persist: bool = False) -> None:
+    def allow(self, node_id: str, persist: bool = False) -> bool:
         logger.info(
             'Whitelist node. node_id=%s, persist=%s',
             common.short_node_id(node_id),
@@ -312,12 +312,11 @@ def allow(self, node_id: str, persist: bool = False) -> None:
 
         if persist:
             try:
-                existNode = ACLAllowedNodes.get(node_id=node_id)
+                ACLAllowedNodes.get(node_id=node_id)
+                return False
             except ACLAllowedNodes.DoesNotExist:
-                existNode = None
-
-            if not existNode:
                 node_model.save()
+        return True
 
     def status(self) -> AclStatus:
         self._read_list()"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -255,8 +255,7 @@ def _handshake_timeout(self, key_id):
         )
         self.disallow_node(
             node_id=key_id,
-            timeout_seconds=variables.ACL_BLOCK_TIMEOUT_RESOURCE,
-            persist=False,
+            timeout_seconds=variables.ACL_BLOCK_TIMEOUT_RESOURCE
         )
         del self.resource_handshakes[key_id]
 "
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -76,8 +76,7 @@ def verification_finished(
                 # providers.
                 self.disallow_node(
                     node_id=node.key,
-                    timeout_seconds=config_desc.disallow_node_timeout_seconds,
-                    persist=False,
+                    timeout_seconds=config_desc.disallow_node_timeout_seconds
                 )
             if config_desc.disallow_ip_timeout_seconds is not None:
                 # Experimental feature. Try to spread subtasks fairly amongst"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -1260,17 +1260,30 @@ def should_accept_requestor(self, node_id):
             return SupportStatus.ok()
         return SupportStatus.err({UnsupportReason.REQUESTOR_TRUST: trust})
 
-    @rpc_utils.expose('net.peer.disallow')
+    @rpc_utils.expose('net.peer.block')
     def disallow_node(
             self,
             node_id: Union[str, list],
             timeout_seconds: int = -1,
-            persist: bool = False
-    ) -> None:
-        if isinstance(node_id, str):
-            node_id = [node_id]
-        for item in node_id:
-            self.acl.disallow(item, timeout_seconds, persist)
+    ) -> Tuple[bool, List[str], Optional[str]]:
+        '''
+        return Tuple
+        (is_disallow_success, list_of_already_disallowed_nodes, err_message)
+
+        Success: (True, [], None)
+        Success with existing node: (True, ['node_id'], None)
+        Error: (False, [], 'message')
+        '''
+        not_changed: List[str] = []
+        try:
+            if isinstance(node_id, str):
+                node_id = [node_id]
+            for item in node_id:
+                if not self.acl.disallow(item, timeout_seconds):
+                    not_changed.append(item)
+            return True, not_changed, None
+        except Exception as e:  # pylint: disable=broad-except
+            return False, not_changed, str(e)
 
     @rpc_utils.expose('net.peer.block_ip')
     def disallow_ip(self, ip: Union[str, list],
@@ -1281,12 +1294,29 @@ def disallow_ip(self, ip: Union[str, list],
             self.acl_ip.disallow(item, timeout_seconds)
 
     @rpc_utils.expose('net.peer.allow')
-    def allow_node(self, node_id: Union[str, list],
-                   persist: bool = True) -> None:
-        if isinstance(node_id, str):
-            node_id = [node_id]
-        for item in node_id:
-            self.acl.allow(item, persist)
+    def allow_node(
+            self,
+            node_id: Union[str, list],
+            persist: bool = True
+    ) -> Tuple[bool, List[str], Optional[str]]:
+        '''
+        return Tuple
+        (is_allow_success, list_of_already_allowed_nodes, err_message)
+
+        Success: (True, [], None)
+        Success with existing node: (True, ['node_id'], None)
+        Error: (False, [], 'message')
+        '''
+        not_changed: List[str] = []
+        try:
+            if isinstance(node_id, str):
+                node_id = [node_id]
+            for item in node_id:
+                if not self.acl.allow(item, persist):
+                    not_changed.append(item)
+            return True, not_changed, None
+        except Exception as e:  # pylint: disable=broad-except
+            return False, not_changed, str(e)
 
     @rpc_utils.expose('net.peer.allow_ip')
     def allow_ip(self, ip: Union[str, list], persist: bool = True) -> None:"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -159,5 +159,4 @@ def test_timeout(self, disallow_mock, *_):
         disallow_mock.assert_called_once_with(
             node_id=self.key_id,
             timeout_seconds=mock.ANY,
-            persist=False,
         )"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -72,7 +72,7 @@ def test_deny_always(self):
         assert acl.is_allowed(""Node1"")[0]
         assert acl.is_allowed(""Node2"")[0]
 
-        acl.disallow(""Node1"")
+        acl.disallow(""Node1"", timeout_seconds=10000)
         # should not throw
         acl.disallow(""Node1"")
         acl.disallow(""Node1"", 5)
@@ -198,7 +198,7 @@ def test_allow_allow_dissalow(self):
 
         acl = setup_acl(self.client, AclRule.deny, [])
         acl.allow('Node5', persist=True)
-        acl.disallow('Node4', persist=True)
+        acl.disallow('Node4')
 
         allowed_nodes = [r[0] for r in acl.status().rules]
         self.assertCountEqual(
@@ -222,9 +222,9 @@ def test_deny_disallow_allow_persistence(self):
         node_4.save()
 
         acl = get_acl(self.client)
-        acl.disallow('Node1', persist=True)
-        acl.disallow('Node2', persist=True)
-        acl.disallow('Node1', persist=True)
+        acl.disallow('Node1')
+        acl.disallow('Node2')
+        self.assertEqual(False, acl.disallow('Node1'))
         acl.allow('Node1')
         acl.allow('Node4', persist=True)
 
@@ -246,10 +246,10 @@ def test_deny_disallow_allow_persistence(self):
     def test_allow_disallow_persistence(self):
 
         acl = setup_acl(self.client, AclRule.deny, ['Node1', 'Node2'])
+        acl.disallow('Node1')
+        self.assertEqual(False, acl.disallow(""Node3""))
+        self.assertEqual(False, acl.disallow(""Node1""))
 
-        acl.disallow('Node1', persist=True)
-        acl.disallow('Node3', persist=True)
-        acl.disallow('Node1', persist=True)
         acl.allow('Node4')
 
         allowed_nodes = [r[0] for r in acl.status().rules]
@@ -291,12 +291,12 @@ def test_allow_disallow_ip_persistence(self):
         self.assertCountEqual([node['node_name'] for node in allowed_ips],
                               ['Node2', 'Node1', None])
         # Remove uknown IP
-        acl.disallow('4.247.45.93', persist=True)
+        acl.disallow('4.247.45.93')
         allowed_ips = [r[0] for r in acl.status().rules]
         self.assertCountEqual(
             [node['node_name'] for node in allowed_ips], ['Node1', 'Node2'])
         # Remove known IP
-        acl.disallow('34.107.145.130', persist=True)
+        acl.disallow('34.107.145.130')
         allowed_ips = [r[0] for r in acl.status().rules]
         self.assertCountEqual(
             [node['node_name'] for node in allowed_ips], ['Node2'])"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -150,7 +150,7 @@ def setUp(self, docker_env, *_):
         self.client.concent_service.enabled = False
         self.client.keys_auth.key_id = 'key_id'
         self.client.keys_auth.eth_addr = 'eth_addr'
-
+        self.client.p2pservice.incoming_peers = MagicMock()
         self.ts = TaskServer(
             node=dt_p2p_factory.Node(),
             config_desc=self.ccd,
@@ -721,10 +721,10 @@ def test_disallow_node(self, *_):
         ts.acl = Mock()
 
         # when
-        ts.disallow_node('ABC', 314, True)
+        ts.disallow_node('ABC', 314)
 
         # then
-        ts.acl.disallow.assert_called_once_with('ABC', 314, True)
+        ts.acl.disallow.assert_called_once_with('ABC', 314)
 
     def test_disallow_ip(self, *_):
         # given"
26;golemfactory;clay;b5f401a50ea9dc56e7cf294f05f7289a2c026eb9;"ACL improvement (#5095)

* Fix node_name check, drop 'net.peer.disallow' duplication of the 'net.peer.block'

* Raise exception if node is exist on block and not exist on allow

* Recontstuct net.peer.block in acl.py

* lint

* CI fix

* CI fix more

* lint test

* Remove persist flag from disallow definitions, use timeout -1 instead, update test

* Re-add persist information to the disallow log, to differentiate the ban types

* syntax fix

* Fix readability

* Return 'already exist' node ids as result if any while blocking/allowing nodes in ACL, test update

* type fix

* update supertype Acl

* lint

* lint more

* lint more and more

* lint more, more and more

* Change return disallow and allow definitions as bool, add info for returned tuple

* update missing types

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Update golem/task/taskserver.py

Co-Authored-By: Adam Mizerski <amizerski@golem.network>

* Return false if any node in memory already banned persistently

Co-authored-by: Adam Mizerski <amizerski@golem.network>
Co-authored-by: shadeofblue <blue@wave460.net>";"@@ -1246,12 +1246,13 @@ def test_port_status_other(self, *_):
 
     def test_block_node(self, *_):
         self.client.task_server.acl = Mock(spec=Acl)
-        self.client.block_node('node_id')
+        self.client.task_server.disallow_node('node_id', -1)
         self.client.task_server.acl.disallow.assert_called_once_with(
-            'node_id', -1, True)
-        self.client.block_node(['node_id_1', 'node_id_2'])
+            'node_id', -1)
+        self.client.task_server.disallow_node(
+            ['node_id_1', 'node_id_2'], -1)
         self.client.task_server.acl.disallow.assert_called_with(
-            'node_id_2', -1, True)
+            'node_id_2', -1)
 
     @classmethod
     def __new_incoming_peer(cls):"
26;ponyorm;pony;448b210055285afca13a04821bc277593c57f0b7;"PonyORM release 0.7.15 (2022-01-22)

# Features

* Add Python 3.10 support, drop support of Python < 3.7
* You can create an in-memory SQLite database accessible from multiple threads by specifying "":sharedmemory:"" as a file name

# Bugfixes

* Do not perform optimistic checks when deleting an object (it is OK if it was already deleted by concurrent transaction)
* Validation of int fields should take into account field size and check that the value is fit into the range
* More tests for hybrid methods added
* Fix incorrect assertion check `assert t is translator.`
* Fix aggregated query `sum(x.field for x in previous_query)`
* #594: Use a clearly synthetic filename when compiling dynamic code to not confuse coverage.py
* Use DecompileError exception instead of AssertionError when a function cannot be decompiled";"@@ -3,7 +3,7 @@
 import os, sys
 from os.path import dirname
 
-__version__ = '0.7.15rc1'
+__version__ = '0.7.15'
 
 def detect_mode():
     try: import google.appengine"
26;ponyorm;pony;8a6ccb7c6273ca173779cf8993ff05ffbada305e;SQLite memory database can be shared between connections;"@@ -724,11 +724,16 @@ def __call__(self, func=None, provider=None):
         OnConnectDecorator.check_provider(provider)
         return OnConnectDecorator(self.database, provider)
 
+
+db_id_counter = itertools.count(1)
+
+
 class Database(object):
     def __deepcopy__(self, memo):
         return self  # Database cannot be cloned by deepcopy()
     @cut_traceback
     def __init__(self, *args, **kwargs):
+        self.id = next(db_id_counter)
         # argument 'self' cannot be named 'database', because 'database' can be in kwargs
         self.priority = 0
         self._insert_cache = {}
@@ -779,7 +784,7 @@ def _bind(self, *args, **kwargs):
             provider_module = import_module('pony.orm.dbproviders.' + provider)
             provider_cls = provider_module.provider_cls
         kwargs['pony_call_on_connect'] = self.call_on_connect
-        self.provider = provider_cls(*args, **kwargs)
+        self.provider = provider_cls(self, *args, **kwargs)
     @property
     def last_sql(database):
         return database._dblocal.last_sql"
26;ponyorm;pony;8a6ccb7c6273ca173779cf8993ff05ffbada305e;SQLite memory database can be shared between connections;"@@ -122,7 +122,8 @@ class DBAPIProvider(object):
 
     fk_types = { 'SERIAL' : 'INTEGER', 'BIGSERIAL' : 'BIGINT' }
 
-    def __init__(provider, *args, **kwargs):
+    def __init__(provider, database, *args, **kwargs):
+        provider.database = database
         pool_mockup = kwargs.pop('pony_pool_mockup', None)
         call_on_connect = kwargs.pop('pony_call_on_connect', None)
         if pool_mockup: provider.pool = pool_mockup"
26;ponyorm;pony;8a6ccb7c6273ca173779cf8993ff05ffbada305e;SQLite memory database can be shared between connections;"@@ -1,7 +1,7 @@
 from __future__ import absolute_import
 from pony.py23compat import buffer, int_types
 
-import os.path, sys, re, json, datetime, time
+import os, os.path, sys, re, json, datetime, time
 import sqlite3 as sqlite
 from decimal import Decimal
 from random import random
@@ -337,8 +337,12 @@ class SQLiteProvider(DBAPIProvider):
         (Json, SQLiteJsonConverter)
     ]
 
-    def __init__(provider, *args, **kwargs):
-        DBAPIProvider.__init__(provider, *args, **kwargs)
+    def __init__(provider, database, filename, **kwargs):
+        is_shared_memory_db = filename == ':sharedmemory:'
+        if is_shared_memory_db:
+            filename = ""file:memdb%d_%s?mode=memory&cache=shared"" % (database.id, os.urandom(8).hex())
+            kwargs[""uri""] = True
+        DBAPIProvider.__init__(provider, database, is_shared_memory_db, filename, **kwargs)
         provider.pre_transaction_lock = Lock()
         provider.transaction_lock = Lock()
 
@@ -434,8 +438,10 @@ def release(provider, connection, cache=None):
                     raise
         DBAPIProvider.release(provider, connection, cache)
 
-    def get_pool(provider, filename, create_db=False, **kwargs):
-        if filename != ':memory:':
+    def get_pool(provider, is_shared_memory_db, filename, create_db=False, **kwargs):
+        if is_shared_memory_db or filename == ':memory:':
+            pass
+        else:
             # When relative filename is specified, it is considered
             # not relative to cwd, but to user module where
             # Database instance is created
@@ -450,7 +456,7 @@ def get_pool(provider, filename, create_db=False, **kwargs):
             # 1 - SQLiteProvider.__init__()
             # 0 - pony.dbproviders.sqlite.get_pool()
             filename = absolutize_path(filename, frame_depth=cut_traceback_depth+5)
-        return SQLitePool(filename, create_db, **kwargs)
+        return SQLitePool(is_shared_memory_db, filename, create_db, **kwargs)
 
     def table_exists(provider, connection, table_name, case_sensitive=True):
         return provider._exists(connection, table_name, None, case_sensitive)
@@ -645,15 +651,19 @@ def py_string_slice(s, start, end):
     return s[start:end]
 
 class SQLitePool(Pool):
-    def __init__(pool, filename, create_db, **kwargs): # called separately in each thread
+    def __init__(pool, is_shared_memory_db, filename, create_db, **kwargs): # called separately in each thread
+        pool.is_shared_memory_db = is_shared_memory_db
         pool.filename = filename
         pool.create_db = create_db
         pool.kwargs = kwargs
         pool.con = None
     def _connect(pool):
         filename = pool.filename
-        if filename != ':memory:' and not pool.create_db and not os.path.exists(filename):
+        if pool.is_shared_memory_db or pool.filename == ':memory:':
+            pass
+        elif not pool.create_db and not os.path.exists(filename):
             throw(IOError, ""Database file is not found: %r"" % filename)
+
         pool.con = con = sqlite.connect(filename, isolation_level=None, **pool.kwargs)
         con.text_factory = _text_factory
 
@@ -685,10 +695,12 @@ def create_function(name, num_params, func):
 
         con.execute('PRAGMA case_sensitive_like = true')
     def disconnect(pool):
-        if pool.filename != ':memory:':
+        if pool.is_shared_memory_db or pool.filename == ':memory:':
+            pass
+        else:
             Pool.disconnect(pool)
     def drop(pool, con):
-        if pool.filename != ':memory:':
-            Pool.drop(pool, con)
-        else:
+        if pool.is_shared_memory_db or pool.filename == ':memory:':
             con.rollback()
+        else:
+            Pool.drop(pool, con)"
26;ponyorm;pony;8a6ccb7c6273ca173779cf8993ff05ffbada305e;SQLite memory database can be shared between connections;"@@ -10,7 +10,7 @@ class TestFormatStyles(unittest.TestCase):
     def setUp(self):
         self.key1 = 'KEY1'
         self.key2 = 'KEY2'
-        self.provider = DBAPIProvider(pony_pool_mockup=TestPool(None))
+        self.provider = DBAPIProvider(database=None, pony_pool_mockup=TestPool(None))
         self.ast = [ 'SELECT', [ 'ALL', ['COLUMN', None, 'A']], [ 'FROM', [None, 'TABLE', 'T1']],
                      [ 'WHERE', [ 'EQ', ['COLUMN', None, 'B'], [ 'PARAM', self.key1 ] ],
                               [ 'EQ', ['COLUMN', None, 'C'], [ 'PARAM', self.key2 ] ],"
26;ponyorm;pony;8a6ccb7c6273ca173779cf8993ff05ffbada305e;SQLite memory database can be shared between connections;"@@ -0,0 +1,37 @@
+from __future__ import absolute_import, print_function, division
+
+import threading
+import unittest
+
+from pony.orm.core import *
+
+
+db = Database('sqlite', ':sharedmemory:')
+
+
+class Person(db.Entity):
+    name = Required(str)
+
+db.generate_mapping(create_tables=True)
+
+with db_session:
+    Person(name='John')
+    Person(name='Mike')
+
+
+class TestThread(threading.Thread):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, *kwargs)
+        self.result = []
+    def run(self):
+        with db_session:
+            persons = Person.select().fetch()
+            self.result.extend(p.name for p in persons)
+
+
+class TestFlush(unittest.TestCase):
+    def test1(self):
+        thread1 = TestThread()
+        thread1.start()
+        thread1.join()
+        self.assertEqual(set(thread1.result), {'John', 'Mike'})"
27;formatc1702;WireViz;80b7a5407bf2774ac899625367df78d8dc5b3e4a;"Improve gracefulness when invoking `wireviz.parse()` without `file_out`

This happened to be a regression for WireViz-Web [1], which aims to do
as much in memory as possible.

[1] https://github.com/daq-tools/wireviz-web.";"@@ -40,8 +40,13 @@ def parse(yaml_input: str, file_out: (str, Path) = None, return_types: (None, st
         options = Options(**yaml_data.get('options', {})),
         tweak = Tweak(**yaml_data.get('tweak', {})),
     )
+
+    # When title is not given, either deduce it from filename, or use default text.
     if 'title' not in harness.metadata:
-        harness.metadata['title'] = Path(file_out).stem
+        if file_out is None:
+            harness.metadata['title'] = ""WireViz diagram and BOM""
+        else:
+            harness.metadata['title'] = Path(file_out).stem
 
     # add items
     sections = ['connectors', 'cables', 'connections']"
27;JDAI-CV;fast-reid;62ad5e1a8b69a14663b6e8cfc06cbbd2d9c984f6;"bugfix for cuda tensor to numpy

Move cuda tensor to host memory in predictor which can be used for following process.";"@@ -151,7 +151,7 @@ def __call__(self, image):
         inputs = {""images"": image.to(self.model.device)}
         with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258
             predictions = self.model(inputs)
-        return predictions
+        return predictions.cpu()
 
 
 class DefaultTrainer(TrainerBase):"
27;JDAI-CV;fast-reid;7ed6240e2cb5e56e5ccd61744a3045f24ea7e62d;"fix ReidEvalution too much memory cost

Move `matches` matrix computation in each iteration to reduce the extra memory cost

#420 #404";"@@ -107,9 +107,6 @@ def eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank):
         print('Note: number of gallery samples is quite small, got {}'.format(num_g))
 
     indices = np.argsort(distmat, axis=1)
-
-    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)
-
     # compute cmc curve for each query
     all_cmc = []
     all_AP = []
@@ -127,7 +124,8 @@ def eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank):
         keep = np.invert(remove)
 
         # compute cmc curve
-        raw_cmc = matches[q_idx][keep]  # binary vector, positions with value 1 are correct matches
+        matches = (g_pids[order] == q_pid).astype(np.int32)
+        raw_cmc = matches[keep]  # binary vector, positions with value 1 are correct matches
         if not np.any(raw_cmc):
             # this condition is true when query identity does not appear in gallery
             continue
@@ -163,7 +161,7 @@ def eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank):
 
 def evaluate_py(distmat, q_pids, g_pids, q_camids, g_camids, max_rank, use_metric_cuhk03):
     if use_metric_cuhk03:
-        return eval_cuhk03(distmat, g_pids, q_camids, g_camids, max_rank)
+        return eval_cuhk03(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)
     else:
         return eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)
 
@@ -176,7 +174,7 @@ def evaluate_rank(
         g_camids,
         max_rank=50,
         use_metric_cuhk03=False,
-        use_cython=True
+        use_cython=True,
 ):
     """"""Evaluates CMC rank.
     Args:"
27;JDAI-CV;fast-reid;7ed6240e2cb5e56e5ccd61744a3045f24ea7e62d;"fix ReidEvalution too much memory cost

Move `matches` matrix computation in each iteration to reduce the extra memory cost

#420 #404";"@@ -159,7 +159,7 @@ cpdef eval_market1501_cy(float[:,:] distmat, long[:] q_pids, long[:]g_pids,
 
     cdef:
         long[:,:] indices = np.argsort(distmat, axis=1)
-        long[:,:] matches = (np.asarray(g_pids)[np.asarray(indices)] == np.asarray(q_pids)[:, np.newaxis]).astype(np.int64)
+        long[:] matches
 
         float[:,:] all_cmc = np.zeros((num_q, max_rank), dtype=np.float32)
         float[:] all_AP = np.zeros(num_q, dtype=np.float32)
@@ -192,14 +192,15 @@ cpdef eval_market1501_cy(float[:,:] distmat, long[:] q_pids, long[:]g_pids,
             order[g_idx] = indices[q_idx, g_idx]
         num_g_real = 0
         meet_condition = 0
+        matches = (np.asarray(g_pids)[np.asarray(order)] == q_pid).astype(np.int64)
 
         # remove gallery samples that have the same pid and camid with query
         for g_idx in range(num_g):
             if (g_pids[order[g_idx]] != q_pid) or (g_camids[order[g_idx]] != q_camid):
-                raw_cmc[num_g_real] = matches[q_idx][g_idx]
+                raw_cmc[num_g_real] = matches[g_idx]
                 num_g_real += 1
                 # this condition is true if query appear in gallery
-                if matches[q_idx][g_idx] > 1e-31:
+                if matches[g_idx] > 1e-31:
                     meet_condition = 1
 
         if not meet_condition:"
27;JDAI-CV;fast-reid;7ed6240e2cb5e56e5ccd61744a3045f24ea7e62d;"fix ReidEvalution too much memory cost

Move `matches` matrix computation in each iteration to reduce the extra memory cost

#420 #404";"@@ -185,7 +185,7 @@ def get_norm(norm, out_channels, **kwargs):
     """"""
     Args:
         norm (str or callable): either one of BN, GhostBN, FrozenBN, GN or SyncBN;
-            or a callable that thakes a channel number and returns
+            or a callable that takes a channel number and returns
             the normalization layer as a nn.Module
         out_channels: number of channels for normalization layer
 "
27;JDAI-CV;fast-reid;de81b3dbaa4cfc66c0ead7141ab364cefca88d2b;"fix ClasEvalution too much memory cost

Compute total number of correct predictions on each batch avoiding keeping all predicted logits, which will cost too much memory when the number of classes is large

#503";"@@ -46,11 +46,14 @@ def reset(self):
         self._predictions = []
 
     def process(self, inputs, outputs):
-        predictions = {
-            ""logits"": outputs.to(self._cpu_device, torch.float32),
-            ""labels"": inputs[""targets""].to(self._cpu_device),
-        }
-        self._predictions.append(predictions)
+        pred_logits = outputs.to(self._cpu_device, torch.float32)
+        labels = inputs[""targets""].to(self._cpu_device)
+
+        # measure accuracy
+        acc1, = accuracy(pred_logits, labels, topk=(1,))
+        num_correct_acc1 = acc1 * labels.size(0) / 100
+
+        self._predictions.append({""num_correct"": num_correct_acc1, ""num_samples"": labels.size(0)})
 
     def evaluate(self):
         if comm.get_world_size() > 1:
@@ -63,21 +66,16 @@ def evaluate(self):
         else:
             predictions = self._predictions
 
-        pred_logits = []
-        labels = []
+        total_correct_num = 0
+        total_samples = 0
         for prediction in predictions:
-            pred_logits.append(prediction['logits'])
-            labels.append(prediction['labels'])
+            total_correct_num += prediction[""num_correct""]
+            total_samples += prediction[""num_samples""]
 
-        pred_logits = torch.cat(pred_logits, dim=0)
-        labels = torch.cat(labels, dim=0)
-
-        # measure accuracy and record loss
-        acc1, = accuracy(pred_logits, labels, topk=(1,))
+        acc1 = total_correct_num / total_samples * 100
 
         self._results = OrderedDict()
         self._results[""Acc@1""] = acc1
-
         self._results[""metric""] = acc1
 
         return copy.deepcopy(self._results)"
27;hluwa;frida-dexdump;2d6d964806a6e872df0766f0c513e2a7ab655a70;mprotect target memory range.;"@@ -6,14 +6,32 @@
 
 import {searchDex} from ""./search"";
 
+function setReadPermission(base: NativePointer, size: number) {
+    const end = base.add(size);
+    Process.enumerateRanges(""---"").forEach(function (range) {
+        const range_end = range.base.add(range.size)
+        if (range.base < base || range_end > end) {
+            return
+        }
+        if (!range.protection.startsWith(""r"")) {
+            console.log(""Set read permission for memory range: "" + base + ""-"" + range_end)
+            Memory.protect(range.base, range.size, ""r"" + range.protection.substr(1, 2))
+        }
+
+    })
+}
+
+
 rpc.exports = {
     memorydump: function (address, size) {
-        return new NativePointer(address).readByteArray(size);
+        const ptr = new NativePointer(address);
+        setReadPermission(ptr, size);
+        return ptr.readByteArray(size);
     },
     searchdex: function (enableDeepSearch: boolean) {
         return searchDex(enableDeepSearch);
     },
-    stopthreads: function(){
+    stopthreads: function () {
         Process.enumerateThreads().forEach(function (thread) {
 
         })"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -13,6 +13,7 @@
 # limitations under the License.
 
 import random
+import tracemalloc
 
 import mars.tensor as mt
 import mars.dataframe as md
@@ -30,6 +31,7 @@ class ChunkGraphAssignerSuite:
     repeat = 10
 
     def setup(self):
+        tracemalloc.start()
         random.seed()
 
         num_rows = 10000
@@ -45,6 +47,8 @@ def setup(self):
         graph = TileableGraph([merged_df.data])
         next(TileableGraphBuilder(graph).build())
         self.chunk_graph = next(ChunkGraphBuilder(graph, fuse_enabled=False).build())
+        self.mem_size, self.mem_peak = tracemalloc.get_traced_memory()
+        tracemalloc.stop()
 
     def time_assigner(self):
         start_ops = list(GraphAnalyzer._iter_start_ops(self.chunk_graph))
@@ -55,3 +59,26 @@ def time_assigner(self):
         assigner = GraphAssigner(self.chunk_graph, start_ops, band_resource)
         assigned_result = assigner.assign(current_assign)
         assert len(assigned_result) == len(start_ops)
+
+    def peakmem_setup(self):
+        """"""peakmem includes the memory used by setup.
+        Peakmem benchmarks measure the maximum amount of RAM used by a
+        function. However, this maximum also includes the memory used
+        by ``setup`` (as of asv 0.2.1; see [1]_)
+        Measuring an empty peakmem function might allow us to disambiguate
+        between the memory used by setup and the memory used by slic (see
+        ``peakmem_slic_basic``, below).
+        References
+        ----------
+        .. [1]: https://asv.readthedocs.io/en/stable/writing_benchmarks.html#peak-memory
+        """"""
+        pass
+
+    def mem_chunk_graph(self):
+        return self.chunk_graph
+
+    def track_traced_mem_size(self):
+        return self.mem_size
+
+    def track_traced_mem_peak(self):
+        return self.mem_peak"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -40,6 +40,8 @@
     Timedelta64Field,
     TupleField,
     DictField,
+    Complex64Field,
+    Complex128Field,
 )
 from mars.services.subtask import Subtask, SubtaskResult, SubtaskStatus
 from mars.services.task import new_task_id
@@ -72,11 +74,13 @@ class MySerializable(Serializable):
     _int64_val = Int64Field(""f3"")
     _float32_val = Float32Field(""f4"")
     _float64_val = Float64Field(""f5"")
-    _string_val = StringField(""f6"")
-    _datetime64_val = Datetime64Field(""f7"")
-    _timedelta64_val = Timedelta64Field(""f8"")
-    _datatype_val = DataTypeField(""f9"")
-    _slice_val = SliceField(""f10"")
+    _complex64_val = Complex64Field(""f6"")
+    _complex128_val = Complex128Field(""f7"")
+    _string_val = StringField(""f8"")
+    _datetime64_val = Datetime64Field(""f9"")
+    _timedelta64_val = Timedelta64Field(""f10"")
+    _datatype_val = DataTypeField(""f11"")
+    _slice_val = SliceField(""f12"")
     _list_val = ListField(""list_val"", FieldTypes.int64)
     _tuple_val = TupleField(""tuple_val"", FieldTypes.string)
     _dict_val = DictField(""dict_val"", FieldTypes.string, FieldTypes.bytes)"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -55,16 +55,21 @@ def _copy_tags_(self):
             return getattr(cls, member)
         except AttributeError:
             slots = sorted(
-                f.attr_name
-                for k, f in self._FIELDS.items()
-                if k not in self._no_copy_attrs_
+                f.name for k, f in self._FIELDS.items() if k not in self._no_copy_attrs_
             )
             setattr(cls, member, slots)
             return slots
 
     @property
     def _values_(self):
-        return [self._FIELD_VALUES.get(k) for k in self._copy_tags_]
+        values = []
+        fields = self._FIELDS
+        for k in self._copy_tags_:
+            try:
+                values.append(fields[k].get(self))
+            except AttributeError:
+                values.append(None)
+        return values
 
     def __mars_tokenize__(self):
         try:
@@ -91,19 +96,18 @@ def copy(self):
         return self.copy_to(type(self)(_key=self.key))
 
     def copy_to(self, target: ""Base""):
-        new_values = dict()
-        values = self._FIELD_VALUES
-        for k in self._FIELDS:
-            if k in self._no_copy_attrs_:
+        target_fields = target._FIELDS
+        no_copy_attrs = self._no_copy_attrs_
+        for k, field in self._FIELDS.items():
+            if k in no_copy_attrs:
+                continue
+            try:
+                # Slightly faster than getattr.
+                value = field.__get__(self, k)
+                target_fields[k].set(target, value)
+            except AttributeError:
                 continue
-            if k in values:
-                new_values[k] = values[k]
-            else:
-                try:
-                    new_values[k] = getattr(self, k)
-                except AttributeError:
-                    continue
-        target._FIELD_VALUES.update(new_values)
+
         return target
 
     def copy_from(self, obj):
@@ -119,12 +123,14 @@ def id(self):
 
     def to_kv(self, exclude_fields: Tuple[str], accept_value_types: Tuple[Type]):
         fields = self._FIELDS
-        field_values = self._FIELD_VALUES
-        return {
-            fields[attr_name].tag: value
-            for attr_name, value in field_values.items()
-            if attr_name not in exclude_fields and isinstance(value, accept_value_types)
-        }
+        kv = {}
+        no_value = object()
+        for name, field in fields.items():
+            if name not in exclude_fields:
+                value = getattr(self, name, no_value)
+                if value is not no_value and isinstance(value, accept_value_types):
+                    kv[field.tag] = value
+        return kv
 
 
 def buffered_base(func):"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -1523,12 +1523,15 @@ def test_arithmetic_lazy_chunk_meta():
     df2 = tile(df2)
 
     chunk = df2.chunks[0].data
-    assert chunk._FIELD_VALUES.get(""_dtypes"") is None
+    assert chunk._FIELDS[""_dtypes""].get(chunk) is None
     pd.testing.assert_series_equal(chunk.dtypes, df.dtypes)
-    assert chunk._FIELD_VALUES.get(""_index_value"") is None
+    assert chunk._FIELDS[""_dtypes""].get(chunk) is not None
+    assert chunk._FIELDS[""_index_value""].get(chunk) is None
     pd.testing.assert_index_equal(chunk.index_value.to_pandas(), pd.RangeIndex(3))
-    assert chunk._FIELD_VALUES.get(""_columns_value"") is None
+    assert chunk._FIELDS[""_index_value""].get(chunk) is not None
+    assert chunk._FIELDS[""_columns_value""].get(chunk) is None
     pd.testing.assert_index_equal(chunk.columns_value.to_pandas(), pd.RangeIndex(3))
+    assert chunk._FIELDS[""_columns_value""].get(chunk) is not None
 
 
 def test_datetime_arithmetic():"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -505,15 +505,16 @@ def _gen_chunk_dtypes(instance: Chunk, index: int) -> Optional[pd.Series]:
             cache[tileable_key, index] = dtypes
             return dtypes
 
-    def __get__(self, instance, owner):
+    def __get__(self, instance, owner=None):
         if not issubclass(owner, LazyMetaChunkData):  # pragma: no cover
             return super().__get__(instance, owner)
 
-        values = instance._FIELD_VALUES
-        dtypes = values.get(""_dtypes"", None)
-        if dtypes is not None:
-            # been set before
-            return dtypes
+        try:
+            value = self.get(instance, owner)
+            if value is not None:
+                return value
+        except AttributeError:  # pragma: no cover
+            pass
 
         if instance.index is None:
             return super().__get__(instance, owner)
@@ -522,7 +523,7 @@ def __get__(self, instance, owner):
         index = instance.index[1]
         dtypes = self._gen_chunk_dtypes(instance, index)
         # cache dtypes
-        values[""_dtypes""] = dtypes
+        self.set(instance, dtypes)
         return dtypes
 
 
@@ -557,15 +558,16 @@ def _gen_chunk_index_value(instance: Chunk, index: int) -> Optional[IndexValue]:
             cache[tileable_key, index] = index_value
             return index_value
 
-    def __get__(self, instance, owner):
+    def __get__(self, instance, owner=None):
         if not issubclass(owner, LazyMetaChunkData):  # pragma: no cover
             return super().__get__(instance, owner)
 
-        values = instance._FIELD_VALUES
-        index_value = values.get(""_index_value"", None)
-        if index_value is not None:
-            # been set before
-            return index_value
+        try:
+            value = self.get(instance, owner)
+            if value is not None:
+                return value
+        except AttributeError:  # pragma: no cover
+            pass
 
         if instance.index is None:
             return super().__get__(instance, owner)
@@ -574,7 +576,7 @@ def __get__(self, instance, owner):
         index = instance.index[0]
         index_value = self._gen_chunk_index_value(instance, index)
         # cache index_value
-        values[""_index_value""] = index_value
+        self.set(instance, index_value)
         return index_value
 
 
@@ -605,15 +607,16 @@ def _gen_chunk_columns_value(instance: Chunk, index: int) -> Optional[IndexValue
             cache[tileable_key, index] = columns_value
             return columns_value
 
-    def __get__(self, instance, owner):
+    def __get__(self, instance, owner=None):
         if not issubclass(owner, LazyMetaChunkData):  # pragma: no cover
             return super().__get__(instance, owner)
 
-        values = instance._FIELD_VALUES
-        columns_value = values.get(""_columns_value"", None)
-        if columns_value is not None:
-            # been set before
-            return columns_value
+        try:
+            value = self.get(instance, owner)
+            if value is not None:
+                return value
+        except AttributeError:  # pragma: no cover
+            pass
 
         if instance.index is None:
             return super().__get__(instance, owner)
@@ -622,7 +625,7 @@ def __get__(self, instance, owner):
         index = instance.index[1]
         columns_value = self._gen_chunk_columns_value(instance, index)
         # cache columns_value
-        values[""_columns_value""] = columns_value
+        self.set(instance, columns_value)
         return columns_value
 
 "
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -940,16 +940,20 @@ def test_getitem_lazy_chunk_meta():
     df2 = tile(df2)
 
     chunk = df2.chunks[0].data
-    assert chunk._FIELD_VALUES.get(""_dtypes"") is None
+    assert chunk._FIELDS[""_dtypes""].get(chunk) is None
     pd.testing.assert_series_equal(chunk.dtypes, df.dtypes[[0, 2]])
-    assert chunk._FIELD_VALUES.get(""_index_value"") is None
+    assert chunk._FIELDS[""_dtypes""].get(chunk) is not None
+    assert chunk._FIELDS[""_index_value""].get(chunk) is None
     pd.testing.assert_index_equal(chunk.index_value.to_pandas(), pd.RangeIndex(3))
-    assert chunk._FIELD_VALUES.get(""_columns_value"") is None
+    assert chunk._FIELDS[""_index_value""].get(chunk) is not None
+    assert chunk._FIELDS[""_columns_value""].get(chunk) is None
     pd.testing.assert_index_equal(chunk.columns_value.to_pandas(), pd.Index([0, 2]))
+    assert chunk._FIELDS[""_columns_value""].get(chunk) is not None
 
     df2 = df[2]
     df2 = tile(df2)
 
     chunk = df2.chunks[0].data
-    assert chunk._FIELD_VALUES.get(""_index_value"") is None
+    assert chunk._FIELDS[""_index_value""].get(chunk) is None
     pd.testing.assert_index_equal(chunk.index_value.to_pandas(), pd.RangeIndex(3))
+    assert chunk._FIELDS[""_index_value""].get(chunk) is not None"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -11,19 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
 import os
+import operator
 import weakref
-from collections import OrderedDict
-from functools import partial
-from typing import Any, Dict, List, Type, Tuple
+from typing import Dict, List, Type, Tuple
 
 import cloudpickle
 
-from ...core.mode import is_kernel_mode, is_build_mode
 from ...utils import no_default
 from ..core import Serializer, Placeholder, buffered
-from .field import Field, OneOfField
+from .field import Field
 from .field_type import (
     PrimitiveFieldType,
     ListType,
@@ -74,41 +71,60 @@ def check_type(field_type):
 
 class SerializableMeta(type):
     def __new__(mcs, name: str, bases: Tuple[Type], properties: Dict):
-        new_properties = dict()
+        # All the fields including base fields.
+        all_fields = dict()
+
         for base in bases:
             if hasattr(base, ""_FIELDS""):
-                new_properties.update(base._FIELDS)
-        new_properties.update(properties)
-        # make field order deterministic to serialize it as list instead of dict
-        properties = OrderedDict()
-        for k, v in sorted(new_properties.items(), key=lambda item: item[0]):
-            properties[k] = v
-
-        # make field order deterministic to serialize it as list instead of dict
-        property_to_fields = OrderedDict()
-        pickle_fields = []
-        non_pickle_fields = []
-        # filter out all fields
+                all_fields.update(base._FIELDS)
+
+        properties_without_fields = {}
+        properties_field_slot_names = []
         for k, v in properties.items():
             if not isinstance(v, Field):
+                properties_without_fields[k] = v
                 continue
 
-            property_to_fields[k] = v
-            v._attr_name = k
+            field = all_fields.get(k)
+            if field is None:
+                properties_field_slot_names.append(k)
+            else:
+                v.name = field.name
+                v.get = field.get
+                v.set = field.set
+                v.__delete__ = field.__delete__
+            all_fields[k] = v
+
+        # Make field order deterministic to serialize it as list instead of dict.
+        all_fields = dict(sorted(all_fields.items(), key=operator.itemgetter(0)))
+        pickle_fields = []
+        non_pickle_fields = []
+        for v in all_fields.values():
             if _is_field_primitive_compound(v):
                 pickle_fields.append(v)
             else:
                 non_pickle_fields.append(v)
 
-        properties[""_FIELDS""] = property_to_fields
+        slots = set(properties.pop(""__slots__"", set()))
+        slots.update(properties_field_slot_names)
+
+        properties = properties_without_fields
+        properties[""_FIELDS""] = all_fields
         properties[""_PRIMITIVE_FIELDS""] = pickle_fields
         properties[""_NON_PRIMITIVE_FIELDS""] = non_pickle_fields
-        slots = set(properties.pop(""__slots__"", set()))
-        if property_to_fields:
-            slots.add(""_FIELD_VALUES"")
         properties[""__slots__""] = tuple(slots)
 
         clz = type.__new__(mcs, name, bases, properties)
+        # Bind slot member_descriptor with field.
+        for name in properties_field_slot_names:
+            member_descriptor = getattr(clz, name)
+            field = all_fields[name]
+            field.name = member_descriptor.__name__
+            field.get = member_descriptor.__get__
+            field.set = member_descriptor.__set__
+            field.__delete__ = member_descriptor.__delete__
+            setattr(clz, name, field)
+
         return clz
 
 
@@ -118,22 +134,18 @@ class Serializable(metaclass=SerializableMeta):
     _cache_primitive_serial = False
 
     _FIELDS: Dict[str, Field]
-    _FIELD_VALUES: Dict[str, Any]
     _PRIMITIVE_FIELDS: List[str]
     _NON_PRIMITIVE_FIELDS: List[str]
 
     def __init__(self, *args, **kwargs):
+        fields = self._FIELDS
         if args:  # pragma: no cover
-            values = dict(zip(self._FIELDS, args))
+            values = dict(zip(fields, args))
             values.update(kwargs)
         else:
             values = kwargs
-        if not _is_ci or is_kernel_mode() or is_build_mode():
-            self._FIELD_VALUES = values
-        else:
-            self._FIELD_VALUES = dict()
-            for k, v in values.items():
-                setattr(self, k, v)
+        for k, v in values.items():
+            fields[k].set(self, v)
 
     def __repr__(self):
         values = "", "".join(
@@ -146,7 +158,14 @@ def __repr__(self):
 
     def copy(self) -> ""Serializable"":
         copied = type(self)()
-        copied._FIELD_VALUES = self._FIELD_VALUES.copy()
+        copied_fields = copied._FIELDS
+        for k, field in self._FIELDS.items():
+            try:
+                # Slightly faster than getattr.
+                value = field.get(self, k)
+                copied_fields[k].set(copied, value)
+            except AttributeError:
+                continue
         return copied
 
 
@@ -160,15 +179,13 @@ class SerializableSerializer(Serializer):
 
     @classmethod
     def _get_field_values(cls, obj: Serializable, fields):
-        attr_to_values = obj._FIELD_VALUES
         values = []
         for field in fields:
-            attr_name = field.attr_name
             try:
-                value = attr_to_values[attr_name]
+                value = field.get(obj)
                 if field.on_serialize:
                     value = field.on_serialize(value)
-            except KeyError:
+            except AttributeError:
                 # Most field values are not None, serialize by list is more efficient than dict.
                 value = no_default
             values.append(value)
@@ -187,45 +204,39 @@ def serial(self, obj: Serializable, context: Dict):
         compound_vals = self._get_field_values(obj, obj._NON_PRIMITIVE_FIELDS)
         return (type(obj), primitive_vals), [compound_vals], False
 
-    @classmethod
-    def _set_field_value(cls, attr_to_values: dict, field: Field, value):
+    @staticmethod
+    def _set_field_value(obj: Serializable, field: Field, value):
         if value is no_default:
             return
-        attr_to_values[field.attr_name] = value
-        if type(field) is not OneOfField:
-            if value is not None:
-                if field.on_deserialize:
-
-                    def cb(v, field_):
-                        attr_to_values[field_.attr_name] = field_.on_deserialize(v)
-
-                else:
-
-                    def cb(v, field_):
-                        attr_to_values[field_.attr_name] = v
-
-                if type(value) is Placeholder:
-                    value.callbacks.append(partial(cb, field_=field))
-                else:
-                    cb(value, field)
+        if type(value) is Placeholder:
+            if field.on_deserialize:
+                value.callbacks.append(
+                    lambda v: field.set(obj, field.on_deserialize(v))
+                )
+            else:
+                value.callbacks.append(lambda v: field.set(obj, v))
+        else:
+            if field.on_deserialize:
+                field.set(obj, field.on_deserialize(value))
+            else:
+                field.set(obj, value)
 
     def deserial(self, serialized: Tuple, context: Dict, subs: List) -> Serializable:
         obj_class, primitives = serialized
-        attr_to_values = dict()
 
         if type(primitives) is not list:
             primitives = cloudpickle.loads(primitives)
 
+        obj = obj_class()
+
         if primitives:
             for field, value in zip(obj_class._PRIMITIVE_FIELDS, primitives):
-                self._set_field_value(attr_to_values, field, value)
+                self._set_field_value(obj, field, value)
 
         if obj_class._NON_PRIMITIVE_FIELDS:
             for field, value in zip(obj_class._NON_PRIMITIVE_FIELDS, subs[0]):
-                self._set_field_value(attr_to_values, field, value)
+                self._set_field_value(obj, field, value)
 
-        obj = obj_class()
-        obj._FIELD_VALUES = attr_to_values
         return obj
 
 "
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -37,15 +37,17 @@ class Field(ABC):
         ""_tag"",
         ""_default_value"",
         ""_default_factory"",
-        ""_attr_name"",
         ""_on_serialize"",
         ""_on_deserialize"",
+        ""name"",  # The __name__ of member_descriptor
+        ""get"",  # The __get__ of member_descriptor
+        ""set"",  # The __set__ of member_descriptor
+        ""__delete__"",  # The __delete__ of member_descriptor
     )
 
     _tag: str
     _default_value: Any
     _default_factory: Optional[Callable]
-    _attr_name: str  # attribute name that set to
 
     def __init__(
         self,
@@ -54,7 +56,6 @@ def __init__(
         default_factory: Optional[Callable] = None,
         on_serialize: Callable[[Any], Any] = None,
         on_deserialize: Callable[[Any], Any] = None,
-        attr_name: str = None,
     ):
         if (
             default is not no_default and default_factory is not None
@@ -66,7 +67,6 @@ def __init__(
         self._default_factory = default_factory
         self._on_serialize = on_serialize
         self._on_deserialize = on_deserialize
-        self._attr_name = attr_name
 
     @property
     def tag(self):
@@ -80,10 +80,6 @@ def on_serialize(self):
     def on_deserialize(self):
         return self._on_deserialize
 
-    @property
-    def attr_name(self):
-        return self._attr_name
-
     @property
     @abstractmethod
     def field_type(self) -> AbstractFieldType:
@@ -96,23 +92,22 @@ def field_type(self) -> AbstractFieldType:
              Field type.
         """"""
 
-    def __get__(self, instance, owner):
-        tag_to_values = instance._FIELD_VALUES
+    def __get__(self, instance, owner=None):
         try:
-            return tag_to_values[self._attr_name]
-        except KeyError:
+            return self.get(instance, owner)
+        except AttributeError:
             if self._default_value is not no_default:
-                val = tag_to_values[self._attr_name] = self._default_value
+                val = self._default_value
+                self.set(instance, val)
                 return val
             elif self._default_factory is not None:
-                val = tag_to_values[self._attr_name] = self._default_factory()
+                val = self._default_factory()
+                self.set(instance, val)
                 return val
             else:
-                raise AttributeError(
-                    f""'{type(instance)}' has no attribute {self._attr_name}""
-                )
+                raise
 
-    def __set__(self, instance, value):
+    def __set__(self, instance, value) -> None:
         if _is_ci:  # pragma: no branch
             from ...core import is_kernel_mode
 
@@ -125,18 +120,10 @@ def __set__(self, instance, value):
                     field_type.validate(to_check_value)
                 except (TypeError, ValueError) as e:
                     raise type(e)(
-                        f""Failed to set `{self._attr_name}` for {type(instance).__name__} ""
+                        f""Failed to set `{self.name}` for {type(instance).__name__} ""
                         f""when environ CI=true is set: {str(e)}""
                     )
-        instance._FIELD_VALUES[self._attr_name] = value
-
-    def __delete__(self, instance):
-        try:
-            del instance._FIELD_VALUES[self._attr_name]
-        except KeyError:
-            raise AttributeError(
-                f""'{type(instance)}' has no attribute {self._attr_name}""
-            ) from None
+        self.set(instance, value)
 
 
 class AnyField(Field):
@@ -515,7 +502,7 @@ def get_field_type(self, instance):
         return self._field_type
 
     def __set__(self, instance, value):
-        if _is_ci and self._field_type is None:
+        if _is_ci:
             from ...core import is_kernel_mode
 
             if not is_kernel_mode():
@@ -526,16 +513,11 @@ def __set__(self, instance, value):
                         to_check_value = self._on_serialize(to_check_value)
                     field_type.validate(to_check_value)
                 except (TypeError, ValueError) as e:
-                    if not self._attr_name:
-                        raise
-                    else:
-                        raise type(e)(
-                            f""Failed to set `{self._attr_name}` for {type(instance).__name__} ""
-                            f""when environ CI=true is set: {e}""
-                        )
-            instance._FIELD_VALUES[self._attr_name] = value
-        else:
-            super().__set__(instance, value)
+                    raise type(e)(
+                        f""Failed to set `{self.name}` for {type(instance).__name__} ""
+                        f""when environ CI=true is set: {e}""
+                    )
+        self.set(instance, value)
 
 
 class OneOfField(Field):
@@ -547,15 +529,13 @@ def __init__(
         default: Any = no_default,
         on_serialize: Callable[[Any], Any] = None,
         on_deserialize: Callable[[Any], Any] = None,
-        attr_name: str = None,
         **tag_to_reference_types,
     ):
         super().__init__(
             tag,
             default=default,
             on_serialize=on_serialize,
             on_deserialize=on_deserialize,
-            attr_name=attr_name,
         )
         self._reference_fields = [
             ReferenceField(t, ref_type)
@@ -574,16 +554,15 @@ def field_type(self) -> AbstractFieldType:  # pragma: no cover
 
     def __set__(self, instance, value):
         if not _is_ci:  # pragma: no cover
-            return super().__set__(instance, value)
+            return self.set(instance, value)
 
-        field_values = instance._FIELD_VALUES
         for reference_field in self._reference_fields:
             try:
                 to_check_value = value
                 if to_check_value is not None and self._on_serialize:
                     to_check_value = self._on_serialize(to_check_value)
                 reference_field.get_field_type(instance).validate(to_check_value)
-                field_values[self._attr_name] = value
+                self.set(instance, value)
                 return
             except TypeError:
                 continue
@@ -596,7 +575,7 @@ def __set__(self, instance, value):
             )
         )
         raise TypeError(
-            f""Failed to set `{self._attr_name}` for {type(instance).__name__} ""
+            f""Failed to set `{self.name}` for {type(instance).__name__} ""
             f""when environ CI=true is set: type of instance cannot match any ""
             f""of {valid_types}, got {type(value).__name__}""
         )"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -201,7 +201,7 @@ def test_serializable(set_environ):
 
 def _assert_serializable_eq(my_serializable, my_serializable2):
     for field_name, field in my_serializable._FIELDS.items():
-        if field.tag not in my_serializable._FIELD_VALUES:
+        if not hasattr(my_serializable, field.tag):
             continue
         expect_value = getattr(my_serializable, field_name)
         actual_value = getattr(my_serializable2, field_name)"
27;mars-project;mars;cd22c4c44e8c5ca6d6e0ff1aa54fe005eca99b2a;"Optimize serializable memory (#3120)

* Optimize serializable memory

* Fix lint

* Fix

* Optimize

* Rename attr_name to name of Field

* Fix

* Fix

* Fix benchmark

* Add asv bench

* Optimize copy

* Improve coverage

* Fix

Co-authored-by: åˆ˜å® <po.lb@antfin.com>";"@@ -245,8 +245,8 @@ def destroy(self):
         self._config = None
         self._task = None
         self._tile_context = None
-        self._task_context = None
-        self._task_chunks_meta = None
+        self._task_context = {}
+        self._task_chunks_meta = {}
         self._ray_executor = None
 
         # api"
28;fastnlp;fastNLP;c72bc070a951c8f79650746cf8edb9243cfc3919;"1.ä¿®å¤yxgåŒå­¦å‘çŽ°çš„DataSet copy Fieldå¯¼è‡´çš„bug; 2. ä¿®æ”¹DistTrainerä¸­update_everyçš„bug; 3.Trainerå’ŒDistTrainerä¿®æ”¹pin_memoryé»˜è®¤å€¼ä¸ºTrue";"@@ -474,8 +474,8 @@ def __getitem__(self, idx):
             if idx.start is not None and (idx.start >= len(self) or idx.start <= -len(self)):
                 raise RuntimeError(f""Start index {idx.start} out of range 0-{len(self) - 1}"")
             data_set = DataSet()
-            for field in self.field_arrays.values():
-                data_set.add_field(field_name=field.name, fields=field.content[idx], padder=field.padder,
+            for field_name, field in self.field_arrays.items():
+                data_set.add_field(field_name=field_name, fields=field.content[idx], padder=field.padder,
                                    is_input=field.is_input, is_target=field.is_target, ignore_type=field.ignore_type)
             data_set.collater = self.collater.copy_from(self.collater)
             return data_set
@@ -616,6 +616,7 @@ def add_fieldarray(self, field_name, fieldarray):
         if len(self) != len(fieldarray):
             raise RuntimeError(f""The field to add must have the same size as dataset. ""
                                f""Dataset size {len(self)} != field size {len(fieldarray)}"")
+        fieldarray.name = field_name
         self.field_arrays[field_name] = fieldarray
 
     def add_field(self, field_name, fields, padder=AutoPadder(), is_input=False, is_target=False, ignore_type=False):
@@ -673,6 +674,7 @@ def copy_field(self, field_name, new_field_name):
         if not self.has_field(field_name):
             raise KeyError(f""Field:{field_name} not found in DataSet."")
         fieldarray = deepcopy(self.get_field(field_name))
+        fieldarray.name = new_field_name
         self.add_fieldarray(field_name=new_field_name, fieldarray=fieldarray)
         return self
 "
28;fastnlp;fastNLP;c72bc070a951c8f79650746cf8edb9243cfc3919;"1.ä¿®å¤yxgåŒå­¦å‘çŽ°çš„DataSet copy Fieldå¯¼è‡´çš„bug; 2. ä¿®æ”¹DistTrainerä¸­update_everyçš„bug; 3.Trainerå’ŒDistTrainerä¿®æ”¹pin_memoryé»˜è®¤å€¼ä¸ºTrue";"@@ -55,7 +55,7 @@ def get_local_rank():
     raise RuntimeError('Please use ""python -m torch.distributed.launch --nproc_per_node=N train_script.py')
 
 
-class DistTrainer():
+class DistTrainer:
     r""""""
     åˆ†å¸ƒå¼çš„ Trainerï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦çš„è®­ç»ƒã€‚å…·ä½“å®žçŽ°åŽŸç†è¯·é˜…è¯» pytorch å®˜æ–¹æ–‡æ¡£ã€‚
 
@@ -110,7 +110,7 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
             int dev_batch_size: åœ¨evaluateæ—¶ï¼Œä½¿ç”¨çš„evaluateçš„batchå¤§å°
             bool test_use_fp16: testæ—¶ä½¿ç”¨fp16
             bool set_grad_to_none: zero_gradæ—¶å°†gradè®¾ä¸ºNoneè€Œä¸æ˜¯0
-            GradScaler gradscaler: è‡ªå®šä¹‰çš„æ¢¯åº¦ scaler
+            GradScaler grad_scaler: è‡ªå®šä¹‰çš„æ¢¯åº¦ scaler
             bool pin_memory: æ˜¯å¦å°†äº§ç”Ÿçš„tensorä½¿ç”¨pin memory, å¯èƒ½ä¼šåŠ å¿«æ•°æ®é€Ÿåº¦ã€‚ä¸€èˆ¬åœ¨tensorè¾ƒå¤šæˆ–tensorç»´åº¦è¾ƒå¤§æ—¶ï¼Œæœ‰é€Ÿåº¦å¢žç›Šã€‚
             bool find_unused_parameters: åœ¨å°†modelè½¬åŒ–ä¸ºDistributedDataParallelç±»åž‹çš„æ—¶å€™ï¼Œéœ€è¦å¡«å…¥è¯¥å‚æ•°ï¼Œé™¤éžmodelå†…ç¡®å®žæœ‰
                 forwardæ²¡ç”¨ä¸Šçš„å‚æ•°ï¼Œå¦åˆ™åº”è¯¥ä¸éœ€è¦ç”¨åˆ°è¯¥å‚æ•°ã€‚
@@ -132,6 +132,7 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         self.rank = dist.get_rank() # unique id for each process
 
         self.train_data = train_data
+        self.kwargs = kwargs
         if kwargs.get('batch_size', None):
             batch_size_per_gpu = int(kwargs.get('batch_size'))
         self.batch_size_per_gpu = int(batch_size_per_gpu)
@@ -158,15 +159,15 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         # init fp16, must before DataParallel init
         autocast, GradScaler = _build_fp16_env(dummy=not self.fp16)
         self.auto_cast = autocast
-        user_grad_scaler = getattr(kwargs, 'gradscaler', None)
+        user_grad_scaler = kwargs.get('grad_scaler', None)
         if user_grad_scaler is not None:
-            assert self.fp16, ""must set fp16=True to enable gradscaler""
+            assert self.fp16, ""must set fp16=True to enable grad_scaler""
             grad_scaler = user_grad_scaler
         else:
             grad_scaler = GradScaler()
         self.grad_scaler = grad_scaler
 
-        self.set_grad_to_none = getattr(kwargs, 'set_grad_to_none', True)
+        self.set_grad_to_none = kwargs.get('set_grad_to_none', False)
 
         # init DataParallel
         if parse_version(torch.__version__)>=parse_version('1.1'):
@@ -191,15 +192,15 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                 elif hasattr(sampler, 'set_batch_size'):
                     sampler.set_batch_size(batch_size_per_gpu)
                 self.sampler = sampler
-        self.pin_memory = kwargs.get('pin_memory', True)
+        # concerning issue from https://github.com/pytorch/pytorch/issues/57273
+        self.pin_memory = kwargs.get('pin_memory', False if parse_version(torch.__version__)==parse_version('1.9') else True)
         self.data_iterator = self._get_data_iter(self.train_data)
         self.batch_size = self.world_size * self.batch_size_per_gpu
         self.n_steps = self._get_n_steps()
 
         self.dev_data = dev_data
         self.metrics = metrics
         self.test_use_tqdm = True
-        self.kwargs = kwargs
         self.test_use_tqdm = kwargs.get('test_use_tqdm', self.use_tqdm)
         dev_batch_size = kwargs.get('dev_batch_size', batch_size_per_gpu)
 
@@ -229,22 +230,6 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         self.logger.info(""Num of processes: {}"".format(self.world_size))
         self.logger.info(""Use device: {}"".format(device))
 
-    def _maybe_no_sync(self):
-        """"""
-        Whenever *samples* contains more than one mini-batch, we
-        want to accumulate gradients locally and only call
-        all-reduce in the last backwards pass.
-        """"""
-        i = self.step % self.update_every
-        if (
-                self.world_size > 1
-                and hasattr(self.ddp_model, ""no_sync"")
-                and i != 0
-        ):
-            return self.ddp_model.no_sync()
-        else:
-            return contextlib.ExitStack()  # dummy contextmanager
-
     def _get_n_steps(self):
         return len(self.data_iterator) * self.n_epochs
 
@@ -365,37 +350,42 @@ def _train(self):
             self.callback_manager.on_epoch_begin()
             for batch_x, batch_y in data_iterator:
                 self.step += 1
-                self.ddp_model.train()
-                _move_dict_value_to_device(batch_x, batch_y, device=self.device, non_blocking=self.pin_memory)
-                indices = data_iterator.get_batch_indices()
-                # negative sampling; replace unknown; re-weight batch_y
-                self.callback_manager.on_batch_begin(batch_x, batch_y, indices)
-                with self.auto_cast():
-                    prediction = self._data_forward(self.ddp_model, batch_x)
-                    # edit prediction
-                    self.callback_manager.on_loss_begin(batch_y, prediction)
-                    loss = self._compute_loss(prediction, batch_y)
-
-                avg_loss += loss.detach()
-
-                # Is loss NaN or inf? requires_grad = False
-                self.callback_manager.on_backward_begin(loss)
-                self._grad_backward(loss)
-                self.callback_manager.on_backward_end()
-                self._update()
-                self.callback_manager.on_step_end()
-
-                if self.step % self.print_every == 0:
-                    avg_loss = float(avg_loss) / self.print_every
-                    print_output = ""loss:{:<6.5f}"".format(avg_loss)
-                    pbar.update(self.print_every)
-                    pbar.set_postfix_str(print_output)
-                    avg_loss = 0
-
-                self.callback_manager.on_batch_end()
-
-                if (self.validate_every > 0 and self.step % self.validate_every == 0) and len(self.test_manager.callbacks):
-                    self._do_validation()
+                if self.step%self.update_every!=0:
+                    no_sync = self.ddp_model.no_sync
+                else:
+                    no_sync = contextlib.ExitStack
+                with no_sync():
+                    self.ddp_model.train()
+                    _move_dict_value_to_device(batch_x, batch_y, device=self.device, non_blocking=self.pin_memory)
+                    indices = data_iterator.get_batch_indices()
+                    # negative sampling; replace unknown; re-weight batch_y
+                    self.callback_manager.on_batch_begin(batch_x, batch_y, indices)
+                    with self.auto_cast():
+                        prediction = self._data_forward(self.ddp_model, batch_x)
+                        # edit prediction
+                        self.callback_manager.on_loss_begin(batch_y, prediction)
+                        loss = self._compute_loss(prediction, batch_y)
+
+                    avg_loss += loss.detach()
+
+                    # Is loss NaN or inf? requires_grad = False
+                    self.callback_manager.on_backward_begin(loss)
+                    self._grad_backward(loss)
+                    self.callback_manager.on_backward_end()
+                    self._update()
+                    self.callback_manager.on_step_end()
+
+                    if self.step % self.print_every == 0:
+                        avg_loss = float(avg_loss) / self.print_every
+                        print_output = ""loss:{:<6.5f}"".format(avg_loss)
+                        pbar.update(self.print_every)
+                        pbar.set_postfix_str(print_output)
+                        avg_loss = 0
+
+                    self.callback_manager.on_batch_end()
+
+                    if (self.validate_every > 0 and self.step % self.validate_every == 0) and len(self.test_manager.callbacks):
+                        self._do_validation()
 
             # ================= mini-batch end ==================== #
             if self.validate_every < 0 and len(self.test_manager.callbacks):"
28;fastnlp;fastNLP;c72bc070a951c8f79650746cf8edb9243cfc3919;"1.ä¿®å¤yxgåŒå­¦å‘çŽ°çš„DataSet copy Fieldå¯¼è‡´çš„bug; 2. ä¿®æ”¹DistTrainerä¸­update_everyçš„bug; 3.Trainerå’ŒDistTrainerä¿®æ”¹pin_memoryé»˜è®¤å€¼ä¸ºTrue";"@@ -334,6 +334,7 @@ def on_epoch_end(self):
 except:
     from .utils import _pseudo_tqdm as tqdm
 import warnings
+from pkg_resources import parse_version
 
 from .batch import DataSetIter, BatchIter
 from .callback import CallbackManager, CallbackException, Callback
@@ -473,7 +474,8 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                 warnings.warn(""num_workers is ignored when train_data is BatchIter."")
             if drop_last:
                 warnings.warn(""drop_last is ignored when train_data is BatchIter."")
-        self.pin_memory = kwargs.get('pin_memory', True)
+        # concerning issue from https://github.com/pytorch/pytorch/issues/57273
+        self.pin_memory = kwargs.get('pin_memory', False if parse_version(torch.__version__)==parse_version('1.9') else True)
         if isinstance(model, nn.parallel.DistributedDataParallel):  # å¦‚æžœæ˜¯åˆ†å¸ƒå¼çš„
             # deviceä¸ºNone
             if device is not None:"
28;fastnlp;fastNLP;c72bc070a951c8f79650746cf8edb9243cfc3919;"1.ä¿®å¤yxgåŒå­¦å‘çŽ°çš„DataSet copy Fieldå¯¼è‡´çš„bug; 2. ä¿®æ”¹DistTrainerä¸­update_everyçš„bug; 3.Trainerå’ŒDistTrainerä¿®æ”¹pin_memoryé»˜è®¤å€¼ä¸ºTrue";"@@ -31,7 +31,8 @@ def __init__(self, vocabs: dict = None, datasets: dict = None):
         r""""""
         
         :param vocabs: ä»Žåç§°(å­—ç¬¦ä¸²)åˆ° :class:`~fastNLP.Vocabulary` ç±»åž‹çš„dict
-        :param datasets: ä»Žåç§°(å­—ç¬¦ä¸²)åˆ° :class:`~fastNLP.DataSet` ç±»åž‹çš„dict
+        :param datasets: ä»Žåç§°(å­—ç¬¦ä¸²)åˆ° :class:`~fastNLP.DataSet` ç±»åž‹çš„dictã€‚å»ºè®®ä¸è¦å°†ç›¸åŒçš„DataSetå¯¹è±¡é‡å¤ä¼ å…¥ï¼Œå¯èƒ½ä¼šåœ¨
+            ä½¿ç”¨Pipeå¤„ç†æ•°æ®çš„æ—¶å€™é‡åˆ°é—®é¢˜ã€‚
         """"""
         self.vocabs = vocabs or {}
         self.datasets = datasets or {}"
28;fastnlp;fastNLP;c72bc070a951c8f79650746cf8edb9243cfc3919;"1.ä¿®å¤yxgåŒå­¦å‘çŽ°çš„DataSet copy Fieldå¯¼è‡´çš„bug; 2. ä¿®æ”¹DistTrainerä¸­update_everyçš„bug; 3.Trainerå’ŒDistTrainerä¿®æ”¹pin_memoryé»˜è®¤å€¼ä¸ºTrue";"@@ -345,6 +345,14 @@ def test_copy_padder(self):
         ds.apply_field(lambda x: x, 'idx', 'idx')
         self.assertTrue(isinstance(ds.get_field('idx').padder, AutoPadder))  # should be None, but AutoPadder
 
+    def test_instance_field_disappear_bug(self):
+        data = DataSet({'raw_chars': [[0,1],[2]], 'target': [0, 1]})
+        data.copy_field(field_name='raw_chars', new_field_name='chars')
+        _data = data[:1]
+        for field_name in ['raw_chars', 'target', 'chars']:
+            self.assertTrue(_data.has_field(field_name))
+
+
 class TestDataSetIter(unittest.TestCase):
     def test__repr__(self):
         ds = DataSet({""x"": [[1, 2, 3, 4]] * 10, ""y"": [[5, 6]] * 10})"
28;fastnlp;fastNLP;7ce03a601b702ce759dd3e13cf8878d1f273f4d9;"1.ä¿®æ”¹Trainerä¸­çš„pin_memoryå‚æ•°; 2.ä¿®æ”¹DistTrainerä½¿å¾—DistTrainerå’ŒTrainerçš„apiä½¿ç”¨å¯ä»¥å°½é‡æŽ¥è¿‘";"@@ -32,6 +32,7 @@
 from .utils import _build_fp16_env
 from .utils import _get_func_signature
 from .utils import _move_dict_value_to_device
+from .sampler import Sampler
 
 __all__ = [
     'get_local_rank',
@@ -68,7 +69,7 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                  dev_data=None, metrics=None, metric_key=None,
                  update_every=1, print_every=10, validate_every=-1,
                  save_path=None, device='auto',
-                 fp16=False, use_tqdm=True, **kwargs):
+                 fp16=False, use_tqdm=True, sampler=None, **kwargs):
         r""""""
 
         :param train_data: è®­ç»ƒé›†ï¼Œ :class:`~fastNLP.DataSet` ç±»åž‹ã€‚
@@ -101,13 +102,18 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         :param str device: æŒ‡å®š deviceï¼Œå¯ä»¥æ˜¯ gpuï¼Œcpu æˆ– auto
         :param bool fp16: æŒ‡å®šæ˜¯å¦ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒã€‚
         :param bool use_tqdm: æ˜¯å¦ä½¿ç”¨tqdmæ¥æ˜¾ç¤ºè®­ç»ƒè¿›åº¦; å¦‚æžœä¸ºFalseï¼Œåˆ™å°†lossæ‰“å°åœ¨ç»ˆç«¯ä¸­ã€‚
+        :param Sampler sampler: ä½¿ç”¨çš„samplerï¼Œå¦‚æžœä¸æŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨çš„DistributedSamplerã€‚ä½¿ç”¨è¿™ä¸ªå‚æ•°çš„æƒ…å†µä¸€èˆ¬ä¸ºï¼Œæ˜Žç¡®ä¿®æ”¹äº†æ¯ä¸ª
+            rankçš„Datasetï¼Œä½¿å¾—æ¯ä¸ªrankä¸Šçš„datasetè™½ç„¶sampleæ•°é‡ä¸€æ ·å¤šï¼Œä½†æ˜¯sampleå…¶å®žä¸ä¸€æ ·ã€‚
         :param kwargs: æ”¯æŒé…ç½®å¯é€‰å‚æ•°
             bool test_use_tqdm: åœ¨devä¸ŠéªŒè¯çš„æ—¶å€™æ˜¯å¦å¼€å¯tqdm
             Sampler test_sampler: åœ¨evaluateçš„æ—¶å€™ä½¿ç”¨çš„sampler
             int dev_batch_size: åœ¨evaluateæ—¶ï¼Œä½¿ç”¨çš„evaluateçš„batchå¤§å°
             bool test_use_fp16: testæ—¶ä½¿ç”¨fp16
             bool set_grad_to_none: zero_gradæ—¶å°†gradè®¾ä¸ºNoneè€Œä¸æ˜¯0
             GradScaler gradscaler: è‡ªå®šä¹‰çš„æ¢¯åº¦ scaler
+            bool pin_memory: æ˜¯å¦å°†äº§ç”Ÿçš„tensorä½¿ç”¨pin memory, å¯èƒ½ä¼šåŠ å¿«æ•°æ®é€Ÿåº¦ã€‚ä¸€èˆ¬åœ¨tensorè¾ƒå¤šæˆ–tensorç»´åº¦è¾ƒå¤§æ—¶ï¼Œæœ‰é€Ÿåº¦å¢žç›Šã€‚
+            bool find_unused_parameters: åœ¨å°†modelè½¬åŒ–ä¸ºDistributedDataParallelç±»åž‹çš„æ—¶å€™ï¼Œéœ€è¦å¡«å…¥è¯¥å‚æ•°ï¼Œé™¤éžmodelå†…ç¡®å®žæœ‰
+                forwardæ²¡ç”¨ä¸Šçš„å‚æ•°ï¼Œå¦åˆ™åº”è¯¥ä¸éœ€è¦ç”¨åˆ°è¯¥å‚æ•°ã€‚
         """"""
         assert device in ['auto', 'cuda', 'cpu'], ""Please set correct device in [auto', 'cuda', 'cpu']""
         if device == 'auto':
@@ -126,6 +132,8 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         self.rank = dist.get_rank() # unique id for each process
 
         self.train_data = train_data
+        if kwargs.get('batch_size', None):
+            batch_size_per_gpu = int(kwargs.get('batch_size'))
         self.batch_size_per_gpu = int(batch_size_per_gpu)
         self.n_epochs = int(n_epochs)
         self.num_data_workers = int(num_workers)
@@ -163,7 +171,8 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         # init DataParallel
         if parse_version(torch.__version__)>=parse_version('1.1'):
             self.ddp_model = DDP(model, device_ids=[self.local_rank],
-                             output_device=self.local_rank, find_unused_parameters=True)
+                                 output_device=self.local_rank,
+                                 find_unused_parameters=kwargs.get('find_unused_parameters', False))
         else:
             self.ddp_model = DDP(model, device_ids=[self.local_rank],
                              output_device=self.local_rank)
@@ -172,7 +181,17 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
         optimizer = self._get_optimizer(optimizer)
         self.optimizer = optimizer
         if isinstance(self.train_data, DataSet):
-            self.sampler = DistributedSampler(self.train_data)
+            if sampler is None:
+                self.sampler = DistributedSampler(self.train_data)
+            else:
+                # sampler check
+                if sampler is not None and not isinstance(sampler, (Sampler, torch.utils.data.Sampler)):
+                    raise ValueError(
+                        f""The type of sampler should be fastNLP.BaseSampler or pytorch's Sampler, got {type(sampler)}"")
+                elif hasattr(sampler, 'set_batch_size'):
+                    sampler.set_batch_size(batch_size_per_gpu)
+                self.sampler = sampler
+        self.pin_memory = kwargs.get('pin_memory', True)
         self.data_iterator = self._get_data_iter(self.train_data)
         self.batch_size = self.world_size * self.batch_size_per_gpu
         self.n_steps = self._get_n_steps()
@@ -191,7 +210,6 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                 batch_size=dev_batch_size, num_workers=num_workers, sampler=kwargs.get('test_sampler', None),
                 use_tqdm=self.test_use_tqdm)
             self.test_manager.add_callback([cb], master=True)
-
         # Setup logging
         # åŒæ­¥start_time
         sync_time = torch.tensor(time.time(), dtype=torch.double).to(self.device)
@@ -233,7 +251,8 @@ def _get_n_steps(self):
     def _get_data_iter(self, dataset):
         if isinstance(dataset, DataSet):
             return DataSetIter(dataset=dataset, batch_size=self.batch_size_per_gpu, sampler=self.sampler,
-                               num_workers=self.num_data_workers, drop_last=self.drop_last)
+                               num_workers=self.num_data_workers, drop_last=self.drop_last,
+                               pin_memory=self.pin_memory)
         elif isinstance(dataset, BatchIter):
             return dataset
         else:
@@ -347,7 +366,7 @@ def _train(self):
             for batch_x, batch_y in data_iterator:
                 self.step += 1
                 self.ddp_model.train()
-                _move_dict_value_to_device(batch_x, batch_y, device=self.device)
+                _move_dict_value_to_device(batch_x, batch_y, device=self.device, non_blocking=self.pin_memory)
                 indices = data_iterator.get_batch_indices()
                 # negative sampling; replace unknown; re-weight batch_y
                 self.callback_manager.on_batch_begin(batch_x, batch_y, indices)
@@ -361,10 +380,9 @@ def _train(self):
 
                 # Is loss NaN or inf? requires_grad = False
                 self.callback_manager.on_backward_begin(loss)
-                self.grad_scaler.scale(loss).backward()
+                self._grad_backward(loss)
                 self.callback_manager.on_backward_end()
-                if self.step % self.update_every == 0:
-                    self._update()
+                self._update()
                 self.callback_manager.on_step_end()
 
                 if self.step % self.print_every == 0:
@@ -390,7 +408,7 @@ def _train(self):
         self.pbar = None
     # ============ tqdm end ============== #
 
-    def _clear_grad_opt(self, optimizer):
+    def _clear_grad(self, optimizer):
         if self.set_grad_to_none:
             for group in optimizer.param_groups:
                 for p in group['params']:
@@ -399,13 +417,24 @@ def _clear_grad_opt(self, optimizer):
         else:
             optimizer.zero_grad()
 
+    def _grad_backward(self, loss):
+        r""""""Compute gradient with link rules.
+
+        :param loss: a scalar where back-prop starts
+
+        For PyTorch, just do ""loss.backward()""
+        """"""
+        if (self.step-1) % self.update_every == 0:
+            self._clear_grad(self.optimizer)
+        self.grad_scaler.scale(loss).backward()
+
     def _update(self):
         r""""""Perform weight update on a model.
 
         """"""
-        self.grad_scaler.step(self.optimizer)
-        self.grad_scaler.update()
-        self._clear_grad_opt(self.optimizer)
+        if self.step % self.update_every == 0:
+            self.grad_scaler.step(self.optimizer)
+            self.grad_scaler.update()
 
     def _data_forward(self, network, x):
         x = _build_args(self._forward_func, **x)"
28;fastnlp;fastNLP;7ce03a601b702ce759dd3e13cf8878d1f273f4d9;"1.ä¿®æ”¹Trainerä¸­çš„pin_memoryå‚æ•°; 2.ä¿®æ”¹DistTrainerä½¿å¾—DistTrainerå’ŒTrainerçš„apiä½¿ç”¨å¯ä»¥å°½é‡æŽ¥è¿‘";"@@ -98,6 +98,7 @@ def __init__(self, data, model, metrics, batch_size=16, num_workers=0, device=No
         :param bool fp16: æ˜¯å¦ä½¿ç”¨float16è¿›è¡ŒéªŒè¯
         :param kwargs:
             Sampler sampler: æ”¯æŒä¼ å…¥sampleræŽ§åˆ¶æµ‹è¯•é¡ºåº
+            bool pin_memory: æ˜¯å¦å°†äº§ç”Ÿçš„tensorä½¿ç”¨pin memory, å¯èƒ½ä¼šåŠ å¿«æ•°æ®é€Ÿåº¦ã€‚
         """"""
         super(Tester, self).__init__()
 
@@ -112,6 +113,7 @@ def __init__(self, data, model, metrics, batch_size=16, num_workers=0, device=No
         self.verbose = verbose
         self.use_tqdm = use_tqdm
         self.logger = logger
+        self.pin_memory = kwargs.get('pin_memory', True)
 
         if isinstance(data, DataSet):
             sampler = kwargs.get('sampler', None)
@@ -122,7 +124,8 @@ def __init__(self, data, model, metrics, batch_size=16, num_workers=0, device=No
             if hasattr(sampler, 'set_batch_size'):
                 sampler.set_batch_size(batch_size)
             self.data_iterator = DataSetIter(dataset=data, batch_size=batch_size, sampler=sampler,
-                                             num_workers=num_workers)
+                                             num_workers=num_workers,
+                                             pin_memory=self.pin_memory)
         elif isinstance(data, BatchIter):
             self.data_iterator = data
         else:
@@ -179,7 +182,8 @@ def test(self):
                     start_time = time.time()
 
                     for batch_x, batch_y in data_iterator:
-                        _move_dict_value_to_device(batch_x, batch_y, device=self._model_device)
+                        _move_dict_value_to_device(batch_x, batch_y, device=self._model_device,
+                                                   non_blocking=self.pin_memory)
                         with self.auto_cast():
                             pred_dict = self._data_forward(self._predict_func, batch_x)
                             if not isinstance(pred_dict, dict):"
28;fastnlp;fastNLP;7ce03a601b702ce759dd3e13cf8878d1f273f4d9;"1.ä¿®æ”¹Trainerä¸­çš„pin_memoryå‚æ•°; 2.ä¿®æ”¹DistTrainerä½¿å¾—DistTrainerå’ŒTrainerçš„apiä½¿ç”¨å¯ä»¥å°½é‡æŽ¥è¿‘";"@@ -432,6 +432,7 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
             bool set_grad_to_none: åœ¨zero_gradçš„æ—¶å€™æ˜¯å¦å°†gradientè®¾ç½®ä¸ºNoneï¼Œè€Œä¸æ˜¯è®¾ç½®ä¸ºzero
             GradScaler grad_scaler: ä»…åœ¨fp16ä¸ºTrueæ—¶æœ‰æ•ˆï¼Œå¦‚æžœä¸ä½¿ç”¨torch.cuda.amp.GradScalerçš„åˆå§‹åŒ–å‚æ•°ï¼Œå¯ä¼ å…¥ä¸€ä¸ªå·²ç»åˆå§‹åŒ–åŽçš„
                 grad_scalerã€‚
+            bool pin_memory: æ˜¯å¦å°†äº§ç”Ÿçš„tensorä½¿ç”¨pin memory, å¯èƒ½ä¼šåŠ å¿«æ•°æ®é€Ÿåº¦ã€‚
         """"""
         super(Trainer, self).__init__()
         if not isinstance(model, nn.Module):
@@ -472,7 +473,7 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                 warnings.warn(""num_workers is ignored when train_data is BatchIter."")
             if drop_last:
                 warnings.warn(""drop_last is ignored when train_data is BatchIter."")
-
+        self.pin_memory = kwargs.get('pin_memory', True)
         if isinstance(model, nn.parallel.DistributedDataParallel):  # å¦‚æžœæ˜¯åˆ†å¸ƒå¼çš„
             # deviceä¸ºNone
             if device is not None:
@@ -502,12 +503,13 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                 sampler(train_data)
                 train_data = DataSetIter(train_data,
                                          batch_size=1, sampler=None, as_numpy=False, num_workers=num_workers,
-                                         pin_memory=False, drop_last=drop_last, timeout=0, worker_init_fn=None,
+                                         pin_memory=self.pin_memory, drop_last=drop_last, timeout=0, worker_init_fn=None,
                                          batch_sampler=sampler)
 
         if isinstance(train_data, DataSet):
             self.data_iterator = DataSetIter(dataset=train_data, batch_size=batch_size, sampler=sampler,
-                                             num_workers=num_workers, drop_last=drop_last)
+                                             num_workers=num_workers, drop_last=drop_last,
+                                             pin_memory=self.pin_memory)
         elif isinstance(train_data, BatchIter):
             self.data_iterator = train_data
             train_data = train_data.dataset
@@ -600,7 +602,8 @@ def __init__(self, train_data, model, optimizer=None, loss=None,
                                  use_tqdm=self.test_use_tqdm,
                                  sampler=kwargs.get('test_sampler', None),
                                  fp16=self.test_use_fp16,
-                                 num_workers=num_workers)
+                                 num_workers=num_workers,
+                                 pin_memory=self.pin_memory)
 
         self.start_time = None  # start timestamp
 "
28;tensorflow;graphics;d0817aec7dee35635814e925a59d83955459d93c;"Allows for using map_fn instead of vectorized_map.

map_fn uses significantly less memory.

PiperOrigin-RevId: 403409412";"@@ -23,8 +23,10 @@
 
 
 def differentiable_barycentrics(
-    framebuffer: fb.Framebuffer, clip_space_vertices: type_alias.TensorLike,
-    triangles: type_alias.TensorLike) -> fb.Framebuffer:
+    framebuffer: fb.Framebuffer,
+    clip_space_vertices: type_alias.TensorLike,
+    triangles: type_alias.TensorLike,
+    use_vectorized_map: bool = True) -> fb.Framebuffer:
   """"""Computes differentiable barycentric coordinates from a Framebuffer.
 
   The barycentric coordinates will be differentiable w.r.t. the input vertices.
@@ -39,6 +41,7 @@ def differentiable_barycentrics(
     triangles: a 2-D int32 tensor with shape [triangle_count, 3] or a 3-D tensor
       with shape [batch, triangle_count, 3] containing per-triangle vertex
       indices in counter-clockwise order.
+    use_vectorized_map: If true uses vectorized_map otherwise uses map_fn.
 
   Returns:
     a copy of `framebuffer`, but the differentiable barycentric coordinates will
@@ -95,9 +98,17 @@ def compute_barycentrics_fn(
     barycentric_coords = tf.transpose(barycentric_coords, perm=[1, 2, 3, 0])
     return barycentric_coords
 
-  per_image_barycentrics = tf.vectorized_map(
-      compute_barycentrics_fn,
-      (clip_space_vertices, triangles, framebuffer.triangle_id))
+  if use_vectorized_map:
+    per_image_barycentrics = tf.vectorized_map(
+        compute_barycentrics_fn,
+        (clip_space_vertices, triangles, framebuffer.triangle_id))
+  else:
+    num_meshes = tf.shape(clip_space_vertices)[0]
+    triangles_repeated = tf.repeat(triangles, repeats=num_meshes, axis=0)
+    per_image_barycentrics = tf.map_fn(
+        compute_barycentrics_fn,
+        (clip_space_vertices, triangles_repeated, framebuffer.triangle_id),
+        fn_output_signature=tf.TensorSpec(shape=(1, None, None, 3)))
 
   barycentric_coords = tf.stack(per_image_barycentrics, axis=0)
   # After stacking barycentrics will have layers dimension no matter what."
28;tensorflow;graphics;d0817aec7dee35635814e925a59d83955459d93c;"Allows for using map_fn instead of vectorized_map.

map_fn uses significantly less memory.

PiperOrigin-RevId: 403409412";"@@ -41,6 +41,7 @@ def rasterize(
     view_projection_matrix: type_alias.TensorLike,
     image_size: Tuple[int, int],
     enable_cull_face: bool = True,
+    use_vectorized_map: bool = True,
     backend: enum.Enum = rasterization_backend.RasterizationBackends.OPENGL,
     name: str = ""triangle_rasterizer_rasterize""
 ) -> Dict[str, type_alias.TensorLike]:
@@ -64,6 +65,8 @@ def rasterize(
       the rasterized image.
     enable_cull_face: Enables BACK face culling when True, and no culling when
       False.
+    use_vectorized_map: If true uses vectorized_map for barycentrics
+      computations otherwise uses map_fn.
     backend: A rasterization_backend.RasterizationBackends enum containing the
       backend method to use for rasterization.
     name: A name for this op. Defaults to ""triangle_rasterizer_rasterize"".
@@ -124,7 +127,7 @@ def rasterize(
     clip_space_vertices = utils.transform_homogeneous(view_projection_matrix,
                                                       vertices)
     rasterized = barycentrics_module.differentiable_barycentrics(
-        rasterized, clip_space_vertices, triangles)
+        rasterized, clip_space_vertices, triangles, use_vectorized_map)
     barycentrics = rasterized.barycentrics.value
     outputs[""barycentrics""] = utils.restore_batch_dims(
         rasterized.foreground_mask * barycentrics, input_batch_shape)"
28;NVlabs;stylegan2-ada-pytorch;d3a616a9c86f9a72087caca088dec7d045f44a4b;"Specify --shm-size=2g and fix typo in code comments

Fix OOM crash in data loader workers caused by docker's small default
shared memory size.";"@@ -17,11 +17,11 @@ set -e
 #
 # Use it like:
 #
-# ./run_docker.sh python generate.py --help
+# ./docker_run.sh python generate.py --help
 #
 # To override the default `stylegan2ada:latest` image, run:
 #
-# IMAGE=my_image:v1.0 ./run_docker.sh python generate.py --help
+# IMAGE=my_image:v1.0 ./docker_run.sh python generate.py --help
 #
 
 rest=$@
@@ -30,7 +30,7 @@ IMAGE=""${IMAGE:-sg2ada:latest}""
 
 CONTAINER_ID=$(docker inspect --format=""{{.Id}}"" ${IMAGE} 2> /dev/null)
 if [[ ""${CONTAINER_ID}"" ]]; then
-    docker run --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \
+    docker run --shm-size=2g --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \
         --workdir=/scratch -e HOME=/scratch $IMAGE $@
 else
     echo ""Unknown container image: ${IMAGE}"""
28;grantjenks;python-sortedcontainers;2b037039bb16c81ddc1ee94e88c44aa85eed2810;"Fix for issue #147 -- Stop caching dict methods using super

Caching dict methods using super creates a reference cycle which increases
memory pressure. User reported increased latencies due to GC pauses.";"@@ -151,17 +151,6 @@ def __init__(self, *args, **kwargs):
 
         self._list = SortedList(key=_key)
 
-        # Calls to super() are expensive so cache references to dict methods on
-        # sorted dict instances.
-
-        _dict = super(SortedDict, self)
-        self._dict_clear = _dict.clear
-        self._dict_delitem = _dict.__delitem__
-        self._dict_iter = _dict.__iter__
-        self._dict_pop = _dict.pop
-        self._dict_setitem = _dict.__setitem__
-        self._dict_update = _dict.update
-
         # Reaching through ``self._list`` repeatedly adds unnecessary overhead
         # so cache references to sorted list methods.
 
@@ -232,7 +221,7 @@ def clear(self):
         Runtime complexity: `O(n)`
 
         """"""
-        self._dict_clear()
+        dict.clear(self)
         self._list_clear()
 
 
@@ -256,7 +245,7 @@ def __delitem__(self, key):
         :raises KeyError: if key not found
 
         """"""
-        self._dict_delitem(key)
+        dict.__delitem__(self, key)
         self._list_remove(key)
 
 
@@ -304,7 +293,7 @@ def __setitem__(self, key, value):
         """"""
         if key not in self:
             self._list_add(key)
-        self._dict_setitem(key, value)
+        dict.__setitem__(self, key, value)
 
     _setitem = __setitem__
 
@@ -425,7 +414,7 @@ def pop(self, key, default=__not_given):
         """"""
         if key in self:
             self._list_remove(key)
-            return self._dict_pop(key)
+            return dict.pop(self, key)
         else:
             if default is self.__not_given:
                 raise KeyError(key)
@@ -464,7 +453,7 @@ def popitem(self, index=-1):
             raise KeyError('popitem(): dictionary is empty')
 
         key = self._list_pop(index)
-        value = self._dict_pop(key)
+        value = dict.pop(self, key)
         return (key, value)
 
 
@@ -525,7 +514,7 @@ def setdefault(self, key, default=None):
         """"""
         if key in self:
             return self[key]
-        self._dict_setitem(key, default)
+        dict.__setitem__(self, key, default)
         self._list_add(key)
         return default
 
@@ -544,8 +533,8 @@ def update(self, *args, **kwargs):
 
         """"""
         if not self:
-            self._dict_update(*args, **kwargs)
-            self._list_update(self._dict_iter())
+            dict.update(self, *args, **kwargs)
+            self._list_update(dict.__iter__(self))
             return
 
         if not kwargs and len(args) == 1 and isinstance(args[0], dict):
@@ -554,9 +543,9 @@ def update(self, *args, **kwargs):
             pairs = dict(*args, **kwargs)
 
         if (10 * len(pairs)) > len(self):
-            self._dict_update(pairs)
+            dict.update(self, pairs)
             self._list_clear()
-            self._list_update(self._dict_iter())
+            self._list_update(dict.__iter__(self))
         else:
             for key in pairs:
                 self._setitem(key, pairs[key])
@@ -631,15 +620,15 @@ def _view_delitem(self, index):
     """"""
     _mapping = self._mapping
     _list = _mapping._list
-    _dict_delitem = _mapping._dict_delitem
+    dict_delitem = dict.__delitem__
     if isinstance(index, slice):
         keys = _list[index]
         del _list[index]
         for key in keys:
-            _dict_delitem(key)
+            dict_delitem(_mapping, key)
     else:
         key = _list.pop(index)
-        _dict_delitem(key)
+        dict_delitem(_mapping, key)
 
 
 class SortedKeysView(KeysView, Sequence):"
28;grantjenks;python-sortedcontainers;2b037039bb16c81ddc1ee94e88c44aa85eed2810;"Fix for issue #147 -- Stop caching dict methods using super

Caching dict methods using super creates a reference cycle which increases
memory pressure. User reported increased latencies due to GC pauses.";"@@ -0,0 +1,123 @@
+from sortedcontainers import SortedDict, SortedList
+import gc
+
+def check(f):
+    print('start')
+
+    a = f()
+    t = type(a)
+
+    print('post-setup')
+    for obj in gc.get_objects():
+        if type(obj) == t:
+            print(obj)
+
+    del a
+
+    print('post-delete')
+    for obj in gc.get_objects():
+        if type(obj) == t:
+            print(obj)
+
+    gc.collect()
+
+    print('post-collect')
+    for obj in gc.get_objects():
+        if type(obj) == t:
+            print(obj)
+
+    print('finish')
+
+check(lambda: SortedDict({'a': 1, 'b': 2}))
+
+class MyDict(dict):
+    pass
+
+check(lambda: MyDict({'a': 1, 'b': 2}))
+
+check(lambda: SortedList([1, 2, 3]))
+
+from functools import partial
+
+class SortedDictSub(SortedDict):
+
+    def __init__(self, *args, **kwargs):
+        """"""Initialize sorted dict instance.
+        Optional key-function argument defines a callable that, like the `key`
+        argument to the built-in `sorted` function, extracts a comparison key
+        from each dictionary key. If no function is specified, the default
+        compares the dictionary keys directly. The key-function argument must
+        be provided as a positional argument and must come before all other
+        arguments.
+        Optional iterable argument provides an initial sequence of pairs to
+        initialize the sorted dict. Each pair in the sequence defines the key
+        and corresponding value. If a key is seen more than once, the last
+        value associated with it is stored in the new sorted dict.
+        Optional mapping argument provides an initial mapping of items to
+        initialize the sorted dict.
+        If keyword arguments are given, the keywords themselves, with their
+        associated values, are added as items to the dictionary. If a key is
+        specified both in the positional argument and as a keyword argument,
+        the value associated with the keyword is stored in the
+        sorted dict.
+        Sorted dict keys must be hashable, per the requirement for Python's
+        dictionaries. Keys (or the result of the key-function) must also be
+        comparable, per the requirement for sorted lists.
+        >>> d = {'alpha': 1, 'beta': 2}
+        >>> SortedDict([('alpha', 1), ('beta', 2)]) == d
+        True
+        >>> SortedDict({'alpha': 1, 'beta': 2}) == d
+        True
+        >>> SortedDict(alpha=1, beta=2) == d
+        True
+        """"""
+        if args and (args[0] is None or callable(args[0])):
+            _key = self._key = args[0]
+            args = args[1:]
+        else:
+            _key = self._key = None
+
+        self._list = SortedList(key=_key)
+
+        # Calls to super() are expensive so cache references to dict methods on
+        # sorted dict instances.
+
+        _dict = super(SortedDict, self)
+        # self._dict_clear = _dict.clear
+        # self._dict_delitem = _dict.__delitem__
+        self._dict_iter = partial(dict.__iter__, self)  # _dict.__iter__
+        # self._dict_pop = _dict.pop
+        # self._dict_setitem = _dict.__setitem__
+        self._dict_update = partial(dict.update, self)  # _dict.update
+
+        # Reaching through ``self._list`` repeatedly adds unnecessary overhead
+        # so cache references to sorted list methods.
+
+        _list = self._list
+        self._list_add = _list.add
+        self._list_clear = _list.clear
+        self._list_iter = _list.__iter__
+        self._list_reversed = _list.__reversed__
+        self._list_pop = _list.pop
+        self._list_remove = _list.remove
+        self._list_update = _list.update
+
+        # Expose some sorted list methods publicly.
+
+        self.bisect_left = _list.bisect_left
+        self.bisect = _list.bisect_right
+        self.bisect_right = _list.bisect_right
+        self.index = _list.index
+        self.irange = _list.irange
+        self.islice = _list.islice
+        self._reset = _list._reset
+
+        if _key is not None:
+            self.bisect_key_left = _list.bisect_key_left
+            self.bisect_key_right = _list.bisect_key_right
+            self.bisect_key = _list.bisect_key
+            self.irange_key = _list.irange_key
+
+        self._update(*args, **kwargs)
+
+check(lambda: SortedDictSub({'a': 1, 'b': 2}))"
28;grantjenks;python-sortedcontainers;2b037039bb16c81ddc1ee94e88c44aa85eed2810;"Fix for issue #147 -- Stop caching dict methods using super

Caching dict methods using super creates a reference cycle which increases
memory pressure. User reported increased latencies due to GC pauses.";"@@ -5,6 +5,7 @@
 from sortedcontainers import SortedDict
 import pytest
 from sys import hexversion
+import gc
 
 if hexversion < 0x03000000:
     range = xrange
@@ -474,3 +475,12 @@ def test_pickle():
     beta = pickle.loads(pickle.dumps(alpha))
     assert alpha == beta
     assert alpha._key == beta._key
+
+def test_ref_counts():
+    start_count = len(gc.get_objects())
+    temp = SortedDict()
+    init_count = len(gc.get_objects())
+    assert init_count > start_count
+    del temp
+    del_count = len(gc.get_objects())
+    assert start_count == del_count"
29;kenshohara;3D-ResNets-PyTorch;b162d135f2af94aa231696930db11ae6863c9419;change resume order to avoid out of memory;"@@ -82,27 +82,30 @@ def get_opt():
     return opt
 
 
-def resume(resume_path,
-           arch,
-           begin_epoch,
-           model,
-           optimizer=None,
-           scheduler=None):
-    print('loading checkpoint {}'.format(resume_path))
-    checkpoint = torch.load(resume_path)
+def resume_model(resume_path, arch, model):
+    print('loading checkpoint {} model'.format(resume_path))
+    checkpoint = torch.load(resume_path, map_location='cpu')
     assert arch == checkpoint['arch']
 
-    begin_epoch = checkpoint['epoch'] + 1
     if hasattr(model, 'module'):
         model.module.load_state_dict(checkpoint['state_dict'])
     else:
         model.load_state_dict(checkpoint['state_dict'])
+
+    return model
+
+
+def resume_train_utils(resume_path, begin_epoch, optimizer, scheduler):
+    print('loading checkpoint {} train utils'.format(resume_path))
+    checkpoint = torch.load(resume_path, map_location='cpu')
+
+    begin_epoch = checkpoint['epoch'] + 1
     if optimizer is not None and 'optimizer' in checkpoint:
         optimizer.load_state_dict(checkpoint['optimizer'])
     if scheduler is not None and 'scheduler' in checkpoint:
         scheduler.load_state_dict(checkpoint['scheduler'])
 
-    return begin_epoch, model, optimizer, scheduler
+    return begin_epoch, optimizer, scheduler
 
 
 def get_normalize_method(mean, std, no_mean_norm, no_std_norm):
@@ -337,7 +340,8 @@ def main_worker(index, opt):
     if opt.pretrain_path:
         model = load_pretrained_model(model, opt.pretrain_path, opt.model,
                                       opt.n_finetune_classes)
-
+    if opt.resume_path is not None:
+        model = resume_model(opt.resume_path, opt.arch, model)
     model = make_data_parallel(model, opt.distributed, opt.device)
 
     if opt.pretrain_path:
@@ -353,19 +357,13 @@ def main_worker(index, opt):
     if not opt.no_train:
         (train_loader, train_sampler, train_logger, train_batch_logger,
          optimizer, scheduler) = get_train_utils(opt, parameters)
-    if not opt.no_val:
-        val_loader, val_logger = get_val_utils(opt)
-
-    if opt.resume_path is not None:
-        if not opt.no_train:
-            opt.begin_epoch, model, optimizer, scheduler = resume(
-                opt.resume_path, opt.arch, opt.begin_epoch, model, optimizer,
-                scheduler)
+        if opt.resume_path is not None:
+            opt.begin_epoch, optimizer, scheduler = resume_train_utils(
+                opt.resume_path, opt.begin_epoch, optimizer, scheduler)
             if opt.overwrite_milestones:
                 scheduler.milestones = opt.multistep_milestones
-        else:
-            opt.begin_epoch, model, _, _ = resume(opt.resume_path, opt.arch,
-                                                  opt.begin_epoch, model)
+    if not opt.no_val:
+        val_loader, val_logger = get_val_utils(opt)
 
     if opt.tensorboard and opt.is_master_node:
         from torch.utils.tensorboard import SummaryWriter"
30;pythonprofilers;memory_profiler;45291539108358045b6f2422252856115c531949;"Add Quick Start to README

Adapted from an answer to ""Graphing a process's memory usage"" on [Stack Overflow](https://stackoverflow.com/a/62876993/111424).

Memory profiler is exactly the tool I've been looking for. But the Usage section of the README makes it seem so complicated to use that it scared me away!

The Stack Overflow answer gets straight to the point and shows how simple it is to use. I think it deserves to be at the top of the README.";"@@ -28,6 +28,20 @@ To install from source, download the package, extract and type::
 
     $ pip install .
 
+===========
+Quick Start
+===========
+
+Use `mprof` to generate a full memory usage report of your executable and to plot it.
+
+.. code-block:: bash
+
+    mprof run executable
+    mprof plot
+
+The plot would be something like this:
+
+.. image:: https://i.stack.imgur.com/ixCH4.png
 
 =======
  Usage"
30;pythonprofilers;memory_profiler;ea08ce874aad301c5c68d959400c8aa006cb63d6;included memory_metric;"@@ -164,7 +164,7 @@ def _ps_util_full_tool(memory_metric):
             mem = getattr(meminfo, memory_metric) / _TWO_20
 
             if include_children:
-                mem +=  sum([mem for (pid, mem) in _get_child_memory(process, meminfo_attr)])
+                mem +=  sum([mem for (pid, mem) in _get_child_memory(process, meminfo_attr, memory_metric)])
 
             if timestamps:
                 return mem, time.time()"
30;pythonprofilers;memory_profiler;461334622e438b136f4fc9bae84708eadda3f861;Merge branch 'master' of github.com:pythonprofilers/memory_profiler;"@@ -411,7 +411,7 @@ file ~/.ipython/ipy_user_conf.py to add the following lines::
 Memory tracking backends
 ===============================
 `memory_profiler` supports different memory tracking backends including: 'psutil', 'psutil_pss', 'psutil_uss', 'posix', 'tracemalloc'.
-If no specific backend is specified the default is to use ""psutil"" which measures RSS aka â€œResident Set Sizeâ€. 
+If no specific backend is specified the default is to use ""psutil"" which measures RSS aka ""Resident Set Size"". 
 In some cases (particularly when tracking child processes) RSS may overestimate memory usage (see `example/example_psutil_memory_full_info.py` for an example).
 For more information on ""psutil_pss"" (measuring PSS) and ""psutil_uss"" please refer to:
 https://psutil.readthedocs.io/en/latest/index.html?highlight=memory_info#psutil.Process.memory_full_info "
30;pythonprofilers;memory_profiler;461334622e438b136f4fc9bae84708eadda3f861;Merge branch 'master' of github.com:pythonprofilers/memory_profiler;"@@ -311,7 +311,7 @@ def memory_usage(proc=-1, interval=.1, timeout=None, timestamps=False,
 
     backend : str, optional
         Current supported backends: 'psutil', 'psutil_pss', 'psutil_uss', 'posix', 'tracemalloc'
-        If `backend=None` the default is ""psutil"" which measures RSS aka â€œResident Set Sizeâ€. 
+        If `backend=None` the default is ""psutil"" which measures RSS aka ""Resident Set Size"". 
         For more information on ""psutil_pss"" (measuring PSS) and ""psutil_uss"" please refer to:
         https://psutil.readthedocs.io/en/latest/index.html?highlight=memory_info#psutil.Process.memory_full_info 
 "
30;pythonprofilers;memory_profiler;d7e5486c459fc7ba7d69139afb3ac36d8f653615;"%memit: fix repeats for short-living statements

Based on the comment by leguyader:
https://github.com/pythonprofilers/memory_profiler/issues/212#issuecomment-782054915

Fixes #212";"@@ -1110,7 +1110,7 @@ def memit(self, line='', cell=None):
             counter += 1
             tmp = memory_usage((_func_exec, (stmt, self.shell.user_ns)),
                                timeout=timeout, interval=interval,
-                               max_usage=True,
+                               max_usage=True, max_iterations=1,
                                include_children=include_children)
             mem_usage.append(tmp)
 "
30;pythonprofilers;memory_profiler;03a9088159d5c898aff4791f6410af6bc1326c87;"feat(peak): adds --func optional argument

Filter the peak memory usage by function. This doesn't isolate the
contribution of the function by itself, but it screens out noise
from setting up the test so an accurate result can be obtained
by mprof peak if the test is otherwise well isolated.

Example usage:

mprof peak --func some.function

The function name is the fully qualified python import name.
You can also figure it out by grepping the dat file for FUNC
entries.";"@@ -835,19 +835,47 @@ def xlim_type(value):
     else:
         pl.show()
 
+def filter_mprofile_mem_usage_by_function(prof, func):
+    if func is None:
+        return prof[""mem_usage""]
+
+    if func not in prof[""func_timestamp""]:
+        raise ValueError(str(func) + "" was not found."")
+
+    time_ranges = prof[""func_timestamp""][func]
+    filtered_memory = []
+    
+    # The check here could be improved, but it's done in this
+    # inefficient way to make sure we don't miss overlapping
+    # ranges.
+    for mib, ts in zip(prof[""mem_usage""], prof[""timestamp""]):
+        for rng in time_ranges:
+            if rng[0] <= ts <= rng[1]:
+                filtered_memory.append(mib)
+
+    return filtered_memory
+
 def peak_action():
     desc = """"""Prints the peak memory used in data file `file.dat` generated
 using `mprof run`. If no .dat file is given, it will take the most recent
 such file in the current directory.""""""
     parser = ArgumentParser(usage=""mprof peak [options] [file.dat]"", description=desc)
     parser.add_argument(""profiles"", nargs=""*"",
                     help=""profiles made by mprof run"")
+    parser.add_argument(""--func"", dest=""func"", default=None,
+                        help=""""""Show the peak for this function. Does not support child processes."""""")
     args = parser.parse_args()
     filenames = get_profiles(args)
 
     for filename in filenames:
         prof = read_mprofile_file(filename)
-        print(""{}\t{:.3f} MiB"".format(prof[""filename""], max(prof[""mem_usage""])))
+        try:
+            mem_usage = filter_mprofile_mem_usage_by_function(prof, args.func)
+        except ValueError:
+            print(""{}\tNaN MiB"".format(prof[""filename""]))
+            continue
+
+        print(""{}\t{:.3f} MiB"".format(prof[""filename""], max(mem_usage)))
         for child, values in prof[""children""].items():
             child_peak = max([ mem_ts[0] for mem_ts in values ])
             print(""  Child {}\t\t\t{:.3f} MiB"".format(child, child_peak))"
32;facebookresearch;PyTorch-BigGraph;3c57f3d1578e0db4d5229e62a948e774ddc1310a;"fix the number of embedding_storage for unpartitioned entities that exist in both lhs and rhs

Summary: Allocating shared memory is expensive. We do this once at the beginning and then cache these storages and re-use them repeatedly during training. hence we need to estimate how many storages we need and the case of 1x1 partitions is handled poorly (because, it's rather rare) and the code decides it needs two storages for the those entities that appear on both sides but are alwasys the same partition on both sides. So only one storage is needed.

Reviewed By: lw

Differential Revision: D25179561

fbshipit-source-id: 1b3f9b896efb6127bcf03e8154bc26e8875e8553";"@@ -279,16 +279,22 @@ def __init__(  # noqa
         ] = defaultdict(set)
         for entity_type, counts in entity_counts.items():
             max_count = max(counts)
-            num_sides = (
-                (1 if entity_type in holder.lhs_partitioned_types else 0)
-                + (1 if entity_type in holder.rhs_partitioned_types else 0)
-                + (
-                    1
-                    if entity_type
-                    in (holder.lhs_unpartitioned_types | holder.rhs_unpartitioned_types)
-                    else 0
+            if holder.nparts_lhs == 1 and holder.nparts_rhs == 1:
+                num_sides = 1
+            else:
+                num_sides = (
+                    (1 if entity_type in holder.lhs_partitioned_types else 0)
+                    + (1 if entity_type in holder.rhs_partitioned_types else 0)
+                    + (
+                        1
+                        if entity_type
+                        in (
+                            holder.lhs_unpartitioned_types
+                            | holder.rhs_unpartitioned_types
+                        )
+                        else 0
+                    )
                 )
-            )
             for _ in range(num_sides):
                 embedding_storage_freelist[entity_type].add(
                     allocate_shared_tensor("
33;facebookresearch;higher;f06986c9d3af4667a2d66fb3560e4fabeee8c7be;"Fix memory leak when using `track_higher_grad=False` (#48)

* Fix spelling. Add track_higher_grads property to patched modules. Fix initialization.
* Fix how parameters() returns iterable. Add track_higher_grads to patched modules. Fix logic when not tracking higher grads.
* Fix test time MAML test.";"@@ -83,7 +83,10 @@ def innerloop_ctx(
         ``DifferentiableOptimizer`` instance of the right subtype.
     """"""
     fmodel = monkeypatch(
-        model, device, copy_initial_weights=copy_initial_weights
+        model, 
+        device, 
+        copy_initial_weights=copy_initial_weights,
+        track_higher_grads=track_higher_grads
     )
     diffopt = optim.get_diff_optim(
         opt,"
33;facebookresearch;higher;f06986c9d3af4667a2d66fb3560e4fabeee8c7be;"Fix memory leak when using `track_higher_grad=False` (#48)

* Fix spelling. Add track_higher_grads property to patched modules. Fix initialization.
* Fix how parameters() returns iterable. Add track_higher_grads to patched modules. Fix logic when not tracking higher grads.
* Fix test time MAML test.";"@@ -40,9 +40,9 @@
 
 @_contextmanager
 def _modify_internally(fmodule):
-    fmodule._being_modifed_internally = True
+    fmodule._being_modified_internally = True
     yield
-    fmodule._being_modifed_internally = False
+    fmodule._being_modified_internally = False
 
 
 def _patched_parameters(
@@ -79,15 +79,21 @@ def _patched_parameters(
 
     time = -1 if time is None else time
 
-    for p in self._fast_params[time]:
-        yield p
+    if not self.track_higher_grads and time not in (-1, 0):
+        raise ValueError(
+            ""The patched model is not tracking higher gradients. Only the ""
+            ""latest parameters are available.""
+        )
+
+    return iter(self._fast_params[time])
 
 
 class _MonkeyPatchBase(_abc.ABC, _torch.nn.Module):
     @_abc.abstractmethod
     def __init__(self) -> None:
         self._param_mapping: _typing.List[int] = []
-        self._being_modifed_internally = True
+        self._being_modified_internally: bool = True
+        self._track_higher_grads: bool = True
 
     def forward(self):
         raise NotImplementedError(
@@ -106,6 +112,11 @@ def _expand_params(
 
     @property
     def init_fast_params(self):
+        if not self.track_higher_grads:
+            raise Exception(
+                ""Cannot get initial parameters when not tracking higher ""
+                ""gradients.""
+            )
         return self._fast_params[0]
 
     @property
@@ -117,7 +128,22 @@ def fast_params(self, value):
         value = list(value)
         if self._fast_params is None:
             self._fast_params = []
-        self._fast_params.append(value)
+        if self.track_higher_grads:
+            self._fast_params.append(value)
+        else:
+            self._fast_params[0] = value
+
+    @property
+    def track_higher_grads(self):
+        return self._track_higher_grads
+
+    @track_higher_grads.setter
+    def track_higher_grads(self, value):
+        if not isinstance(value, bool):
+            raise ValueError(
+                ""Expected boolean argument. Got: {}."".format(type(value))
+            )
+        self._track_higher_grads = value
 
 
 def buffer_sync(
@@ -194,6 +220,7 @@ class MonkeyPatched(_ModuleType, _MonkeyPatchBase):  # type: ignore
 
         def __init__(self, original_params, root) -> None:
             _torch.nn.Module.__init__(self)
+            _MonkeyPatchBase.__init__(self)
             self._root_ref = _weakref.ref(root) if root else None
 
             self._fast_params = None
@@ -235,7 +262,7 @@ def remove_from(*dicts):
                     raise TypeError(""Require Tensor as fast weights. ""
                                     ""Got {}"".format(_torch.typename(value)))
 
-                if not self._being_modifed_internally:
+                if not self._being_modified_internally:
                     # Additional behaviour for when fast weights are being
                     # directly modified goes here:
                     old_value = self._parameters[name]
@@ -454,7 +481,8 @@ def _update_params(self, params):
 def monkeypatch(
     module: _torch.nn.Module,
     device: _typing.Optional[_torch.device] = None,
-    copy_initial_weights: bool = True
+    copy_initial_weights: bool = True,
+    track_higher_grads: bool = True
 ) -> _MonkeyPatchBase:
     r""""""Create a monkey-patched stateless version of a module.
 
@@ -473,6 +501,13 @@ def monkeypatch(
             If this is set to False, the actual module weights will be the
             initial weights of the patched module. This is useful when doing
             MAML, for example.
+        track_higher_grads: if True, during unrolled optimization the graph be
+            retained, and the fast weights will bear grad funcs, so as to permit
+            backpropagation through the optimization process. Setting this to
+            False allows ``monkeypatch`` to be used in ""test mode"", without
+            potentially tracking higher order gradients. This can be useful when
+            running the training loop at test time, e.g. in k-shot learning
+            experiments, without incurring a significant memory overhead.
 
     Returns:
         ``fmodule``: a ""stateless"" version of the original module, for which calls
@@ -496,5 +531,6 @@ def encapsulator(
         fmodule.update_params(params)
 
     fmodule = make_functional(module, encapsulator=encapsulator)
+    fmodule.track_higher_grads = track_higher_grads
 
     return fmodule"
33;facebookresearch;higher;f06986c9d3af4667a2d66fb3560e4fabeee8c7be;"Fix memory leak when using `track_higher_grad=False` (#48)

* Fix spelling. Add track_higher_grads property to patched modules. Fix initialization.
* Fix how parameters() returns iterable. Add track_higher_grads to patched modules. Fix logic when not tracking higher grads.
* Fix test time MAML test.";"@@ -335,13 +335,14 @@ def testMiniMAMLTestTime(self, _, model_builder):
 
         ctx_opts = {""copy_initial_weights"": False, ""track_higher_grads"": False}
         with higher.innerloop_ctx(model, opt, **ctx_opts) as (fmodel, diffopt):
+            init_params = fmodel.parameters(time=0)
             for _ in range(10):
                 inputs = torch.rand(8, 4)
                 loss = fmodel(inputs).sum().pow(2)
                 diffopt.step(loss)
             param_sum = sum(p.sum() for p in fmodel.parameters())
             final_grads = torch.autograd.grad(
-                param_sum, fmodel.parameters(time=0), allow_unused=True
+                param_sum, init_params, allow_unused=True
             )
             param_sum.backward()
             for p, g in zip(model.parameters(), final_grads):"
33;hoxu;gitstats;e56e7b6f9116e1b5e3a19eacb668211476084e77;"properly terminate created subprocesses

This fixes a memory / ressource leak that manifests when computing
stats over big sets of repositories. It was eating more than 8G of
memory for ~15 git repositories.

Signed-off-by: Heikki Hokkanen <hoxu@users.sf.net>";"@@ -56,12 +56,14 @@ def getpipeoutput(cmds, quiet = False):
 	if not quiet and ON_LINUX and os.isatty(1):
 		print '>> ' + ' | '.join(cmds),
 		sys.stdout.flush()
-	p0 = subprocess.Popen(cmds[0], stdout = subprocess.PIPE, shell = True)
-	p = p0
+	p = subprocess.Popen(cmds[0], stdout = subprocess.PIPE, shell = True)
+	processes=[p]
 	for x in cmds[1:]:
-		p = subprocess.Popen(x, stdin = p0.stdout, stdout = subprocess.PIPE, shell = True)
-		p0 = p
+		p = subprocess.Popen(x, stdin = p.stdout, stdout = subprocess.PIPE, shell = True)
+		processes.append(p)
 	output = p.communicate()[0]
+	for p in processes:
+		p.wait()
 	end = time.time()
 	if not quiet:
 		if ON_LINUX and os.isatty(1):
@@ -449,7 +451,10 @@ class GitDataCollector(DataCollector):
 				revs_to_read.append((time,rev))
 
 		#Read revisions from repo
-		time_rev_count = Pool(processes=conf['processes']).map(getnumoffilesfromrev, revs_to_read)
+		pool = Pool(processes=conf['processes'])
+		time_rev_count = pool.map(getnumoffilesfromrev, revs_to_read)
+		pool.terminate()
+		pool.join()
 
 		#Update cache with new revisions and append then to general list
 		for (time, rev, count) in time_rev_count:
@@ -507,7 +512,10 @@ class GitDataCollector(DataCollector):
 				blobs_to_read.append((ext,blob_id))
 
 		#Get info abount line count for new blob's that wasn't found in cache
-		ext_blob_linecount = Pool(processes=conf['processes']).map(getnumoflinesinblob, blobs_to_read)
+		pool = Pool(processes=conf['processes'])
+		ext_blob_linecount = pool.map(getnumoflinesinblob, blobs_to_read)
+		pool.terminate()
+		pool.join()
 
 		#Update cache and write down info about number of number of lines
 		for (ext, blob_id, linecount) in ext_blob_linecount:"
34;spec-first;connexion;28c33d88ce70e22c0bc7f5b5bacc83c855f2f72d;"Save memory on unused schema strings (#1482)

Nodoby needs the schema strings vs. the parsed contents, yet those strings consume quite some memory for big specs.";"@@ -208,6 +208,7 @@ class Swagger2Specification(Specification):
 
     schema_string = pkg_resources.resource_string('connexion', 'resources/schemas/v2.0/schema.json')
     openapi_schema = json.loads(schema_string.decode('utf-8'))
+    del schema_string
 
     @classmethod
     def _set_defaults(cls, spec):
@@ -260,6 +261,7 @@ class OpenAPISpecification(Specification):
 
     schema_string = pkg_resources.resource_string('connexion', 'resources/schemas/v3.0/schema.json')
     openapi_schema = json.loads(schema_string.decode('utf-8'))
+    del schema_string
 
     @classmethod
     def _set_defaults(cls, spec):"
34;qiyeboy;IPProxyPool;e620c5fc3770d523b184ee3c606acc11464594f1;limit speed and save memory;"@@ -41,6 +41,18 @@ def validator(queue1, queue2, myip):
     proc_pool = {}     # æ‰€æœ‰è¿›ç¨‹åˆ—è¡¨
     cntl_q = Queue()   # æŽ§åˆ¶ä¿¡æ¯é˜Ÿåˆ—
     while True:
+        if not cntl_q.empty():
+            # å¤„ç†å·²ç»“æŸçš„è¿›ç¨‹
+            try:
+                pid = cntl_q.get()
+                proc = proc_pool.pop(pid)
+                proc_ps = psutil.Process(pid)
+                proc_ps.kill()
+                proc_ps.wait()
+            except Exception as e:
+                pass
+                # print(e)
+                # print("" we are unable to kill pid:%s"" % (pid))
         try:
             # proxy_dict = {'source':'crawl','data':proxy}
             if len(proc_pool) >= config.MAX_CHECK_PROCESS:
@@ -61,20 +73,6 @@ def validator(queue1, queue2, myip):
                 proc_pool[p.pid] = p
                 tasklist = []
 
-        if not cntl_q.empty():
-            # å¤„ç†å·²ç»“æŸçš„è¿›ç¨‹
-            try:
-                pid = cntl_q.get()
-                proc = proc_pool.pop(pid)
-                proc_ps = psutil.Process(pid)
-                proc_ps.kill()
-                proc_ps.wait()
-            except Exception as e:
-                pass
-                # print(e)
-                # print("" we are unable to kill pid:%s"" % (pid))
-
-
 def process_start(tasks, myip, queue2, cntl):
     spawns = []
     for task in tasks:"
34;qiyeboy;IPProxyPool;fcbd1293530bb276759c8b903c1cc8b1e5d9ba13;limit speed and save memory;"@@ -204,4 +204,5 @@ def get_header():
 MAX_CHECK_PROCESS = 2 # CHECK_PROXYæœ€å¤§è¿›ç¨‹æ•°
 MAX_CHECK_CONCURRENT_PER_PROCESS = 30 # CHECK_PROXYæ—¶æ¯ä¸ªè¿›ç¨‹çš„æœ€å¤§å¹¶å‘
 TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
-MAX_DOWNLOAD_CONCURRENT = 3 # ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file
+MAX_DOWNLOAD_CONCURRENT = 3 # ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
+CHECK_WATI_TIME = 1#è¿›ç¨‹æ•°è¾¾åˆ°ä¸Šé™æ—¶çš„ç­‰å¾…æ—¶é—´
\ No newline at end of file"
34;qiyeboy;IPProxyPool;1b6b823e992a39c2272b73ee03949b08d23bc8c9;limit speed and save memory;"@@ -204,4 +204,4 @@ def get_header():
 MAX_CHECK_PROCESS = 2 # CHECK_PROXYæœ€å¤§è¿›ç¨‹æ•°
 MAX_CHECK_CONCURRENT_PER_PROCESS = 30 # CHECK_PROXYæ—¶æ¯ä¸ªè¿›ç¨‹çš„æœ€å¤§å¹¶å‘
 TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
-MAX_DOWNLOAD_CURRENT = 3 # ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file
+MAX_DOWNLOAD_CONCURRENT = 3 # ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file"
34;qiyeboy;IPProxyPool;1b6b823e992a39c2272b73ee03949b08d23bc8c9;limit speed and save memory;"@@ -10,7 +10,7 @@
 from multiprocessing import Queue, Process, Value
 
 from api.apiServer import start_api_server
-from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CONCURRENT_PER_PROCESS, MAX_DOWNLOAD_CURRENT
+from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CONCURRENT_PER_PROCESS, MAX_DOWNLOAD_CONCURRENT
 from db.DataStore import store_data, sqlhelper
 from spider.HtmlDownloader import Html_Downloader
 from spider.HtmlPraser import Html_Parser
@@ -61,7 +61,7 @@ def run(self):
                 spawns = []
                 for p in parserList:
                     spawns.append(gevent.spawn(self.crawl, p))
-                    if len(spawns) >= MAX_DOWNLOAD_CURRENT:
+                    if len(spawns) >= MAX_DOWNLOAD_CONCURRENT:
                         gevent.joinall(spawns)
                         spawns= []
                 gevent.joinall(spawns)"
34;qiyeboy;IPProxyPool;f77bc557ca28521166f137da0da8d46501e636b4;limit speed and save memory;"@@ -201,7 +201,7 @@ def get_header():
 #ä¸‹é¢é…ç½®squid,çŽ°åœ¨è¿˜æ²¡å®žçŽ°
 #SQUID={'path':None,'confpath':'C:/squid/etc/squid.conf'}
 
-
-MAX_CHECK_CURRENT = 30 # CHECK_PROXYæ—¶çš„æœ€å¤§å¹¶å‘
+MAX_CHECK_PROCESS = 2 # CHECK_PROXYæœ€å¤§è¿›ç¨‹æ•°
+MAX_CHECK_CONCURRENT_PER_PROCESS = 30 # CHECK_PROXYæ—¶æ¯ä¸ªè¿›ç¨‹çš„æœ€å¤§å¹¶å‘
 TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
-MAX_DOWNLOAD_CURRENT = 3#ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file
+MAX_DOWNLOAD_CURRENT = 3 # ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file"
34;qiyeboy;IPProxyPool;f77bc557ca28521166f137da0da8d46501e636b4;limit speed and save memory;"@@ -10,7 +10,7 @@
 from multiprocessing import Queue, Process, Value
 
 from api.apiServer import start_api_server
-from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CURRENT, MAX_DOWNLOAD_CURRENT
+from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CONCURRENT_PER_PROCESS, MAX_DOWNLOAD_CURRENT
 from db.DataStore import store_data, sqlhelper
 from spider.HtmlDownloader import Html_Downloader
 from spider.HtmlPraser import Html_Parser
@@ -47,7 +47,7 @@ def run(self):
             spawns = []
             for proxy in proxylist:
                 spawns.append(gevent.spawn(detect_from_db, self.myip, proxy, self.proxies))
-                if len(spawns) >= MAX_CHECK_CURRENT:
+                if len(spawns) >= MAX_CHECK_CONCURRENT_PER_PROCESS:
                     gevent.joinall(spawns)
                     spawns= []
             gevent.joinall(spawns)"
34;qiyeboy;IPProxyPool;f77bc557ca28521166f137da0da8d46501e636b4;limit speed and save memory;"@@ -43,13 +43,15 @@ def validator(queue1, queue2, myip):
     while True:
         try:
             # proxy_dict = {'source':'crawl','data':proxy}
+            if len(proc_pool) >= config.MAX_CHECK_PROCESS:
+                time.sleep(config.CHECK_WATI_TIME)
+                continue
             proxy = queue1.get()
             tasklist.append(proxy)
-            if len(tasklist) >= config.MAX_CHECK_CURRENT:
-                normal_start(tasklist, myip, queue2)
-                # p = Process(target=process_start, args=(tasklist, myip, queue2, cntl_q))
-                # p.start()
-                # proc_pool[p.pid] = p
+            if len(tasklist) >= config.MAX_CHECK_CONCURRENT_PER_PROCESS:
+                p = Process(target=process_start, args=(tasklist, myip, queue2, cntl_q))
+                p.start()
+                proc_pool[p.pid] = p
                 tasklist = []
 
         except Exception as e:
@@ -73,12 +75,6 @@ def validator(queue1, queue2, myip):
                 # print("" we are unable to kill pid:%s"" % (pid))
 
 
-def normal_start(tasks, myip, queue2):
-    spawns = []
-    for task in tasks:
-        spawns.append(gevent.spawn(detect_proxy, myip, task, queue2))
-    gevent.joinall(spawns)
-
 def process_start(tasks, myip, queue2, cntl):
     spawns = []
     for task in tasks:"
34;qiyeboy;IPProxyPool;d4327f32ecbe2b2795bdf5af52aa5392e909711f;limit speed and save memory;"@@ -204,4 +204,4 @@ def get_header():
 
 MAX_CHECK_CURRENT = 30 # CHECK_PROXYæ—¶çš„æœ€å¤§å¹¶å‘
 TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
-MAX_DOWNLOAD_CURRENT = 1#ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file
+MAX_DOWNLOAD_CURRENT = 3#ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file"
34;qiyeboy;IPProxyPool;d28eb188ba2c502cabb3f2cb72bf30f1566bc3d3;limit speed and save memory;"@@ -204,4 +204,4 @@ def get_header():
 
 MAX_CHECK_CURRENT = 30 # CHECK_PROXYæ—¶çš„æœ€å¤§å¹¶å‘
 TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
-MAX_DOWNLOAD_CURRENT = 3#ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file
+MAX_DOWNLOAD_CURRENT = 1#ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file"
34;qiyeboy;IPProxyPool;7004c142025e4d69a97cc17ae5ac40e791079d18;limit speed and save memory;"@@ -203,4 +203,5 @@ def get_header():
 
 
 MAX_CHECK_CURRENT = 30 # CHECK_PROXYæ—¶çš„æœ€å¤§å¹¶å‘
-TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
\ No newline at end of file
+TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
+MAX_DOWNLOAD_CURRENT = 3#ä»Žå…è´¹ä»£ç†ç½‘ç«™ä¸‹è½½æ—¶çš„æœ€å¤§å¹¶å‘ 
\ No newline at end of file"
34;qiyeboy;IPProxyPool;7004c142025e4d69a97cc17ae5ac40e791079d18;limit speed and save memory;"@@ -10,7 +10,7 @@
 from multiprocessing import Queue, Process, Value
 
 from api.apiServer import start_api_server
-from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CURRENT
+from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CURRENT, MAX_DOWNLOAD_CURRENT
 from db.DataStore import store_data, sqlhelper
 from spider.HtmlDownloader import Html_Downloader
 from spider.HtmlPraser import Html_Parser
@@ -58,9 +58,13 @@ def run(self):
                 str += '\r\nIPProxyPool----->>>>>>>>now ip num < MINNUM,start crawling...'
                 sys.stdout.write(str + ""\r\n"")
                 sys.stdout.flush()
-                self.crawl_pool.map(self.crawl, parserList)
-                # for p in parserList:
-                #     self.crawl(p)
+                spawns = []
+                for p in parserList:
+                    spawns.append(gevent.spawn(self.crawl, p))
+                    if len(spawns) >= MAX_DOWNLOAD_CURRENT:
+                        gevent.joinall(spawns)
+                        spawns= []
+                gevent.joinall(spawns)
             else:
                 str += '\r\nIPProxyPool----->>>>>>>>now ip num meet the requirement,wait UPDATE_TIME...'
                 sys.stdout.write(str + ""\r\n"")"
34;qiyeboy;IPProxyPool;179def3b646ee52940d19f59b62c3db0aaf6fb6d;limit speed and save memory;"@@ -7,20 +7,22 @@
 from validator.Validator import validator, getMyIP
 from spider.ProxyCrawl import startProxyCrawl
 
+from config import TASK_QUEUE_SIZE
+
 if __name__ == ""__main__"":
     myip = getMyIP()
     DB_PROXY_NUM = Value('i', 0)
-    q1 = Queue(maxsize=50)
-    q2 = Queue(maxsize=50)
-    # p0 = Process(target=start_api_server)
+    q1 = Queue(maxsize=TASK_QUEUE_SIZE)
+    q2 = Queue()
+    p0 = Process(target=start_api_server)
     p1 = Process(target=startProxyCrawl, args=(q1, DB_PROXY_NUM,myip))
     p2 = Process(target=validator, args=(q1, q2, myip))
     p3 = Process(target=store_data, args=(q2, DB_PROXY_NUM))
-    # p0.start()
+    p0.start()
     p1.start()
     p2.start()
     p3.start()
-    # p0.join()
+    p0.join()
     p1.join()
     p2.join()
     p3.join()"
34;qiyeboy;IPProxyPool;179def3b646ee52940d19f59b62c3db0aaf6fb6d;limit speed and save memory;"@@ -200,3 +200,7 @@ def get_header():
 
 #ä¸‹é¢é…ç½®squid,çŽ°åœ¨è¿˜æ²¡å®žçŽ°
 #SQUID={'path':None,'confpath':'C:/squid/etc/squid.conf'}
+
+
+MAX_CHECK_CURRENT = 30 # CHECK_PROXYæ—¶çš„æœ€å¤§å¹¶å‘
+TASK_QUEUE_SIZE = 50 # ä»»åŠ¡é˜Ÿåˆ—SIZE
\ No newline at end of file"
34;qiyeboy;IPProxyPool;179def3b646ee52940d19f59b62c3db0aaf6fb6d;limit speed and save memory;"@@ -27,8 +27,7 @@ def store_data(queue2, db_proxy_num):
     failNum = 0
     while True:
         try:
-            # proxy = queue2.get(timeout=300)
-            proxy = queue2.get()
+            proxy = queue2.get(timeout=300)
             if proxy:
 
                 sqlhelper.insert(proxy)"
34;qiyeboy;IPProxyPool;179def3b646ee52940d19f59b62c3db0aaf6fb6d;limit speed and save memory;"@@ -10,7 +10,7 @@
 from multiprocessing import Queue, Process, Value
 
 from api.apiServer import start_api_server
-from config import THREADNUM, parserList, UPDATE_TIME, MINNUM
+from config import THREADNUM, parserList, UPDATE_TIME, MINNUM, MAX_CHECK_CURRENT
 from db.DataStore import store_data, sqlhelper
 from spider.HtmlDownloader import Html_Downloader
 from spider.HtmlPraser import Html_Parser
@@ -47,7 +47,7 @@ def run(self):
             spawns = []
             for proxy in proxylist:
                 spawns.append(gevent.spawn(detect_from_db, self.myip, proxy, self.proxies))
-                if len(spawns) >= 30:
+                if len(spawns) >= MAX_CHECK_CURRENT:
                     gevent.joinall(spawns)
                     spawns= []
             gevent.joinall(spawns)
@@ -58,9 +58,9 @@ def run(self):
                 str += '\r\nIPProxyPool----->>>>>>>>now ip num < MINNUM,start crawling...'
                 sys.stdout.write(str + ""\r\n"")
                 sys.stdout.flush()
-                # self.crawl_pool.map(self.crawl, parserList)
-                for p in parserList:
-                    self.crawl(p)
+                self.crawl_pool.map(self.crawl, parserList)
+                # for p in parserList:
+                #     self.crawl(p)
             else:
                 str += '\r\nIPProxyPool----->>>>>>>>now ip num meet the requirement,wait UPDATE_TIME...'
                 sys.stdout.write(str + ""\r\n"")"
34;qiyeboy;IPProxyPool;179def3b646ee52940d19f59b62c3db0aaf6fb6d;limit speed and save memory;"@@ -43,10 +43,9 @@ def validator(queue1, queue2, myip):
     while True:
         try:
             # proxy_dict = {'source':'crawl','data':proxy}
-            # proxy = queue1.get(timeout=10)
             proxy = queue1.get()
             tasklist.append(proxy)
-            if len(tasklist) >= 30:
+            if len(tasklist) >= config.MAX_CHECK_CURRENT:
                 normal_start(tasklist, myip, queue2)
                 # p = Process(target=process_start, args=(tasklist, myip, queue2, cntl_q))
                 # p.start()"
34;qiyeboy;IPProxyPool;99b5a246a3b62b9b1396b3682e5666a1d5532da8;limit speed and save memory;"@@ -47,6 +47,9 @@ def run(self):
             spawns = []
             for proxy in proxylist:
                 spawns.append(gevent.spawn(detect_from_db, self.myip, proxy, self.proxies))
+                if len(spawns) >= 30:
+                    gevent.joinall(spawns)
+                    spawns= []
             gevent.joinall(spawns)
             self.db_proxy_num.value = len(self.proxies)
             str = 'IPProxyPool----->>>>>>>>db exists ip:%d' % len(self.proxies)
@@ -55,7 +58,9 @@ def run(self):
                 str += '\r\nIPProxyPool----->>>>>>>>now ip num < MINNUM,start crawling...'
                 sys.stdout.write(str + ""\r\n"")
                 sys.stdout.flush()
-                self.crawl_pool.map(self.crawl, parserList)
+                # self.crawl_pool.map(self.crawl, parserList)
+                for p in parserList:
+                    self.crawl(p)
             else:
                 str += '\r\nIPProxyPool----->>>>>>>>now ip num meet the requirement,wait UPDATE_TIME...'
                 sys.stdout.write(str + ""\r\n"")"
34;qiyeboy;IPProxyPool;07081b71c2d036c640ec238ab29f51e1ae93fcfd;limit speed and save memory;"@@ -10,17 +10,17 @@
 if __name__ == ""__main__"":
     myip = getMyIP()
     DB_PROXY_NUM = Value('i', 0)
-    q1 = Queue()
-    q2 = Queue()
-    p0 = Process(target=start_api_server)
+    q1 = Queue(maxsize=50)
+    q2 = Queue(maxsize=50)
+    # p0 = Process(target=start_api_server)
     p1 = Process(target=startProxyCrawl, args=(q1, DB_PROXY_NUM,myip))
     p2 = Process(target=validator, args=(q1, q2, myip))
     p3 = Process(target=store_data, args=(q2, DB_PROXY_NUM))
-    p0.start()
+    # p0.start()
     p1.start()
     p2.start()
     p3.start()
-    p0.join()
+    # p0.join()
     p1.join()
     p2.join()
     p3.join()"
34;qiyeboy;IPProxyPool;07081b71c2d036c640ec238ab29f51e1ae93fcfd;limit speed and save memory;"@@ -27,7 +27,8 @@ def store_data(queue2, db_proxy_num):
     failNum = 0
     while True:
         try:
-            proxy = queue2.get(timeout=300)
+            # proxy = queue2.get(timeout=300)
+            proxy = queue2.get()
             if proxy:
 
                 sqlhelper.insert(proxy)"
34;qiyeboy;IPProxyPool;07081b71c2d036c640ec238ab29f51e1ae93fcfd;limit speed and save memory;"@@ -43,12 +43,14 @@ def validator(queue1, queue2, myip):
     while True:
         try:
             # proxy_dict = {'source':'crawl','data':proxy}
-            proxy = queue1.get(timeout=10)
+            # proxy = queue1.get(timeout=10)
+            proxy = queue1.get()
             tasklist.append(proxy)
-            if len(tasklist) > 500:
-                p = Process(target=process_start, args=(tasklist, myip, queue2, cntl_q))
-                p.start()
-                proc_pool[p.pid] = p
+            if len(tasklist) >= 30:
+                normal_start(tasklist, myip, queue2)
+                # p = Process(target=process_start, args=(tasklist, myip, queue2, cntl_q))
+                # p.start()
+                # proc_pool[p.pid] = p
                 tasklist = []
 
         except Exception as e:
@@ -72,6 +74,12 @@ def validator(queue1, queue2, myip):
                 # print("" we are unable to kill pid:%s"" % (pid))
 
 
+def normal_start(tasks, myip, queue2):
+    spawns = []
+    for task in tasks:
+        spawns.append(gevent.spawn(detect_proxy, myip, task, queue2))
+    gevent.joinall(spawns)
+
 def process_start(tasks, myip, queue2, cntl):
     spawns = []
     for task in tasks:"
34;aaugustin;websockets;8850eb06c7955462dd641acf50d1dcfcfc95b0ee;"Fix logging error when sending a memoryview.

See also: https://bugs.python.org/issue15945";"@@ -139,7 +139,7 @@ def __str__(self) -> str:
             # Encode just what we need, plus two dummy bytes to elide later.
             binary = self.data
             if len(binary) > 25:
-                binary = binary[:16] + b""\x00\x00"" + binary[-8:]
+                binary = b"""".join([binary[:16], b""\x00\x00"", binary[-8:]])
             data = "" "".join(f""{byte:02x}"" for byte in binary)
         elif self.opcode is OP_CLOSE:
             data = str(Close.parse(self.data))
@@ -153,7 +153,7 @@ def __str__(self) -> str:
             except UnicodeDecodeError:
                 binary = self.data
                 if len(binary) > 25:
-                    binary = binary[:16] + b""\x00\x00"" + binary[-8:]
+                    binary = b"""".join([binary[:16], b""\x00\x00"", binary[-8:]])
                 data = "" "".join(f""{byte:02x}"" for byte in binary)
                 coding = ""binary""
         else:"
35;tensorflow;graphics;d0817aec7dee35635814e925a59d83955459d93c;"Allows for using map_fn instead of vectorized_map.

map_fn uses significantly less memory.

PiperOrigin-RevId: 403409412";"@@ -23,8 +23,10 @@
 
 
 def differentiable_barycentrics(
-    framebuffer: fb.Framebuffer, clip_space_vertices: type_alias.TensorLike,
-    triangles: type_alias.TensorLike) -> fb.Framebuffer:
+    framebuffer: fb.Framebuffer,
+    clip_space_vertices: type_alias.TensorLike,
+    triangles: type_alias.TensorLike,
+    use_vectorized_map: bool = True) -> fb.Framebuffer:
   """"""Computes differentiable barycentric coordinates from a Framebuffer.
 
   The barycentric coordinates will be differentiable w.r.t. the input vertices.
@@ -39,6 +41,7 @@ def differentiable_barycentrics(
     triangles: a 2-D int32 tensor with shape [triangle_count, 3] or a 3-D tensor
       with shape [batch, triangle_count, 3] containing per-triangle vertex
       indices in counter-clockwise order.
+    use_vectorized_map: If true uses vectorized_map otherwise uses map_fn.
 
   Returns:
     a copy of `framebuffer`, but the differentiable barycentric coordinates will
@@ -95,9 +98,17 @@ def compute_barycentrics_fn(
     barycentric_coords = tf.transpose(barycentric_coords, perm=[1, 2, 3, 0])
     return barycentric_coords
 
-  per_image_barycentrics = tf.vectorized_map(
-      compute_barycentrics_fn,
-      (clip_space_vertices, triangles, framebuffer.triangle_id))
+  if use_vectorized_map:
+    per_image_barycentrics = tf.vectorized_map(
+        compute_barycentrics_fn,
+        (clip_space_vertices, triangles, framebuffer.triangle_id))
+  else:
+    num_meshes = tf.shape(clip_space_vertices)[0]
+    triangles_repeated = tf.repeat(triangles, repeats=num_meshes, axis=0)
+    per_image_barycentrics = tf.map_fn(
+        compute_barycentrics_fn,
+        (clip_space_vertices, triangles_repeated, framebuffer.triangle_id),
+        fn_output_signature=tf.TensorSpec(shape=(1, None, None, 3)))
 
   barycentric_coords = tf.stack(per_image_barycentrics, axis=0)
   # After stacking barycentrics will have layers dimension no matter what."
35;tensorflow;graphics;d0817aec7dee35635814e925a59d83955459d93c;"Allows for using map_fn instead of vectorized_map.

map_fn uses significantly less memory.

PiperOrigin-RevId: 403409412";"@@ -41,6 +41,7 @@ def rasterize(
     view_projection_matrix: type_alias.TensorLike,
     image_size: Tuple[int, int],
     enable_cull_face: bool = True,
+    use_vectorized_map: bool = True,
     backend: enum.Enum = rasterization_backend.RasterizationBackends.OPENGL,
     name: str = ""triangle_rasterizer_rasterize""
 ) -> Dict[str, type_alias.TensorLike]:
@@ -64,6 +65,8 @@ def rasterize(
       the rasterized image.
     enable_cull_face: Enables BACK face culling when True, and no culling when
       False.
+    use_vectorized_map: If true uses vectorized_map for barycentrics
+      computations otherwise uses map_fn.
     backend: A rasterization_backend.RasterizationBackends enum containing the
       backend method to use for rasterization.
     name: A name for this op. Defaults to ""triangle_rasterizer_rasterize"".
@@ -124,7 +127,7 @@ def rasterize(
     clip_space_vertices = utils.transform_homogeneous(view_projection_matrix,
                                                       vertices)
     rasterized = barycentrics_module.differentiable_barycentrics(
-        rasterized, clip_space_vertices, triangles)
+        rasterized, clip_space_vertices, triangles, use_vectorized_map)
     barycentrics = rasterized.barycentrics.value
     outputs[""barycentrics""] = utils.restore_batch_dims(
         rasterized.foreground_mask * barycentrics, input_batch_shape)"
35;NVlabs;stylegan2-ada-pytorch;d3a616a9c86f9a72087caca088dec7d045f44a4b;"Specify --shm-size=2g and fix typo in code comments

Fix OOM crash in data loader workers caused by docker's small default
shared memory size.";"@@ -17,11 +17,11 @@ set -e
 #
 # Use it like:
 #
-# ./run_docker.sh python generate.py --help
+# ./docker_run.sh python generate.py --help
 #
 # To override the default `stylegan2ada:latest` image, run:
 #
-# IMAGE=my_image:v1.0 ./run_docker.sh python generate.py --help
+# IMAGE=my_image:v1.0 ./docker_run.sh python generate.py --help
 #
 
 rest=$@
@@ -30,7 +30,7 @@ IMAGE=""${IMAGE:-sg2ada:latest}""
 
 CONTAINER_ID=$(docker inspect --format=""{{.Id}}"" ${IMAGE} 2> /dev/null)
 if [[ ""${CONTAINER_ID}"" ]]; then
-    docker run --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \
+    docker run --shm-size=2g --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \
         --workdir=/scratch -e HOME=/scratch $IMAGE $@
 else
     echo ""Unknown container image: ${IMAGE}"""
35;grantjenks;python-sortedcontainers;2b037039bb16c81ddc1ee94e88c44aa85eed2810;"Fix for issue #147 -- Stop caching dict methods using super

Caching dict methods using super creates a reference cycle which increases
memory pressure. User reported increased latencies due to GC pauses.";"@@ -151,17 +151,6 @@ def __init__(self, *args, **kwargs):
 
         self._list = SortedList(key=_key)
 
-        # Calls to super() are expensive so cache references to dict methods on
-        # sorted dict instances.
-
-        _dict = super(SortedDict, self)
-        self._dict_clear = _dict.clear
-        self._dict_delitem = _dict.__delitem__
-        self._dict_iter = _dict.__iter__
-        self._dict_pop = _dict.pop
-        self._dict_setitem = _dict.__setitem__
-        self._dict_update = _dict.update
-
         # Reaching through ``self._list`` repeatedly adds unnecessary overhead
         # so cache references to sorted list methods.
 
@@ -232,7 +221,7 @@ def clear(self):
         Runtime complexity: `O(n)`
 
         """"""
-        self._dict_clear()
+        dict.clear(self)
         self._list_clear()
 
 
@@ -256,7 +245,7 @@ def __delitem__(self, key):
         :raises KeyError: if key not found
 
         """"""
-        self._dict_delitem(key)
+        dict.__delitem__(self, key)
         self._list_remove(key)
 
 
@@ -304,7 +293,7 @@ def __setitem__(self, key, value):
         """"""
         if key not in self:
             self._list_add(key)
-        self._dict_setitem(key, value)
+        dict.__setitem__(self, key, value)
 
     _setitem = __setitem__
 
@@ -425,7 +414,7 @@ def pop(self, key, default=__not_given):
         """"""
         if key in self:
             self._list_remove(key)
-            return self._dict_pop(key)
+            return dict.pop(self, key)
         else:
             if default is self.__not_given:
                 raise KeyError(key)
@@ -464,7 +453,7 @@ def popitem(self, index=-1):
             raise KeyError('popitem(): dictionary is empty')
 
         key = self._list_pop(index)
-        value = self._dict_pop(key)
+        value = dict.pop(self, key)
         return (key, value)
 
 
@@ -525,7 +514,7 @@ def setdefault(self, key, default=None):
         """"""
         if key in self:
             return self[key]
-        self._dict_setitem(key, default)
+        dict.__setitem__(self, key, default)
         self._list_add(key)
         return default
 
@@ -544,8 +533,8 @@ def update(self, *args, **kwargs):
 
         """"""
         if not self:
-            self._dict_update(*args, **kwargs)
-            self._list_update(self._dict_iter())
+            dict.update(self, *args, **kwargs)
+            self._list_update(dict.__iter__(self))
             return
 
         if not kwargs and len(args) == 1 and isinstance(args[0], dict):
@@ -554,9 +543,9 @@ def update(self, *args, **kwargs):
             pairs = dict(*args, **kwargs)
 
         if (10 * len(pairs)) > len(self):
-            self._dict_update(pairs)
+            dict.update(self, pairs)
             self._list_clear()
-            self._list_update(self._dict_iter())
+            self._list_update(dict.__iter__(self))
         else:
             for key in pairs:
                 self._setitem(key, pairs[key])
@@ -631,15 +620,15 @@ def _view_delitem(self, index):
     """"""
     _mapping = self._mapping
     _list = _mapping._list
-    _dict_delitem = _mapping._dict_delitem
+    dict_delitem = dict.__delitem__
     if isinstance(index, slice):
         keys = _list[index]
         del _list[index]
         for key in keys:
-            _dict_delitem(key)
+            dict_delitem(_mapping, key)
     else:
         key = _list.pop(index)
-        _dict_delitem(key)
+        dict_delitem(_mapping, key)
 
 
 class SortedKeysView(KeysView, Sequence):"
35;grantjenks;python-sortedcontainers;2b037039bb16c81ddc1ee94e88c44aa85eed2810;"Fix for issue #147 -- Stop caching dict methods using super

Caching dict methods using super creates a reference cycle which increases
memory pressure. User reported increased latencies due to GC pauses.";"@@ -0,0 +1,123 @@
+from sortedcontainers import SortedDict, SortedList
+import gc
+
+def check(f):
+    print('start')
+
+    a = f()
+    t = type(a)
+
+    print('post-setup')
+    for obj in gc.get_objects():
+        if type(obj) == t:
+            print(obj)
+
+    del a
+
+    print('post-delete')
+    for obj in gc.get_objects():
+        if type(obj) == t:
+            print(obj)
+
+    gc.collect()
+
+    print('post-collect')
+    for obj in gc.get_objects():
+        if type(obj) == t:
+            print(obj)
+
+    print('finish')
+
+check(lambda: SortedDict({'a': 1, 'b': 2}))
+
+class MyDict(dict):
+    pass
+
+check(lambda: MyDict({'a': 1, 'b': 2}))
+
+check(lambda: SortedList([1, 2, 3]))
+
+from functools import partial
+
+class SortedDictSub(SortedDict):
+
+    def __init__(self, *args, **kwargs):
+        """"""Initialize sorted dict instance.
+        Optional key-function argument defines a callable that, like the `key`
+        argument to the built-in `sorted` function, extracts a comparison key
+        from each dictionary key. If no function is specified, the default
+        compares the dictionary keys directly. The key-function argument must
+        be provided as a positional argument and must come before all other
+        arguments.
+        Optional iterable argument provides an initial sequence of pairs to
+        initialize the sorted dict. Each pair in the sequence defines the key
+        and corresponding value. If a key is seen more than once, the last
+        value associated with it is stored in the new sorted dict.
+        Optional mapping argument provides an initial mapping of items to
+        initialize the sorted dict.
+        If keyword arguments are given, the keywords themselves, with their
+        associated values, are added as items to the dictionary. If a key is
+        specified both in the positional argument and as a keyword argument,
+        the value associated with the keyword is stored in the
+        sorted dict.
+        Sorted dict keys must be hashable, per the requirement for Python's
+        dictionaries. Keys (or the result of the key-function) must also be
+        comparable, per the requirement for sorted lists.
+        >>> d = {'alpha': 1, 'beta': 2}
+        >>> SortedDict([('alpha', 1), ('beta', 2)]) == d
+        True
+        >>> SortedDict({'alpha': 1, 'beta': 2}) == d
+        True
+        >>> SortedDict(alpha=1, beta=2) == d
+        True
+        """"""
+        if args and (args[0] is None or callable(args[0])):
+            _key = self._key = args[0]
+            args = args[1:]
+        else:
+            _key = self._key = None
+
+        self._list = SortedList(key=_key)
+
+        # Calls to super() are expensive so cache references to dict methods on
+        # sorted dict instances.
+
+        _dict = super(SortedDict, self)
+        # self._dict_clear = _dict.clear
+        # self._dict_delitem = _dict.__delitem__
+        self._dict_iter = partial(dict.__iter__, self)  # _dict.__iter__
+        # self._dict_pop = _dict.pop
+        # self._dict_setitem = _dict.__setitem__
+        self._dict_update = partial(dict.update, self)  # _dict.update
+
+        # Reaching through ``self._list`` repeatedly adds unnecessary overhead
+        # so cache references to sorted list methods.
+
+        _list = self._list
+        self._list_add = _list.add
+        self._list_clear = _list.clear
+        self._list_iter = _list.__iter__
+        self._list_reversed = _list.__reversed__
+        self._list_pop = _list.pop
+        self._list_remove = _list.remove
+        self._list_update = _list.update
+
+        # Expose some sorted list methods publicly.
+
+        self.bisect_left = _list.bisect_left
+        self.bisect = _list.bisect_right
+        self.bisect_right = _list.bisect_right
+        self.index = _list.index
+        self.irange = _list.irange
+        self.islice = _list.islice
+        self._reset = _list._reset
+
+        if _key is not None:
+            self.bisect_key_left = _list.bisect_key_left
+            self.bisect_key_right = _list.bisect_key_right
+            self.bisect_key = _list.bisect_key
+            self.irange_key = _list.irange_key
+
+        self._update(*args, **kwargs)
+
+check(lambda: SortedDictSub({'a': 1, 'b': 2}))"
35;grantjenks;python-sortedcontainers;2b037039bb16c81ddc1ee94e88c44aa85eed2810;"Fix for issue #147 -- Stop caching dict methods using super

Caching dict methods using super creates a reference cycle which increases
memory pressure. User reported increased latencies due to GC pauses.";"@@ -5,6 +5,7 @@
 from sortedcontainers import SortedDict
 import pytest
 from sys import hexversion
+import gc
 
 if hexversion < 0x03000000:
     range = xrange
@@ -474,3 +475,12 @@ def test_pickle():
     beta = pickle.loads(pickle.dumps(alpha))
     assert alpha == beta
     assert alpha._key == beta._key
+
+def test_ref_counts():
+    start_count = len(gc.get_objects())
+    temp = SortedDict()
+    init_count = len(gc.get_objects())
+    assert init_count > start_count
+    del temp
+    del_count = len(gc.get_objects())
+    assert start_count == del_count"
35;sightmachine;SimpleCV;fcdb3ab8a09a21fe7b29643a13ab5567846bf909;remove a test_vimba_memory.py, move to test_vimba_manyshots.py;"@@ -10,6 +10,33 @@
 def printPrettyHeader(msg):
     print ""*""*80 + ""\n* %s *\n"" % msg + ""*""*80
 
+def _takeShots(cam, numPics, filename):
+    start = time.time()
+    #print ""Taking %d photos..."" % numPics
+    for i in range(numPics):
+        img = cam.getImage()
+        img.save(""%s_%d.png"" % (filename, i))
+    end = time.time()
+    elapsed = end - start
+    print ""Took %f seconds"" % elapsed
+
+def _takeManyVimbaShots(idx):
+    c = VimbaCamera()
+    print ""_takeManyVimbaShots %d"" % idx
+
+    _takeShots(c, 10, ""cam_vimba%d"" % idx)
+
+def _takeAVTManyShots(idx):
+    c = AVTCamera()
+    print ""_takeAVTManyShots %d"" % idx
+
+    _takeShots(c, 10, ""cam_avtnative%d"" % idx)
+
+#_takeAVTManyShots(1)
+#_takeAVTManyShots(2)
+#_takeManyVimbaShots(1)
+#_takeManyVimbaShots(2)
+
 """"""
 def test_getImageDisplay():
     c = VimbaCamera()
@@ -22,19 +49,13 @@ def test_getImageDisplay():
     print ""test_getImage_scv_display2.png saved""
 """"""
 
-def _takeShots(cam, numPics, filename):
-    start = time.time()
-    print ""Taking %d photos..."" % numPics
-    for i in range(numPics):
-        img = cam.getImage()
-        if (i % 100 == 0):
-            print ""At %d"" % i
-        #img.save(""%s_%d.png"" % (filename, i))
-    end = time.time()
-    elapsed = end - start
-    print ""Took %f seconds"" % elapsed
 
-""""""
+def test_createManyCameras():
+    printPrettyHeader(""Test createManyCameras"")
+    numIter = 10 #1000
+    for i in range(numIter):
+        _takeManyVimbaShots(i)
+
 def test_oneGrayShot():
     c = VimbaCamera(properties={""mode"":""gray""})
     printPrettyHeader(""Test oneGrayShot"")
@@ -48,14 +69,19 @@ def test_oneShot():
 
     img = c.getImage()
     img.save(""test_oneShot.png"")
-""""""
 
-""""""
 def test_takeManyShots():
     c = VimbaCamera()
     printPrettyHeader(""Test takeManyShots"")
 
-    _takeShots(c, 100, ""vimba"")
+    _takeShots(c, 5, ""vimba"")
+
+def test_AVT_takeManyShots():
+    c = AVTCamera()
+    printPrettyHeader(""Test AVT_takeManyShots"")
+
+    _takeShots(c, 5, ""avtnative"")
+
 """"""
 
 def test_makeLotsOfCamera():
@@ -71,9 +97,3 @@ def test_makeLotsOfCamera():
     print ""Took %f seconds"" % elapsed
 
 """"""
-def test_AVT_takeManyShots():
-    c = AVTCamera()
-    printPrettyHeader(""Test AVT_takeManyShots"")
-
-    _takeShots(c, 10, ""avtnative"")
-""""""
\ No newline at end of file"
35;sightmachine;SimpleCV;fcdb3ab8a09a21fe7b29643a13ab5567846bf909;remove a test_vimba_memory.py, move to test_vimba_manyshots.py;"@@ -1,63 +0,0 @@
-import gc
-# From bytes.com/topic/python/answers/22097-hunting-memory-leak
-
-import time
-from SimpleCV.Camera import VimbaCamera, AVTCamera
-
-""""""
-# at the beginning of code
-gc.enable()
-gc.set_debug(gc.DEBUG_LEAK)
-""""""
-
-def printPrettyHeader(msg):
-    print ""*""*80 + ""\n* %s *\n"" % msg + ""*""*80
-
-def _takeShots(cam, numPics, filename):
-    start = time.time()
-    #print ""Taking %d photos..."" % numPics
-    for i in range(numPics):
-        img = cam.getImage()
-        #if (i % 1000 == 0):
-        #    img.save(""%s_%d.png"" % (filename, i))
-    end = time.time()
-    elapsed = end - start
-    print ""Took %f seconds"" % elapsed
-
-def test_takeManyShots(idx):
-    c = VimbaCamera()
-    #printPrettyHeader(""Test takeManyShots %d"" % idx)
-    print ""Test takeManyShots %d"" % idx
-
-    _takeShots(c, 1, ""vimba%d"" % idx)
-
-def test_takeAVTManyShots(idx):
-    c = AVTCamera()
-    printPrettyHeader(""Test takeAVTShots %d"" % idx)
-
-    _takeShots(c, 1, ""avtnative%d"" % idx)
-
-#test_takeAVTManyShots(1)
-#test_takeAVTManyShots(2)
-#test_takeManyShots(1)
-#test_takeManyShots(2)
-
-def test_createManyCameras():
-    printPrettyHeader(""Test createManyCameras"")
-    numIter = 1000
-    for i in range(numIter):
-        test_takeManyShots(i)
-
-test_createManyCameras()
-
-""""""
-########################################################################3
-# at the end of code:
-print ""\nGARBAGE:""
-gc.collect()
-
-print ""\nGARBAGE OBJECTS:""
-for x in gc.garbage:
-    s = str(x)
-    print type(x),""\n"",s
-""""""
\ No newline at end of file"
36;wookayin;gpustat;ebd7f463bd2185db1cc86704e7ed0535b3d73d4e;"Add windows support with blessed (#78)

- Use blessed (replacing blessings) for windows support
- Handle corner cases of signal handling, query_time, and gpu_memory_usage";"@@ -5,7 +5,7 @@
 import sys
 import time
 
-from blessings import Terminal
+from blessed import Terminal
 
 from gpustat import __version__
 from .core import GPUStatCollection
@@ -60,9 +60,11 @@ def main(*argv):
         argv = list(sys.argv)
 
     # attach SIGPIPE handler to properly handle broken pipe
-    import signal
-    signal.signal(signal.SIGPIPE, signal.SIG_DFL)
-
+    try: # sigpipe not available under windows. just ignore in this case
+        import signal
+        signal.signal(signal.SIGPIPE, signal.SIG_DFL)
+    except Exception as e:
+        pass
     # arguments to gpustat
     import argparse
     parser = argparse.ArgumentParser()"
36;wookayin;gpustat;ebd7f463bd2185db1cc86704e7ed0535b3d73d4e;"Add windows support with blessed (#78)

- Use blessed (replacing blessings) for windows support
- Handle corner cases of signal handling, query_time, and gpu_memory_usage";"@@ -24,7 +24,7 @@
 from six.moves import cStringIO as StringIO
 import psutil
 import pynvml as N
-from blessings import Terminal
+from blessed import Terminal
 
 import gpustat.util as util
 
@@ -345,7 +345,10 @@ def get_process_info(nv_process):
                     process['command'] = os.path.basename(_cmdline[0])
                     process['full_command'] = _cmdline
                 # Bytes to MBytes
-                process['gpu_memory_usage'] = nv_process.usedGpuMemory // MB
+                # if drivers are not TTC this will be None. 
+                usedmem = nv_process.usedGpuMemory // MB if \
+                          nv_process.usedGpuMemory else None
+                process['gpu_memory_usage'] = usedmem
                 process['cpu_percent'] = ps_process.cpu_percent()
                 process['cpu_memory_usage'] = \
                     round((ps_process.memory_percent() / 100.0) *
@@ -473,6 +476,9 @@ def __repr__(self):
         s += '\n'.join('  ' + str(g) for g in self.gpus)
         s += '\n])'
         return s
+    
+    def is_windows(self):
+        return 'windows' in platform.platform().lower()
 
     # --- Printing Functions ---
 
@@ -491,7 +497,8 @@ def print_formatted(self, fp=sys.stdout, force_color=False, no_color=False,
             t_color = Terminal(kind='linux', force_styling=True)
 
             # workaround of issue #32 (watch doesn't recognize sgr0 characters)
-            t_color.normal = u'\x1b[0;10m'
+            try: t_color.normal = u'\x1b[0;10m'
+            except: pass
         elif no_color:
             t_color = Terminal(force_styling=None)
         else:
@@ -503,16 +510,23 @@ def print_formatted(self, fp=sys.stdout, force_color=False, no_color=False,
 
         # header
         if show_header:
-            time_format = locale.nl_langinfo(locale.D_T_FMT)
-
+            # no localization is available that easily
+            # however,everybody should be able understand the
+            # standard datetime string format %Y-%m-%d %H:%M:%S
+            if self.is_windows():
+                # same as str(timestr) but without ms
+                timestr = self.query_time.strftime('%Y-%m-%d %H:%M:%S')
+            else:
+                time_format = locale.nl_langinfo(locale.D_T_FMT)
+                timestr = self.query_time.strftime(time_format)
             header_template = '{t.bold_white}{hostname:{width}}{t.normal}  '
             header_template += '{timestr}  '
             header_template += '{t.bold_black}{driver_version}{t.normal}'
 
             header_msg = header_template.format(
                     hostname=self.hostname,
                     width=gpuname_width + 3,  # len(""[?]"")
-                    timestr=self.query_time.strftime(time_format),
+                    timestr=timestr,
                     driver_version=self.driver_version,
                     t=t_color,
                 )"
36;wookayin;gpustat;ebd7f463bd2185db1cc86704e7ed0535b3d73d4e;"Add windows support with blessed (#78)

- Use blessed (replacing blessings) for windows support
- Handle corner cases of signal handling, query_time, and gpu_memory_usage";"@@ -1,4 +1,4 @@
 six>=1.7
 nvidia-ml-py3
 psutil
-blessings>=1.6
+blessed>=1.16.1"
36;wookayin;gpustat;ebd7f463bd2185db1cc86704e7ed0535b3d73d4e;"Add windows support with blessed (#78)

- Use blessed (replacing blessings) for windows support
- Handle corner cases of signal handling, query_time, and gpu_memory_usage";"@@ -79,7 +79,7 @@ def run(self):
     'six>=1.7',
     'nvidia-ml-py3>=7.352.0',
     'psutil',
-    'blessings>=1.6',
+    'blessed>=1.16.1',
 ]
 
 tests_requires = ["
36;Netflix;vmaf;8b0782c6029d9acd4f0815421ea51b5a717f2b01;"libvmaf/motion: use single-channel buffers

gives a ~10% memory use reduction for vmaf

before:
87638016  maximum resident set size

after:
79347712  maximum resident set size";"@@ -287,10 +287,10 @@ static int init(VmafFeatureExtractor *fex, enum VmafPixelFormat pix_fmt,
         return 0;
     }
 
-    err |= vmaf_picture_alloc(&s->tmp, pix_fmt, 16, w, h);
-    err |= vmaf_picture_alloc(&s->blur[0], pix_fmt, 16, w, h);
-    err |= vmaf_picture_alloc(&s->blur[1], pix_fmt, 16, w, h);
-    err |= vmaf_picture_alloc(&s->blur[2], pix_fmt, 16, w, h);
+    err |= vmaf_picture_alloc(&s->tmp, VMAF_PIX_FMT_YUV400P, 16, w, h);
+    err |= vmaf_picture_alloc(&s->blur[0], VMAF_PIX_FMT_YUV400P, 16, w, h);
+    err |= vmaf_picture_alloc(&s->blur[1], VMAF_PIX_FMT_YUV400P, 16, w, h);
+    err |= vmaf_picture_alloc(&s->blur[2], VMAF_PIX_FMT_YUV400P, 16, w, h);
     if (err) goto fail;
 
     s->y_convolution = bpc == 8 ? y_convolution_8 : y_convolution_16;"
36;Netflix;vmaf;79ce8a1fe77069bbdfd2c04414cec9e198729d23;"libvmaf: pipelined vif feature computation

memory requirements reduced and performance improvement

Original PR: https://github.com/Netflix/vmaf/pull/944

cleanup-and-testing-by: nilfm <nfonsmiret@netflix.com>
rebased-by: Kyle Swanson <kswanson@netflix.com>";"@@ -40,16 +40,12 @@
 #endif
 
 typedef struct VifState {
-    VifBuffer buf;
-    uint16_t log2_table[65537];
+    VifPublicState public;
     bool debug;
-    double vif_enhn_gain_limit;
-    void (*filter1d_8)(VifBuffer buf, unsigned w, unsigned h);
-    void (*filter1d_16)(VifBuffer buf, unsigned w, unsigned h, int scale,
-                        int bpc);
-    void (*filter1d_rd_8)(VifBuffer buf, unsigned w, unsigned h);
-    void (*filter1d_rd_16)(VifBuffer buf, unsigned w, unsigned h, int scale,
-                           int bpc);
+    void (*subsample_rd_8)(VifBuffer buf, unsigned w, unsigned h);
+    void (*subsample_rd_16)(VifBuffer buf, unsigned w, unsigned h, int scale, int bpc);
+    void (*vif_statistic_8)(VifPublicState *s, float *num, float *den, unsigned w, unsigned h);
+    void (*vif_statistic_16)(VifPublicState *s, float *num, float *den, unsigned w, unsigned h, int bpc, int scale);
     VmafDictionary *feature_name_dict;
 } VifState;
 
@@ -66,7 +62,7 @@ static const VmafOption options[] = {
         .alias = ""egl"",
         .help = ""enhancement gain imposed on vif, must be >= 1.0, ""
                 ""where 1.0 means the gain is completely disabled"",
-        .offset = offsetof(VifState, vif_enhn_gain_limit),
+        .offset = offsetof(VifState, public.vif_enhn_gain_limit),
         .type = VMAF_OPT_TYPE_DOUBLE,
         .default_val.d = DEFAULT_VIF_ENHN_GAIN_LIMIT,
         .min = 1.0,
@@ -80,7 +76,8 @@ static FORCE_INLINE inline void
 pad_top_and_bottom(VifBuffer buf, unsigned h, int fwidth)
 {
     const unsigned fwidth_half = fwidth / 2;
-    void *ref = buf.ref; void *dis = buf.dis;
+    unsigned char *ref = buf.ref;
+    unsigned char *dis = buf.dis;
     for (unsigned i = 1; i <= fwidth_half; ++i) {
         size_t offset = buf.stride * i;
         memcpy(ref - offset, ref + offset, buf.stride);
@@ -111,42 +108,126 @@ decimate_and_pad(VifBuffer buf, unsigned w, unsigned h, int scale)
     pad_top_and_bottom(buf, h / 2, vif_filter1d_width[scale]);
 }
 
-static FORCE_INLINE inline uint16_t
-get_best16_from32(uint32_t temp, int *x)
+static void subsample_rd_8(VifBuffer buf, unsigned w, unsigned h)
 {
-    int k = __builtin_clz(temp);
-    k = 16 - k;
-    temp = temp >> k;
-    *x = -k;
-    return temp;
+    const unsigned fwidth = vif_filter1d_width[1];
+    const uint16_t *vif_filt_s1 = vif_filter1d_table[1];
+
+    for (unsigned i = 0; i < h; ++i) {
+        //VERTICAL
+        for (unsigned j = 0; j < w; ++j) {
+            uint32_t accum_ref = 0;
+            uint32_t accum_dis = 0;
+            for (unsigned fi = 0; fi < fwidth; ++fi) {
+                int ii = i - fwidth / 2;
+                int ii_check = ii + fi;
+                const uint16_t fcoeff = vif_filt_s1[fi];
+                const uint8_t *ref = (uint8_t*)buf.ref;
+                const uint8_t *dis = (uint8_t*)buf.dis;
+                accum_ref += fcoeff * (uint32_t)ref[ii_check * buf.stride + j];
+                accum_dis += fcoeff * (uint32_t)dis[ii_check * buf.stride + j];
+            }
+            buf.tmp.ref_convol[j] = (accum_ref + 128) >> 8;
+            buf.tmp.dis_convol[j] = (accum_dis + 128) >> 8;
+        }
+
+        PADDING_SQ_DATA_2(buf, w, fwidth / 2);
+
+        //HORIZONTAL
+        for (unsigned j = 0; j < w; ++j) {
+            uint32_t accum_ref = 0;
+            uint32_t accum_dis = 0;
+            for (unsigned fj = 0; fj < fwidth; ++fj) {
+                int jj = j - fwidth / 2;
+                int jj_check = jj + fj;
+                const uint16_t fcoeff = vif_filt_s1[fj];
+                accum_ref += fcoeff * buf.tmp.ref_convol[jj_check];
+                accum_dis += fcoeff * buf.tmp.dis_convol[jj_check];
+            }
+            const ptrdiff_t stride = buf.stride_16 / sizeof(uint16_t);
+            buf.mu1[i * stride + j] = (uint16_t)((accum_ref + 32768) >> 16);
+            buf.mu2[i * stride + j] = (uint16_t)((accum_dis + 32768) >> 16);
+        }
+    }
+    decimate_and_pad(buf, w, h, 0);
 }
 
-static FORCE_INLINE inline uint16_t
-get_best16_from64(uint64_t temp, int *x)
+static void subsample_rd_16(VifBuffer buf, unsigned w, unsigned h, int scale, int bpc)
 {
-    int k = __builtin_clzll(temp);
-    if (k > 48) {
-        k -= 48;
-        temp = temp << k;
-        *x = k;
-    } else if (k < 47) {
-        k = 48 - k;
-        temp = temp >> k;
-        *x = -k;
-    } else {
-        *x = 0;
-        if (temp >> 16) {
-            temp = temp >> 1;
-            *x = -1;
+    const unsigned fwidth = vif_filter1d_width[scale + 1];
+    const uint16_t *vif_filt = vif_filter1d_table[scale + 1];
+    int32_t add_shift_round_VP, shift_VP;
+
+    if (scale == 0) {
+        add_shift_round_VP = 1 << (bpc - 1);
+        shift_VP = bpc;
+    }
+    else {
+        add_shift_round_VP = 32768;
+        shift_VP = 16;
+    }
+
+    for (unsigned i = 0; i < h; ++i) {
+        //VERTICAL
+        for (unsigned j = 0; j < w; ++j) {
+            uint32_t accum_ref = 0;
+            uint32_t accum_dis = 0;
+            for (unsigned fi = 0; fi < fwidth; ++fi) {
+                int ii = i - fwidth / 2;
+                int ii_check = ii + fi;
+                const uint16_t fcoeff = vif_filt[fi];
+                const ptrdiff_t stride = buf.stride / sizeof(uint16_t);
+                uint16_t *ref = buf.ref;
+                uint16_t *dis = buf.dis;
+                accum_ref += fcoeff * ((uint32_t)ref[ii_check * stride + j]);
+                accum_dis += fcoeff * ((uint32_t)dis[ii_check * stride + j]);
+            }
+            buf.tmp.ref_convol[j] = (uint16_t)((accum_ref + add_shift_round_VP) >> shift_VP);
+            buf.tmp.dis_convol[j] = (uint16_t)((accum_dis + add_shift_round_VP) >> shift_VP);
+        }
+
+        PADDING_SQ_DATA_2(buf, w, fwidth / 2);
+
+        //HORIZONTAL
+        for (unsigned j = 0; j < w; ++j) {
+            uint32_t accum_ref = 0;
+            uint32_t accum_dis = 0;
+            for (unsigned fj = 0; fj < fwidth; ++fj) {
+                int jj = j - fwidth / 2;
+                int jj_check = jj + fj;
+                const uint16_t fcoeff = vif_filt[fj];
+                accum_ref += fcoeff * ((uint32_t)buf.tmp.ref_convol[jj_check]);
+                accum_dis += fcoeff * ((uint32_t)buf.tmp.dis_convol[jj_check]);
+            }
+            const ptrdiff_t stride = buf.stride_16 / sizeof(uint16_t);
+            buf.mu1[i * stride + j] = (uint16_t)((accum_ref + 32768) >> 16);
+            buf.mu2[i * stride + j] = (uint16_t)((accum_dis + 32768) >> 16);
         }
     }
-    return (uint16_t)temp;
+    decimate_and_pad(buf, w, h, scale);
 }
 
-static void filter1d_8(VifBuffer buf, unsigned w, unsigned h)
+#define MIN(x, y) (((x) < (y)) ? (x) : (y))
+#define MAX(x, y) (((x) > (y)) ? (x) : (y))
+
+static inline void log_generate(uint16_t *log2_table)
 {
+    for (unsigned i = 32767; i < 65536; ++i) {
+        log2_table[i] = (uint16_t)round(log2f((float)i) * 2048);
+    }
+}
+
+void vif_statistic_8(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h) {
     const unsigned fwidth = vif_filter1d_width[0];
     const uint16_t *vif_filt_s0 = vif_filter1d_table[0];
+    VifBuffer buf = s->buf;
+    int64_t accum_num_log = 0.0;
+    int64_t accum_den_log = 0.0;
+    int64_t accum_num_non_log = 0;
+    int64_t accum_den_non_log = 0;
+    static const int32_t sigma_nsq = 65536 << 1;
+    uint16_t *log2_table = s->log2_table;
+    double vif_enhn_gain_limit = s->vif_enhn_gain_limit;
 
     for (unsigned i = 0; i < h; ++i) {
         //VERTICAL
@@ -198,22 +279,78 @@ static void filter1d_8(VifBuffer buf, unsigned w, unsigned h)
                 accum_dis += fcoeff * ((uint64_t)buf.tmp.dis[jj_check]);
                 accum_ref_dis += fcoeff * ((uint64_t)buf.tmp.ref_dis[jj_check]);
             }
-            const ptrdiff_t dst_stride = buf.stride_32 / sizeof(uint32_t);
-            buf.mu1_32[i * dst_stride + j] = accum_mu1;
-            buf.mu2_32[i * dst_stride + j] = accum_mu2;
-            buf.ref_sq[i * dst_stride + j] = (uint32_t)((accum_ref + 32768) >> 16);
-            buf.dis_sq[i * dst_stride + j] = (uint32_t)((accum_dis + 32768) >> 16);
-            buf.ref_dis[i * dst_stride + j] = (uint32_t)((accum_ref_dis + 32768) >> 16);
+
+            uint32_t mu1_val = accum_mu1;
+            uint32_t mu2_val = accum_mu2;
+            uint32_t mu1_sq_val = (uint32_t)((((uint64_t)mu1_val * mu1_val)
+                + 2147483648) >> 32);
+            uint32_t mu2_sq_val = (uint32_t)((((uint64_t)mu2_val * mu2_val)
+                + 2147483648) >> 32);
+            uint32_t mu1_mu2_val = (uint32_t)((((uint64_t)mu1_val * mu2_val)
+                + 2147483648) >> 32);
+
+            uint32_t xx_filt_val = (uint32_t)((accum_ref + 32768) >> 16);
+            uint32_t yy_filt_val = (uint32_t)((accum_dis + 32768) >> 16);
+            uint32_t xy_filt_val = (uint32_t)((accum_ref_dis + 32768) >> 16);
+
+            int32_t sigma1_sq = (int32_t)(xx_filt_val - mu1_sq_val);
+            int32_t sigma2_sq = (int32_t)(yy_filt_val - mu2_sq_val);
+            int32_t sigma12 = (int32_t)(xy_filt_val - mu1_mu2_val);
+
+            sigma2_sq = MAX(sigma2_sq, 0);
+            if (sigma1_sq >= sigma_nsq) {
+                /**
+                * log values are taken from the look-up table generated by
+                * log_generate() function which is called in integer_combo_threadfunc
+                * den_val in float is log2(1 + sigma1_sq/2)
+                * here it is converted to equivalent of log2(2+sigma1_sq) - log2(2) i.e log2(2*65536+sigma1_sq) - 17
+                * multiplied by 2048 as log_value = log2(i)*2048 i=16384 to 65535 generated using log_value
+                * x because best 16 bits are taken
+                */
+                accum_den_log += log2_32(log2_table, sigma_nsq + sigma1_sq) - 2048 * 17;
+
+                if (sigma12 > 0 && sigma2_sq > 0) {
+                    /**
+                    * In floating-point numerator = log2((1.0f + (g * g * sigma1_sq)/(sv_sq + sigma_nsq))
+                    *
+                    * In Fixed-point the above is converted to
+                    * numerator = log2((sv_sq + sigma_nsq)+(g * g * sigma1_sq))- log2(sv_sq + sigma_nsq)
+                    */
+
+                    const double eps = 65536 * 1.0e-10;
+                    double g = sigma12 / (sigma1_sq + eps); // this epsilon can go away
+                    int32_t sv_sq = sigma2_sq - g * sigma12;
+
+                    sv_sq = (uint32_t)(MAX(sv_sq, 0));
+
+                    g = MIN(g, vif_enhn_gain_limit);
+
+                    uint32_t numer1 = (sv_sq + sigma_nsq);
+                    int64_t numer1_tmp = (int64_t)((g * g * sigma1_sq)) + numer1; //numerator
+                    accum_num_log += log2_64(log2_table, numer1_tmp) - log2_64(log2_table, numer1);
+                }
+            }
+            else {
+                accum_num_non_log += sigma2_sq;
+                accum_den_non_log++;
+            }
         }
     }
+    num[0] = accum_num_log / 2048.0 + (accum_den_non_log - ((accum_num_non_log) / 16384.0) / (65025.0));
+    den[0] = accum_den_log / 2048.0 + accum_den_non_log;
 }
 
-static void filter1d_16(VifBuffer buf, unsigned w, unsigned h, int scale,
-                        int bpc)
-{
+void vif_statistic_16(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h, int bpc, int scale) {
     const unsigned fwidth = vif_filter1d_width[scale];
     const uint16_t *vif_filt = vif_filter1d_table[scale];
-
+    VifBuffer buf = s->buf;
+    int64_t accum_num_log = 0.0;
+    int64_t accum_den_log = 0.0;
+    int64_t accum_num_non_log = 0;
+    int64_t accum_den_non_log = 0;
+    static const int32_t sigma_nsq = 65536 << 1;
+    uint16_t *log2_table = s->log2_table;
+    double vif_enhn_gain_limit = s->vif_enhn_gain_limit;
     int32_t add_shift_round_HP, shift_HP;
     int32_t add_shift_round_VP, shift_VP;
     int32_t add_shift_round_VP_sq, shift_VP_sq;
@@ -224,7 +361,8 @@ static void filter1d_16(VifBuffer buf, unsigned w, unsigned h, int scale,
         add_shift_round_VP = 1 << (bpc - 1);
         shift_VP_sq = (bpc - 8) * 2;
         add_shift_round_VP_sq = (bpc == 8) ? 0 : 1 << (shift_VP_sq - 1);
-    } else {
+    }
+    else {
         shift_HP = 16;
         add_shift_round_HP = 32768;
         shift_VP = 16;
@@ -284,103 +422,26 @@ static void filter1d_16(VifBuffer buf, unsigned w, unsigned h, int scale,
                 accum_dis += fcoeff * ((uint64_t)buf.tmp.dis[jj_check]);
                 accum_ref_dis += fcoeff * ((uint64_t)buf.tmp.ref_dis[jj_check]);
             }
-            const ptrdiff_t dst_stride = buf.stride_32 / sizeof(uint32_t);
-            buf.mu1_32[i * dst_stride + j] = accum_mu1;
-            buf.mu2_32[i * dst_stride + j] = accum_mu2;
-            buf.ref_sq[i * dst_stride + j] = (uint32_t)((accum_ref + add_shift_round_HP) >> shift_HP);
-            buf.dis_sq[i * dst_stride + j] = (uint32_t)((accum_dis + add_shift_round_HP) >> shift_HP);
-            buf.ref_dis[i * dst_stride + j] = (uint32_t)((accum_ref_dis + add_shift_round_HP) >> shift_HP);
-        }
-    }
-}
-
-#define MIN(x, y) (((x) < (y)) ? (x) : (y))
-#define MAX(x, y) (((x) > (y)) ? (x) : (y))
-
-static void vif_statistic(VifBuffer buf, float *num, float *den,
-                          unsigned w, unsigned h, uint16_t *log2_table,
-                          double vif_enhn_gain_limit)
-{
-    uint32_t *xx_filt = buf.ref_sq;
-    uint32_t *yy_filt = buf.dis_sq;
-    uint32_t *xy_filt = buf.ref_dis;
-
-    //float equivalent of 2. (2 * 65536)
-    static const int32_t sigma_nsq = 65536 << 1;
 
-    int64_t num_val, den_val;
-    int64_t accum_x = 0, accum_x2 = 0;
-    int64_t num_accum_x = 0;
-    int64_t accum_num_log = 0.0;
-    int64_t accum_den_log = 0.0;
-    int64_t accum_num_non_log = 0;
-    int64_t accum_den_non_log = 0;
-    /**
-        * In floating-point there are two types of numerator scores and denominator scores
-        * 1. num = 1 - sigma1_sq * constant den =1  when sigma1_sq<2  here constant=4/(255*255)
-        * 2. num = log2(((sigma2_sq+2)*sigma1_sq)/((sigma2_sq+2)*sigma1_sq-sigma12*sigma12) den=log2(1+(sigma1_sq/2)) else
-        *
-        * In fixed-point separate accumulator is used for non-log score accumulations and log-based score accumulation
-        * For non-log accumulator of numerator, only sigma1_sq * constant in fixed-point is accumulated
-        * log based values are separately accumulated.
-        * While adding both accumulator values the non-log accumulator is converted such that it is equivalent to 1 - sigma1_sq * constant(1's are accumulated with non-log denominator accumulator)
-    */
-    for (unsigned i = 0; i < h; ++i) {
-        for (unsigned j = 0; j < w; ++j) {
-            const ptrdiff_t stride = buf.stride_32 / sizeof(uint32_t);
-            uint32_t mu1_val = buf.mu1_32[i * stride + j];
-            uint32_t mu2_val = buf.mu2_32[i * stride + j];
+            uint32_t mu1_val = accum_mu1;
+            uint32_t mu2_val = accum_mu2;
             uint32_t mu1_sq_val = (uint32_t)((((uint64_t)mu1_val * mu1_val)
                                     + 2147483648) >> 32);
             uint32_t mu2_sq_val = (uint32_t)((((uint64_t)mu2_val * mu2_val)
                                     + 2147483648) >> 32);
             uint32_t mu1_mu2_val = (uint32_t)((((uint64_t)mu1_val * mu2_val)
                                     + 2147483648) >> 32);
 
-            uint32_t xx_filt_val = xx_filt[i * stride + j];
-            uint32_t yy_filt_val = yy_filt[i * stride + j];
-            uint32_t xy_filt_val = xy_filt[i * stride + j];
+            uint32_t xx_filt_val = (uint32_t)((accum_ref + add_shift_round_HP) >> shift_HP);
+            uint32_t yy_filt_val = (uint32_t)((accum_dis + add_shift_round_HP) >> shift_HP);
+            uint32_t xy_filt_val = (uint32_t)((accum_ref_dis + add_shift_round_HP) >> shift_HP);
 
             int32_t sigma1_sq = (int32_t)(xx_filt_val - mu1_sq_val);
             int32_t sigma2_sq = (int32_t)(yy_filt_val - mu2_sq_val);
             int32_t sigma12 = (int32_t)(xy_filt_val - mu1_mu2_val);
 
-            sigma1_sq = MAX(sigma1_sq, 0.0);
-            sigma2_sq = MAX(sigma2_sq, 0.0);
-
-            //eps is zero, an int will not be less then 1.0e-10, it can be changed to one
-            const double eps = 65536 * 1.0e-10;
-            double g = sigma12 / (sigma1_sq + eps);
-            int32_t sv_sq = sigma2_sq - g * sigma12;
-
-			sv_sq = sigma2_sq - g * sigma12;
-
-
-			if (sigma1_sq < eps) {
-			    g = 0.0;
-                sv_sq = sigma2_sq;
-                sigma1_sq = 0.0;
-			}
-
-			if (sigma2_sq < eps) {
-			    g = 0.0;
-			    sv_sq = 0.0;
-			}
-
-			if (g < 0.0) {
-			    sv_sq = sigma2_sq;
-			    g = 0.0;
-			}
-
-			sv_sq = (uint32_t)(MAX(sv_sq, eps));
-
-            g = MIN(g, vif_enhn_gain_limit);
-
+            sigma2_sq = MAX(sigma2_sq, 0);
             if (sigma1_sq >= sigma_nsq) {
-                uint32_t log_den_stage1 = (uint32_t)(sigma_nsq + sigma1_sq);
-                int x;
-                uint16_t log_den1 = get_best16_from32(log_den_stage1, &x);
-
                 /**
                 * log values are taken from the look-up table generated by
                 * log_generate() function which is called in integer_combo_threadfunc
@@ -389,224 +450,211 @@ static void vif_statistic(VifBuffer buf, float *num, float *den,
                 * multiplied by 2048 as log_value = log2(i)*2048 i=16384 to 65535 generated using log_value
                 * x because best 16 bits are taken
                 */
-                num_accum_x++;
-                accum_x += x;
-                den_val = log2_table[log_den1];
+                accum_den_log += log2_32(log2_table, sigma_nsq + sigma1_sq) - 2048 * 17;
 
-                if (sigma12 >= 0) {
-                    // num_val = log2f(1.0f + (g * g * sigma1_sq) / (sv_sq + sigma_nsq));
+                if (sigma12 > 0 && sigma2_sq > 0) {
                     /**
                     * In floating-point numerator = log2((1.0f + (g * g * sigma1_sq)/(sv_sq + sigma_nsq))
                     *
                     * In Fixed-point the above is converted to
                     * numerator = log2((sv_sq + sigma_nsq)+(g * g * sigma1_sq))- log2(sv_sq + sigma_nsq)
                     */
-                    int x1, x2;
+
+                    const double eps = 65536 * 1.0e-10;
+                    double g = sigma12 / (sigma1_sq + eps); // this epsilon can go away
+                    int32_t sv_sq = sigma2_sq - g * sigma12;
+
+                    sv_sq = (uint32_t)(MAX(sv_sq, 0));
+
+                    g = MIN(g, vif_enhn_gain_limit);
+
                     uint32_t numer1 = (sv_sq + sigma_nsq);
                     int64_t numer1_tmp = (int64_t)((g * g * sigma1_sq)) + numer1; //numerator
-                    uint16_t numlog = get_best16_from64((uint64_t)numer1_tmp, &x1);
-                    if (numer1 > 0) {
-                        uint16_t denlog = get_best16_from64((uint64_t)numer1, &x2);
-                        accum_x2 += (x2 - x1);
-                        num_val = log2_table[numlog] - log2_table[denlog];
-                        accum_num_log += num_val;
-                        accum_den_log += den_val;
-                    } else {
-                        den_val = 1;
-                        accum_num_non_log += sigma2_sq;
-                        accum_den_non_log += den_val;
-                    }
-
-                }
-                else {
-                    num_val = 0;
-                    accum_num_log += num_val;
-                    accum_den_log += den_val;
+                    accum_num_log += log2_64(log2_table, numer1_tmp) - log2_64(log2_table, numer1);
                 }
             }
             else {
-                den_val = 1;
                 accum_num_non_log += sigma2_sq;
-                accum_den_non_log += den_val;
-            }
-        }
-    }
-    //log has to be divided by 2048 as log_value = log2(i*2048)  i=16384 to 65535
-    //num[0] = accum_num_log / 2048.0 + (accum_den_non_log - (accum_num_non_log / 65536.0) / (255.0*255.0));
-    //den[0] = accum_den_log / 2048.0 + accum_den_non_log;
-
-    //changed calculation to increase performance
-    num[0] = accum_num_log / 2048.0  + accum_x2 + (accum_den_non_log - ((accum_num_non_log) / 16384.0) / (65025.0));
-    den[0] = accum_den_log / 2048.0  - (accum_x + (num_accum_x * 17)) + accum_den_non_log;
-}
-
-static void filter1d_rd_8(VifBuffer buf, unsigned w, unsigned h)
-{
-    const unsigned fwidth = vif_filter1d_width[1];
-    const uint16_t *vif_filt_s1 = vif_filter1d_table[1];
-
-    for (unsigned i = 0; i < h; ++i) {
-        //VERTICAL
-        for (unsigned j = 0; j < w; ++j) {
-            uint32_t accum_ref = 0;
-            uint32_t accum_dis = 0;
-            for (unsigned fi = 0; fi < fwidth; ++fi) {
-                int ii = i - fwidth / 2;
-                int ii_check = ii + fi;
-                const uint16_t fcoeff = vif_filt_s1[fi];
-                const uint8_t *ref = (uint8_t*)buf.ref;
-                const uint8_t *dis = (uint8_t*)buf.dis;
-                accum_ref += fcoeff * (uint32_t)ref[ii_check * buf.stride + j];
-                accum_dis += fcoeff * (uint32_t)dis[ii_check * buf.stride + j];
-            }
-            buf.tmp.ref_convol[j] = (accum_ref + 128) >> 8;
-            buf.tmp.dis_convol[j] = (accum_dis + 128) >> 8;
-        }
-
-        PADDING_SQ_DATA_2(buf, w, fwidth / 2);
-
-        //HORIZONTAL
-        for (unsigned j = 0; j < w; ++j) {
-            uint32_t accum_ref = 0;
-            uint32_t accum_dis = 0;
-            for (unsigned fj = 0; fj < fwidth; ++fj) {
-                int jj = j - fwidth / 2;
-                int jj_check = jj + fj;
-                const uint16_t fcoeff = vif_filt_s1[fj];
-                accum_ref += fcoeff * buf.tmp.ref_convol[jj_check];
-                accum_dis += fcoeff * buf.tmp.dis_convol[jj_check];
+                accum_den_non_log++;
             }
-            const ptrdiff_t stride = buf.stride_16 / sizeof(uint16_t);
-            buf.mu1[i * stride + j] = (uint16_t)((accum_ref + 32768) >> 16);
-            buf.mu2[i * stride + j] = (uint16_t)((accum_dis + 32768) >> 16);
         }
     }
+    num[0] = accum_num_log / 2048.0 + (accum_den_non_log - ((accum_num_non_log) / 16384.0) / (65025.0));
+    den[0] = accum_den_log / 2048.0 + accum_den_non_log;
 }
 
-static void filter1d_rd_16(VifBuffer buf, unsigned w, unsigned h, int scale,
-                           int bpc)
+VifResiduals vif_compute_line_residuals(VifPublicState *s, unsigned from,
+                                        unsigned to, int bpc, int scale)
 {
-    const unsigned fwidth = vif_filter1d_width[scale + 1];
-    const uint16_t *vif_filt = vif_filter1d_table[scale + 1];
+    VifResiduals residuals = { 0 };
+    const unsigned fwidth = vif_filter1d_width[scale];
+    const uint16_t *vif_filt = vif_filter1d_table[scale];
+    VifBuffer buf = s->buf;
+    int32_t add_shift_round_HP, shift_HP;
     int32_t add_shift_round_VP, shift_VP;
+    int32_t add_shift_round_VP_sq, shift_VP_sq;
+    const uint16_t *log2_table = s->log2_table;
+    double vif_enhn_gain_limit = s->vif_enhn_gain_limit;
+    static const int32_t sigma_nsq = 65536 << 1;
 
     if (scale == 0) {
-        add_shift_round_VP = 1 << (bpc - 1);
+        shift_HP = 16;
+        add_shift_round_HP = 32768;
         shift_VP = bpc;
-    } else {
-        add_shift_round_VP = 32768;
+        add_shift_round_VP = 1 << (bpc - 1);
+        shift_VP_sq = (bpc - 8) * 2;
+        add_shift_round_VP_sq = (bpc == 8) ? 0 : 1 << (shift_VP_sq - 1);
+    }
+    else {
+        shift_HP = 16;
+        add_shift_round_HP = 32768;
         shift_VP = 16;
+        add_shift_round_VP = 32768;
+        shift_VP_sq = 16;
+        add_shift_round_VP_sq = 32768;
     }
 
-    for (unsigned i = 0; i < h; ++i) {
-        //VERTICAL
-        for (unsigned j = 0; j < w; ++j) {
-            uint32_t accum_ref = 0;
-            uint32_t accum_dis = 0;
-            for (unsigned fi = 0; fi < fwidth; ++fi) {
-                int ii = i - fwidth / 2;
-                int ii_check = ii + fi;
-                const uint16_t fcoeff = vif_filt[fi];
-                const ptrdiff_t stride = buf.stride / sizeof(uint16_t);
-                uint16_t *ref = buf.ref;
-                uint16_t *dis = buf.dis;
-                accum_ref += fcoeff * ((uint32_t)ref[ii_check * stride + j]);
-                accum_dis += fcoeff * ((uint32_t)dis[ii_check * stride + j]);
-            }
-            buf.tmp.ref_convol[j] = (uint16_t)((accum_ref + add_shift_round_VP) >> shift_VP);
-            buf.tmp.dis_convol[j] = (uint16_t)((accum_dis + add_shift_round_VP) >> shift_VP);
+    //HORIZONTAL
+    for (unsigned j = from; j < to; ++j) {
+        uint32_t accum_mu1 = 0;
+        uint32_t accum_mu2 = 0;
+        uint64_t accum_ref = 0;
+        uint64_t accum_dis = 0;
+        uint64_t accum_ref_dis = 0;
+        for (unsigned fj = 0; fj < fwidth; ++fj) {
+            int jj = j - fwidth / 2;
+            int jj_check = jj + fj;
+            const uint16_t fcoeff = vif_filt[fj];
+            accum_mu1 += fcoeff * ((uint32_t)buf.tmp.mu1[jj_check]);
+            accum_mu2 += fcoeff * ((uint32_t)buf.tmp.mu2[jj_check]);
+            accum_ref += fcoeff * ((uint64_t)buf.tmp.ref[jj_check]);
+            accum_dis += fcoeff * ((uint64_t)buf.tmp.dis[jj_check]);
+            accum_ref_dis += fcoeff * ((uint64_t)buf.tmp.ref_dis[jj_check]);
         }
+        uint32_t mu1_val = accum_mu1;
+        uint32_t mu2_val = accum_mu2;
+        uint32_t mu1_sq_val = (uint32_t)((((uint64_t)mu1_val * mu1_val)
+            + 2147483648) >> 32);
+        uint32_t mu2_sq_val = (uint32_t)((((uint64_t)mu2_val * mu2_val)
+            + 2147483648) >> 32);
+        uint32_t mu1_mu2_val = (uint32_t)((((uint64_t)mu1_val * mu2_val)
+            + 2147483648) >> 32);
+
+        uint32_t xx_filt_val = (uint32_t)((accum_ref + add_shift_round_HP) >> shift_HP);
+        uint32_t yy_filt_val = (uint32_t)((accum_dis + add_shift_round_HP) >> shift_HP);
+        uint32_t xy_filt_val = (uint32_t)((accum_ref_dis + add_shift_round_HP) >> shift_HP);
+
+        int32_t sigma1_sq = (int32_t)(xx_filt_val - mu1_sq_val);
+        int32_t sigma2_sq = (int32_t)(yy_filt_val - mu2_sq_val);
+        int32_t sigma12 = (int32_t)(xy_filt_val - mu1_mu2_val);
+
+        sigma2_sq = MAX(sigma2_sq, 0);
+        if (sigma1_sq >= sigma_nsq) {
+            /**
+            * log values are taken from the look-up table generated by
+            * log_generate() function which is called in integer_combo_threadfunc
+            * den_val in float is log2(1 + sigma1_sq/2)
+            * here it is converted to equivalent of log2(2+sigma1_sq) - log2(2) i.e log2(2*65536+sigma1_sq) - 17
+            * multiplied by 2048 as log_value = log2(i)*2048 i=16384 to 65535 generated using log_value
+            * x because best 16 bits are taken
+            */
+            residuals.accum_den_log += log2_32(log2_table, sigma_nsq + sigma1_sq) - 2048 * 17;
+
+            if (sigma12 > 0 && sigma2_sq > 0) {
+                /**
+                * In floating-point numerator = log2((1.0f + (g * g * sigma1_sq)/(sv_sq + sigma_nsq))
+                *
+                * In Fixed-point the above is converted to
+                * numerator = log2((sv_sq + sigma_nsq)+(g * g * sigma1_sq))- log2(sv_sq + sigma_nsq)
+                */
 
-        PADDING_SQ_DATA_2(buf, w, fwidth / 2);
+                const double eps = 65536 * 1.0e-10;
+                double g = sigma12 / (sigma1_sq + eps); // this epsilon can go away
+                int32_t sv_sq = sigma2_sq - g * sigma12;
 
-        //HORIZONTAL
-        for (unsigned j = 0; j < w; ++j) {
-            uint32_t accum_ref = 0;
-            uint32_t accum_dis = 0;
-            for (unsigned fj = 0; fj < fwidth; ++fj) {
-                int jj = j - fwidth / 2;
-                int jj_check = jj + fj;
-                const uint16_t fcoeff = vif_filt[fj];
-                accum_ref += fcoeff * ((uint32_t)buf.tmp.ref_convol[jj_check]);
-                accum_dis += fcoeff * ((uint32_t)buf.tmp.dis_convol[jj_check]);
+                sv_sq = (uint32_t)(MAX(sv_sq, 0));
+
+                g = MIN(g, vif_enhn_gain_limit);
+
+                uint32_t numer1 = (sv_sq + sigma_nsq);
+                int64_t numer1_tmp = (int64_t)((g * g * sigma1_sq)) + numer1; //numerator
+                residuals.accum_num_log += log2_64(log2_table, numer1_tmp) - log2_64(log2_table, numer1);
             }
-            const ptrdiff_t stride = buf.stride_16 / sizeof(uint16_t);
-            buf.mu1[i * stride + j] = (uint16_t)((accum_ref + 32768) >> 16);
-            buf.mu2[i * stride + j] = (uint16_t)((accum_dis + 32768) >> 16);
+        }
+        else {
+            residuals.accum_num_non_log += sigma2_sq;
+            residuals.accum_den_non_log++;
         }
     }
+    return residuals;
 }
 
-static inline void log_generate(uint16_t *log2_table)
-{
-    for (unsigned i = 32767; i < 65536; ++i) {
-        log2_table[i] = (uint16_t)round(log2f((float)i) * 2048);
-    }
-}
 
 static int init(VmafFeatureExtractor *fex, enum VmafPixelFormat pix_fmt,
                 unsigned bpc, unsigned w, unsigned h)
 {
     VifState *s = fex->priv;
 
-    s->filter1d_8 = filter1d_8;
-    s->filter1d_16 = filter1d_16;
-    s->filter1d_rd_8 = filter1d_rd_8;
-    s->filter1d_rd_16 = filter1d_rd_16;
+    s->subsample_rd_8 = subsample_rd_8;
+    s->subsample_rd_16 = subsample_rd_16;
+    s->vif_statistic_8 = vif_statistic_8;
+    s->vif_statistic_16 = vif_statistic_16;
 
 #if ARCH_X86
     unsigned flags = vmaf_get_cpu_flags();
     if (flags & VMAF_X86_CPU_FLAG_AVX2) {
-        s->filter1d_8 = vif_filter1d_8_avx2;
-        s->filter1d_16 = vif_filter1d_16_avx2;
-        s->filter1d_rd_8 = vif_filter1d_rd_8_avx2;
-        s->filter1d_rd_16 = vif_filter1d_rd_16_avx2;
+        s->subsample_rd_8 = vif_subsample_rd_8_avx2;
+        s->subsample_rd_16 = vif_subsample_rd_16_avx2;
+        s->vif_statistic_8 = vif_statistic_8_avx2;
+        s->vif_statistic_16 = vif_statistic_16_avx2;
     }
+/*
 #if HAVE_AVX512
     if (flags & VMAF_X86_CPU_FLAG_AVX512) {
-        s->filter1d_8 = vif_filter1d_8_avx512;
-        s->filter1d_16 = vif_filter1d_16_avx512;
-        s->filter1d_rd_8 = vif_filter1d_rd_8_avx512;
-        s->filter1d_rd_16 = vif_filter1d_rd_16_avx512;
+        s->subsample_rd_8 = vif_subsample_rd_8_avx2;
+        s->subsample_rd_16 = vif_subsample_rd_16_avx512;
+        s->vif_statistic_8 = vif_statistic_8_avx512;
+        s->vif_statistic_16 = vif_statistic_16_avx512;
     }
 #endif
+*/
 #endif
 
-    log_generate(s->log2_table);
+    log_generate(s->public.log2_table);
 
-    (void) pix_fmt;
+    (void)pix_fmt;
     const bool hbd = bpc > 8;
 
-    s->buf.stride = ALIGN_CEIL(w << hbd);
-    s->buf.stride_16 = ALIGN_CEIL(w * sizeof(uint16_t));
-    s->buf.stride_32 = ALIGN_CEIL(w * sizeof(uint32_t));
-    s->buf.stride_tmp =
+    s->public.buf.stride = ALIGN_CEIL(w << hbd);
+    s->public.buf.stride_16 = ALIGN_CEIL(w * sizeof(uint16_t));
+    s->public.buf.stride_32 = ALIGN_CEIL(w * sizeof(uint32_t));
+    s->public.buf.stride_tmp =
         ALIGN_CEIL((MAX_ALIGN + w + MAX_ALIGN) * sizeof(uint32_t));
-    const size_t frame_size = s->buf.stride * h;
-    const size_t pad_size = s->buf.stride * 8;
+    const size_t frame_size = s->public.buf.stride * h;
+    const size_t pad_size = s->public.buf.stride * 8;
     const size_t data_sz =
-        2 * (pad_size + frame_size + pad_size) + 2 * (h * s->buf.stride_16) +
-        5 * (h * s->buf.stride_32) + 7 * s->buf.stride_tmp;
+        2 * (pad_size + frame_size + pad_size) + 2 * (h * s->public.buf.stride_16) +
+        5 * (s->public.buf.stride_32) + 7 * s->public.buf.stride_tmp;
     void *data = aligned_malloc(data_sz, MAX_ALIGN);
-    if (!data) goto fail;
-
-    s->buf.data = data; data += pad_size;
-    s->buf.ref = data; data += frame_size + pad_size + pad_size;
-    s->buf.dis = data; data += frame_size + pad_size;
-    s->buf.mu1 = data; data += h * s->buf.stride_16;
-    s->buf.mu2 = data; data += h * s->buf.stride_16;
-    s->buf.mu1_32 = data; data += h * s->buf.stride_32;
-    s->buf.mu2_32 = data; data += h * s->buf.stride_32;
-    s->buf.ref_sq = data; data += h * s->buf.stride_32;
-    s->buf.dis_sq = data; data += h * s->buf.stride_32;
-    s->buf.ref_dis = data; data += h * s->buf.stride_32;
-    s->buf.tmp.mu1 = data; data += s->buf.stride_tmp;
-    s->buf.tmp.mu2 = data; data += s->buf.stride_tmp;
-    s->buf.tmp.ref = data; data += s->buf.stride_tmp;
-    s->buf.tmp.dis = data; data += s->buf.stride_tmp;
-    s->buf.tmp.ref_dis = data; data += s->buf.stride_tmp;
-    s->buf.tmp.ref_convol = data; data += s->buf.stride_tmp;
-    s->buf.tmp.dis_convol = data;
+    if (!data) return -ENOMEM;
+
+    s->public.buf.data = data; data += pad_size;
+    s->public.buf.ref = data; data += frame_size + pad_size + pad_size;
+    s->public.buf.dis = data; data += frame_size + pad_size;
+    s->public.buf.mu1 = data; data += h * s->public.buf.stride_16;
+    s->public.buf.mu2 = data; data += h * s->public.buf.stride_16;
+    s->public.buf.mu1_32 = data; data += s->public.buf.stride_32;
+    s->public.buf.mu2_32 = data; data += s->public.buf.stride_32;
+    s->public.buf.ref_sq = data; data += s->public.buf.stride_32;
+    s->public.buf.dis_sq = data; data += s->public.buf.stride_32;
+    s->public.buf.ref_dis = data; data += s->public.buf.stride_32;
+    s->public.buf.tmp.mu1 = data; data += s->public.buf.stride_tmp;
+    s->public.buf.tmp.mu2 = data; data += s->public.buf.stride_tmp;
+    s->public.buf.tmp.ref = data; data += s->public.buf.stride_tmp;
+    s->public.buf.tmp.dis = data; data += s->public.buf.stride_tmp;
+    s->public.buf.tmp.ref_dis = data; data += s->public.buf.stride_tmp;
+    s->public.buf.tmp.ref_convol = data; data += s->public.buf.stride_tmp;
+    s->public.buf.tmp.dis_convol = data;
 
     s->feature_name_dict =
         vmaf_feature_name_dict_from_provided_features(fex->provided_features,
@@ -713,47 +761,45 @@ static int extract(VmafFeatureExtractor *fex,
 {
     VifState *s = fex->priv;
 
-    (void) ref_pic_90;
-    (void) dist_pic_90;
+    (void)ref_pic_90;
+    (void)dist_pic_90;
 
     unsigned w = ref_pic->w[0];
     unsigned h = dist_pic->h[0];
 
-    void *ref_in = ref_pic->data[0];
-    void *dis_in = dist_pic->data[0];
-    void *ref_out = s->buf.ref;
-    void *dis_out = s->buf.dis;
+    unsigned char *ref_in = ref_pic->data[0];
+    unsigned char *dis_in = dist_pic->data[0];
+    unsigned char *ref_out = s->public.buf.ref;
+    unsigned char *dis_out = s->public.buf.dis;
 
     for (unsigned i = 0; i < h; i++) {
         memcpy(ref_out, ref_in, ref_pic->stride[0]);
         memcpy(dis_out, dis_in, dist_pic->stride[0]);
         ref_in += ref_pic->stride[0];
         dis_in += dist_pic->stride[0];
-        ref_out += s->buf.stride;
-        dis_out += s->buf.stride;
+        ref_out += s->public.buf.stride;
+        dis_out += s->public.buf.stride;
     }
-    pad_top_and_bottom(s->buf, h, vif_filter1d_width[0]);
+    pad_top_and_bottom(s->public.buf, h, vif_filter1d_width[0]);
 
     VifScore vif_score;
     for (unsigned scale = 0; scale < 4; ++scale) {
         if (scale > 0) {
             if (ref_pic->bpc == 8 && scale == 1)
-                s->filter1d_rd_8(s->buf, w, h);
+                s->subsample_rd_8(s->public.buf, w, h);
             else
-                s->filter1d_rd_16(s->buf, w, h, scale - 1, ref_pic->bpc);
+                s->subsample_rd_16(s->public.buf, w, h, scale - 1, ref_pic->bpc);
 
-            decimate_and_pad(s->buf, w, h, scale);
             w /= 2; h /= 2;
         }
 
-        if (ref_pic->bpc == 8 && scale == 0)
-            s->filter1d_8(s->buf, w, h);
-        else
-            s->filter1d_16(s->buf, w, h, scale, ref_pic->bpc);
+        if (ref_pic->bpc == 8 && scale == 0) {
+            s->vif_statistic_8(&s->public, &vif_score.scale[scale].num, &vif_score.scale[scale].den, w, h);
+        }
+        else {
+            s->vif_statistic_16(&s->public, &vif_score.scale[scale].num, &vif_score.scale[scale].den, w, h, ref_pic->bpc, scale);
+        }
 
-        vif_statistic(s->buf, &vif_score.scale[scale].num,
-                      &vif_score.scale[scale].den, w, h, s->log2_table,
-                      s->vif_enhn_gain_limit);
     }
 
     return write_scores(feature_collector, index, vif_score, s);
@@ -762,8 +808,7 @@ static int extract(VmafFeatureExtractor *fex,
 static int close(VmafFeatureExtractor *fex)
 {
     VifState *s = fex->priv;
-    if (s->buf.data) aligned_free(s->buf.data);
-    vmaf_dictionary_free(&s->feature_name_dict);
+    if (s->public.buf.data) aligned_free(s->public.buf.data);
     return 0;
 }
 "
36;Netflix;vmaf;79ce8a1fe77069bbdfd2c04414cec9e198729d23;"libvmaf: pipelined vif feature computation

memory requirements reduced and performance improvement

Original PR: https://github.com/Netflix/vmaf/pull/944

cleanup-and-testing-by: nilfm <nfonsmiret@netflix.com>
rebased-by: Kyle Swanson <kswanson@netflix.com>";"@@ -20,6 +20,8 @@
 #define FEATURE_VIF_H_
 
 #include <stdint.h>
+#include <stdbool.h>
+#include <assert.h>
 
 /* Enhancement gain imposed on vif, must be >= 1.0, where 1.0 means the gain is completely disabled */
 #ifndef DEFAULT_VIF_ENHN_GAIN_LIMIT
@@ -64,10 +66,23 @@ typedef struct VifBuffer {
     ptrdiff_t stride_tmp;
 } VifBuffer;
 
+typedef struct VifResiduals {
+    int64_t accum_num_log;
+    int64_t accum_den_log;
+    int64_t accum_num_non_log;
+    int64_t accum_den_non_log;
+} VifResiduals;
+
+typedef struct VifPublicState {
+    VifBuffer buf;
+    uint16_t log2_table[65537];
+    double vif_enhn_gain_limit;
+} VifPublicState;
+
 static inline void PADDING_SQ_DATA(VifBuffer buf, int w, unsigned fwidth_half)
 {
     for (unsigned f = 1; f <= fwidth_half; ++f) {
-        int left_point = -f;
+        int left_point = -(int)f;
         int right_point = f;
         buf.tmp.mu1[left_point] = buf.tmp.mu1[right_point];
         buf.tmp.mu2[left_point] = buf.tmp.mu2[right_point];
@@ -88,7 +103,7 @@ static inline void PADDING_SQ_DATA(VifBuffer buf, int w, unsigned fwidth_half)
 static inline void PADDING_SQ_DATA_2(VifBuffer buf, int w, unsigned fwidth_half)
 {
     for (unsigned f = 1; f <= fwidth_half; ++f) {
-        int left_point = -f;
+        int left_point = -(int)f;
         int right_point = f;
         buf.tmp.ref_convol[left_point] = buf.tmp.ref_convol[right_point];
         buf.tmp.dis_convol[left_point] = buf.tmp.dis_convol[right_point];
@@ -100,4 +115,46 @@ static inline void PADDING_SQ_DATA_2(VifBuffer buf, int w, unsigned fwidth_half)
     }
 }
 
+void vif_statistic_8(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h);
+void vif_statistic_16(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h, int bpc, int scale);
+
+/*
+ * Compute vif residuals on a vertically filtered line 
+ * This is a support method for block based vip_statistic_xxx method and is typically called
+ * only when to is not a multiple of the block size, with from = (to / block_size) + block_size
+ */
+VifResiduals vif_compute_line_residuals(VifPublicState *s, unsigned from,
+                                        unsigned to, int bpc, int scale);
+
+
+#ifdef _MSC_VER
+#include <intrin.h>
+
+static inline int __builtin_clz(unsigned x) {
+    return (int)__lzcnt(x);
+}
+
+static inline int __builtin_clzll(unsigned long long x) {
+    return (int)__lzcnt64(x);
+}
+
+#endif
+
+static inline int32_t log2_32(const uint16_t *log2_table, uint32_t temp)
+{
+    int k = __builtin_clz(temp);
+    k = 16 - k;
+    temp = temp >> k;
+    return log2_table[temp] + 2048 * k;
+}
+
+static inline int32_t log2_64(const uint16_t *log2_table, uint64_t temp)
+{
+    assert(temp >= 0x20000);
+    int k = __builtin_clzll(temp);
+    k = 48 - k;
+    temp = temp >> k;
+    return log2_table[temp] + 2048 * k;
+}
+
 #endif /* _FEATURE_VIF_H_ */"
36;Netflix;vmaf;79ce8a1fe77069bbdfd2c04414cec9e198729d23;"libvmaf: pipelined vif feature computation

memory requirements reduced and performance improvement

Original PR: https://github.com/Netflix/vmaf/pull/944

cleanup-and-testing-by: nilfm <nfonsmiret@netflix.com>
rebased-by: Kyle Swanson <kswanson@netflix.com>";"@@ -23,12 +23,15 @@
 
 void vif_filter1d_8_avx2(VifBuffer buf, unsigned w, unsigned h);
 
-void vif_filter1d_rd_8_avx2(VifBuffer buf, unsigned w, unsigned h);
+void vif_subsample_rd_8_avx2(VifBuffer buf, unsigned w, unsigned h);
 
-void vif_filter1d_rd_16_avx2(VifBuffer buf, unsigned w, unsigned h, int scale,
+void vif_subsample_rd_16_avx2(VifBuffer buf, unsigned w, unsigned h, int scale,
                              int bpc);
 
-void vif_filter1d_16_avx2(VifBuffer buf, unsigned w, unsigned h, int scale,
-                            int bpc);
+void vif_filter1d_16_avx2(VifBuffer buf, unsigned w, unsigned h, int scale, int bpc);
+
+void vif_statistic_8_avx2(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h);
+
+void vif_statistic_16_avx2(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h, int bpc, int scale);
 
 #endif /* X86_AVX2_VIF_H_ */"
36;Netflix;vmaf;79ce8a1fe77069bbdfd2c04414cec9e198729d23;"libvmaf: pipelined vif feature computation

memory requirements reduced and performance improvement

Original PR: https://github.com/Netflix/vmaf/pull/944

cleanup-and-testing-by: nilfm <nfonsmiret@netflix.com>
rebased-by: Kyle Swanson <kswanson@netflix.com>";"@@ -21,14 +21,13 @@
 
 #include ""feature/integer_vif.h""
 
-void vif_filter1d_8_avx512(VifBuffer buf, unsigned w, unsigned h);
+void vif_subsample_rd_8_avx2(VifBuffer buf, unsigned w, unsigned h);
 
-void vif_filter1d_rd_8_avx512(VifBuffer buf, unsigned w, unsigned h);
-
-void vif_filter1d_rd_16_avx512(VifBuffer buf, unsigned w, unsigned h, int scale,
+void vif_subsample_rd_16_avx512(VifBuffer buf, unsigned w, unsigned h, int scale,
                              int bpc);
 
-void vif_filter1d_16_avx512(VifBuffer buf, unsigned w, unsigned h, int scale,
-                            int bpc);
+void vif_statistic_8_avx512(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h);
+
+void vif_statistic_16_avx512(struct VifPublicState *s, float *num, float *den, unsigned w, unsigned h, int bpc, int scale);
 
 #endif /* X86_AVX512_VIF_H_ */"
36;Netflix;vmaf;79ce8a1fe77069bbdfd2c04414cec9e198729d23;"libvmaf: pipelined vif feature computation

memory requirements reduced and performance improvement

Original PR: https://github.com/Netflix/vmaf/pull/944

cleanup-and-testing-by: nilfm <nfonsmiret@netflix.com>
rebased-by: Kyle Swanson <kswanson@netflix.com>";"@@ -203,8 +203,8 @@ if is_asm_enabled
             'x86_avx512',
             x86_avx512_sources,
             include_directories : vmaf_base_include,
-            c_args : ['-mavx512f', '-mavx512dq', '-mavx512bw',
-                      '-mavx512vbmi', '-mavx512vbmi2', '-mavx512vl'] +
+            c_args : ['-mavx512f', '-mavx512dq', '-mavx512bw', '-mavx512cd', '-mavx512dq',
+                      '-mavx512vbmi', '-mavx512vl'] +
                      vmaf_cflags_common,
         )
 "
36;vispy;vispy;f0af63083b25c27bd02ce3778bd9e62096c3ccd1;Reduce memory copies in LineVisual (#2327);"@@ -327,8 +327,7 @@ def _prepare_draw(self, view):
         if self._parent._changed['pos']:
             if self._parent._pos is None:
                 return False
-            # todo: does this result in unnecessary copies?
-            pos = np.ascontiguousarray(self._parent._pos.astype(np.float32))
+            pos = np.ascontiguousarray(self._parent._pos, dtype=np.float32)
             self._pos_vbo.set_data(pos)
             self._program.vert['position'] = self._pos_vbo
             self._program.vert['to_vec4'] = self._ensure_vec4_func(pos.shape[-1])
@@ -437,9 +436,7 @@ def _prepare_draw(self, view):
         if self._parent._changed['pos']:
             if self._parent._pos is None:
                 return False
-            # todo: does this result in unnecessary copies?
-            self._pos = np.ascontiguousarray(
-                self._parent._pos.astype(np.float32))
+            self._pos = np.ascontiguousarray(self._parent._pos, dtype=np.float32)
             bake = True
 
         if self._parent._changed['color']:"
37;coala;coala;7e3e890f13aa8cf33e1fd751f9ba741c30007910;"settings.py: Return memory location in `__repr__`

Modify the `__repr__` method to return
memory location as well to comply with the
default behavior of the method.

Closes https://github.com/coala/coala/issues/6050";"@@ -77,7 +77,9 @@ def __call__(self, setting):
                     for elem in setting]
 
         def __repr__(self):
-            return 'typed_list(%s)' % conversion_func.__name__
+            return (
+                f'typed_list({conversion_func.__name__}) at ({hex(id(self))})'
+            )
 
     return Converter()
 
@@ -113,8 +115,10 @@ def __call__(self, setting):
                     for key, value in dict(setting).items()}
 
         def __repr__(self):
-            return 'typed_dict(%s, %s, default=%s)' % (
-                key_type.__name__, value_type.__name__, default)
+            return (
+                f'typed_dict({key_type.__name__}, {value_type.__name__}, ' +
+                f'default={default}) at ({hex(id(self))})'
+            )
 
     return Converter()
 
@@ -139,8 +143,11 @@ def __call__(self, setting):
                                for key, value in OrderedDict(setting).items())
 
         def __repr__(self):
-            return 'typed_ordered_dict(%s, %s, default=%s)' % (
-                key_type.__name__, value_type.__name__, default)
+            return (
+                f'typed_ordered_dict({key_type.__name__}, ' +
+                f'{value_type.__name__}, default={default}) ' +
+                f'at ({hex(id(self))})'
+            )
 
     return Converter()
 "
37;coala;coala;7e3e890f13aa8cf33e1fd751f9ba741c30007910;"settings.py: Return memory location in `__repr__`

Modify the `__repr__` method to return
memory location as well to comply with the
default behavior of the method.

Closes https://github.com/coala/coala/issues/6050";"@@ -125,7 +125,8 @@ def test_typed_list(self):
             self.uut = Setting('key', '1, a, 3')
             typed_list(int)(self.uut)
 
-        self.assertEqual(repr(typed_list(int)), 'typed_list(int)')
+        self.assertRegex(repr(typed_list(int)),
+                         'typed_list\\(int\\) at \\(0x[a-fA-F0-9]+\\)')
 
     def test_int_list(self):
         self.uut = Setting('key', '1, 2, 3')
@@ -135,12 +136,14 @@ def test_int_list(self):
             self.uut = Setting('key', '1, a, 3')
             int_list(self.uut)
 
-        self.assertEqual(repr(int_list), 'typed_list(int)')
+        self.assertRegex(
+            repr(int_list), 'typed_list\\(int\\) at \\(0x[a-fA-F0-9]+\\)')
 
     def test_str_list(self):
         self.uut = Setting('key', 'a, b, c')
         self.assertEqual(str_list(self.uut), ['a', 'b', 'c'])
-        self.assertEqual(repr(str_list), 'typed_list(str)')
+        self.assertRegex(
+            repr(str_list), 'typed_list\\(str\\) at \\(0x[a-fA-F0-9]+\\)')
 
     def test_float_list(self):
         self.uut = Setting('key', '0.8, 1.3, 5.87')
@@ -150,7 +153,8 @@ def test_float_list(self):
             self.uut = Setting('key', '1.987, a, 3')
             float_list(self.uut)
 
-        self.assertEqual(repr(float_list), 'typed_list(float)')
+        self.assertRegex(
+            repr(float_list), 'typed_list\\(float\\) at \\(0x[a-fA-F0-9]+\\)')
 
     def test_bool_list(self):
         self.uut = Setting('key', 'true, nope, yeah')
@@ -160,7 +164,8 @@ def test_bool_list(self):
             self.uut = Setting('key', 'true, false, 78, 89.0')
             bool_list(self.uut)
 
-        self.assertEqual(repr(bool_list), 'typed_list(bool)')
+        self.assertRegex(
+            repr(bool_list), 'typed_list\\(bool\\) at \\(0x[a-fA-F0-9]+\\)')
 
     def test_typed_dict(self):
         self.uut = Setting('key', '1, 2: t, 3')
@@ -171,8 +176,10 @@ def test_typed_dict(self):
             self.uut = Setting('key', '1, a, 3')
             typed_dict(int, str, '')(self.uut)
 
-        self.assertEqual(repr(typed_dict(int, str, None)),
-                         'typed_dict(int, str, default=None)')
+        self.assertRegex(
+            repr(typed_dict(int, str, None)),
+            'typed_dict\\(int, str, default=None\\) at \\(0x[a-fA-F0-9]+\\)'
+        )
 
     def test_typed_ordered_dict(self):
         self.uut = Setting('key', '1, 2: t, 3')
@@ -183,8 +190,12 @@ def test_typed_ordered_dict(self):
             self.uut = Setting('key', '1, a, 3')
             typed_ordered_dict(int, str, '')(self.uut)
 
-        self.assertEqual(repr(typed_ordered_dict(int, str, None)),
-                         'typed_ordered_dict(int, str, default=None)')
+        self.assertRegex(
+            repr(typed_ordered_dict(int, str, None)),
+            # Start ignoring LineLengthBear, PyCodeStyleBear
+            'typed_ordered_dict\\(int, str, default=None\\) at \\(0x[a-fA-F0-9]+\\)'
+            # Stop ignoring
+        )
 
     def test_inherited_conversions(self):
         self.uut = Setting('key', ' 22\n', '.', strip_whitespaces=True)"
37;coala;coala;c640fe5c1c2748ed046aff4c61f2c228b1c77e3c;"Linter.py: Include memory location with __repr__

This enables the default behavior of
__repr__ method to include memory location
as coala overrides this method in Linter.py.

Closes https://github.com/coala/coala/issues/6051";"@@ -167,8 +167,8 @@ def _create_linter(klass, options):
     class LinterMeta(type):
 
         def __repr__(cls):
-            return '<{} linter class (wrapping {!r})>'.format(
-                cls.__name__, options['executable'])
+            return '<{} linter class (wrapping {!r}) at ({})>'.format(
+                cls.__name__, options['executable'], hex(id(cls)))
 
     class LinterBase(metaclass=LinterMeta):
 "
37;coala;coala;c640fe5c1c2748ed046aff4c61f2c228b1c77e3c;"Linter.py: Include memory location with __repr__

This enables the default behavior of
__repr__ method to include memory location
as coala overrides this method in Linter.py.

Closes https://github.com/coala/coala/issues/6051";"@@ -1090,15 +1090,20 @@ class LinterOtherTest(LinterTestBase):
 
     def test_metaclass_repr(self):
         uut = linter('my-tool')(self.ManualProcessingTestLinter)
-        self.assertEqual(
+        self.assertRegex(
             repr(uut),
-            ""<ManualProcessingTestLinter linter class (wrapping 'my-tool')>"")
+            '<ManualProcessingTestLinter linter class \\(wrapping ' +
+            '\'my-tool\'\\) at \\(0x[a-fA-F0-9]+\\)>'
+        )
 
         # Test also whether derivatives change the class name accordingly.
         class DerivedLinter(uut):
             pass
-        self.assertEqual(repr(DerivedLinter),
-                         ""<DerivedLinter linter class (wrapping 'my-tool')>"")
+        self.assertRegex(
+            repr(DerivedLinter),
+            '<DerivedLinter linter class \\(wrapping \'my-tool\'\\)' +
+            ' at \\(0x[a-fA-F0-9]+\\)>'
+        )
 
     def test_repr(self):
         uut = (linter(sys.executable)"
37;coala;coala;c640fe5c1c2748ed046aff4c61f2c228b1c77e3c;"Linter.py: Include memory location with __repr__

This enables the default behavior of
__repr__ method to include memory location
as coala overrides this method in Linter.py.

Closes https://github.com/coala/coala/issues/6051";"@@ -466,9 +466,15 @@ def test_get_filtered_bears(self):
 
         self.assertEqual(len(local_bears['cli']), TEST_BEARS_COUNT)
 
-        self.assertEqual(
-            [str(bear) for bear in local_bears['cli']],
-            TEST_BEAR_NAME_REPRS)
+        test_string = [str(bear) for bear in local_bears['cli']]
+        test_bear_name_reprs_regex = [test.replace(
+            '(', '\\(').replace(')', '\\)') for test in TEST_BEAR_NAME_REPRS]
+        pattern_string = [f'{test}'[
+            0:-1] + '( at \\(0x[a-fA-F0-9]+\\))?>'
+            for test in test_bear_name_reprs_regex]
+
+        for test, pattern in zip(test_string, pattern_string):
+            self.assertRegex(test, pattern)
 
         with bear_test_module():
             local_bears, global_bears = get_filtered_bears("
38;microsoft;restler-fuzzer;e63d4b2667326d809e5a9f2bc9a10b8f0b5804a4;"Fix out of memory for large OpenAPI spec. (#515)

The stream-based serialization was not implemented for every output file.
This change modifies all output files that may be large to be written
via serializing To/FromStream.

Testing: manual testing on the specification that found the issue -
https://github.com/microsoftgraph/msgraph-metadata/tree/master/openapi/v1.0";"@@ -1871,12 +1871,14 @@ let writeDependencies dependenciesFilePath dependencies (unresolvedOnly:bool) =
                    )
         |> Map.ofSeq
 
-    Microsoft.FSharpLu.Json.Compact.serializeToFile dependenciesFilePath grouped
+    Restler.Utilities.Stream.serializeToFile dependenciesFilePath grouped
 
 let writeDependenciesDebug dependenciesFilePath dependencies =
+    Restler.Utilities.Stream.serializeToFile dependenciesFilePath dependencies
 
-    Microsoft.FSharpLu.Json.Compact.serializeToFile dependenciesFilePath dependencies
-    // The below statement is present as an assertion, to check for deserialization issues for
-    // specific grammars.
-    Microsoft.FSharpLu.Json.Compact.deserializeFile<ProducerConsumerDependency list> dependenciesFilePath
+    // The below statement is present as an assertion, to check for deserialization issues
+#if TEST_GRAMMAR
+    use f = System.IO.File.OpenRead(dependenciesFilePath)
+    Microsoft.FSharpLu.Json.Compact.deserializeStream<ProducerConsumerDependency list> f
     |> ignore
+#endif"
38;microsoft;restler-fuzzer;e63d4b2667326d809e5a9f2bc9a10b8f0b5804a4;"Fix out of memory for large OpenAPI spec. (#515)

The stream-based serialization was not implemented for every output file.
This change modifies all output files that may be large to be written
via serializing To/FromStream.

Testing: manual testing on the specification that found the issue -
https://github.com/microsoftgraph/msgraph-metadata/tree/master/openapi/v1.0";"@@ -93,3 +93,10 @@ module Stream =
                 ()
             else
                 base.Write(s, offset, count)
+
+    let serializeToFile filePath data = 
+        use fs = new FileStreamWithoutPreamble(filePath, System.IO.FileMode.Create)
+        Microsoft.FSharpLu.Json.Compact.serializeToStream fs data
+        fs.Flush()
+        fs.Dispose()
+"
38;microsoft;restler-fuzzer;e63d4b2667326d809e5a9f2bc9a10b8f0b5804a4;"Fix out of memory for large OpenAPI spec. (#515)

The stream-based serialization was not implemented for every output file.
This change modifies all output files that may be large to be written
via serializing To/FromStream.

Testing: manual testing on the specification that found the issue -
https://github.com/microsoftgraph/msgraph-metadata/tree/master/openapi/v1.0";"@@ -186,17 +186,15 @@ let generateGrammarFromSwagger grammarOutputDirectoryPath (swaggerDoc, specMetad
                         userSpecifiedExamples
 
     let grammarFilePath = Path.Combine(grammarOutputDirectoryPath, Constants.DefaultJsonGrammarFileName)
+    Restler.Utilities.Stream.serializeToFile grammarFilePath grammar
 
-    use fs = new Restler.Utilities.Stream.FileStreamWithoutPreamble(grammarFilePath, IO.FileMode.Create)
-    Microsoft.FSharpLu.Json.Compact.serializeToStream fs grammar
-    fs.Flush()
-    fs.Dispose()
     // The below statement is present as an assertion, to check for deserialization issues for
     // specific grammars.
-
-    let ignoreStream = new System.IO.MemoryStream()
-    Microsoft.FSharpLu.Json.Compact.deserializeStream<GrammarDefinition>(ignoreStream)
+#if TEST_GRAMMAR
+    use f = System.IO.File.OpenRead(grammarFilePath)
+    Microsoft.FSharpLu.Json.Compact.deserializeStream<GrammarDefinition> f
     |> ignore
+#endif
 
     // If examples were discovered, create a new examples file
     if config.DiscoverExamples then"
40;r9y9;deepvoice3_pytorch;f6f87aa102d8f209106b4a611fcfc291f4882f08;"Fix RuntimeError: Caught RuntimeError in pin memory thread for device
0.""

ref #172";"@@ -340,8 +340,10 @@ def collate_fn(batch):
     s, e = 1, max_decoder_target_len + 1
     # if b_pad > 0:
     #    s, e = s - 1, e - 1
+    # NOTE: needs clone to supress RuntimeError in dataloarder...
+    # ref: https://github.com/pytorch/pytorch/issues/10756
     frame_positions = torch.arange(s, e).long().unsqueeze(0).expand(
-        len(batch), max_decoder_target_len)
+        len(batch), max_decoder_target_len).clone()
 
     # done flags
     done = np.array([_pad(np.zeros(len(x[1]) // r // downsample_step - 1),"
41;lucidrains;reformer-pytorch;9f6ad0674567a521edbb07f8bfce68da43865487;make sure rotary embeddings is only applied to original sequence in the presence of memory key / values and contextual key / values;"@@ -656,9 +656,11 @@ def rotate_every_two(x):
 def apply_rotary_pos_emb(qk, sinu_pos):
     sinu_pos = rearrange(sinu_pos, '() n (j d) -> n j d', j = 2)
     sin, cos = sinu_pos.unbind(dim = -2)
-    sin, cos = map(lambda t: repeat(t, 'b n -> b (n j)', j = 2), (sin, cos))
+    sin, cos = map(lambda t: repeat(t, 'n d -> n (d j)', j = 2), (sin, cos))
+    seq_len = sin.shape[0]
+    qk, qk_pass = qk[:, :seq_len], qk[:, seq_len:]
     qk = (qk * cos) + (rotate_every_two(qk) * sin)
-    return qk
+    return torch.cat((qk, qk_pass), dim = 1)
 
 # reformer lm
 "
41;lucidrains;reformer-pytorch;9f6ad0674567a521edbb07f8bfce68da43865487;make sure rotary embeddings is only applied to original sequence in the presence of memory key / values and contextual key / values;"@@ -3,7 +3,7 @@
 setup(
   name = 'reformer_pytorch',
   packages = find_packages(exclude=['examples', 'pretraining']),
-  version = '1.4.0',
+  version = '1.4.1',
   license='MIT',
   description = 'Reformer, the Efficient Transformer, Pytorch',
   author = 'Phil Wang',"
41;lucidrains;reformer-pytorch;b7dab93dd634380a655ed65c1fda03e83e3c1a96;fix bug with memory key/values;"@@ -512,7 +512,7 @@ def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, con
         b, t, e, h, dh, m, l_h = *x.shape, self.heads, self.dim_head, self.num_mem_kv, self.n_local_attn_heads
 
         mem_kv = default(self.mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))
-        mem = mem_kv.expand(-1, m, -1)
+        mem = mem_kv.expand(b, m, -1)
 
         keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))
         c = keys.shape[1]
@@ -699,7 +699,7 @@ def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, dim_head = No
         elif fixed_position_emb:
             self.pos_emb = FixedPositionalEmbedding(emb_dim)
         else:
-            axial_position_shape = default(axial_position_shape, (max_seq_len // bucket_size, bucket_size))
+            axial_position_shape = default(axial_position_shape, (math.ceil(max_seq_len / bucket_size), bucket_size))
             self.pos_emb = AxialPositionalEmbedding(emb_dim, axial_position_shape)
 
         self.reformer = Reformer(dim, depth, max_seq_len, heads = heads, dim_head = dim_head, bucket_size = bucket_size, n_hashes = n_hashes, ff_chunks = ff_chunks, attn_chunks = attn_chunks, causal = causal, weight_tie = weight_tie, lsh_dropout = lsh_dropout, ff_mult = ff_mult, ff_activation = ff_activation, ff_glu = ff_glu, ff_dropout = ff_dropout, post_attn_dropout = 0., layer_dropout = layer_dropout, random_rotations_per_head = random_rotations_per_head, twin_attention = twin_attention, use_scale_norm = use_scale_norm, use_rezero = use_rezero, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, reverse_thres = reverse_thres, num_mem_kv = num_mem_kv, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads, pkm_layers = pkm_layers, pkm_num_keys = pkm_num_keys)"
41;lucidrains;reformer-pytorch;b7dab93dd634380a655ed65c1fda03e83e3c1a96;fix bug with memory key/values;"@@ -3,7 +3,7 @@
 setup(
   name = 'reformer_pytorch',
   packages = find_packages(exclude=['examples', 'pretraining']),
-  version = '1.1.3',
+  version = '1.1.4',
   license='MIT',
   description = 'Reformer, the Efficient Transformer, Pytorch',
   author = 'Phil Wang',"
42;andersbll;neural_artistic_style;13b95ddbfd727a3dd02e16f96ae29f5263cced40;reduce memory footprint by calculating gram matrices during bprop;"@@ -126,14 +126,10 @@ def _update(self):
         # Forward propagation
         next_x = self.x.array
         x_feats = [None]*len(self.layers)
-        x_grams = [None]*len(self.layers)
         for l, layer in enumerate(self.layers):
             next_x = layer.fprop(next_x)
-            if self.subject_weights[l] > 0:
-                x_feats[l] = next_x
-            if self.style_weights[l] > 0:
+            if self.subject_weights[l] > 0 or self.style_weights[l] > 0:
                 x_feats[l] = next_x
-                x_grams[l] = gram_matrix(next_x)
 
         # Backward propagation
         grad = ca.zeros_like(next_x)
@@ -146,7 +142,7 @@ def _update(self):
                 grad += diff * weight
                 loss += 0.5*weight*ca.sum(diff**2)
             if self.style_weights[l] > 0:
-                diff = x_grams[l] - self.style_grams[l]
+                diff = gram_matrix(x_feats[l]) - self.style_grams[l]
                 n_channels = diff.shape[0]
                 x_feat = ca.reshape(x_feats[l], (n_channels, -1))
                 style_grad = ca.reshape(ca.dot(diff, x_feat), x_feats[l].shape)"
42;google;youtube-8m;19a075ede1ede560489ef228e02f14e0cb17855f;Improve inference memory usage.;"@@ -319,6 +319,9 @@ def set_up_init_ops(variables):
               float(preds[idx]) for idx in range(1, len(preds), 2)
           ]
           for cls, score in zip(pred_cls_ids, pred_cls_scores):
+            if not whitelisted_cls_mask[cls]:
+              # Skip non-whitelisted classes.
+              continue
             if cls not in heaps:
               heaps[cls] = []
             if len(heaps[cls]) >= FLAGS.segment_max_pred:"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,2 +1,3 @@
 # Port to serve app on.
-PORT = 5000
\ No newline at end of file
+PORT = 5000
+LOCAL_REDIS_URL = redis://localhost:6379"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -20,15 +20,17 @@ pytest-cov = ""*""
 responses = ""*""
 
 [packages]
+aiocache = {extras = [""redis""],version = ""*""}
+aiofiles = ""*""
 aiohttp = ""*""
 asyncache = ""*""
 cachetools = ""*""
 dataclasses = {version = ""*"",markers = ""python_version<'3.7'""}
 fastapi = ""*""
 gunicorn = ""*""
 idna_ssl = {version = ""*"",markers = ""python_version<'3.7'""}
+pydantic = {extras = [""dotenv""],version = ""*""}
 python-dateutil = ""*""
-python-dotenv = ""*""
 requests = ""*""
 uvicorn = ""*""
 "
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,7 +1,7 @@
 {
     ""_meta"": {
         ""hash"": {
-            ""sha256"": ""9c469c96db1ae3a7e4c239d3a9c7028ecf49a0ab5e3ea50aed304ea2ab1a113e""
+            ""sha256"": ""596c0a497d4f2cfa9e3a3e8b38b2cf018ab3b6d9a26f04a949ced6b025e05f62""
         },
         ""pipfile-spec"": 6,
         ""requires"": {
@@ -16,6 +16,25 @@
         ]
     },
     ""default"": {
+        ""aiocache"": {
+            ""extras"": [
+                ""redis""
+            ],
+            ""hashes"": [
+                ""sha256:e55c7caaa5753794fd301c3a2e592737fa1d036db9f8d04ae154facdfb48a157"",
+                ""sha256:f2ebe0b05cec45782e7b5ea0bb74640f157dd4bb1028b4565364dda9fe33be7f""
+            ],
+            ""index"": ""pypi"",
+            ""version"": ""==0.11.1""
+        },
+        ""aiofiles"": {
+            ""hashes"": [
+                ""sha256:377fdf7815cc611870c59cbd07b68b180841d2a2b79812d8c218be02448c2acb"",
+                ""sha256:98e6bcfd1b50f97db4980e182ddd509b7cc35909e903a8fe50d8849e02d815af""
+            ],
+            ""index"": ""pypi"",
+            ""version"": ""==0.5.0""
+        },
         ""aiohttp"": {
             ""hashes"": [
                 ""sha256:1e984191d1ec186881ffaed4581092ba04f7c61582a177b187d3a2f07ed9719e"",
@@ -34,6 +53,13 @@
             ""index"": ""pypi"",
             ""version"": ""==3.6.2""
         },
+        ""aioredis"": {
+            ""hashes"": [
+                ""sha256:15f8af30b044c771aee6787e5ec24694c048184c7b9e54c3b60c750a4b93273a"",
+                ""sha256:b61808d7e97b7cd5a92ed574937a079c9387fdadd22bfbfa7ad2fd319ecc26e3""
+            ],
+            ""version"": ""==1.3.1""
+        },
         ""async-timeout"": {
             ""hashes"": [
                 ""sha256:0c3c816a028d47f659d6ff5c745cb2acf1f966da1fe5c19c77a70282b25f4c5f"",
@@ -79,10 +105,10 @@
         },
         ""click"": {
             ""hashes"": [
-                ""sha256:8a18b4ea89d8820c5d0c7da8a64b2c324b4dabb695804dbfea19b9be9d88c0cc"",
-                ""sha256:e345d143d80bf5ee7534056164e5e112ea5e22716bbb1ce727941f4c8b471b9a""
+                ""sha256:d2b5255c7c6349bc1bd1e59e08cd12acbbd63ce649f2588755783aa94dfb6b1a"",
+                ""sha256:dacca89f4bfadd5de3d7489b7c8a566eee0d3676333fbb50030263894c38c0dc""
             ],
-            ""version"": ""==7.1.1""
+            ""version"": ""==7.1.2""
         },
         ""dataclasses"": {
             ""hashes"": [
@@ -116,6 +142,51 @@
             ],
             ""version"": ""==0.9.0""
         },
+        ""hiredis"": {
+            ""hashes"": [
+                ""sha256:01b577f84c20ecc9c07fc4c184231b08e3c3942de096fa99978e053de231c423"",
+                ""sha256:01ff0900134166961c9e339df77c33b72f7edc5cb41739f0babcd9faa345926e"",
+                ""sha256:03ed34a13316d0c34213c4fd46e0fa3a5299073f4d4f08e93fed8c2108b399b3"",
+                ""sha256:040436e91df5143aff9e0debb49530d0b17a6bd52200ce568621c31ef581b10d"",
+                ""sha256:091eb38fbf968d1c5b703e412bbbd25f43a7967d8400842cee33a5a07b33c27b"",
+                ""sha256:102f9b9dc6ed57feb3a7c9bdf7e71cb7c278fe8df1edfcfe896bc3e0c2be9447"",
+                ""sha256:2b4b392c7e3082860c8371fab3ae762139090f9115819e12d9f56060f9ede05d"",
+                ""sha256:2c9cc0b986397b833073f466e6b9e9c70d1d4dc2c2c1b3e9cae3a23102ff296c"",
+                ""sha256:2fa65a9df683bca72073cd77709ddeb289ea2b114d3775d225fbbcc5faf808c5"",
+                ""sha256:38437a681f17c975fd22349e72c29bc643f8e7eb2d6dc5df419eac59afa4d7ce"",
+                ""sha256:3b3428fa3cf1ee178807b52c9bee8950ab94cd4eaa9bfae8c1bbae3c49501d34"",
+                ""sha256:3dd8c2fae7f5494978facb0e93297dd627b1a3f536f3b070cf0a7d9157a07dcb"",
+                ""sha256:4414a96c212e732723b5c3d7c04d386ebbb2ec359e1de646322cbc3f875cbd0d"",
+                ""sha256:48c627581ad4ef60adbac980981407939acf13a0e18f093502c7b542223c4f19"",
+                ""sha256:4a60e71625a2d78d8ab84dfb2fa2cfd9458c964b6e6c04fea76d9ade153fb371"",
+                ""sha256:585ace09f434e43d8a8dbeb366865b1a044d7c06319b3c7372a0a00e63b860f4"",
+                ""sha256:74b364b3f06c9cf0a53f7df611045bc9437ed972a283fa1f0b12537236d23ddc"",
+                ""sha256:75c65c3850e89e9daa68d1b9bedd5806f177d60aa5a7b0953b4829481cfc1f72"",
+                ""sha256:7f052de8bf744730a9120dbdc67bfeb7605a01f69fb8e7ba5c475af33c24e145"",
+                ""sha256:8113a7d5e87ecf57cd4ae263cc9e429adb9a3e59f5a7768da5d3312a8d0a051a"",
+                ""sha256:84857ce239eb8ed191ac78e77ff65d52902f00f30f4ee83bf80eb71da73b70e6"",
+                ""sha256:8644a48ddc4a40b3e3a6b9443f396c2ee353afb2d45656c4fc68d04a82e8e3f7"",
+                ""sha256:936aa565e673536e8a211e43ec43197406f24cd1f290138bd143765079c8ba00"",
+                ""sha256:9afeb88c67bbc663b9f27385c496da056d06ad87f55df6e393e1516cfecb0461"",
+                ""sha256:9d62cc7880110e4f83b0a51d218f465d3095e2751fbddd34e553dbd106a929ff"",
+                ""sha256:a1fadd062fc8d647ff39220c57ea2b48c99bb73f18223828ec97f88fc27e7898"",
+                ""sha256:a7754a783b1e5d6f627c19d099b178059c62f782ab62b4d8ba165b9fbc2ee34c"",
+                ""sha256:aa59dd63bb3f736de4fc2d080114429d5d369dfb3265f771778e8349d67a97a4"",
+                ""sha256:ae2ee0992f8de249715435942137843a93db204dd7db1e7cc9bdc5a8436443e8"",
+                ""sha256:b36842d7cf32929d568f37ec5b3173b72b2ec6572dec4d6be6ce774762215aee"",
+                ""sha256:bcbf9379c553b5facc6c04c1e5569b44b38ff16bcbf354676287698d61ee0c92"",
+                ""sha256:cbccbda6f1c62ab460449d9c85fdf24d0d32a6bf45176581151e53cc26a5d910"",
+                ""sha256:d0caf98dfb8af395d6732bd16561c0a2458851bea522e39f12f04802dbf6f502"",
+                ""sha256:d6456afeddba036def1a36d8a2758eca53202308d83db20ab5d0b66590919627"",
+                ""sha256:dbaef9a21a4f10bc281684ee4124f169e62bb533c2a92b55f8c06f64f9af7b8f"",
+                ""sha256:dce84916c09aaece006272b37234ae84a8ed13abb3a4d341a23933b8701abfb5"",
+                ""sha256:eb8c9c8b9869539d58d60ff4a28373a22514d40495911451343971cb4835b7a9"",
+                ""sha256:efc98b14ee3a8595e40b1425e8d42f5fd26f11a7b215a81ef9259068931754f4"",
+                ""sha256:fa2dc05b87d97acc1c6ae63f3e0f39eae5246565232484b08db6bf2dc1580678"",
+                ""sha256:fe7d6ce9f6a5fbe24f09d95ea93e9c7271abc4e1565da511e1449b107b4d7848""
+            ],
+            ""version"": ""==1.0.1""
+        },
         ""httptools"": {
             ""hashes"": [
                 ""sha256:0a4b1b2012b28e68306575ad14ad5e9120b34fccd02a81eb08838d7e3bbb48be"",
@@ -172,26 +243,30 @@
             ""version"": ""==4.7.5""
         },
         ""pydantic"": {
-            ""hashes"": [
-                ""sha256:0b7aadfa1de28057656064e04d9f018d1b186fe2a8e953a2fb41545873b7cf95"",
-                ""sha256:0f61e67291b99a927816558a218a4e794db72a33621c836e63d12613a2202cd4"",
-                ""sha256:20946280c750753b3e3177c748825ef189d7ab86c514f6a0b118621110d5f0d3"",
-                ""sha256:22139ee446992c222977ac0a9269c4da2e9ecc1834f84804ebde008a4649b929"",
-                ""sha256:3c0f39e884d7a3572d5cc8322b0fe9bf66114283e22e05a5c4b8961c19588945"",
-                ""sha256:446ce773a552a2cb90065d4aa645e16fa7494369b5f0d199e4d41a992a98204d"",
-                ""sha256:475e6606873e40717cc3b0eebc7d1101cbfc774e01dadeeea24c121eb5826b86"",
-                ""sha256:66124752662de0479a9d0c17bdebdc8a889bccad8846626fb66d8669e8eafb63"",
-                ""sha256:896637b7d8e4cdc0bcee1704fcadacdd167c35ac29f02a4395fce7a033925f26"",
-                ""sha256:9af44d06db33896a2176603c9cb876df3a60297a292a24d3018956a910cc1402"",
-                ""sha256:9e46fac8a4674db0777fd0133aa56817e1481beee50971bab39dded7639f9b2b"",
-                ""sha256:ae206e103e976c40ec294cd6c8fcbfbdaced3ab9b736bc53d03fa11b5aaa1628"",
-                ""sha256:b11d0bd7ecf41098894e8777ee623c29554dbaa37e862c51bcc5a2b950d1bf77"",
-                ""sha256:d73070028f7b046a5b2e611a9799c238d7bd245f8fe30f4ad7ff29ddb63aac40"",
-                ""sha256:ddedcdf9d5c24939578449a8e099ceeec3b3d76243fc143aff63ebf6d5aade10"",
-                ""sha256:e08e21f4d5395ac17cde19de26be63fb16fb870f0cfde1481ddc22d5e2353548"",
-                ""sha256:e6239199b363bc53262bcb57f1441206d4b2d46b392eccba2213d8358d6e284a""
-            ],
-            ""version"": ""==1.5""
+            ""extras"": [
+                ""dotenv""
+            ],
+            ""hashes"": [
+                ""sha256:0a1cdf24e567d42dc762d3fed399bd211a13db2e8462af9dfa93b34c41648efb"",
+                ""sha256:2007eb062ed0e57875ce8ead12760a6e44bf5836e6a1a7ea81d71eeecf3ede0f"",
+                ""sha256:20a15a303ce1e4d831b4e79c17a4a29cb6740b12524f5bba3ea363bff65732bc"",
+                ""sha256:2a6904e9f18dea58f76f16b95cba6a2f20b72d787abd84ecd67ebc526e61dce6"",
+                ""sha256:3714a4056f5bdbecf3a41e0706ec9b228c9513eee2ad884dc2c568c4dfa540e9"",
+                ""sha256:473101121b1bd454c8effc9fe66d54812fdc128184d9015c5aaa0d4e58a6d338"",
+                ""sha256:68dece67bff2b3a5cc188258e46b49f676a722304f1c6148ae08e9291e284d98"",
+                ""sha256:70f27d2f0268f490fe3de0a9b6fca7b7492b8fd6623f9fecd25b221ebee385e3"",
+                ""sha256:8433dbb87246c0f562af75d00fa80155b74e4f6924b0db6a2078a3cd2f11c6c4"",
+                ""sha256:8be325fc9da897029ee48d1b5e40df817d97fe969f3ac3fd2434ba7e198c55d5"",
+                ""sha256:93b9f265329d9827f39f0fca68f5d72cc8321881cdc519a1304fa73b9f8a75bd"",
+                ""sha256:9be755919258d5d168aeffbe913ed6e8bd562e018df7724b68cabdee3371e331"",
+                ""sha256:ab863853cb502480b118187d670f753be65ec144e1654924bec33d63bc8b3ce2"",
+                ""sha256:b96ce81c4b5ca62ab81181212edfd057beaa41411cd9700fbcb48a6ba6564b4e"",
+                ""sha256:da8099fca5ee339d5572cfa8af12cf0856ae993406f0b1eb9bb38c8a660e7416"",
+                ""sha256:e2c753d355126ddd1eefeb167fa61c7037ecd30b98e7ebecdc0d1da463b4ea09"",
+                ""sha256:f0018613c7a0d19df3240c2a913849786f21b6539b9f23d85ce4067489dfacfa""
+            ],
+            ""index"": ""pypi"",
+            ""version"": ""==1.5.1""
         },
         ""python-dateutil"": {
             ""hashes"": [
@@ -206,7 +281,6 @@
                 ""sha256:25c0ff1a3e12f4bde8d592cc254ab075cfe734fc5dd989036716fd17ee7e5ec7"",
                 ""sha256:3b9909bc96b0edc6b01586e1eed05e71174ef4e04c71da5786370cebea53ad74""
             ],
-            ""index"": ""pypi"",
             ""version"": ""==0.13.0""
         },
         ""requests"": {
@@ -240,11 +314,11 @@
         },
         ""uvicorn"": {
             ""hashes"": [
-                ""sha256:0f58170165c4495f563d8224b2f415a0829af0412baa034d6f777904613087fd"",
-                ""sha256:6fdaf8e53bf1b2ddf0fe9ed06079b5348d7d1d87b3365fe2549e6de0d49e631c""
+                ""sha256:50577d599775dac2301bac8bd5b540d19a9560144143c5bdab13cba92783b6e7"",
+                ""sha256:596eaa8645b6dbc24d6610e335f8ddf5f925b4c4b86fdc7146abb0bf0da65d17""
             ],
             ""index"": ""pypi"",
-            ""version"": ""==0.11.3""
+            ""version"": ""==0.11.5""
         },
         ""uvloop"": {
             ""hashes"": [
@@ -321,10 +395,10 @@
         },
         ""astroid"": {
             ""hashes"": [
-                ""sha256:71ea07f44df9568a75d0f354c49143a4575d90645e9fead6dfb52c26a85ed13a"",
-                ""sha256:840947ebfa8b58f318d42301cf8c0a20fd794a33b61cc4638e28e9e61ba32f42""
+                ""sha256:29fa5d46a2404d01c834fcb802a3943685f1fc538eb2a02a161349f5505ac196"",
+                ""sha256:2fecea42b20abb1922ed65c7b5be27edfba97211b04b2b6abc6a43549a024ea6""
             ],
-            ""version"": ""==2.3.3""
+            ""version"": ""==2.4.0""
         },
         ""async-asgi-testclient"": {
             ""hashes"": [
@@ -388,10 +462,10 @@
         },
         ""click"": {
             ""hashes"": [
-                ""sha256:8a18b4ea89d8820c5d0c7da8a64b2c324b4dabb695804dbfea19b9be9d88c0cc"",
-                ""sha256:e345d143d80bf5ee7534056164e5e112ea5e22716bbb1ce727941f4c8b471b9a""
+                ""sha256:d2b5255c7c6349bc1bd1e59e08cd12acbbd63ce649f2588755783aa94dfb6b1a"",
+                ""sha256:dacca89f4bfadd5de3d7489b7c8a566eee0d3676333fbb50030263894c38c0dc""
             ],
-            ""version"": ""==7.1.1""
+            ""version"": ""==7.1.2""
         },
         ""coverage"": {
             ""hashes"": [
@@ -596,11 +670,11 @@
         },
         ""pylint"": {
             ""hashes"": [
-                ""sha256:3db5468ad013380e987410a8d6956226963aed94ecb5f9d3a28acca6d9ac36cd"",
-                ""sha256:886e6afc935ea2590b462664b161ca9a5e40168ea99e5300935f6591ad467df4""
+                ""sha256:588e114e3f9a1630428c35b7dd1c82c1c93e1b0e78ee312ae4724c5e1a1e0245"",
+                ""sha256:bd556ba95a4cf55a1fc0004c00cf4560b1e70598a54a74c6904d933c8f3bd5a8""
             ],
             ""index"": ""pypi"",
-            ""version"": ""==2.4.4""
+            ""version"": ""==2.5.0""
         },
         ""pyparsing"": {
             ""hashes"": [
@@ -619,11 +693,11 @@
         },
         ""pytest-asyncio"": {
             ""hashes"": [
-                ""sha256:9fac5100fd716cbecf6ef89233e8590a4ad61d729d1732e0a96b84182df1daaf"",
-                ""sha256:d734718e25cfc32d2bf78d346e99d33724deeba774cc4afdf491530c6184b63b""
+                ""sha256:6096d101a1ae350d971df05e25f4a8b4d3cd13ffb1b32e42d902ac49670d2bfa"",
+                ""sha256:c54866f3cf5dd2063992ba2c34784edae11d3ed19e006d220a3cf0bfc4191fcb""
             ],
             ""index"": ""pypi"",
-            ""version"": ""==0.10.0""
+            ""version"": ""==0.11.0""
         },
         ""pytest-cov"": {
             ""hashes"": [
@@ -685,11 +759,11 @@
         },
         ""responses"": {
             ""hashes"": [
-                ""sha256:0474ce3c897fbcc1aef286117c93499882d5c440f06a805947e4b1cb5ab3d474"",
-                ""sha256:f83613479a021e233e82d52ffb3e2e0e2836d24b0cc88a0fa31978789f78d0e5""
+                ""sha256:1a78bc010b20a5022a2c0cb76b8ee6dc1e34d887972615ebd725ab9a166a4960"",
+                ""sha256:3d596d0be06151330cb230a2d630717ab20f7a81f205019481e206eb5db79915""
             ],
             ""index"": ""pypi"",
-            ""version"": ""==0.10.12""
+            ""version"": ""==0.10.14""
         },
         ""six"": {
             ""hashes"": [
@@ -761,9 +835,9 @@
         },
         ""wrapt"": {
             ""hashes"": [
-                ""sha256:565a021fd19419476b9362b05eeaa094178de64f8361e44468f9e9d7843901e1""
+                ""sha256:b62ffa81fb85f4332a4f609cab4ac40709470da05643a082ec1eb88e6d9b97d7""
             ],
-            ""version"": ""==1.11.2""
+            ""version"": ""==1.12.1""
         },
         ""zipp"": {
             ""hashes"": ["
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -0,0 +1,52 @@
+""""""app.caches.py""""""
+import functools
+import logging
+from typing import Union
+
+import aiocache
+
+from .config import get_settings
+
+LOGGER = logging.getLogger(name=""app.caches"")
+
+SETTINGS = get_settings()
+
+if SETTINGS.rediscloud_url:
+    REDIS_URL = SETTINGS.rediscloud_url
+    LOGGER.info(""Using Rediscloud"")
+else:
+    REDIS_URL = SETTINGS.local_redis_url
+    LOGGER.info(""Using Local Redis"")
+
+
+@functools.lru_cache()
+def get_cache(namespace) -> Union[aiocache.RedisCache, aiocache.SimpleMemoryCache]:
+    """"""Retunr """"""
+    if REDIS_URL:
+        LOGGER.info(""using RedisCache"")
+        return aiocache.RedisCache(
+            endpoint=REDIS_URL.host,
+            port=REDIS_URL.port,
+            password=REDIS_URL.password,
+            namespace=namespace,
+            create_connection_timeout=5,
+        )
+    LOGGER.info(""using SimpleMemoryCache"")
+    return aiocache.SimpleMemoryCache(namespace=namespace)
+
+
+async def check_cache(data_id: str, namespace: str = None):
+    """"""Check the data of a cache given an id.""""""
+    cache = get_cache(namespace)
+    result = await cache.get(data_id, None)
+    LOGGER.info(f""{data_id} cache pulled"")
+    await cache.close()
+    return result
+
+
+async def load_cache(data_id: str, data, namespace: str = None, cache_life: int = 3600):
+    """"""Load data into the cache.""""""
+    cache = get_cache(namespace)
+    await cache.set(data_id, data, ttl=cache_life)
+    LOGGER.info(f""{data_id} cache loaded"")
+    await cache.close()"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -0,0 +1,29 @@
+""""""app.config.py""""""
+import functools
+import logging
+
+from pydantic import AnyUrl, BaseSettings
+
+CFG_LOGGER = logging.getLogger(""app.config"")
+
+
+class _Settings(BaseSettings):
+    port: int = 5000
+    rediscloud_url: AnyUrl = None
+    local_redis_url: AnyUrl = None
+
+
+@functools.lru_cache()
+def get_settings(**kwargs) -> BaseSettings:
+    """"""
+    Read settings from the environment or `.env` file.
+    https://pydantic-docs.helpmanual.io/usage/settings/#dotenv-env-support
+
+    Usage:
+        import app.config
+
+        settings = app.config.get_settings(_env_file="""")
+        port_number = settings.port
+    """"""
+    CFG_LOGGER.info(""Loading Config settings from Environment ..."")
+    return _Settings(**kwargs)"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,10 +0,0 @@
-""""""app.config.settings.py""""""
-import os
-
-# Load enviroment variables from .env file.
-from dotenv import load_dotenv
-
-load_dotenv()
-
-# The port to serve the app application on.
-PORT = int(os.getenv(""PORT"", ""5000""))"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,28 +1,56 @@
 """"""app.io.py""""""
 import json
 import pathlib
-from typing import Dict, Union
+from typing import Dict, List, Union
+
+import aiofiles
 
 HERE = pathlib.Path(__file__)
 DATA = HERE.joinpath("".."", ""data"").resolve()
 
 
 def save(
-    name: str, content: Union[str, Dict], write_mode: str = ""w"", indent: int = 2, **json_dumps_kwargs
+    name: str, content: Union[str, Dict, List], write_mode: str = ""w"", indent: int = 2, **json_dumps_kwargs
 ) -> pathlib.Path:
     """"""Save content to a file. If content is a dictionary, use json.dumps().""""""
     path = DATA / name
-    if isinstance(content, dict):
+    if isinstance(content, (dict, list)):
         content = json.dumps(content, indent=indent, **json_dumps_kwargs)
     with open(DATA / name, mode=write_mode) as f_out:
         f_out.write(content)
     return path
 
 
-def load(name: str, **json_kwargs) -> Union[str, Dict]:
+def load(name: str, **json_kwargs) -> Union[str, Dict, List]:
     """"""Loads content from a file. If file ends with '.json', call json.load() and return a Dictionary.""""""
     path = DATA / name
     with open(path) as f_in:
         if path.suffix == "".json"":
             return json.load(f_in, **json_kwargs)
         return f_in.read()
+
+
+class AIO:
+    """"""Asynsc compatible file io operations.""""""
+
+    @classmethod
+    async def save(
+        cls, name: str, content: Union[str, Dict, List], write_mode: str = ""w"", indent: int = 2, **json_dumps_kwargs
+    ):
+        """"""Save content to a file. If content is a dictionary, use json.dumps().""""""
+        path = DATA / name
+        if isinstance(content, (dict, list)):
+            content = json.dumps(content, indent=indent, **json_dumps_kwargs)
+        async with aiofiles.open(DATA / name, mode=write_mode) as f_out:
+            await f_out.write(content)
+        return path
+
+    @classmethod
+    async def load(cls, name: str, **json_kwargs) -> Union[str, Dict, List]:
+        """"""Loads content from a file. If file ends with '.json', call json.load() and return a Dictionary.""""""
+        path = DATA / name
+        async with aiofiles.open(path) as f_in:
+            content = await f_in.read()
+        if path.suffix == "".json"":
+            content = json.loads(content, **json_kwargs)
+        return content"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -2,7 +2,6 @@
 app.main.py
 """"""
 import logging
-import os
 
 import pydantic
 import uvicorn
@@ -11,6 +10,7 @@
 from fastapi.middleware.gzip import GZipMiddleware
 from fastapi.responses import JSONResponse
 
+from .config import get_settings
 from .data import data_source
 from .routers import V1, V2
 from .utils.httputils import setup_client_session, teardown_client_session
@@ -20,6 +20,8 @@
 # ############
 LOGGER = logging.getLogger(""api"")
 
+SETTINGS = get_settings()
+
 APP = FastAPI(
     title=""Coronavirus Tracker"",
     description=(
@@ -93,5 +95,5 @@ async def handle_validation_error(
 # Running of app.
 if __name__ == ""__main__"":
     uvicorn.run(
-        ""app.main:APP"", host=""127.0.0.1"", port=int(os.getenv(""PORT"", ""5000"")), log_level=""info"",
+        ""app.main:APP"", host=""127.0.0.1"", port=SETTINGS.port, log_level=""info"",
     )"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -8,6 +8,7 @@
 from asyncache import cached
 from cachetools import TTLCache
 
+from ...caches import check_cache, load_cache
 from ...coordinates import Coordinates
 from ...location import TimelinedLocation
 from ...timeline import Timeline
@@ -19,6 +20,7 @@
 LOGGER = logging.getLogger(""services.location.jhu"")
 PID = os.getpid()
 
+
 class JhuLocationService(LocationService):
     """"""
     Service for retrieving locations from Johns Hopkins CSSE (https://github.com/CSSEGISandData/COVID-19).
@@ -44,7 +46,7 @@ async def get(self, loc_id):  # pylint: disable=arguments-differ
 )
 
 
-@cached(cache=TTLCache(maxsize=1024, ttl=3600))
+@cached(cache=TTLCache(maxsize=1024, ttl=1800))
 async def get_category(category):
     """"""
     Retrieves the data for the provided category. The data is cached for 1 hour.
@@ -56,68 +58,78 @@ async def get_category(category):
     category = category.lower()
     data_id = f""jhu.{category}""
 
-    # URL to request data from.
-    url = BASE_URL + ""time_series_covid19_%s_global.csv"" % category
+    # check shared cache
+    cache_results = await check_cache(data_id)
+    if cache_results:
+        LOGGER.info(f""{data_id} using shared cache results"")
+        results = cache_results
+    else:
+        LOGGER.info(f""{data_id} shared cache empty"")
+        # URL to request data from.
+        url = BASE_URL + ""time_series_covid19_%s_global.csv"" % category
 
-    # Request the data
-    LOGGER.info(f""{data_id} Requesting data..."")
-    async with httputils.CLIENT_SESSION.get(url) as response:
-        text = await response.text()
+        # Request the data
+        LOGGER.info(f""{data_id} Requesting data..."")
+        async with httputils.CLIENT_SESSION.get(url) as response:
+            text = await response.text()
 
-    LOGGER.debug(f""{data_id} Data received"")
+        LOGGER.debug(f""{data_id} Data received"")
 
-    # Parse the CSV.
-    data = list(csv.DictReader(text.splitlines()))
-    LOGGER.debug(f""{data_id} CSV parsed"")
+        # Parse the CSV.
+        data = list(csv.DictReader(text.splitlines()))
+        LOGGER.debug(f""{data_id} CSV parsed"")
 
-    # The normalized locations.
-    locations = []
+        # The normalized locations.
+        locations = []
 
-    for item in data:
-        # Filter out all the dates.
-        dates = dict(filter(lambda element: date_util.is_date(element[0]), item.items()))
+        for item in data:
+            # Filter out all the dates.
+            dates = dict(filter(lambda element: date_util.is_date(element[0]), item.items()))
 
-        # Make location history from dates.
-        history = {date: int(amount or 0) for date, amount in dates.items()}
+            # Make location history from dates.
+            history = {date: int(amount or 0) for date, amount in dates.items()}
 
-        # Country for this location.
-        country = item[""Country/Region""]
+            # Country for this location.
+            country = item[""Country/Region""]
 
-        # Latest data insert value.
-        latest = list(history.values())[-1]
+            # Latest data insert value.
+            latest = list(history.values())[-1]
+
+            # Normalize the item and append to locations.
+            locations.append(
+                {
+                    # General info.
+                    ""country"": country,
+                    ""country_code"": countries.country_code(country),
+                    ""province"": item[""Province/State""],
+                    # Coordinates.
+                    ""coordinates"": {""lat"": item[""Lat""], ""long"": item[""Long""],},
+                    # History.
+                    ""history"": history,
+                    # Latest statistic.
+                    ""latest"": int(latest or 0),
+                }
+            )
+        LOGGER.debug(f""{data_id} Data normalized"")
+
+        # Latest total.
+        latest = sum(map(lambda location: location[""latest""], locations))
+
+        # Return the final data.
+        results = {
+            ""locations"": locations,
+            ""latest"": latest,
+            ""last_updated"": datetime.utcnow().isoformat() + ""Z"",
+            ""source"": ""https://github.com/ExpDev07/coronavirus-tracker-api"",
+        }
+        # save the results to distributed cache
+        await load_cache(data_id, results)
 
-        # Normalize the item and append to locations.
-        locations.append(
-            {
-                # General info.
-                ""country"": country,
-                ""country_code"": countries.country_code(country),
-                ""province"": item[""Province/State""],
-                # Coordinates.
-                ""coordinates"": {""lat"": item[""Lat""], ""long"": item[""Long""],},
-                # History.
-                ""history"": history,
-                # Latest statistic.
-                ""latest"": int(latest or 0),
-            }
-        )
-    LOGGER.debug(f""{data_id} Data normalized"")
-
-    # Latest total.
-    latest = sum(map(lambda location: location[""latest""], locations))
-
-    # Return the final data.
-    results = {
-        ""locations"": locations,
-        ""latest"": latest,
-        ""last_updated"": datetime.utcnow().isoformat() + ""Z"",
-        ""source"": ""https://github.com/ExpDev07/coronavirus-tracker-api"",
-    }
     LOGGER.info(f""{data_id} results:\n{pf(results, depth=1)}"")
     return results
 
 
-@cached(cache=TTLCache(maxsize=1024, ttl=3600))
+@cached(cache=TTLCache(maxsize=1024, ttl=1800))
 async def get_locations():
     """"""
     Retrieves the locations from the categories. The locations are cached for 1 hour."
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,582 +0,0 @@
-[MASTER]
-
-# A comma-separated list of package or module names from where C extensions may
-# be loaded. Extensions are loading into the active Python interpreter and may
-# run arbitrary code.
-extension-pkg-whitelist=pydantic
-
-# Add files or directories to the blacklist. They should be base names, not
-# paths.
-ignore=CVS
-
-# Add files or directories matching the regex patterns to the blacklist. The
-# regex matches against base names, not paths.
-ignore-patterns=
-
-# Python code to execute, usually for sys.path manipulation such as
-# pygtk.require().
-#init-hook=
-
-# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
-# number of processors available to use.
-jobs=1
-
-# Control the amount of potential inferred values when inferring a single
-# object. This can help the performance when dealing with large functions or
-# complex, nested conditions.
-limit-inference-results=100
-
-# List of plugins (as comma separated values of python module names) to load,
-# usually to register additional checkers.
-load-plugins=
-
-# Pickle collected data for later comparisons.
-persistent=yes
-
-# Specify a configuration file.
-#rcfile=
-
-# When enabled, pylint would attempt to guess common misconfiguration and emit
-# user-friendly hints instead of false-positive error messages.
-suggestion-mode=yes
-
-# Allow loading of arbitrary C extensions. Extensions are imported into the
-# active Python interpreter and may run arbitrary code.
-unsafe-load-any-extension=no
-
-
-[MESSAGES CONTROL]
-
-# Only show warnings with the listed confidence levels. Leave empty to show
-# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED.
-confidence=
-
-# Disable the message, report, category or checker with the given id(s). You
-# can either give multiple identifiers separated by comma (,) or put this
-# option multiple times (only on the command line, not in the configuration
-# file where it should appear only once). You can also use ""--disable=all"" to
-# disable everything first and then reenable specific checks. For example, if
-# you want to run only the similarities checker, you can use ""--disable=all
-# --enable=similarities"". If you want to run only the classes checker, but have
-# no Warning level messages displayed, use ""--disable=all --enable=classes
-# --disable=W"".
-disable=print-statement,
-        parameter-unpacking,
-        unpacking-in-except,
-        old-raise-syntax,
-        backtick,
-        long-suffix,
-        old-ne-operator,
-        old-octal-literal,
-        import-star-module-level,
-        non-ascii-bytes-literal,
-        raw-checker-failed,
-        bad-inline-option,
-        locally-disabled,
-        file-ignored,
-        suppressed-message,
-        useless-suppression,
-        deprecated-pragma,
-        use-symbolic-message-instead,
-        apply-builtin,
-        basestring-builtin,
-        buffer-builtin,
-        cmp-builtin,
-        coerce-builtin,
-        execfile-builtin,
-        file-builtin,
-        long-builtin,
-        raw_input-builtin,
-        reduce-builtin,
-        standarderror-builtin,
-        unicode-builtin,
-        xrange-builtin,
-        coerce-method,
-        delslice-method,
-        getslice-method,
-        setslice-method,
-        no-absolute-import,
-        old-division,
-        dict-iter-method,
-        dict-view-method,
-        next-method-called,
-        metaclass-assignment,
-        indexing-exception,
-        raising-string,
-        reload-builtin,
-        oct-method,
-        hex-method,
-        nonzero-method,
-        cmp-method,
-        input-builtin,
-        round-builtin,
-        intern-builtin,
-        unichr-builtin,
-        map-builtin-not-iterating,
-        zip-builtin-not-iterating,
-        range-builtin-not-iterating,
-        filter-builtin-not-iterating,
-        using-cmp-argument,
-        eq-without-hash,
-        div-method,
-        idiv-method,
-        rdiv-method,
-        exception-message-attribute,
-        invalid-str-codec,
-        sys-max-int,
-        bad-python3-import,
-        deprecated-string-function,
-        deprecated-str-translate-call,
-        deprecated-itertools-function,
-        deprecated-types-field,
-        next-method-defined,
-        dict-items-not-iterating,
-        dict-keys-not-iterating,
-        dict-values-not-iterating,
-        deprecated-operator-function,
-        deprecated-urllib-function,
-        xreadlines-attribute,
-        deprecated-sys-function,
-        exception-escape,
-        comprehension-escape,
-        bad-continuation,  # conflicts with black
-        duplicate-code  # turn back on ASAP
-
-# Enable the message, report, category or checker with the given id(s). You can
-# either give multiple identifier separated by comma (,) or put this option
-# multiple time (only on the command line, not in the configuration file where
-# it should appear only once). See also the ""--disable"" option for examples.
-enable=c-extension-no-member
-
-
-[REPORTS]
-
-# Python expression which should return a score less than or equal to 10. You
-# have access to the variables 'error', 'warning', 'refactor', and 'convention'
-# which contain the number of messages in each category, as well as 'statement'
-# which is the total number of statements analyzed. This score is used by the
-# global evaluation report (RP0004).
-evaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)
-
-# Template used to display messages. This is a python new-style format string
-# used to format the message information. See doc for all details.
-#msg-template=
-
-# Set the output format. Available formats are text, parseable, colorized, json
-# and msvs (visual studio). You can also give a reporter class, e.g.
-# mypackage.mymodule.MyReporterClass.
-output-format=text
-
-# Tells whether to display a full report or only the messages.
-reports=no
-
-# Activate the evaluation score.
-score=yes
-
-
-[REFACTORING]
-
-# Maximum number of nested blocks for function / method body
-max-nested-blocks=5
-
-# Complete name of functions that never returns. When checking for
-# inconsistent-return-statements if a never returning function is called then
-# it will be considered as an explicit return statement and no message will be
-# printed.
-never-returning-functions=sys.exit
-
-
-[BASIC]
-
-# Naming style matching correct argument names.
-argument-naming-style=snake_case
-
-# Regular expression matching correct argument names. Overrides argument-
-# naming-style.
-#argument-rgx=
-
-# Naming style matching correct attribute names.
-attr-naming-style=snake_case
-
-# Regular expression matching correct attribute names. Overrides attr-naming-
-# style.
-#attr-rgx=
-
-# Bad variable names which should always be refused, separated by a comma.
-bad-names=foo,
-          bar,
-          baz,
-          toto,
-          tutu,
-          tata
-
-# Naming style matching correct class attribute names.
-class-attribute-naming-style=any
-
-# Regular expression matching correct class attribute names. Overrides class-
-# attribute-naming-style.
-#class-attribute-rgx=
-
-# Naming style matching correct class names.
-class-naming-style=PascalCase
-
-# Regular expression matching correct class names. Overrides class-naming-
-# style.
-#class-rgx=
-
-# Naming style matching correct constant names.
-const-naming-style=UPPER_CASE
-
-# Regular expression matching correct constant names. Overrides const-naming-
-# style.
-#const-rgx=
-
-# Minimum line length for functions/classes that require docstrings, shorter
-# ones are exempt.
-docstring-min-length=-1
-
-# Naming style matching correct function names.
-function-naming-style=snake_case
-
-# Regular expression matching correct function names. Overrides function-
-# naming-style.
-#function-rgx=
-
-# Good variable names which should always be accepted, separated by a comma.
-good-names=i,
-           j,
-           k,
-           ex,
-           Run,
-           _
-
-# Include a hint for the correct naming format with invalid-name.
-include-naming-hint=no
-
-# Naming style matching correct inline iteration names.
-inlinevar-naming-style=any
-
-# Regular expression matching correct inline iteration names. Overrides
-# inlinevar-naming-style.
-#inlinevar-rgx=
-
-# Naming style matching correct method names.
-method-naming-style=snake_case
-
-# Regular expression matching correct method names. Overrides method-naming-
-# style.
-#method-rgx=
-
-# Naming style matching correct module names.
-module-naming-style=snake_case
-
-# Regular expression matching correct module names. Overrides module-naming-
-# style.
-#module-rgx=
-
-# Colon-delimited sets of names that determine each other's naming style when
-# the name regexes allow several styles.
-name-group=
-
-# Regular expression which should only match function or class names that do
-# not require a docstring.
-no-docstring-rgx=^_
-
-# List of decorators that produce properties, such as abc.abstractproperty. Add
-# to this list to register other decorators that produce valid properties.
-# These decorators are taken in consideration only for invalid-name.
-property-classes=abc.abstractproperty
-
-# Naming style matching correct variable names.
-variable-naming-style=snake_case
-
-# Regular expression matching correct variable names. Overrides variable-
-# naming-style.
-#variable-rgx=
-
-
-[FORMAT]
-
-# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
-expected-line-ending-format=
-
-# Regexp for a line that is allowed to be longer than the limit.
-ignore-long-lines=^\s*(# )?<?https?://\S+>?$
-
-# Number of spaces of indent required inside a hanging or continued line.
-indent-after-paren=4
-
-# String used as indentation unit. This is usually ""    "" (4 spaces) or ""\t"" (1
-# tab).
-indent-string='    '
-
-# Maximum number of characters on a single line.
-max-line-length=120  # matches black setting
-
-# Maximum number of lines in a module.
-max-module-lines=1000
-
-# List of optional constructs for which whitespace checking is disabled. `dict-
-# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\n222: 2}.
-# `trailing-comma` allows a space between comma and closing bracket: (a, ).
-# `empty-line` allows space-only lines.
-no-space-check=trailing-comma,
-               dict-separator
-
-# Allow the body of a class to be on the same line as the declaration if body
-# contains single statement.
-single-line-class-stmt=no
-
-# Allow the body of an if to be on the same line as the test if there is no
-# else.
-single-line-if-stmt=no
-
-
-[LOGGING]
-
-# Format style used to check logging format string. `old` means using %
-# formatting, `new` is for `{}` formatting,and `fstr` is for f-strings.
-logging-format-style=fstr
-
-# Logging modules to check that the string format arguments are in logging
-# function parameter format.
-logging-modules=logging
-
-
-[MISCELLANEOUS]
-
-# List of note tags to take in consideration, separated by a comma.
-notes=FIXME,
-      XXX
-
-
-[SIMILARITIES]
-
-# Ignore comments when computing similarities.
-ignore-comments=yes
-
-# Ignore docstrings when computing similarities.
-ignore-docstrings=yes
-
-# Ignore imports when computing similarities.
-ignore-imports=no
-
-# Minimum lines number of a similarity.
-min-similarity-lines=4
-
-
-[SPELLING]
-
-# Limits count of emitted suggestions for spelling mistakes.
-max-spelling-suggestions=4
-
-# Spelling dictionary name. Available dictionaries: none. To make it work,
-# install the python-enchant package.
-spelling-dict=
-
-# List of comma separated words that should not be checked.
-spelling-ignore-words=
-
-# A path to a file that contains the private dictionary; one word per line.
-spelling-private-dict-file=
-
-# Tells whether to store unknown words to the private dictionary (see the
-# --spelling-private-dict-file option) instead of raising a message.
-spelling-store-unknown-words=no
-
-
-[STRING]
-
-# This flag controls whether the implicit-str-concat-in-sequence should
-# generate a warning on implicit string concatenation in sequences defined over
-# several lines.
-check-str-concat-over-line-jumps=no
-
-
-[TYPECHECK]
-
-# List of decorators that produce context managers, such as
-# contextlib.contextmanager. Add to this list to register other decorators that
-# produce valid context managers.
-contextmanager-decorators=contextlib.contextmanager
-
-# List of members which are set dynamically and missed by pylint inference
-# system, and so shouldn't trigger E1101 when accessed. Python regular
-# expressions are accepted.
-generated-members=
-
-# Tells whether missing members accessed in mixin class should be ignored. A
-# mixin class is detected if its name ends with ""mixin"" (case insensitive).
-ignore-mixin-members=yes
-
-# Tells whether to warn about missing members when the owner of the attribute
-# is inferred to be None.
-ignore-none=yes
-
-# This flag controls whether pylint should warn about no-member and similar
-# checks whenever an opaque object is returned when inferring. The inference
-# can return multiple potential results while evaluating a Python object, but
-# some branches might not be evaluated, which results in partial inference. In
-# that case, it might be useful to still emit no-member and other checks for
-# the rest of the inferred objects.
-ignore-on-opaque-inference=yes
-
-# List of class names for which member attributes should not be checked (useful
-# for classes with dynamically set attributes). This supports the use of
-# qualified names.
-ignored-classes=optparse.Values,thread._local,_thread._local
-
-# List of module names for which member attributes should not be checked
-# (useful for modules/projects where namespaces are manipulated during runtime
-# and thus existing member attributes cannot be deduced by static analysis). It
-# supports qualified module names, as well as Unix pattern matching.
-ignored-modules=
-
-# Show a hint with possible names when a member name was not found. The aspect
-# of finding the hint is based on edit distance.
-missing-member-hint=yes
-
-# The minimum edit distance a name should have in order to be considered a
-# similar match for a missing member name.
-missing-member-hint-distance=1
-
-# The total number of similar names that should be taken in consideration when
-# showing a hint for a missing member.
-missing-member-max-choices=1
-
-# List of decorators that change the signature of a decorated function.
-signature-mutators=
-
-
-[VARIABLES]
-
-# List of additional names supposed to be defined in builtins. Remember that
-# you should avoid defining new builtins when possible.
-additional-builtins=
-
-# Tells whether unused global variables should be treated as a violation.
-allow-global-unused-variables=yes
-
-# List of strings which can identify a callback function by name. A callback
-# name must start or end with one of those strings.
-callbacks=cb_,
-          _cb
-
-# A regular expression matching the name of dummy variables (i.e. expected to
-# not be used).
-dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_
-
-# Argument names that match this expression will be ignored. Default to name
-# with leading underscore.
-ignored-argument-names=_.*|^ignored_|^unused_
-
-# Tells whether we should check for unused import in __init__ files.
-init-import=no
-
-# List of qualified module names which can have objects that can redefine
-# builtins.
-redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
-
-
-[CLASSES]
-
-# List of method names used to declare (i.e. assign) instance attributes.
-defining-attr-methods=__init__,
-                      __new__,
-                      setUp,
-                      __post_init__
-
-# List of member names, which should be excluded from the protected access
-# warning.
-exclude-protected=_asdict,
-                  _fields,
-                  _replace,
-                  _source,
-                  _make
-
-# List of valid names for the first argument in a class method.
-valid-classmethod-first-arg=cls
-
-# List of valid names for the first argument in a metaclass class method.
-valid-metaclass-classmethod-first-arg=cls
-
-
-[DESIGN]
-
-# Maximum number of arguments for function / method.
-max-args=5
-
-# Maximum number of attributes for a class (see R0902).
-max-attributes=7
-
-# Maximum number of boolean expressions in an if statement (see R0916).
-max-bool-expr=5
-
-# Maximum number of branch for function / method body.
-max-branches=12
-
-# Maximum number of locals for function / method body.
-max-locals=15
-
-# Maximum number of parents for a class (see R0901).
-max-parents=7
-
-# Maximum number of public methods for a class (see R0904).
-max-public-methods=20
-
-# Maximum number of return / yield for function / method body.
-max-returns=6
-
-# Maximum number of statements in function / method body.
-max-statements=50
-
-# Minimum number of public methods for a class (see R0903).
-min-public-methods=2
-
-
-[IMPORTS]
-
-# List of modules that can be imported at any level, not just the top level
-# one.
-allow-any-import-level=
-
-# Allow wildcard imports from modules that define __all__.
-allow-wildcard-with-all=no
-
-# Analyse import fallback blocks. This can be used to support both Python 2 and
-# 3 compatible code, which means that the block might have code that exists
-# only in one or another interpreter, leading to false positives when analysed.
-analyse-fallback-blocks=no
-
-# Deprecated modules which should not be used, separated by a comma.
-deprecated-modules=optparse,tkinter.tix
-
-# Create a graph of external dependencies in the given file (report RP0402 must
-# not be disabled).
-ext-import-graph=
-
-# Create a graph of every (i.e. internal and external) dependencies in the
-# given file (report RP0402 must not be disabled).
-import-graph=
-
-# Create a graph of internal dependencies in the given file (report RP0402 must
-# not be disabled).
-int-import-graph=
-
-# Force import order to recognize a module as part of the standard
-# compatibility libraries.
-known-standard-library=
-
-# Force import order to recognize a module as part of a third party library.
-known-third-party=enchant
-
-# Couples of modules and preferred modules, separated by a comma.
-preferred-modules=
-
-
-[EXCEPTIONS]
-
-# Exceptions that will emit a warning when being caught. Defaults to
-# ""BaseException, Exception"".
-overgeneral-exceptions=BaseException,
-                       Exception"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -24,3 +24,39 @@ include_trailing_comma = ""True""
 force_grid_wrap = 0
 use_parentheses = ""True""
 line_length = 120
+
+[tool.pylint.master]
+extension-pkg-whitelist = ""pydantic""
+ignore = ""CVS""
+suggestion-mode = ""yes""
+[tool.pylint.messages_control]
+disable = '''
+duplicate-code,
+line-too-long,
+logging-fstring-interpolation,
+bad-continuation,
+'''
+[tool.pylint.logging]
+logging-modules = ""logging""
+[tool.pylint.imports]
+allow-wildcard-with-all = ""no""
+[tool.pylint.format]
+indent-after-paren = ""4""
+max-line-length = ""120""  # matches black setting
+max-module-lines = ""800""
+no-space-check = '''
+trailing-comma,
+dict-separator
+'''
+single-line-class-stmt = ""no""
+single-line-if-stmt = ""no""
+[tool.pylint.miscellaneous]
+notes= '''
+FIXME,
+XXX
+'''
+[tool.pylint.similarities]
+ignore-comments = ""yes""
+ignore-docstrings = ""yes""
+ignore-imports = ""no""
+min-similarity-lines = ""4"""
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,6 +1,6 @@
 -i https://pypi.org/simple
 appdirs==1.4.3
-astroid==2.3.3
+astroid==2.4.0
 async-asgi-testclient==1.4.4
 async-generator==1.10
 asyncmock==0.4.2
@@ -9,7 +9,7 @@ bandit==1.6.2
 black==19.10b0
 certifi==2020.4.5.1
 chardet==3.0.4
-click==7.1.1
+click==7.1.2
 coverage==5.1
 coveralls==2.0.0
 docopt==0.6.2
@@ -29,21 +29,21 @@ pathspec==0.8.0
 pbr==5.4.5
 pluggy==0.13.1
 py==1.8.1
-pylint==2.4.4
+pylint==2.5.0
 pyparsing==2.4.7
-pytest-asyncio==0.10.0
+pytest-asyncio==0.11.0
 pytest-cov==2.8.1
 pytest==5.4.1
 pyyaml==5.3.1
 regex==2020.4.4
 requests==2.23.0
-responses==0.10.12
+responses==0.10.14
 six==1.14.0
 smmap==3.0.2
 stevedore==1.32.0
 toml==0.10.0
 typed-ast==1.4.1
 urllib3==1.25.9
 wcwidth==0.1.9
-wrapt==1.11.2
+wrapt==1.12.1
 zipp==3.1.0"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -1,28 +1,32 @@
 -i https://pypi.org/simple
+aiocache[redis]==0.11.1
+aiofiles==0.5.0
 aiohttp==3.6.2
+aioredis==1.3.1
 async-timeout==3.0.1
 asyncache==0.1.1
 attrs==19.3.0
 cachetools==4.1.0
 certifi==2020.4.5.1
 chardet==3.0.4
-click==7.1.1
+click==7.1.2
 dataclasses==0.6 ; python_version < '3.7'
 fastapi==0.54.1
 gunicorn==20.0.4
 h11==0.9.0
+hiredis==1.0.1
 httptools==0.1.1 ; sys_platform != 'win32' and sys_platform != 'cygwin' and platform_python_implementation != 'PyPy'
 idna-ssl==1.1.0 ; python_version < '3.7'
 idna==2.9
 multidict==4.7.5
-pydantic==1.5
+pydantic[dotenv]==1.5.1
 python-dateutil==2.8.1
 python-dotenv==0.13.0
 requests==2.23.0
 six==1.14.0
 starlette==0.13.2
 urllib3==1.25.9
-uvicorn==0.11.3
+uvicorn==0.11.5
 uvloop==0.14.0 ; sys_platform != 'win32' and sys_platform != 'cygwin' and platform_python_implementation != 'PyPy'
 websockets==8.1
 yarl==1.4.2"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -46,6 +46,7 @@ def check(ctx, fmt=False, sort=False, diff=False):  # pylint: disable=redefined-
         fmt_args.append(""--diff"")
         sort_args.append(""--diff"")
 
+    # FIXME: run each command and check return code
     cmd_args = []
     if fmt:
         cmd_args.extend(fmt_args)"
44;ExpDev07;coronavirus-tracker-api;983fa5c22b40e8607c18158e2b61fabca4be76d1;"Use shared Redis cache for jhu data (#306)

* add & log a data_id string for each cache
* dev mode should run with debug log level
* log missing country code at sub debug level
* use pydantic for config managment
* add redis config settings
* add aiocache with redis support
* use memory cache
* use shared cache for jhu data
* cleanup
* fix url type
* add aiofiles
* add async save/read
* update tests
* update dependencies
* change pylint config to pyproject.toml
* cache jhu data (locally) for 30 minutes";"@@ -5,15 +5,17 @@
 
 import app.io
 
-
-@pytest.mark.parametrize(
+IO_PARAMS = (
     ""name, content, kwargs"",
     [
         (""test_file.txt"", string.ascii_lowercase, {}),
         (""test_json_file.json"", {""a"": 0, ""b"": 1, ""c"": 2}, {}),
         (""test_custom_json.json"", {""z"": -1, ""b"": 1, ""y"": -2, ""a"": 0}, {""indent"": 4, ""sort_keys"": True}),
     ],
 )
+
+
+@pytest.mark.parametrize(*IO_PARAMS)
 def test_save(tmp_path, name, content, kwargs):
     test_path = tmp_path / name
     assert not test_path.exists()
@@ -23,17 +25,32 @@ def test_save(tmp_path, name, content, kwargs):
     assert test_path.exists()
 
 
-@pytest.mark.parametrize(
-    ""name, content, kwargs"",
-    [
-        (""test_file.txt"", string.ascii_lowercase, {}),
-        (""test_json_file.json"", {""a"": 0, ""b"": 1, ""c"": 2}, {}),
-        (""test_custom_json.json"", {""z"": -1, ""b"": 1, ""y"": -2, ""a"": 0}, {""indent"": 4, ""sort_keys"": True}),
-    ],
-)
+@pytest.mark.parametrize(*IO_PARAMS)
 def test_round_trip(tmp_path, name, content, kwargs):
     test_path = tmp_path / name
     assert not test_path.exists()
 
     app.io.save(test_path, content, **kwargs)
     assert app.io.load(test_path) == content
+
+
+@pytest.mark.asyncio
+@pytest.mark.parametrize(*IO_PARAMS)
+async def test_async_save(tmp_path, name, content, kwargs):
+    test_path = tmp_path / name
+    assert not test_path.exists()
+
+    result = await app.io.AIO.save(test_path, content, **kwargs)
+    assert result == test_path
+    assert test_path.exists()
+
+
+@pytest.mark.asyncio
+@pytest.mark.parametrize(*IO_PARAMS)
+async def test_async_round_trip(tmp_path, name, content, kwargs):
+    test_path = tmp_path / name
+    assert not test_path.exists()
+
+    await app.io.AIO.save(test_path, content, **kwargs)
+    load_results = await app.io.AIO.load(test_path)
+    assert load_results == content"
45;facebookincubator;cinder;db693420baf9e6db145ec7e1590fccba16f9be9c;"JIT: rewrite index operand in memory indirect when it is a linked operand

Summary: In JIT register allocator, the index operand in a memory indirect operand needs to be rewritten with the assigned register when it is a linked operand.

Reviewed By: tekknolagi

Differential Revision: D36883863

fbshipit-source-id: 6a481dedddfc9e2ae3c51b2298e7150d4541881c";"@@ -1117,7 +1117,7 @@ void LinearScanAllocator::rewriteInstrOneIndirectOperand(
   PhyLocation index_phy_reg = PhyLocation::REG_INVALID;
   bool index_last_use = false;
   if (index != nullptr) {
-    index_phy_reg = index->isVreg()
+    index_phy_reg = (index->isVreg() || index->isLinked())
         ? map_get(mapping, index->getDefine())->allocated_loc
         : PhyLocation(index->getPhyRegister());
 "
45;facebookincubator;cinder;52d7b141661e4e48d08b9b02797c3939e9996aaf;"Tighten up deopt metadata

Summary:
Some simple changes to reduce the size of de-opt metadata, as it seems like we allocate a lot of it.  Changes small enums to be `char` sized and improves the layout of `DeoptMetadata`.  Also removes the `inline_depth` field as this can be inferred from the `frame_meta` field (unfortunately due to 8 byte alignment this doesn't save us memory, but would set us up to do so if we can get another field removed).

This reduces the size of `DeoptMetadata` from 104 bytes to 88, and `LiveValue` from 16 to 8.

Reviewed By: swtaarrs

Differential Revision: D36941657

fbshipit-source-id: 5971752ba8160a5ed99295cb7cf4ad4a39b32eeb";"@@ -1443,7 +1443,7 @@ prepareForDeopt(const uint64_t* regs, Runtime* runtime, std::size_t deopt_idx) {
   PyFrameObject* frame_iter = frame;
   _PyShadowFrame* sf_iter = tstate->shadow_frame;
   // Iterate one past the inline depth because that is the caller frame.
-  for (int i = deopt_meta.inline_depth; i >= 0; i--) {
+  for (int i = deopt_meta.inline_depth(); i >= 0; i--) {
     // Transfer ownership of shadow frame to the interpreter. The associated
     // Python frame will be ignored during future attempts to materialize the
     // stack.
@@ -1515,7 +1515,7 @@ static PyObject* resumeInInterpreter(
   PyObject* result = nullptr;
   // Resume all of the inlined frames and the caller
   const DeoptMetadata& deopt_meta = runtime->getDeoptMetadata(deopt_idx);
-  int inline_depth = deopt_meta.inline_depth;
+  int inline_depth = deopt_meta.inline_depth();
   int err_occurred = (deopt_meta.reason != DeoptReason::kGuardFailure);
   while (inline_depth >= 0) {
     // TODO(emacs): Investigate skipping resuming frames that do not have"
45;facebookincubator;cinder;52d7b141661e4e48d08b9b02797c3939e9996aaf;"Tighten up deopt metadata

Summary:
Some simple changes to reduce the size of de-opt metadata, as it seems like we allocate a lot of it.  Changes small enums to be `char` sized and improves the layout of `DeoptMetadata`.  Also removes the `inline_depth` field as this can be inferred from the `frame_meta` field (unfortunately due to 8 byte alignment this doesn't save us memory, but would set us up to do so if we can get another field removed).

This reduces the size of `DeoptMetadata` from 104 bytes to 88, and `LiveValue` from 16 to 8.

Reviewed By: swtaarrs

Differential Revision: D36941657

fbshipit-source-id: 5971752ba8160a5ed99295cb7cf4ad4a39b32eeb";"@@ -317,8 +317,7 @@ DeoptMetadata DeoptMetadata::fromInstr(
 
   auto fs = instr.frameState();
 
-  meta.inline_depth = fs->inlineDepth();
-  int num_frames = meta.inline_depth;
+  int num_frames = fs->inlineDepth();
   meta.frame_meta.resize(num_frames + 1); // +1 for caller
   for (hir::FrameState* frame = fs; frame != NULL; frame = frame->parent) {
     int i = num_frames--;"
45;facebookincubator;cinder;52d7b141661e4e48d08b9b02797c3939e9996aaf;"Tighten up deopt metadata

Summary:
Some simple changes to reduce the size of de-opt metadata, as it seems like we allocate a lot of it.  Changes small enums to be `char` sized and improves the layout of `DeoptMetadata`.  Also removes the `inline_depth` field as this can be inferred from the `frame_meta` field (unfortunately due to 8 byte alignment this doesn't save us memory, but would set us up to do so if we can get another field removed).

This reduces the size of `DeoptMetadata` from 104 bytes to 88, and `LiveValue` from 16 to 8.

Reviewed By: swtaarrs

Differential Revision: D36941657

fbshipit-source-id: 5971752ba8160a5ed99295cb7cf4ad4a39b32eeb";"@@ -44,7 +44,7 @@ struct LiveValue {
   // During deoptimization we need to translate this stack layout into the
   // form expected by the interpreter. To do so, we tag the `LiveValue` for
   // the stack slot that contains `<callable>` with this field.
-  enum class Source {
+  enum class Source : char {
     kLoadMethod,
     kUnknown,
   };
@@ -83,15 +83,15 @@ struct LiveValue {
   X(UnhandledUnboundFreevar) \
   X(UnhandledNullField)
 
-enum class DeoptReason {
+enum class DeoptReason : char {
 #define REASON(name) k##name,
   DEOPT_REASONS(REASON)
 #undef REASON
 };
 
 const char* deoptReasonName(DeoptReason reason);
 
-enum class DeoptAction {
+enum class DeoptAction : char {
   kResumeInInterpreter,
   kUnwind,
 };
@@ -128,40 +128,42 @@ struct DeoptFrameMetadata {
 // DeoptMetadata captures all the information necessary to reconstruct a
 // PyFrameObject when deoptimization occurs.
 struct DeoptMetadata {
-  // Why we are de-opting
-  DeoptReason reason{DeoptReason::kUnhandledException};
-
-  // What to do when we de-opt
-  DeoptAction action{DeoptAction::kUnwind};
-
   // The name index of the unbound local or attribute, if we are deopting
   // because of an undefined value.
   BorrowedRef<> eh_name;
 
   // All live values
   std::vector<LiveValue> live_values;
 
-  // If not -1, index into live_values for a context-dependent value that is
-  // relevant to this deopt event.
-  int guilty_value{-1};
-
   // Stack of inlined frame metadata unwound from the deopting instruction.
   std::vector<DeoptFrameMetadata> frame_meta;
 
-  // An identifier that can be used to map back to the guard from which
-  // this was generated.
-  int nonce{-1};
-
   // Runtime metadata associated with the JIT-compiled function from which this
   // was generated.
   CodeRuntime* code_rt{nullptr};
 
   // A human-readable description of why this deopt happened.
   const char* descr{nullptr};
 
+  // If not -1, index into live_values for a context-dependent value that is
+  // relevant to this deopt event.
+  int guilty_value{-1};
+
+  // An identifier that can be used to map back to the guard from which
+  // this was generated.
+  int nonce{-1};
+
+  // Why we are de-opting
+  DeoptReason reason{DeoptReason::kUnhandledException};
+
+  // What to do when we de-opt
+  DeoptAction action{DeoptAction::kUnwind};
+
   // If part of an inlined function, the depth into the call stack that this
   // code *would* be (1, 2, 3, ...). If not inlined, 0.
-  int inline_depth{0};
+  int inline_depth() const {
+    return frame_meta.size() - 1;
+  }
 
   const LiveValue& getStackValue(int i, const DeoptFrameMetadata& frame) const {
     return live_values[frame.stack[i]];
@@ -195,7 +197,7 @@ struct DeoptMetadata {
         deoptReasonName(reason),
         deoptActionName(action),
         descr,
-        inline_depth,
+        inline_depth(),
         fmt::join(live_value_strings, "", ""));
   }
 "
45;facebookincubator;cinder;52d7b141661e4e48d08b9b02797c3939e9996aaf;"Tighten up deopt metadata

Summary:
Some simple changes to reduce the size of de-opt metadata, as it seems like we allocate a lot of it.  Changes small enums to be `char` sized and improves the layout of `DeoptMetadata`.  Also removes the `inline_depth` field as this can be inferred from the `frame_meta` field (unfortunately due to 8 byte alignment this doesn't save us memory, but would set us up to do so if we can get another field removed).

This reduces the size of `DeoptMetadata` from 104 bytes to 88, and `LiveValue` from 16 to 8.

Reviewed By: swtaarrs

Differential Revision: D36941657

fbshipit-source-id: 5971752ba8160a5ed99295cb7cf4ad4a39b32eeb";"@@ -105,7 +105,7 @@ class Register {
 std::ostream& operator<<(std::ostream& os, const Register& reg);
 
 // The refcount semantics of a value held in a Register.
-enum class RefKind {
+enum class RefKind : char {
   // A PyObject* that is either null or points to an immortal object, and
   // doesn't need to be reference counted, or a primitive.
   kUncounted,
@@ -117,7 +117,7 @@ enum class RefKind {
 std::ostream& operator<<(std::ostream& os, RefKind kind);
 
 // The kind of value held in a Register.
-enum class ValueKind {
+enum class ValueKind : char {
   // A PyObject*.
   kObject,
   // A signed 64-bit integer."
45;facebookincubator;cinder;52d7b141661e4e48d08b9b02797c3939e9996aaf;"Tighten up deopt metadata

Summary:
Some simple changes to reduce the size of de-opt metadata, as it seems like we allocate a lot of it.  Changes small enums to be `char` sized and improves the layout of `DeoptMetadata`.  Also removes the `inline_depth` field as this can be inferred from the `frame_meta` field (unfortunately due to 8 byte alignment this doesn't save us memory, but would set us up to do so if we can get another field removed).

This reduces the size of `DeoptMetadata` from 104 bytes to 88, and `LiveValue` from 16 to 8.

Reviewed By: swtaarrs

Differential Revision: D36941657

fbshipit-source-id: 5971752ba8160a5ed99295cb7cf4ad4a39b32eeb";"@@ -943,7 +943,7 @@ Ref<> make_deopt_stats() {
 
   for (auto& pair : runtime->deoptStats()) {
     const DeoptMetadata& meta = runtime->getDeoptMetadata(pair.first);
-    const DeoptFrameMetadata& frame_meta = meta.frame_meta[meta.inline_depth];
+    const DeoptFrameMetadata& frame_meta = meta.frame_meta[meta.inline_depth()];
     const DeoptStat& stat = pair.second;
     BorrowedRef<PyCodeObject> code = frame_meta.code;
 "
45;facebookincubator;cinder;4d7b7cc6c0981715247cc0119d471175f3199344;"stop clearing the generic type cache when the classloader cache is cleared

Summary:
This is incorrect - as discussed in the Static Python slack channel, removing valid types from the generic instruction cache results in us creating multiple copies of valid types, and the equality check of Generic[T] == Generic[T] potentially failing as a result (since we have to mint a new type object, which won't compare equal pointer wise).

If we don't remove generic types from the cache, if you have an invalid type, we will leak some memory, but in the absence of building a mechanism to only clear out generic types with invalid parameters, this is necessary for correctness.

Reviewed By: carljm

Differential Revision: D36824755

fbshipit-source-id: 5260cb7679cf85efcb91283afc0da7643ed98054";"@@ -98,6 +98,7 @@ int _PyClassLoader_AddSubclass(PyTypeObject *base, PyTypeObject *type);
 
 _PyType_VTable *_PyClassLoader_EnsureVtable(PyTypeObject *self, int init_subclasses);
 int _PyClassLoader_ClearVtables(void);
+void _PyClassLoader_ClearGenericTypes(void);
 
 /* Gets an indirect pointer for a function.  This should be used if
 * the given container is mutable, and the indirect pointer will"
45;facebookincubator;cinder;4d7b7cc6c0981715247cc0119d471175f3199344;"stop clearing the generic type cache when the classloader cache is cleared

Summary:
This is incorrect - as discussed in the Static Python slack channel, removing valid types from the generic instruction cache results in us creating multiple copies of valid types, and the equality check of Generic[T] == Generic[T] potentially failing as a result (since we have to mint a new type object, which won't compare equal pointer wise).

If we don't remove generic types from the cache, if you have an invalid type, we will leak some memory, but in the absence of building a mechanism to only clear out generic types with invalid parameters, this is necessary for correctness.

Reviewed By: carljm

Differential Revision: D36824755

fbshipit-source-id: 5260cb7679cf85efcb91283afc0da7643ed98054";"@@ -358,13 +358,15 @@ static PyObject * strict_module_patch_enabled(PyObject *self, PyObject *mod)
 
 
 PyAPI_FUNC(int) _PyClassLoader_ClearVtables(void);
+PyAPI_FUNC(int) _PyClassLoader_ClearGenericTypes(void);
 PyAPI_FUNC(int) _PyClassLoader_ClearCache(void);
 
 static PyObject *
 clear_classloader_caches(PyObject *self, PyObject *obj)
 {
     _PyClassLoader_ClearVtables();
     _PyClassLoader_ClearCache();
+    _PyClassLoader_ClearGenericTypes();
     Py_RETURN_NONE;
 }
 "
45;facebookincubator;cinder;4d7b7cc6c0981715247cc0119d471175f3199344;"stop clearing the generic type cache when the classloader cache is cleared

Summary:
This is incorrect - as discussed in the Static Python slack channel, removing valid types from the generic instruction cache results in us creating multiple copies of valid types, and the equality check of Generic[T] == Generic[T] potentially failing as a result (since we have to mint a new type object, which won't compare equal pointer wise).

If we don't remove generic types from the cache, if you have an invalid type, we will leak some memory, but in the absence of building a mechanism to only clear out generic types with invalid parameters, this is necessary for correctness.

Reviewed By: carljm

Differential Revision: D36824755

fbshipit-source-id: 5260cb7679cf85efcb91283afc0da7643ed98054";"@@ -2510,10 +2510,15 @@ void
 _PyClassLoader_ClearCache()
 {
     Py_CLEAR(classloader_cache);
-    Py_CLEAR(genericinst_cache);
     Py_CLEAR(static_enum);
 }
 
+void
+_PyClassLoader_ClearGenericTypes()
+{
+    Py_CLEAR(genericinst_cache);
+}
+
 /**
     For every slot in the vtable slotmap, this sets the vectorcall entrypoint
     to `type_vtable_lazyinit`."
45;facebookincubator;cinder;a8804cc6e3a5861463ff959abcd09ad60a0763e5;"Use std::back_inserter with format_to

Summary:
While the memory_buffer overload does exist, it's marked as deprecated so breaks the open source build

See https://github.com/facebookincubator/cinder/runs/6568024639?check_suite_focus=true

Reviewed By: tekknolagi

Differential Revision: D36638005

fbshipit-source-id: 1a6bfcaee2d95701627ebf439bc548d8eb9cd1c7";"@@ -46,7 +46,7 @@ class BasicBlockBuilder {
   template <typename... T>
   void AppendCode(fmt::format_string<T...> s, T&&... args) {
     fmt::memory_buffer buf;
-    fmt::format_to(buf, s, std::forward<T>(args)...);
+    fmt::format_to(std::back_inserter(buf), s, std::forward<T>(args)...);
     AppendCode(buf);
   }
 "
45;facebookincubator;cinder;a8804cc6e3a5861463ff959abcd09ad60a0763e5;"Use std::back_inserter with format_to

Summary:
While the memory_buffer overload does exist, it's marked as deprecated so breaks the open source build

See https://github.com/facebookincubator/cinder/runs/6568024639?check_suite_focus=true

Reviewed By: tekknolagi

Differential Revision: D36638005

fbshipit-source-id: 1a6bfcaee2d95701627ebf439bc548d8eb9cd1c7";"@@ -252,7 +252,8 @@ void LIRGenerator::AppendGuard(
   auto id = env_->rt->addDeoptMetadata(std::move(deopt_meta));
 
   fmt::memory_buffer buf;
-  fmt::format_to(buf, ""Guard {}, {}"", kind, id);
+  auto buf_ins = std::back_inserter(buf);
+  fmt::format_to(buf_ins, ""Guard {}, {}"", kind, id);
 
   JIT_CHECK(
       guard_var.empty() == (kind == ""AlwaysFail""),
@@ -269,22 +270,22 @@ void LIRGenerator::AppendGuard(
     const auto& guard = static_cast<const GuardIs&>(instr);
     auto guard_ptr = static_cast<void*>(guard.target());
     env_->code_rt->addReference(static_cast<PyObject*>(guard_ptr));
-    fmt::format_to(buf, "", {}"", guard_ptr);
+    fmt::format_to(buf_ins, "", {}"", guard_ptr);
   } else if (instr.IsGuardType()) {
     const auto& guard = static_cast<const GuardType&>(instr);
     // TODO(T101999851): Handle non-Exact types
     JIT_CHECK(guard.target().isExact(), ""Only exact type guards are supported"");
     PyTypeObject* guard_type = guard.target().uniquePyType();
     JIT_CHECK(guard_type != nullptr, ""Ensure unique representation exists"");
     env_->code_rt->addReference(reinterpret_cast<PyObject*>(guard_type));
-    fmt::format_to(buf, "", {}"", reinterpret_cast<void*>(guard_type));
+    fmt::format_to(buf_ins, "", {}"", reinterpret_cast<void*>(guard_type));
   } else {
     buf.append(std::string_view("", 0""));
   }
 
   auto& regstates = instr.live_regs();
   for (const auto& reg_state : regstates) {
-    fmt::format_to(buf, "", {}"", reg_state.reg->name());
+    fmt::format_to(buf_ins, "", {}"", reg_state.reg->name());
   }
 
   bbb.AppendCode(buf);
@@ -357,14 +358,15 @@ bool LIRGenerator::TranslateSpecializedCall(
   }
 
   fmt::memory_buffer buf;
+  auto buf_ins = std::back_inserter(buf);
   fmt::format_to(
-      buf,
+      buf_ins,
       ""Vectorcall {}, {}, 0, {}"",
       instr.dst()->name(),
       reinterpret_cast<uint64_t>(func),
       reinterpret_cast<uint64_t>(callee));
   for (size_t i = 0, num_args = instr.numArgs(); i < num_args; i++) {
-    fmt::format_to(buf, "", {}"", instr.arg(i));
+    fmt::format_to(buf_ins, "", {}"", instr.arg(i));
   }
   buf.append(std::string_view("", 0""));
   bbb.AppendCode(buf);"
45;facebookincubator;cinder;a5fe9ede8f78c2f2ca93625f765a3c152c9b8e70;"Add an API to touch memory used by profilers

Summary:
Some profilers need to walk the `code_rt->code->qualname` for jitted functions
on the call stack. The JIT rarely touches this memory and, as a result, the
OS may page it out. Out of process profilers (i.e. those that use eBPF) that
attempt to read the memory after it has been paged out will fail as the read
would cause a page fault. We can prevent the memory from being paged out as
aggressively by periodically reading it using the API included in this diff.

Reviewed By: czardoz

Differential Revision: D36580046

fbshipit-source-id: be0b9642bb7a3ed09e0cef3a3b9fac7d8ec61d37";"@@ -889,6 +889,11 @@ static PyObject* mlock_profiler_dependencies(PyObject* /* self */, PyObject*) {
   Py_RETURN_NONE;
 }
 
+static PyObject* page_in_profiler_dependencies(PyObject*, PyObject*) {
+  Ref<> qualnames = Runtime::get()->pageInProfilerDependencies();
+  return qualnames.release();
+}
+
 namespace {
 
 // Simple wrapper functions to turn NULL or -1 return values from C-API
@@ -1234,6 +1239,10 @@ static PyMethodDef jit_methods[] = {
      mlock_profiler_dependencies,
      METH_NOARGS,
      ""Keep profiler dependencies paged in""},
+    {""page_in_profiler_dependencies"",
+     page_in_profiler_dependencies,
+     METH_NOARGS,
+     ""Read the memory needed by ebpf-based profilers.""},
     {NULL, NULL, 0, NULL}};
 
 static PyModuleDef jit_module = {"
45;facebookincubator;cinder;a5fe9ede8f78c2f2ca93625f765a3c152c9b8e70;"Add an API to touch memory used by profilers

Summary:
Some profilers need to walk the `code_rt->code->qualname` for jitted functions
on the call stack. The JIT rarely touches this memory and, as a result, the
OS may page it out. Out of process profilers (i.e. those that use eBPF) that
attempt to read the memory after it has been paged out will fail as the read
would cause a page fault. We can prevent the memory from being paged out as
aggressively by periodically reading it using the API included in this diff.

Reviewed By: czardoz

Differential Revision: D36580046

fbshipit-source-id: be0b9642bb7a3ed09e0cef3a3b9fac7d8ec61d37";"@@ -64,6 +64,28 @@ void Runtime::mlockProfilerDependencies() {
   runtimes_.lock();
 }
 
+Ref<> Runtime::pageInProfilerDependencies() {
+  ThreadedCompileSerialize guard;
+  Ref<> qualnames = Ref<>::steal(PyList_New(0));
+  if (qualnames == nullptr) {
+    return nullptr;
+  }
+  // We want to force the OS to page in the memory on the
+  // code_rt->code->qualname path and keep the compiler from optimizing away
+  // the code to do so. There are probably more efficient ways of doing this
+  // but perf isn't a major concern.
+  for (auto& code_rt : runtimes_) {
+    BorrowedRef<> qualname = code_rt.frameState()->code()->co_qualname;
+    if (qualname == nullptr) {
+      continue;
+    }
+    if (PyList_Append(qualnames, qualname) < 0) {
+      return nullptr;
+    }
+  }
+  return qualnames;
+}
+
 GlobalCache Runtime::findGlobalCache(PyObject* globals, PyObject* name) {
   JIT_CHECK(PyUnicode_CheckExact(name), ""Name must be a str"");
   JIT_CHECK(PyUnicode_CHECK_INTERNED(name), ""Name must be interned"");"
45;facebookincubator;cinder;a5fe9ede8f78c2f2ca93625f765a3c152c9b8e70;"Add an API to touch memory used by profilers

Summary:
Some profilers need to walk the `code_rt->code->qualname` for jitted functions
on the call stack. The JIT rarely touches this memory and, as a result, the
OS may page it out. Out of process profilers (i.e. those that use eBPF) that
attempt to read the memory after it has been paged out will fail as the read
would cause a page fault. We can prevent the memory from being paged out as
aggressively by periodically reading it using the API included in this diff.

Reviewed By: czardoz

Differential Revision: D36580046

fbshipit-source-id: be0b9642bb7a3ed09e0cef3a3b9fac7d8ec61d37";"@@ -390,6 +390,18 @@ class Runtime {
     return static_cast<T*>(deopt_patchers_.back().get());
   }
 
+  // Some profilers need to walk the code_rt->code->qualname chain for jitted
+  // functions on the call stack. The JIT rarely touches this memory and, as a
+  // result, the OS may page it out. Out of process profilers (i.e. those that
+  // use eBPF) that attempt to read the memory after it has been paged out will
+  // fail; the read would cause a page fault which is currently unsupported
+  // inside of an eBPF probe. Periodically calling this function will ensure
+  // that the OS doesn't page out the memory too aggressively.
+  //
+  // Returns a PyListObject containing the qualnames of the units for which
+  // memory was paged in.
+  Ref<> pageInProfilerDependencies();
+
  private:
   static Runtime* s_runtime_;
 "
45;facebookincubator;cinder;a5fe9ede8f78c2f2ca93625f765a3c152c9b8e70;"Add an API to touch memory used by profilers

Summary:
Some profilers need to walk the `code_rt->code->qualname` for jitted functions
on the call stack. The JIT rarely touches this memory and, as a result, the
OS may page it out. Out of process profilers (i.e. those that use eBPF) that
attempt to read the memory after it has been paged out will fail as the read
would cause a page fault. We can prevent the memory from being paged out as
aggressively by periodically reading it using the API included in this diff.

Reviewed By: czardoz

Differential Revision: D36580046

fbshipit-source-id: be0b9642bb7a3ed09e0cef3a3b9fac7d8ec61d37";"@@ -3544,6 +3544,11 @@ def test_type_ready(self):
     def test_mlock_profiler_dependencies(self):
         cinderjit.mlock_profiler_dependencies()
 
+    @unittest.skipIf(cinderjit is None, ""not jitting"")
+    def test_page_in_profiler_dependencies(self):
+        qualnames = cinderjit.page_in_profiler_dependencies()
+        self.assertTrue(len(qualnames) > 0)
+
 
 if __name__ == ""__main__"":
     unittest.main()"
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -19,6 +19,7 @@ Code contributions:
 - Harun Tuncay (haruntuncay)
 - Jeremiah Lowin (jlowin)
 - (jandelgado)
+- Jonas Irgens Kylling (jkylling)
 
 Suggestions and bug reporting:
 "
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -1,6 +1,16 @@
 Changelog
 =========
 
+Version 4.2.0
+-------------
+
+* Adding optimizations for speed ups to creation and inserts
+* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
+* Adding all additional methods specific to `Box` as protected keys
+* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
+* Fixing `copy` and `__copy__` not copying box options
+
+
 Version 4.1.0
 -------------
 "
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -2,7 +2,7 @@
 # -*- coding: UTF-8 -*-
 
 __author__ = 'Chris Griffith'
-__version__ = '4.1.0'
+__version__ = '4.2.0'
 
 from box.box import Box
 from box.box_list import BoxList"
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -5,18 +5,18 @@
 """"""
 Improved dictionary access through dot notation with additional tools.
 """"""
-import string
-import re
 import copy
-from keyword import kwlist
+import re
+import string
 import warnings
 from collections.abc import Iterable, Mapping, Callable
-from typing import Any, Union, Tuple, List, Dict
+from keyword import kwlist
 from pathlib import Path
+from typing import Any, Union, Tuple, List, Dict
 
 import box
-from box.exceptions import BoxError, BoxKeyError, BoxTypeError, BoxValueError, BoxWarning
 from box.converters import (_to_json, _from_json, _from_toml, _to_toml, _from_yaml, _to_yaml, BOX_PARAMETERS)
+from box.exceptions import BoxError, BoxKeyError, BoxTypeError, BoxValueError, BoxWarning
 
 __all__ = ['Box']
 
@@ -29,45 +29,6 @@
 NO_DEFAULT = object()
 
 
-def _safe_attr(attr, camel_killer=False, replacement_char='x'):
-    """"""Convert a key into something that is accessible as an attribute""""""
-    allowed = string.ascii_letters + string.digits + '_'
-
-    if isinstance(attr, tuple):
-        attr = ""_"".join([str(x) for x in attr])
-
-    attr = attr.decode('utf-8', 'ignore') if isinstance(attr, bytes) else str(attr)
-
-    if camel_killer:
-        attr = _camel_killer(attr)
-
-    out = []
-    last_safe = 0
-    for i, character in enumerate(attr):
-        if character in allowed:
-            last_safe = i
-            out.append(character)
-        elif not out:
-            continue
-        else:
-            if last_safe == i - 1:
-                out.append('_')
-
-    out = """".join(out)[:last_safe + 1]
-
-    try:
-        int(out[0])
-    except (ValueError, IndexError):
-        pass
-    else:
-        out = f'{replacement_char}{out}'
-
-    if out in kwlist:
-        out = f'{replacement_char}{out}'
-
-    return out
-
-
 def _camel_killer(attr):
     """"""
     CamelKiller, qu'est-ce que c'est?
@@ -93,41 +54,6 @@ def _recursive_tuples(iterable, box_class, recreate_tuples=False, **kwargs):
     return tuple(out_list)
 
 
-def _conversion_checks(item, keys, box_config):
-    """"""
-    Internal use for checking if a duplicate safe attribute already exists
-
-    :param item: Item to see if a dup exists
-    :param keys: Keys to check against
-    :param box_config: Easier to pass in than ask for specific items
-    :return: the original unmodified key, if exists and not check_only
-    """"""
-    if box_config['box_duplicates'] != 'ignore':
-        keys = list(keys) + [item]
-
-        key_list = [(k,
-                     _safe_attr(k, camel_killer=box_config['camel_killer_box'],
-                                replacement_char=box_config['box_safe_prefix']
-                                )) for k in keys]
-        if len(key_list) > len(set(x[1] for x in key_list)):
-            seen, dups = set(), set()
-            for x in key_list:
-                if x[1] in seen:
-                    dups.add(f'{x[0]}({x[1]})')
-                seen.add(x[1])
-            if box_config['box_duplicates'].startswith('warn'):
-                warnings.warn(f'Duplicate conversion attributes exist: {dups}', BoxWarning)
-            else:
-                raise BoxError(f'Duplicate conversion attributes exist: {dups}')
-
-    # This way will be slower for warnings, as it will have double work
-    # But faster for the default 'ignore'
-    for k in keys:
-        if item == _safe_attr(k, camel_killer=box_config['camel_killer_box'],
-                              replacement_char=box_config['box_safe_prefix']):
-            return k
-
-
 def _parse_box_dots(item):
     for idx, char in enumerate(item):
         if char == '[':
@@ -140,8 +66,8 @@ def _parse_box_dots(item):
 def _get_box_config():
     return {
         # Internal use only
-        '__converted': set(),
-        '__created': False
+        '__created': False,
+        '__safe_keys': {}
     }
 
 
@@ -164,7 +90,9 @@ class Box(dict):
     :param box_dots: access nested Boxes by period separated keys in string
     """"""
 
-    _protected_keys = dir({}) + ['to_dict', 'to_json', 'to_yaml', 'from_yaml', 'from_json', 'from_toml', 'to_toml']
+    _protected_keys = dir({}) + ['to_dict', 'to_json', 'to_yaml', 'from_yaml', 'from_json', 'from_toml', 'to_toml',
+                                 '_Box__convert_and_store', '_Box__recast', '_Box__get_default', '_protected_keys',
+                                 '_conversion_checks', 'merge_update', '_safe_attr']
 
     def __new__(cls, *args: Any, default_box: bool = False, default_box_attr: Any = NO_DEFAULT,
                 default_box_none_transform: bool = True, frozen_box: bool = False, camel_killer_box: bool = False,
@@ -198,7 +126,7 @@ def __init__(self, *args: Any, default_box: bool = False, default_box_attr: Any
                  conversion_box: bool = True, modify_tuples_box: bool = False, box_safe_prefix: str = 'x',
                  box_duplicates: str = 'ignore', box_intact_types: Union[Tuple, List] = (),
                  box_recast: Dict = None, box_dots: bool = False, **kwargs: Any):
-        super(Box, self).__init__()
+        super().__init__()
         self._box_config = _get_box_config()
         self._box_config.update({
             'default_box': default_box,
@@ -258,7 +186,7 @@ def __hash__(self):
 
     def __dir__(self):
         allowed = string.ascii_letters + string.digits + '_'
-        items = set(super(Box, self).__dir__())
+        items = set(super().__dir__())
         # Only show items accessible by dot notation
         for key in self.keys():
             key = str(key)
@@ -272,7 +200,7 @@ def __dir__(self):
         for key in self.keys():
             if key not in items:
                 if self._box_config['conversion_box']:
-                    key = _safe_attr(key, replacement_char=self._box_config['box_safe_prefix'])
+                    key = self._safe_attr(key)
                     if key:
                         items.add(key)
 
@@ -293,10 +221,10 @@ def get(self, key, default=NO_DEFAULT):
         return self[key]
 
     def copy(self):
-        return Box(super(Box, self).copy())
+        return Box(super().copy(), **self.__box_config())
 
     def __copy__(self):
-        return Box(super(Box, self).copy())
+        return Box(super().copy(), **self.__box_config())
 
     def __deepcopy__(self, memodict=None):
         frozen = self._box_config['frozen_box']
@@ -314,31 +242,8 @@ def __setstate__(self, state):
         self._box_config = state['_box_config']
         self.__dict__.update(state)
 
-    def __getitem__(self, item, _ignore_default=False):
-        try:
-            return super(Box, self).__getitem__(item)
-        except KeyError as err:
-            if item == '_box_config':
-                raise BoxKeyError('_box_config should only exist as an attribute and is never defaulted') from None
-            if self._box_config['box_dots'] and isinstance(item, str) and ('.' in item or '[' in item):
-                first_item, children = _parse_box_dots(item)
-                if first_item in self.keys():
-                    if hasattr(self[first_item], '__getitem__'):
-                        return self[first_item][children]
-            if self._box_config['conversion_box'] and item:
-                k = _conversion_checks(item, self.keys(), self._box_config)
-                if k:
-                    return self.__getitem__(k)
-            if self._box_config['camel_killer_box'] and isinstance(item, str):
-                converted = _camel_killer(item)
-                if converted in self.keys():
-                    return super(Box, self).__getitem__(converted)
-            if self._box_config['default_box'] and not _ignore_default:
-                return self.__get_default(item)
-            raise BoxKeyError(str(err)) from None
-
     def keys(self):
-        return super(Box, self).keys()
+        return super().keys()
 
     def values(self):
         return [self[x] for x in self.keys()]
@@ -378,16 +283,18 @@ def __recast(self, item, value):
                 raise BoxValueError(f'Cannot convert {value} to {self._box_config[""box_recast""][item]}') from None
         return value
 
-    def __convert_and_store(self, item, value, force_conversion=False):
+    def __convert_and_store(self, item, value):
+        if self._box_config['conversion_box']:
+            safe_key = self._safe_attr(item)
+            self._box_config['__safe_keys'][safe_key] = item
+        if isinstance(value, (int, float, str, bytes, bytearray, bool, complex, set, frozenset)):
+            return super().__setitem__(item, value)
         # If the value has already been converted or should not be converted, return it as-is
-        if ((item in self._box_config['__converted'] and not force_conversion)
-                or (self._box_config['box_intact_types'] and isinstance(value, self._box_config['box_intact_types']))):
-            return value
-        value = self.__recast(item, value)
+        if self._box_config['box_intact_types'] and isinstance(value, self._box_config['box_intact_types']):
+            return super().__setitem__(item, value)
         # This is the magic sauce that makes sub dictionaries into new box objects
         if isinstance(value, dict) and not isinstance(value, Box):
             value = self.__class__(value, **self.__box_config())
-            super(Box, self).__setitem__(item, value)
         elif isinstance(value, list) and not isinstance(value, box.BoxList):
             if self._box_config['frozen_box']:
                 value = _recursive_tuples(value,
@@ -398,8 +305,26 @@ def __convert_and_store(self, item, value, force_conversion=False):
                 value = box.BoxList(value, box_class=self.__class__, **self.__box_config())
         elif self._box_config['modify_tuples_box'] and isinstance(value, tuple):
             value = _recursive_tuples(value, self.__class__, recreate_tuples=True, **self.__box_config())
-        super(Box, self).__setitem__(item, value)
-        self._box_config['__converted'].add(item)
+        super().__setitem__(item, value)
+
+    def __getitem__(self, item, _ignore_default=False):
+        try:
+            return super().__getitem__(item)
+        except KeyError as err:
+            if item == '_box_config':
+                raise BoxKeyError('_box_config should only exist as an attribute and is never defaulted') from None
+            if self._box_config['box_dots'] and isinstance(item, str) and ('.' in item or '[' in item):
+                first_item, children = _parse_box_dots(item)
+                if first_item in self.keys():
+                    if hasattr(self[first_item], '__getitem__'):
+                        return self[first_item][children]
+            if self._box_config['camel_killer_box'] and isinstance(item, str):
+                converted = _camel_killer(item)
+                if converted in self.keys():
+                    return super().__getitem__(converted)
+            if self._box_config['default_box'] and not _ignore_default:
+                return self.__get_default(item)
+            raise BoxKeyError(str(err)) from None
 
     def __getattr__(self, item):
         try:
@@ -414,6 +339,11 @@ def __getattr__(self, item):
                 raise BoxError('_box_config key must exist') from None
             if self._box_config['default_box']:
                 return self.__get_default(item)
+            if self._box_config['conversion_box']:
+                safe_key = self._safe_attr(item)
+                print(self._box_config['__safe_keys'])
+                if safe_key in self._box_config['__safe_keys']:
+                    return self.__getitem__(self._box_config['__safe_keys'][safe_key])
             raise BoxKeyError(str(err)) from None
         return value
 
@@ -426,12 +356,11 @@ def __setitem__(self, key, value):
                 if hasattr(self[first_item], '__setitem__'):
                     return self[first_item].__setitem__(children, value)
         value = self.__recast(key, value)
-        if key not in self.keys() and (self._box_config['conversion_box'] or self._box_config['camel_killer_box']):
-            if self._box_config['conversion_box']:
-                key = _conversion_checks(key, self.keys(), self._box_config) or key
+        if key not in self.keys() and self._box_config['camel_killer_box']:
             if self._box_config['camel_killer_box'] and isinstance(key, str):
                 key = _camel_killer(key)
-        super(Box, self).__setitem__(key, value)
+        if self._box_config['conversion_box'] and self._box_config['box_duplicates'] != 'ignore':
+            self._conversion_checks(key)
         self.__convert_and_store(key, value)
 
     def __setattr__(self, key, value):
@@ -442,6 +371,9 @@ def __setattr__(self, key, value):
         if key == '_box_config':
             return object.__setattr__(self, key, value)
         value = self.__recast(key, value)
+        safe_key = self._safe_attr(key)
+        if safe_key in self._box_config['__safe_keys']:
+            key = self._box_config['__safe_keys'][safe_key]
         self.__setitem__(key, value)
 
     def __delitem__(self, key):
@@ -451,15 +383,13 @@ def __delitem__(self, key):
             first_item, children = key.split('.', 1)
             if first_item in self.keys() and isinstance(self[first_item], dict):
                 return self[first_item].__delitem__(children)
-        if key not in self.keys() and (self._box_config['conversion_box'] or self._box_config['camel_killer_box']):
-            if self._box_config['conversion_box']:
-                key = _conversion_checks(key, self.keys(), self._box_config) or key
+        if key not in self.keys() and self._box_config['camel_killer_box']:
             if self._box_config['camel_killer_box'] and isinstance(key, str):
                 for each_key in self:
                     if _camel_killer(key) == each_key:
                         key = each_key
                         break
-        super(Box, self).__delitem__(key)
+        super().__delitem__(key)
 
     def __delattr__(self, item):
         if self._box_config['frozen_box']:
@@ -468,7 +398,16 @@ def __delattr__(self, item):
             raise BoxError('""_box_config"" is protected')
         if item in self._protected_keys:
             raise BoxKeyError(f'Key name ""{item}"" is protected')
-        self.__delitem__(item)
+        try:
+            self.__delitem__(item)
+        except KeyError as err:
+            if self._box_config['conversion_box']:
+                safe_key = self._safe_attr(item)
+                if safe_key in self._box_config['__safe_keys']:
+                    self.__delitem__(self._box_config['__safe_keys'][safe_key])
+                    del self._box_config['__safe_keys'][safe_key]
+                    return
+            raise BoxKeyError(err)
 
     def pop(self, key, *args):
         if args:
@@ -490,7 +429,8 @@ def pop(self, key, *args):
             return item
 
     def clear(self):
-        super(Box, self).clear()
+        super().clear()
+        self._box_config['__safe_keys'].clear()
 
     def popitem(self):
         try:
@@ -533,12 +473,12 @@ def update(self, __m=None, **kwargs):
         if __m:
             if hasattr(__m, 'keys'):
                 for k in __m:
-                    self.__convert_and_store(k, __m[k], force_conversion=True)
+                    self.__convert_and_store(k, __m[k])
             else:
                 for k, v in __m:
-                    self.__convert_and_store(k, v, force_conversion=True)
+                    self.__convert_and_store(k, v)
         for k in kwargs:
-            self.__convert_and_store(k, kwargs[k], force_conversion=True)
+            self.__convert_and_store(k, kwargs[k])
 
     def merge_update(self, __m=None, **kwargs):
         def convert_and_set(k, v):
@@ -548,14 +488,14 @@ def convert_and_set(k, v):
                 # in the `converted` box_config set
                 v = self.__class__(v, **self.__box_config())
                 if k in self and isinstance(self[k], dict):
-                    self[k].update(v)
+                    if isinstance(self[k], Box):
+                        self[k].merge_update(v)
+                    else:
+                        self[k].update(v)
                     return
             if isinstance(v, list) and not intact_type:
                 v = box.BoxList(v, **self.__box_config())
-            try:
-                self.__setattr__(k, v)
-            except (AttributeError, TypeError):
-                self.__setitem__(k, v)
+            self.__setitem__(k, v)
 
         if __m:
             if hasattr(__m, 'keys'):
@@ -578,6 +518,59 @@ def setdefault(self, item, default=None):
         self[item] = default
         return default
 
+    def _safe_attr(self, attr):
+        """"""Convert a key into something that is accessible as an attribute""""""
+        allowed = string.ascii_letters + string.digits + '_'
+
+        if isinstance(attr, tuple):
+            attr = ""_"".join([str(x) for x in attr])
+
+        attr = attr.decode('utf-8', 'ignore') if isinstance(attr, bytes) else str(attr)
+        if self.__box_config()['camel_killer_box']:
+            attr = _camel_killer(attr)
+
+        out = []
+        last_safe = 0
+        for i, character in enumerate(attr):
+            if character in allowed:
+                last_safe = i
+                out.append(character)
+            elif not out:
+                continue
+            else:
+                if last_safe == i - 1:
+                    out.append('_')
+
+        out = """".join(out)[:last_safe + 1]
+
+        try:
+            int(out[0])
+        except (ValueError, IndexError):
+            pass
+        else:
+            out = f'{self.__box_config()[""box_safe_prefix""]}{out}'
+
+        if out in kwlist:
+            out = f'{self.__box_config()[""box_safe_prefix""]}{out}'
+
+        return out
+
+    def _conversion_checks(self, item):
+        """"""
+        Internal use for checking if a duplicate safe attribute already exists
+
+        :param item: Item to see if a dup exists
+        :param keys: Keys to check against
+        """"""
+        safe_item = self._safe_attr(item)
+
+        if safe_item in self._box_config['__safe_keys']:
+            dups = [f'{item}({safe_item})', f'{self._box_config[""__safe_keys""][safe_item]}({safe_item})']
+            if self._box_config['box_duplicates'].startswith('warn'):
+                warnings.warn(f'Duplicate conversion attributes exist: {dups}', BoxWarning)
+            else:
+                raise BoxError(f'Duplicate conversion attributes exist: {dups}')
+
     def to_json(self, filename: Union[str, Path] = None, encoding: str = 'utf-8', errors: str = 'strict',
                 **json_kwargs):
         """""""
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -2,15 +2,14 @@
 # -*- coding: UTF-8 -*-
 #
 # Copyright (c) 2017-2020 - Chris Griffith - MIT License
-from typing import Iterable
-import re
 import copy
+import re
+from typing import Iterable
 
-
+import box
 from box.converters import (_to_yaml, _from_yaml, _to_json, _from_json,
                             _to_toml, _from_toml, _to_csv, _from_csv, BOX_PARAMETERS)
 from box.exceptions import BoxError, BoxTypeError, BoxKeyError
-import box
 
 _list_pos_re = re.compile(r'\[(\d+)\]')
 "
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -17,25 +17,20 @@ class ConfigBox(Box):
     cns.list('my_list', mod=lambda x: int(x)) # [5, 4, 3, 3, 2]
     """"""
 
-    _protected_keys = dir({}) + ['to_dict', 'bool', 'int', 'float',
-                                 'list', 'getboolean', 'to_json', 'to_yaml',
-                                 'getfloat', 'getint',
-                                 'from_json', 'from_yaml']
+    _protected_keys = dir(Box) + ['bool', 'int', 'float', 'list', 'getboolean', 'getfloat', 'getint']
 
     def __getattr__(self, item):
         """"""
         Config file keys are stored in lower case, be a little more
         loosey goosey
         """"""
         try:
-            return super(ConfigBox, self).__getattr__(item)
+            return super().__getattr__(item)
         except AttributeError:
-            return super(ConfigBox, self).__getattr__(item.lower())
+            return super().__getattr__(item.lower())
 
     def __dir__(self):
-        return super(ConfigBox, self).__dir__() + ['bool', 'int', 'float',
-                                                   'list', 'getboolean',
-                                                   'getfloat', 'getint']
+        return super().__dir__() + ['bool', 'int', 'float', 'list', 'getboolean', 'getfloat', 'getint']
 
     def bool(self, item, default=None):
         """"""
@@ -132,7 +127,7 @@ def __repr__(self):
         return '<ConfigBox: {0}>'.format(str(self.to_dict()))
 
     def copy(self):
-        return ConfigBox(super(ConfigBox, self).copy())
+        return ConfigBox(super().copy())
 
     def __copy__(self):
-        return ConfigBox(super(ConfigBox, self).copy())
+        return ConfigBox(super().copy())"
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -3,11 +3,11 @@
 
 # Abstract converter functions for use in any Box class
 
-import sys
 import csv
 import json
-from pathlib import Path
+import sys
 import warnings
+from pathlib import Path
 
 from box.exceptions import BoxError, BoxWarning
 
@@ -18,7 +18,6 @@
     warnings.warn(""ruamel.yaml was not detected, using PyYAML instead, which may not support YAML 1.2"", BoxWarning)
 import toml
 
-
 BOX_PARAMETERS = ('default_box', 'default_box_attr', 'conversion_box',
                   'frozen_box', 'camel_killer_box',
                   'box_safe_prefix', 'box_duplicates', 'ordered_box',"
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -1,8 +1,8 @@
 #!/usr/bin/env python
 # -*- coding: UTF-8 -*-
+from json import JSONDecodeError
 from pathlib import Path
 from typing import Union
-from json import JSONDecodeError
 
 from toml import TomlDecodeError
 try:"
45;cdgriffith;Box;ec6cc77390a2b6dbdbc11d5c482f06626c8cd4cb;"Version 4.2.0 (#136)

* Adding optimizations for speed ups to creation and inserts
* Adding internal record of safe attributes for faster lookups, increases memory footprint for speed (thanks to Jonas Irgens Kylling)
* Adding all additional methods specific to `Box` as protected keys
* Fixing `merge_update` from incorrectly calling `__setattr__` which was causing a huge slowdown (thanks to Jonas Irgens Kylling)
* Fixing `copy` and `__copy__` not copying box options";"@@ -37,10 +37,10 @@ def temp_dir_cleanup(self):
         shutil.rmtree(tmp_dir, ignore_errors=True)
 
     def test_safe_attrs(self):
-        assert box._safe_attr(""BAD!KEY!1"", camel_killer=False) == ""BAD_KEY_1""
-        assert box._safe_attr(""BAD!KEY!2"", camel_killer=True) == ""bad_key_2""
-        assert box._safe_attr((5, 6, 7), camel_killer=False) == ""x5_6_7""
-        assert box._safe_attr(356, camel_killer=False) == ""x356""
+        assert Box()._safe_attr(""BAD!KEY!1"") == ""BAD_KEY_1""
+        assert Box(camel_killer_box=True)._safe_attr(""BAD!KEY!2"") == ""bad_key_2""
+        assert Box()._safe_attr((5, 6, 7)) == ""x5_6_7""
+        assert Box()._safe_attr(356) == ""x356""
 
     def test_camel_killer(self):
         assert box._camel_killer(""CamelCase"") == ""camel_case""
@@ -583,10 +583,11 @@ def test_duplicate_errors(self):
             my_box['^a'] = 3
 
     def test_copy(self):
-        my_box = Box(movie_data)
+        my_box = Box(movie_data, camel_killer_box=True)
         bb = my_box.copy()
         assert my_box == bb
         assert isinstance(bb, Box)
+        assert bb._box_config['camel_killer_box']
 
         aa = copy.deepcopy(my_box)
         assert my_box == aa
@@ -595,6 +596,7 @@ def test_copy(self):
         cc = my_box.__copy__()
         assert my_box == cc
         assert isinstance(cc, Box)
+        assert cc._box_config['camel_killer_box']
 
         dd = BoxList([my_box])
         assert dd == copy.copy(dd)"
45;SublimeText;LaTeXTools;58bb2fd61486ff1fbb38f1c7bfc6238d50bf99b4;"Improve Ghostscript handling for large equations

Changes to GS use for generating equation previews:
* Exposes new (advanced) parameters MaxBitmap, BufferSpace as preview_math_max_bitmap and preview_math_bufferspace. Modifying these will change the amount of memory GS uses, enabling it to handle larger images.
* Utilises -dNumRenderingThreads to enable GS to take advantage of multiple cores. By itself, this seems to help with some large equations on modern hardware.
* Try to display GS error messages when GS errors occur to provide better user feedback (note that many GS error messages are very cryptic, but hopefully better than giving no indication what the issue is).
* Moves preview_math_scale_quotient, preview_math_density, and preview_math_hires to the LaTeXTools (Advanced).sublime-settings file by default, where they belong.";"@@ -209,10 +209,33 @@
 	// exceed the size images will be deleted until the folders are 10% of that size.
 	""preview_math_temp_size"": 50,
 	""preview_image_temp_size"": 30,
+
 	// The duration in which this exceeding will be checked in hours.
 	// Use -1 to not automatically delete the files.
 	""preview_temp_delete_period"": 24,
 
+	// --- Settings for Ghostscript conversion to PNG ---
+	// The density of the preview image. The higher the density the larger the phantom.
+	""preview_math_density"": 150,
+	// If the image is not sharp enough increase this scale to get a better resolution.
+	// However also change the density by the same factor to keep the size the
+	// same.
+	""preview_math_scale_quotient"": 1,
+	// If this is true, the image will be rendered at a higher resolution and
+	// then scaled down. This generally results in a clearer image.
+	""preview_math_hires"": true,
+
+	// These settings control the amount of memory used by Ghostscript
+	// when converting a PDF to PNG. You may want to adjust them if you
+	// use complex equations. The defaults for both of these values are the
+	// Ghostscript defaults.
+	// 
+	// Controls the maximum size of the image stored in memory
+	""preview_math_max_bitmap"": 1000000,
+	// Controls the maximum buffer space for images that do not fit
+	// in the `preview_math_max_bitmap` space
+	""preview_math_bufferspace"": 4000000,
+
 
 // ------------------------------------------------------------------
 // Additional fill all helper settings (for experts)
@@ -274,4 +297,4 @@
 	// bracket auto triggering looks 5 lines before the selection and 5 lines
 	// after
 	""smart_bracket_look_around"": 5
-}
\ No newline at end of file
+}"
45;SublimeText;LaTeXTools;58bb2fd61486ff1fbb38f1c7bfc6238d50bf99b4;"Improve Ghostscript handling for large equations

Changes to GS use for generating equation previews:
* Exposes new (advanced) parameters MaxBitmap, BufferSpace as preview_math_max_bitmap and preview_math_bufferspace. Modifying these will change the amount of memory GS uses, enabling it to handle larger images.
* Utilises -dNumRenderingThreads to enable GS to take advantage of multiple cores. By itself, this seems to help with some large equations on modern hardware.
* Try to display GS error messages when GS errors occur to provide better user feedback (note that many GS error messages are very cryptic, but hopefully better than giving no indication what the issue is).
* Moves preview_math_scale_quotient, preview_math_density, and preview_math_hires to the LaTeXTools (Advanced).sublime-settings file by default, where they belong.";"@@ -158,15 +158,6 @@
 	// therefore you can create a blacklist of such environments here.
 	""preview_math_no_star_envs"": [""displaymath""],
 
-	// The density of the preview image. The higher the density the bigger the phantom.
-	""preview_math_density"": 150,
-	// If the image is not sharp enough increase this scale to get a better resolution.
-	// However also change the density by the same factor to keep the size.
-	""preview_math_scale_quotient"": 1,
-	// If this is true, the image will be rendered at a higher resolution and
-	// then scaled down. This generally results in a clearer image.
-	""preview_math_hires"": true,
-
 	// IMAGE PREVIEW
 
 	// The preview mode for image preview, possible values are:"
45;SublimeText;LaTeXTools;58bb2fd61486ff1fbb38f1c7bfc6238d50bf99b4;"Improve Ghostscript handling for large equations

Changes to GS use for generating equation previews:
* Exposes new (advanced) parameters MaxBitmap, BufferSpace as preview_math_max_bitmap and preview_math_bufferspace. Modifying these will change the amount of memory GS uses, enabling it to handle larger images.
* Utilises -dNumRenderingThreads to enable GS to take advantage of multiple cores. By itself, this seems to help with some large equations on modern hardware.
* Try to display GS error messages when GS errors occur to provide better user feedback (note that many GS error messages are very cryptic, but hopefully better than giving no indication what the issue is).
* Moves preview_math_scale_quotient, preview_math_density, and preview_math_hires to the LaTeXTools (Advanced).sublime-settings file by default, where they belong.";"@@ -16,6 +16,7 @@
 
 from ..latextools_utils import cache, get_setting
 from ..latextools_utils.external_command import execute_command
+from ..latextools_utils.utils import cpu_count
 from . import preview_utils
 from .preview_utils import (
     ghostscript_installed, get_ghostscript_version, run_ghostscript_command)
@@ -38,10 +39,10 @@
     def get_color(view):
         try:
             color = mdpopups.scope2style(view, """").get(""color"", ""#CCCCCC"")
-        except:
+        except Exception:
             color = ""#CCCCCC""
         return color
-except:
+except Exception:
     def get_color(view):
         return ""#CCCCCC""
 
@@ -72,17 +73,23 @@ def get_color(view):
 _scale_quotient = 1
 _density = 150
 _hires = True
+# these are the default values derived from gxdevice.h in the
+# Ghostscript source code
+_max_bitmap = 1000000
+_bufferspace = 4000000
 _lt_settings = {}
 
 _name = ""preview_math""
 
 
 def _on_setting_change():
-    global _density, _scale_quotient, _hires
+    global _density, _scale_quotient, _hires, _max_bitmap, _bufferspace
     _scale_quotient = _lt_settings.get(
         ""preview_math_scale_quotient"", _scale_quotient)
     _density = _lt_settings.get(""preview_math_density"", _density)
     _hires = _lt_settings.get(""preview_math_hires"", _hires)
+    _max_bitmap = _lt_settings.get(""preview_math_max_bitmap"", _max_bitmap)
+    _bufferspace = _lt_settings.get(""preview_math_bufferspace"", _bufferspace)
     max_threads = get_setting(
         ""preview_max_convert_threads"", default=None, view={})
     if max_threads is not None:
@@ -117,10 +124,13 @@ def _create_image(latex_program, latex_document, base_name, color,
     pdf_path = os.path.join(temp_path, base_name + "".pdf"")
     image_path = os.path.join(temp_path, base_name + _IMAGE_EXTENSION)
 
-    # do nothing if the pdf already exists
-    if os.path.exists(pdf_path):
+    # do nothing if the image already exists
+    if os.path.exists(image_path):
         return
 
+    err_log = []
+    gs_error_occurred = False
+
     # write the latex document
     source_path = os.path.join(temp_path, rel_source_path)
     with open(source_path, ""w"", encoding=""utf-8"") as f:
@@ -165,13 +175,21 @@ def _create_image(latex_program, latex_document, base_name, color,
         scale_factor = \
             8 if _hires and get_ghostscript_version() >= (9, 14) else 1
 
+        # allow Ghostscript to use multiple CPUs, up to two less than the
+        # total number (so that sublime_text and plugin_host are minimally
+        # affected)
+        cpus = max(cpu_count() - 2, 1)
+
         # convert the pdf to a png image
         command = [
             '-sDEVICE=pngalpha', '-dLastPage=1',
             '-sOutputFile={image_path}'.format(image_path=image_path),
             '-r{density}'.format(density=_density * scale_factor),
             '-dDownScaleFactor={0}'.format(scale_factor),
-            '-dTextAlphaBits=4', '-dGraphicsAlphaBits=4'
+            '-dTextAlphaBits=4', '-dGraphicsAlphaBits=4',
+            '-dNumRenderingThreads={0}'.format(cpus),
+            '-dMaxBitmap={0}'.format(_max_bitmap),
+            '-dBufferSpace={0}'.format(_bufferspace)
         ]
 
         # calculate and apply cropping boundaries, if we have them
@@ -197,10 +215,29 @@ def _create_image(latex_program, latex_document, base_name, color,
 
         command.append(pdf_path)
 
-        run_ghostscript_command(command)
+        rc, output, _ = run_ghostscript_command(
+            command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
+
+        if rc != 0:
+            gs_error_occurred = True
+            err_log.append(
+                'Error while running Ghostscript. {0}'.format(output))
+
+        # Ghostscript will output a 0 byte sized image if certain issues
+        # occur. Deal with this here.
+        if os.path.exists(image_path) and os.path.getsize(image_path) < 1:
+            os.remove(image_path)
+            gs_error_occurred = True
+            err_log.append(
+                'Ghostscript could not produce an image. '
+                'Please try changing the preview_math_bufferspace setting '
+                'to a larger value. Currently: {0}. '.format(_bufferspace) +
+                'This setting can be found in the LaTeXTools (Advanced) '
+                'settings file.')
+
+        if gs_error_occurred:
+            err_log.append('')
 
-    err_file_path = image_path + _ERROR_EXTENSION
-    err_log = []
     if not pdf_exists:
         err_log.append(
             ""Failed to run '{latex_program}' to create pdf to preview.""
@@ -217,11 +254,13 @@ def _create_image(latex_program, latex_document, base_name, color,
         else:
             with open(log_file, ""rb"") as f:
                 log_data = f.read()
+
             try:
                 errors, warnings, _ = parse_tex_log(log_data, temp_path)
-            except:
-                err_log.append(""Error while parsing log file."")
+            except Exception as e:
+                err_log.append(""Error while parsing log file: {0}"".format(e))
                 errors = warnings = []
+
             if errors:
                 err_log.append(""Errors:"")
                 err_log.extend(errors)
@@ -242,10 +281,11 @@ def _create_image(latex_program, latex_document, base_name, color,
             err_log.append(""-----BEGIN LOG-----"")
             err_log.append(log_content)
             err_log.append(""-----END LOG-----"")
-    elif not os.path.exists(image_path):
+    elif not gs_error_occurred and not os.path.exists(image_path):
         err_log.append(""Failed to convert pdf to png to preview."")
 
     if err_log:
+        err_file_path = image_path + _ERROR_EXTENSION
         with open(err_file_path, ""w"", encoding=""utf-8"") as f:
             f.write(""\n"".join(err_log))
 
@@ -476,6 +516,14 @@ def update_template_file(init=False):
             ""_watch_hires"": {
                 ""setting"": ""preview_math_hires"",
                 ""call_after"": self.reset_phantoms
+            },
+            ""_watch_max_bitmap"": {
+                ""setting"": ""preview_math_max_bitmap"",
+                ""call_after"": self.reset_phantoms
+            },
+            ""_watch_bufferspace"": {
+                ""setting"": ""preview_math_bufferspace"",
+                ""call_after"": self.reset_phantoms
             }
         }
         for attr_name, d in watch_attr.items():
@@ -502,7 +550,7 @@ def _read_latex_template_file(self, refresh=False):
                     old_mtime = self.template_mtime[self.latex_template_file]
                     if old_mtime == mtime:
                         return
-                except:
+                except Exception:
                     return
 
             mtime = 0
@@ -708,7 +756,7 @@ def is_dollar_snippet(scope):
                 # update the content and the layout
                 p.content = content
                 p.layout = layout
-            except:
+            except Exception:
                 p = types.SimpleNamespace(
                     id=None,
                     region=region,
@@ -826,7 +874,7 @@ def _create_document(self, scope, color):
             latex_template = self.template_contents[self.latex_template_file]
             if not latex_template:
                 raise Exception(""Template must not be empty!"")
-        except:
+        except Exception:
             latex_template = default_latex_template
 
         if color.startswith(""#""):"
45;RJT1990;pyflux;297f2afc2095acd97c12e827dd500e8ea5da0c0f;"Merge pull request #135 from TerryJey/patch-1

Fix memory leak";"@@ -498,7 +498,7 @@ def _mean_prediction(self, mu, Y, h, t_z):
                 Y_exp = np.append(Y_exp, [self.link(new_value)])
 
             mu_exp = np.append(mu_exp,[0]) # For indexing consistency
-
+        del mu_exp
         return Y_exp
 
     def _sim_prediction(self, mu, Y, h, t_z, simulations):
@@ -562,7 +562,8 @@ def _sim_prediction(self, mu, Y, h, t_z, simulations):
                 mu_exp = np.append(mu_exp, [0]) # For indexing consistency
 
                 sim_vector[n] = Y_exp[-h:]
-
+            del Y_exp
+            del mu_exp
         return np.transpose(sim_vector)
 
     def _sim_prediction_bayes(self, h, simulations):
@@ -622,7 +623,8 @@ def _sim_prediction_bayes(self, h, simulations):
                 mu_exp = np.append(mu_exp, [0]) # For indexing consistency
 
                 sim_vector[n] = Y_exp[-h:]
-
+            del Y_exp
+            del mu_exp
         return np.transpose(sim_vector)
 
     def _summarize_simulations(self, mean_values, sim_vector, date_index, h, past_values):
@@ -1105,4 +1107,4 @@ def plot_ppc(self, nsims=1000, T=np.mean, **kwargs):
             ax.axvline(T_actual)
             sns.distplot(T_sim, kde=False, ax=ax)
             ax.set(title='Posterior predictive' + description, xlabel='T(x)', ylabel='Frequency');
-            plt.show()
\ No newline at end of file
+            plt.show()"
45;RJT1990;pyflux;895863e793d2648d9c1c7288465eb0b59a6a999d;"Fix memory leak

Y_exp = Y.copy()
mu_exp = mu.copy()

The above 2 lines cause memory leaks. Added del statements at the end of method so that the copies are Garbage collected.";"@@ -498,7 +498,7 @@ def _mean_prediction(self, mu, Y, h, t_z):
                 Y_exp = np.append(Y_exp, [self.link(new_value)])
 
             mu_exp = np.append(mu_exp,[0]) # For indexing consistency
-
+        del mu_exp
         return Y_exp
 
     def _sim_prediction(self, mu, Y, h, t_z, simulations):
@@ -562,7 +562,8 @@ def _sim_prediction(self, mu, Y, h, t_z, simulations):
                 mu_exp = np.append(mu_exp, [0]) # For indexing consistency
 
                 sim_vector[n] = Y_exp[-h:]
-
+            del Y_exp
+            del mu_exp
         return np.transpose(sim_vector)
 
     def _sim_prediction_bayes(self, h, simulations):
@@ -622,7 +623,8 @@ def _sim_prediction_bayes(self, h, simulations):
                 mu_exp = np.append(mu_exp, [0]) # For indexing consistency
 
                 sim_vector[n] = Y_exp[-h:]
-
+            del Y_exp
+            del mu_exp
         return np.transpose(sim_vector)
 
     def _summarize_simulations(self, mean_values, sim_vector, date_index, h, past_values):
@@ -1105,4 +1107,4 @@ def plot_ppc(self, nsims=1000, T=np.mean, **kwargs):
             ax.axvline(T_actual)
             sns.distplot(T_sim, kde=False, ax=ax)
             ax.set(title='Posterior predictive' + description, xlabel='T(x)', ylabel='Frequency');
-            plt.show()
\ No newline at end of file
+            plt.show()"
