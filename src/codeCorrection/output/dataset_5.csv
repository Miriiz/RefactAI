Label;Page;Username;Repo;Commit;Bug;Code
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def my_load_checkpoint(model, filename, map_location=None, strict=False, logger=
     elif isinstance(checkpoint, dict) and 'model' in checkpoint:
         state_dict = checkpoint['model']  # for classification weights
     else:
-        raise RuntimeError(
-            'No state_dict found in checkpoint file {}'.format(filename))
     # strip prefix of state_dict
     if list(state_dict.keys())[0].startswith('module.'):
         state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def my_load_checkpoint(model, filename, map_location=None, strict=False, logger=
     elif isinstance(checkpoint, dict) and 'model' in checkpoint:
         state_dict = checkpoint['model']  # for classification weights
     else:
+        state_dict = checkpoint #  fix ""No state_dict found in checkpoint file""
+        # raise RuntimeError(
+        #     'No state_dict found in checkpoint file {}'.format(filename))
     # strip prefix of state_dict
     if list(state_dict.keys())[0].startswith('module.'):
         state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, dim=768):
     def forward(self, x, H, W):
         B, N, C = x.shape
         n = N // 21
-        x1 = x[:, 0:16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2)
-        x2 = x[:, 16 * n:20 * n, :].transpose(1, 2).view(B, C, H, W)
-        x3 = x[:, 20 * n:, :].transpose(1, 2).view(B, C, H // 2, W // 2)
         x1 = self.dwconv(x1).flatten(2).transpose(1, 2)
         x2 = self.dwconv(x2).flatten(2).transpose(1, 2)
         x3 = self.dwconv(x3).flatten(2).transpose(1, 2)
@@ -143,7 +143,8 @@ def __init__(self, dim, num_heads=6, n_points=4, norm_layer=partial(nn.LayerNorm
         if extra_extractor:
             self.extra_extractors = nn.Sequential(*[
                 Extractor(dim=dim, num_heads=num_heads, n_points=n_points, norm_layer=norm_layer,
-                          with_cffn=with_cffn, cffn_ratio=cffn_ratio, deform_ratio=deform_ratio)
                 for _ in range(2)
             ])
         else:
@@ -200,7 +201,7 @@ def __init__(self, inplanes=64, embed_dim=384):
         self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc2 = nn.Conv2d(2 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc3 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
-        self.fc4 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0,  bias=True)
 
     def forward(self, x):
         c1 = self.stem(x)"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, dim=768):
     def forward(self, x, H, W):
         B, N, C = x.shape
         n = N // 21
+        x1 = x[:, 0:16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2).contiguous()
+        x2 = x[:, 16 * n:20 * n, :].transpose(1, 2).view(B, C, H, W).contiguous()
+        x3 = x[:, 20 * n:, :].transpose(1, 2).view(B, C, H // 2, W // 2).contiguous()
         x1 = self.dwconv(x1).flatten(2).transpose(1, 2)
         x2 = self.dwconv(x2).flatten(2).transpose(1, 2)
         x3 = self.dwconv(x3).flatten(2).transpose(1, 2)
@@ -143,7 +143,8 @@ def __init__(self, dim, num_heads=6, n_points=4, norm_layer=partial(nn.LayerNorm
         if extra_extractor:
             self.extra_extractors = nn.Sequential(*[
                 Extractor(dim=dim, num_heads=num_heads, n_points=n_points, norm_layer=norm_layer,
+                          with_cffn=with_cffn, cffn_ratio=cffn_ratio, deform_ratio=deform_ratio,
+                          drop=drop, drop_path=drop_path)
                 for _ in range(2)
             ])
         else:
@@ -200,7 +201,7 @@ def __init__(self, inplanes=64, embed_dim=384):
         self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc2 = nn.Conv2d(2 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc3 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
+        self.fc4 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
 
     def forward(self, x):
         c1 = self.stem(x)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" 
 import torch
 import torch.nn.functional as F
 from mmcv.runner import load_checkpoint
 from mmdet.utils import get_root_logger
 from timm.models.layers import DropPath
@@ -218,9 +219,10 @@ def forward(self, hidden_states, input_tensor):
 
 class BertLayer(nn.Module):
     def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=12,
-                 drop_path_ratio=0.1, windowed=False, window_size=14):
 
         super(BertLayer, self).__init__()
         self.attention = BertAttention(hidden_size, num_attention_heads,
                                        drop_path_ratio, windowed, window_size)
 
@@ -230,10 +232,19 @@ def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=
                                  drop_path_ratio=drop_path_ratio)
 
     def forward(self, hidden_states, H, W):
-        attention_output = self.attention(hidden_states, H, W)
-        intermediate_output = self.intermediate(attention_output)
-        layer_output = self.output(intermediate_output, attention_output)
-        return layer_output
 
 
 class VisualPatchEmbedding(nn.Module):
@@ -299,7 +310,8 @@ def forward(self, x):
 class UnifiedBertEncoder(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,
                  num_heads=12, mlp_ratio=4., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
-                 embed_layer=VisualPatchEmbedding, window_attn=False, window_size=14, pretrained=None):
 
         super(UnifiedBertEncoder, self).__init__()
         self.embed_dim = embed_dim
@@ -316,7 +328,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth
             layers.append(
                 BertLayer(hidden_size=embed_dim, intermediate_size=int(embed_dim * mlp_ratio),
                           num_attention_heads=num_heads, drop_path_ratio=drop_path_rate,
-                          windowed=window_attn[i], window_size=window_size[i])
             )
 
         self.layers = nn.ModuleList(layers)"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" 
 import torch
 import torch.nn.functional as F
+import torch.utils.checkpoint as cp
 from mmcv.runner import load_checkpoint
 from mmdet.utils import get_root_logger
 from timm.models.layers import DropPath
@@ -218,9 +219,10 @@ def forward(self, hidden_states, input_tensor):
 
 class BertLayer(nn.Module):
     def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=12,
+                 drop_path_ratio=0.1, windowed=False, window_size=14, with_cp=False):
 
         super(BertLayer, self).__init__()
+        self.with_cp = with_cp
         self.attention = BertAttention(hidden_size, num_attention_heads,
                                        drop_path_ratio, windowed, window_size)
 
@@ -230,10 +232,19 @@ def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=
                                  drop_path_ratio=drop_path_ratio)
 
     def forward(self, hidden_states, H, W):
+        
+        def _inner_forward(hidden_states):
+            attention_output = self.attention(hidden_states, H, W)
+            intermediate_output = self.intermediate(attention_output)
+            layer_output = self.output(intermediate_output, attention_output)
+            return layer_output
+
+        if self.with_cp and hidden_states.requires_grad:
+            x = cp.checkpoint(_inner_forward, hidden_states)
+        else:
+            x = _inner_forward(hidden_states)
+
+        return x
 
 
 class VisualPatchEmbedding(nn.Module):
@@ -299,7 +310,8 @@ def forward(self, x):
 class UnifiedBertEncoder(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,
                  num_heads=12, mlp_ratio=4., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
+                 embed_layer=VisualPatchEmbedding, window_attn=False, window_size=14,
+                 with_cp=False, pretrained=None):
 
         super(UnifiedBertEncoder, self).__init__()
         self.embed_dim = embed_dim
@@ -316,7 +328,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth
             layers.append(
                 BertLayer(hidden_size=embed_dim, intermediate_size=int(embed_dim * mlp_ratio),
                           num_attention_heads=num_heads, drop_path_ratio=drop_path_rate,
+                          windowed=window_attn[i], window_size=window_size[i], with_cp=with_cp)
             )
 
         self.layers = nn.ModuleList(layers)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.runner import BaseModule
 from mmcv_custom import my_load_checkpoint as load_checkpoint
 from mmdet.utils import get_root_logger
@@ -208,10 +209,11 @@ def forward(self, x, H, W):
 
 
 class Block(nn.Module):
-    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
                  attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                  windowed=False, window_size=14, pad_mode='constant', layer_scale=False):
         super().__init__()
         self.norm1 = norm_layer(dim)
         if windowed:
             self.attn = WindowedAttention(dim, num_heads=num_heads,
@@ -233,12 +235,21 @@ def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
             self.gamma2 = nn.Parameter(torch.ones((dim)), requires_grad=True)
 
     def forward(self, x, H, W):
-        if self.layer_scale:
-            x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), H, W))
-            x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
         else:
-            x = x + self.drop_path(self.attn(self.norm1(x), H, W))
-            x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
@@ -254,7 +265,7 @@ class TIMMVisionTransformer(BaseModule):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768,
                  depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,
                  drop_path_rate=0., layer_scale=True, embed_layer=PatchEmbed, norm_layer=partial(nn.LayerNorm, eps=1e-6),
-                 act_layer=nn.GELU, window_attn=False, window_size=14, pretrained=None):
         """"""
         Args:
             img_size (int, tuple): input image size
@@ -272,6 +283,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
             embed_layer (nn.Module): patch embedding layer
             norm_layer: (nn.Module): normalization layer
             pretrained: (str): pretrained path
         """"""
         super().__init__()
         self.num_classes = num_classes
@@ -307,7 +319,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
                   qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
                   drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer,
                   windowed=window_attn[i], window_size=window_size[i],
-                  layer_scale=layer_scale) for i in range(depth)
         ])
 
         self.init_weights(pretrained)"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" import torch
 import torch.nn as nn
 import torch.nn.functional as F
+import torch.utils.checkpoint as cp
 from mmcv.runner import BaseModule
 from mmcv_custom import my_load_checkpoint as load_checkpoint
 from mmdet.utils import get_root_logger
@@ -208,10 +209,11 @@ def forward(self, x, H, W):
 
 
 class Block(nn.Module):
+    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., with_cp=False,
                  attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                  windowed=False, window_size=14, pad_mode='constant', layer_scale=False):
         super().__init__()
+        self.with_cp = with_cp
         self.norm1 = norm_layer(dim)
         if windowed:
             self.attn = WindowedAttention(dim, num_heads=num_heads,
@@ -233,12 +235,21 @@ def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
             self.gamma2 = nn.Parameter(torch.ones((dim)), requires_grad=True)
 
     def forward(self, x, H, W):
+        
+        def _inner_forward(x):
+            if self.layer_scale:
+                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), H, W))
+                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
+            else:
+                x = x + self.drop_path(self.attn(self.norm1(x), H, W))
+                x = x + self.drop_path(self.mlp(self.norm2(x)))
+            return x
+
+        if self.with_cp and x.requires_grad:
+            x = cp.checkpoint(_inner_forward, x)
         else:
+            x = _inner_forward(x)
+        
         return x
 
 
@@ -254,7 +265,7 @@ class TIMMVisionTransformer(BaseModule):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768,
                  depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,
                  drop_path_rate=0., layer_scale=True, embed_layer=PatchEmbed, norm_layer=partial(nn.LayerNorm, eps=1e-6),
+                 act_layer=nn.GELU, window_attn=False, window_size=14, with_cp=False, pretrained=None):
         """"""
         Args:
             img_size (int, tuple): input image size
@@ -272,6 +283,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
             embed_layer (nn.Module): patch embedding layer
             norm_layer: (nn.Module): normalization layer
             pretrained: (str): pretrained path
+            with_cp: (bool): use checkpoint or not
         """"""
         super().__init__()
         self.num_classes = num_classes
@@ -307,7 +319,7 @@ def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
                   qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
                   drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer,
                   windowed=window_attn[i], window_size=window_size[i],
+                  layer_scale=layer_scale, with_cp=with_cp) for i in range(depth)
         ])
 
         self.init_weights(pretrained)"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, conv_inplane=64, n_points=4, deform_num_he
         self.version = version
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
-        self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, conv_inplane=64, n_points=4, deform_num_he
         self.version = version
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.layers)
         self.pretrain_size = (pretrain_size, pretrain_size)
-        self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.layers)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
-        self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;" CWD = os.path.dirname(__file__)
 sys.path.append(os.path.join(CWD, '..'))
 from scan.modules import *
-from utils.net import get_all_ip
-from utils.base import multi_thread, multi_process, process_bar, save_res
 
 
 class Base:
@@ -43,8 +43,8 @@ class CameraScanner(Base):
     def __init__(self, in_file: str, out_file: str) -> None:
         super().__init__(in_file, out_file)
         self.scanner_name = 'camera scanner'
-        self.ip_list = []
         self.lock = Lock()
         self.total = 0
         self.found = 0
         self.done = 0
@@ -56,31 +56,28 @@ def __init__(self, in_file: str, out_file: str) -> None:
     def _get_ip(self):
         with open(self.in_file, 'r') as f:
             for line in f:
-                if line.strip():
-                    if not line.startswith('#'):
-                        if '-' in line or '/' in line:
-                            self.ip_list.extend(get_all_ip(line.strip()))
-                        else:
-                            self.ip_list.append(line.strip())
-        self.total = len(self.ip_list)
 
     def _step(self, *args, **kwargs):
         with self.lock:
             if kwargs['found']:
                 self.found += 1
             self.bar(self.total, self.done + 1, self.found, timer=True, start_time=self.start_time)
 
-    def scan(self, ip):
-        for mod in self.modules:
-            found = False
-            try:
-                res = mod(ip)
-                if res[0]:
-                    found = True
-                    save_res(self.out_file, [ip] + res[1:])
-            except Exception as e: pass  # print(e)
-            finally: self._step(found=found)
-        with self.lock: self.done += 1
 
 
     def __call__(self, args):
@@ -102,4 +99,5 @@ def __call__(self, args):
             if args.cve_2021_36260: self.modules.append(cve_2021_36260)
             if args.cve_2020_25078: self.modules.append(cve_2020_25078)
             if args.cve_2021_33044: self.modules.append(cve_2021_33044)
         multi_thread(self.scan, self.ip_list, processes=args.th_num)"
OK;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;" CWD = os.path.dirname(__file__)
 sys.path.append(os.path.join(CWD, '..'))
 from scan.modules import *
+from utils.net import get_all_ip, get_ip_seg_len
+from utils.base import multi_thread, process_bar, save_res
 
 
 class Base:
@@ -43,8 +43,8 @@ class CameraScanner(Base):
     def __init__(self, in_file: str, out_file: str) -> None:
         super().__init__(in_file, out_file)
         self.scanner_name = 'camera scanner'
         self.lock = Lock()
+        self.ip_list = []
         self.total = 0
         self.found = 0
         self.done = 0
@@ -56,31 +56,28 @@ def __init__(self, in_file: str, out_file: str) -> None:
     def _get_ip(self):
         with open(self.in_file, 'r') as f:
             for line in f:
+                if line.strip() and not line.startswith('#'):
+                    self.total += get_ip_seg_len(line.strip()) if '-' in line or '/' in line else 1
+                    self.ip_list.append(line.strip())
 
     def _step(self, *args, **kwargs):
         with self.lock:
             if kwargs['found']:
                 self.found += 1
             self.bar(self.total, self.done + 1, self.found, timer=True, start_time=self.start_time)
 
+    def scan(self, ip_term):
+        for ip in get_all_ip(ip_term):
+            for mod in self.modules:
+                found = False
+                try:
+                    res = mod(ip)
+                    if res[0]:
+                        found = True
+                        save_res(self.out_file, [ip] + res[1:])
+                except Exception as e: pass  # print(e)
+                finally: self._step(found=found)
+            with self.lock: self.done += 1
 
 
     def __call__(self, args):
@@ -102,4 +99,5 @@ def __call__(self, args):
             if args.cve_2021_36260: self.modules.append(cve_2021_36260)
             if args.cve_2020_25078: self.modules.append(cve_2020_25078)
             if args.cve_2021_33044: self.modules.append(cve_2021_33044)
+        
         multi_thread(self.scan, self.ip_list, processes=args.th_num)"
KO;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def wrapper(total, done, found=0, timer=False, start_time=0):
         _found = 'Found ' + output_formatter(found, color='red', bold=True) if found else ''
         count = f""{_done}/{_total} ({_percent}) {_found}""
 
-        print(f""\r{icon} {count}  {_time}"", end='')
     return wrapper
 
 "
OK;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def wrapper(total, done, found=0, timer=False, start_time=0):
         _found = 'Found ' + output_formatter(found, color='red', bold=True) if found else ''
         count = f""{_done}/{_total} ({_percent}) {_found}""
 
+        print(f""\r{icon} {count}  {_time:<55}"", end='')
     return wrapper
 
 "
KO;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def get_ip_segment(start: str, end: str) -> str:
     return IPy.IP(f""{start}-{end}"", make_net=True).strNormal()
 
 
 def get_all_ip(ip_seg: str) -> list:
-    return [i.strNormal() for i in IPy.IP(f""{ip_seg}"", make_net=True)]
 
 
 def get_user_agent(name='random'):"
OK;1;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def get_ip_segment(start: str, end: str) -> str:
     return IPy.IP(f""{start}-{end}"", make_net=True).strNormal()
 
 
+def get_ip_seg_len(ip_seg: str) -> int:
+    return IPy.IP(ip_seg, make_net=True).len()
+
+
 def get_all_ip(ip_seg: str) -> list:
+    return [i.strNormal() for i in IPy.IP(ip_seg, make_net=True)]
 
 
 def get_user_agent(name='random'):"
KO;2;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
-the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of current user and software\microsoft\windows\currentVersion\run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
OK;2;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
+the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of HKCU\Software\Microsoft\Windows\CurrentVersion\Run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
KO;2;CarbonCollective;fusion-dUQtools;9e8b10b081cfc7275998055e377496bf1da9a172;"Fix memory issue with IDS mapping (#90)

Fix memory issue with mapping";"def recursive_defaultdict():
 
 
 class IDSMapping:
-    # All fields in the core profile in a single dict
-    flat_fields: dict = {}
-    # All fields, in the core profile in a nested dict
-    fields: dict = defaultdict(recursive_defaultdict)
 
     def __init__(self, ids):
         self.dive(ids, [])
         self.fields = self.ddict_to_dict(self.fields)
 "
OK;2;CarbonCollective;fusion-dUQtools;9e8b10b081cfc7275998055e377496bf1da9a172;"Fix memory issue with IDS mapping (#90)

Fix memory issue with mapping";"def recursive_defaultdict():
 
 
 class IDSMapping:
 
     def __init__(self, ids):
+        # All fields in the core profile in a single dict
+        self.flat_fields: dict = {}
+        # All fields, in the core profile in a nested dict
+        self.fields: dict = defaultdict(recursive_defaultdict)
+
         self.dive(ids, [])
         self.fields = self.ddict_to_dict(self.fields)
 "
KO;2;dansanderson;mega65-welcome-guide;d39da1b335c04b9bf5afe8d71653a2d7f4fad035;Mention screen memory arrays in Recent Features;"Some of the new features that have been added since the factory-installed ROM wa
 - Single-letter BASIC variables are ""fast"" variables stored in fixed memory addresses `$FD00-$FEFF`.
 - The `PLAY` and `SOUND` commands have improved background playback and use of SID voices, so BASIC games can sensibly have both background music and sound effects.
 - Some disk commands can access files on the SD card directly (and not via a mounted D81 disk image) using the virtual device `U12`. `DIR U12` lists the files on the SD card. `DLOAD ""FILE.PRG"",U12` loads a `PRG` file.
 
 ## New BASIC commands
 "
OK;2;dansanderson;mega65-welcome-guide;d39da1b335c04b9bf5afe8d71653a2d7f4fad035;Mention screen memory arrays in Recent Features;"Some of the new features that have been added since the factory-installed ROM wa
 - Single-letter BASIC variables are ""fast"" variables stored in fixed memory addresses `$FD00-$FEFF`.
 - The `PLAY` and `SOUND` commands have improved background playback and use of SID voices, so BASIC games can sensibly have both background music and sound effects.
 - Some disk commands can access files on the SD card directly (and not via a mounted D81 disk image) using the virtual device `U12`. `DIR U12` lists the files on the SD card. `DLOAD ""FILE.PRG"",U12` loads a `PRG` file.
+- BASIC programs can access screen and color memory via special byte arrays `T@&(COLUMN, ROW)` and `C@&(COLUMN, ROW)`. Screen coordinates are intuitive in both 40-column and 80-column modes.
 
 ## New BASIC commands
 "
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
 *.parquet
\ No newline at end of file"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
+*.ipc
 *.parquet
\ No newline at end of file"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
-A smol simulator framework built on top of PyGame.
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
+A smol simulator framework built on top of [PyGame](https://www.pygame.org/docs/).
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
-from vi import Agent, BaseConfig, Simulation, Snapshot, dataclass
 
 
-@dataclass
-class MySnapshot(Snapshot):  # ðŸ‘ˆ inherit Snapshot to collect base metrics.
-    # We want to keep track of how many other agents were in our agent's radius,
-    # so we add an extra `in_radius` metric to our Snapshot!
-    in_radius: int
 
 
-class MyAgent(Agent):
-    def update(self):
         # If at least one agent is within our agent's radius, then we turn red!
-        if len(self.in_radius()) > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
-    def snapshot(self) -> MySnapshot:
-        return MySnapshot(
-            # Automatically fill-in all the Snapshot attributes such as agent, frame, x and y.
-            **super().snapshot().as_dict(),
-            # Then add our own metric: in_radius!
-            in_radius=len(self.in_radius()),
-        )
-
 
 print(
     # We're using a seed to collect the same data every time.
@@ -39,9 +32,7 @@ def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
-    # convert the output of the simulation into a Polars DataFrame
-    .to_polars()
-    .groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
+from vi import Agent, BaseConfig, Simulation
 
 
+class MyAgent(Agent):
+    def every_frame(self):
+        # As radius calculation is quite performance heavy,
+        # we only calculate it once per frame.
+        in_radius = len(self.in_radius())
 
+        # We want to keep track of how many other agents were in our agent's radius,
+        # so we add data to the `in_radius` column of our dataframe!
+        self.save_data(""in_radius"", in_radius)
 
         # If at least one agent is within our agent's radius, then we turn red!
+        if in_radius > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
 
 print(
     # We're using a seed to collect the same data every time.
@@ -39,9 +32,7 @@ def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
+    .snapshots.groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
-optional = true
 python-versions = "">=3.8""
 
-[[package]]
-name = ""pandas""
-version = ""1.4.2""
-description = ""Powerful data structures for data analysis, time series, and statistics""
-category = ""main""
-optional = true
-python-versions = "">=3.8""
-
-[package.dependencies]
-numpy = [
-    {version = "">=1.18.5"", markers = ""platform_machine != \""aarch64\"" and platform_machine != \""arm64\"" and python_version < \""3.10\""""},
-    {version = "">=1.19.2"", markers = ""platform_machine == \""aarch64\"" and python_version < \""3.10\""""},
-    {version = "">=1.20.0"", markers = ""platform_machine == \""arm64\"" and python_version < \""3.10\""""},
-    {version = "">=1.21.0"", markers = ""python_version >= \""3.10\""""},
-]
-python-dateutil = "">=2.8.1""
-pytz = "">=2020.1""
-
-[package.extras]
-test = [""hypothesis (>=5.5.3)"", ""pytest (>=6.0)"", ""pytest-xdist (>=1.31)""]
-
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
@@ -137,7 +116,7 @@ name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
-optional = true
 python-versions = "">=3.7""
 
 [package.dependencies]
@@ -179,33 +158,6 @@ toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
-[[package]]
-name = ""python-dateutil""
-version = ""2.8.2""
-description = ""Extensions to the standard Python datetime module""
-category = ""main""
-optional = true
-python-versions = ""!=3.0.*,!=3.1.*,!=3.2.*,>=2.7""
-
-[package.dependencies]
-six = "">=1.5""
-
-[[package]]
-name = ""pytz""
-version = ""2022.1""
-description = ""World timezone definitions, modern and historical""
-category = ""main""
-optional = true
-python-versions = ""*""
-
-[[package]]
-name = ""six""
-version = ""1.16.0""
-description = ""Python 2 and 3 compatibility utilities""
-category = ""main""
-optional = true
-python-versions = "">=2.7, !=3.0.*, !=3.1.*, !=3.2.*""
-
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
@@ -250,15 +202,10 @@ python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
-[extras]
-full = [""pandas"", ""polars""]
-pandas = [""pandas""]
-polars = [""polars""]
-
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
-content-hash = ""8fda347438c855d8fcb0e5b6319823d331690eba7e5f28f2a7cfaa4d69b2020d""
 
 [metadata.files]
 black = [
@@ -372,29 +319,6 @@ numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
-pandas = [
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_universal2.whl"", hash = ""sha256:be67c782c4f1b1f24c2f16a157e12c2693fd510f8df18e3287c77f33d124ed07""},
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_x86_64.whl"", hash = ""sha256:5a206afa84ed20e07603f50d22b5f0db3fb556486d8c2462d8bc364831a4b417""},
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_11_0_arm64.whl"", hash = ""sha256:0010771bd9223f7afe5f051eb47c4a49534345dfa144f2f5470b27189a4dd3b5""},
-    {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:3228198333dd13c90b6434ddf61aa6d57deaca98cf7b654f4ad68a2db84f8cfe""},
-    {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:5b79af3a69e5175c6fa7b4e046b21a646c8b74e92c6581a9d825687d92071b51""},
-    {file = ""pandas-1.4.2-cp310-cp310-win_amd64.whl"", hash = ""sha256:5586cc95692564b441f4747c47c8a9746792e87b40a4680a2feb7794defb1ce3""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_universal2.whl"", hash = ""sha256:061609334a8182ab500a90fe66d46f6f387de62d3a9cb9aa7e62e3146c712167""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_x86_64.whl"", hash = ""sha256:b8134651258bce418cb79c71adeff0a44090c98d955f6953168ba16cc285d9f7""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_11_0_arm64.whl"", hash = ""sha256:df82739e00bb6daf4bba4479a40f38c718b598a84654cbd8bb498fd6b0aa8c16""},
-    {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:385c52e85aaa8ea6a4c600a9b2821181a51f8be0aee3af6f2dcb41dafc4fc1d0""},
-    {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:295872bf1a09758aba199992c3ecde455f01caf32266d50abc1a073e828a7b9d""},
-    {file = ""pandas-1.4.2-cp38-cp38-win32.whl"", hash = ""sha256:95c1e422ced0199cf4a34385ff124b69412c4bc912011ce895582bee620dfcaa""},
-    {file = ""pandas-1.4.2-cp38-cp38-win_amd64.whl"", hash = ""sha256:5c54ea4ef3823108cd4ec7fb27ccba4c3a775e0f83e39c5e17f5094cb17748bc""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_universal2.whl"", hash = ""sha256:c072c7f06b9242c855ed8021ff970c0e8f8b10b35e2640c657d2a541c5950f59""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_x86_64.whl"", hash = ""sha256:f549097993744ff8c41b5e8f2f0d3cbfaabe89b4ae32c8c08ead6cc535b80139""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_11_0_arm64.whl"", hash = ""sha256:ff08a14ef21d94cdf18eef7c569d66f2e24e0bc89350bcd7d243dd804e3b5eb2""},
-    {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:8c5bf555b6b0075294b73965adaafb39cf71c312e38c5935c93d78f41c19828a""},
-    {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:51649ef604a945f781105a6d2ecf88db7da0f4868ac5d45c51cb66081c4d9c73""},
-    {file = ""pandas-1.4.2-cp39-cp39-win32.whl"", hash = ""sha256:d0d4f13e4be7ce89d7057a786023c461dd9370040bdb5efa0a7fe76b556867a0""},
-    {file = ""pandas-1.4.2-cp39-cp39-win_amd64.whl"", hash = ""sha256:09d8be7dd9e1c4c98224c4dfe8abd60d145d934e9fc1f5f411266308ae683e6a""},
-    {file = ""pandas-1.4.2.tar.gz"", hash = ""sha256:92bc1fc585f1463ca827b45535957815b7deb218c549b7c18402c322c7549a12""},
-]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
@@ -475,18 +399,6 @@ pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
-python-dateutil = [
-    {file = ""python-dateutil-2.8.2.tar.gz"", hash = ""sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86""},
-    {file = ""python_dateutil-2.8.2-py2.py3-none-any.whl"", hash = ""sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9""},
-]
-pytz = [
-    {file = ""pytz-2022.1-py2.py3-none-any.whl"", hash = ""sha256:e68985985296d9a66a881eb3193b0906246245294a881e7c8afe623866ac6a5c""},
-    {file = ""pytz-2022.1.tar.gz"", hash = ""sha256:1e760e2fe6a8163bc0b3d9a19c4f84342afa0a2affebfaa84b01b978a02ecaa7""},
-]
-six = [
-    {file = ""six-1.16.0-py2.py3-none-any.whl"", hash = ""sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254""},
-    {file = ""six-1.16.0.tar.gz"", hash = ""sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926""},
-]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
+optional = false
 python-versions = "">=3.8""
 
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
@@ -137,7 +116,7 @@ name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
+optional = false
 python-versions = "">=3.7""
 
 [package.dependencies]
@@ -179,33 +158,6 @@ toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
@@ -250,15 +202,10 @@ python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
+content-hash = ""5509c339bc3154af4b5c349b84ad6a79801ec11bd63a83881a76c9c664791262""
 
 [metadata.files]
 black = [
@@ -372,29 +319,6 @@ numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
@@ -475,18 +399,6 @@ pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
-
-# Optional DataFrame libraries
-pandas = { version = ""^1.4.2"", optional = true }
-polars = { version = ""^0.13.38"", optional = true }
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
-[tool.poetry.extras]
-pandas = [""pandas""]
-polars = [""polars""]
-full = [""pandas"", ""polars""] # all features
-
 [tool.isort]
 profile = ""black""
 "
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
+polars = ""^0.13.38""
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
 [tool.isort]
 profile = ""black""
 "
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
-from .metrics import Snapshot
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
-from typing import TYPE_CHECKING, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask
@@ -10,7 +10,7 @@
 from pygame.surface import Surface
 
 from .config import BaseConfig
-from .metrics import Snapshot
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
@@ -48,19 +48,16 @@ class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
-    # Sites
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
-    # Proximity
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
-    # Config (shared with other agents too)
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
@@ -74,6 +71,9 @@ class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
@@ -86,12 +86,14 @@ def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
 
         self.__proximity = proximity
 
@@ -156,15 +158,19 @@ def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
-    def update(self):
         """"""Run your own agent logic at every tick of the simulation.
-        Every frame of the simulation, this update method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
@@ -202,8 +208,8 @@ def there_is_no_escape(self) -> bool:
 
         return changed
 
-    def update_position(self):
-        """"""Update the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
@@ -331,24 +337,42 @@ def change_image(self, index: int):
 
         self._image_index = index
 
-    def snapshot(self) -> Snapshot:
-        """"""Create a Snapshot of agent data that you're interested in.
 
-        By default the Agent will produce a Snapshot with the following data:
         - agent identifier
         - current frame
         - x and y coordinates
 
-        However, you can also add your own data by inheriting the Snapshot dataclass.
-        Add any fields that you like and then overwrite this method to produce your custom Snapshot.
 
-        Make sure to call `super().snapshot()` to collect the default Snapshot data.
         """"""
 
-        return Snapshot(
-            x=self.pos.x,
-            y=self.pos.y,
-            id=self.id,
-            frame=self.shared.counter,
-            image_index=self._image_index,
-        )"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
+from typing import TYPE_CHECKING, Any, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask
@@ -10,7 +10,7 @@
 from pygame.surface import Surface
 
 from .config import BaseConfig
+from .metrics import Metrics
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
@@ -48,19 +48,16 @@ class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
@@ -74,6 +71,9 @@ class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
+    __metrics: Metrics
+    """"""Data collection of the snapshots.""""""
+
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
@@ -86,12 +86,14 @@ def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
+        metrics: Metrics,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
+        self.__metrics = metrics
 
         self.__proximity = proximity
 
@@ -156,15 +158,19 @@ def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
+    def every_frame(self):
         """"""Run your own agent logic at every tick of the simulation.
+        Every frame of the simulation, this method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
+    def update(self):
+        self._collect_replay_data()
+        self.every_frame()
+
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
@@ -202,8 +208,8 @@ def there_is_no_escape(self) -> bool:
 
         return changed
 
+    def change_position(self):
+        """"""Change the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
@@ -331,24 +337,42 @@ def change_image(self, index: int):
 
         self._image_index = index
 
+    def save_data(self, column: str, value: Any):
+        """"""Add extra data to the simulation's metrics.
 
+        The following data is collected automatically:
         - agent identifier
         - current frame
         - x and y coordinates
 
+        Examples
+        --------
 
+        Saving the number of agents in radius:
+
+        >>> from vi import Agent
+        >>> class MyAgent(Agent):
+        ...     def every_frame(self):
+        ...         in_radius = len(self.in_radius())
+        ...         self.save_data(""in_radius"", in_radius)
         """"""
 
+        self.__metrics._temporary_snapshots[column].append(value)
+
+    def _collect_replay_data(self):
+        """"""Add the minimum data needed for the replay simulation to the dataframe.""""""
+
+        x, y = round_pos(self.pos)
+        snapshots = self.__metrics._temporary_snapshots
+
+        snapshots[""frame""].append(self.shared.counter)
+        snapshots[""id""].append(self.id)
+
+        snapshots[""x""].append(x)
+        snapshots[""y""].append(y)
+
+        snapshots[""image_index""].append(self._image_index)
+
+        if self.config.image_rotation:
+            angle = self.move.angle_to(Vector2((0, -1)))
+            snapshots[""angle""].append(round(angle))"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
-import dataclasses
 from dataclasses import dataclass, field
-from typing import TYPE_CHECKING, Any
 
-if TYPE_CHECKING:
-    from pandas import DataFrame as PandasDataFrame
-    from polars import DataFrame as PolarsDataFrame
-    from polars import Series as PolarsSeries
-
-
-@dataclass
-class Snapshot:
-    """"""Data that's collected for every agent in every frame of the simulation.""""""
-
-    frame: int
-    """"""The current frame of the simulation.""""""
-
-    id: int
-    """"""The identifier of the agent.""""""
-
-    x: float
-    """"""The x coordinate of the agent.""""""
-
-    y: float
-    """"""The y coordinate of the agent.""""""
-
-    image_index: int
-    """"""The current index of the image list.""""""
-
-    def as_dict(self) -> dict[str, Any]:
-        """"""Convert this Snapshot into a dictionary.""""""
-
-        return dataclasses.asdict(self)
 
 
 @dataclass
@@ -42,28 +14,29 @@ class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
-    def to_polars(self) -> PolarsSeries:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
-@dataclass
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
-    fps: Fps = field(default_factory=Fps)
     """"""The frames-per-second history to analyse performance.""""""
 
-    snapshots: list[dict[str, Any]] = field(default_factory=list)
-    """"""The most important data (snapshot) of every agent at every moment in time.""""""
 
-    def to_pandas(self) -> PandasDataFrame:
-        import pandas as pd
 
-        return pd.DataFrame(self.snapshots)
 
-    def to_polars(self) -> PolarsDataFrame:
-        import polars as pl
 
-        return pl.from_dicts(self.snapshots)"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
+from collections import defaultdict
 from dataclasses import dataclass, field
+from typing import Any
 
+import polars as pl
 
 
 @dataclass
@@ -42,28 +14,29 @@ class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
+    def to_polars(self) -> pl.Series:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
+    fps: Fps
     """"""The frames-per-second history to analyse performance.""""""
 
+    _temporary_snapshots: defaultdict[str, list[Any]]
+    snapshots: pl.DataFrame
 
+    def __init__(self):
+        self.fps = Fps()
+        self._temporary_snapshots = defaultdict(list)
+        self.snapshots = pl.DataFrame()
 
+    def merge(self):
+        df = pl.from_dict(self._temporary_snapshots)
 
+        self.snapshots.vstack(df, in_place=True)
 
+        self._temporary_snapshots = defaultdict(list)"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
-from .util import load_image, load_images, round_pos
 
 if TYPE_CHECKING:
     from .agent import Agent
@@ -148,6 +148,7 @@ def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
             )
 
         return self
@@ -173,6 +174,7 @@ def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
         )
 
         return self
@@ -272,8 +274,8 @@ def tick(self):
         # Update all agents
         self._all.update()
 
-        # Snapshot marked agent data
-        self.__save_snapshots()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
@@ -306,16 +308,7 @@ def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
-            agent.update_position()
-
-    def __save_snapshots(self):
-        """"""Save a Snapshot of each agent and add it to Metrics.""""""
-
-        for sprite in self._agents.sprites():
-            agent: Agent = sprite  # type: ignore
-            snapshot = agent.snapshot()
-
-            self.__metrics.snapshots.append(snapshot.as_dict())
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
+from .util import load_image, load_images
 
 if TYPE_CHECKING:
     from .agent import Agent
@@ -148,6 +148,7 @@ def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
+                metrics=self.__metrics,
             )
 
         return self
@@ -173,6 +174,7 @@ def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
+            metrics=self.__metrics,
         )
 
         return self
@@ -272,8 +274,8 @@ def tick(self):
         # Update all agents
         self._all.update()
 
+        # Merge the collected snapshots into the dataframe.
+        self.__metrics.merge()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
@@ -306,16 +308,7 @@ def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
+            agent.change_position()
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
-                ],
-                ""uses"": 0
             }
         }
     },
     ""2"": {
         ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
             ""deadline"": 3,
             ""pages"": 10,
-            ""already_exec"": 4
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -76,8 +79,10 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -89,35 +94,46 @@
                     8,
                     9
                 ],
-                ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 2,
-            ""execution_time"": 2,
-            ""deadline"": 3,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
-            ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
-        ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -128,8 +144,15 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -141,40 +164,48 @@
                     8,
                     9
                 ],
-                ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
-            ""name"": ""C"",
-            ""arrival_time"": 4,
-            ""execution_time"": 1,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 1
         },
-        ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
-            ""D""
         ],
-        ""done_in_this_cicle"": true,
-        ""time"": 9,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -185,8 +216,20 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -198,44 +241,46 @@
                     8,
                     9
                 ],
-                ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
-            ""name"": ""D"",
-            ""arrival_time"": 6,
-            ""execution_time"": 3,
-            ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
-        ""done_in_this_cicle"": false,
-        ""time"": 12,
-        ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""D"": {
                 ""real"": [
                     0,
                     1,
@@ -259,9 +304,39 @@
                     7,
                     8,
                     9
-                ],
                 ""uses"": 0
             }
         }
     },
     ""6"": {
@@ -274,11 +349,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 13,
-        ""started_time"": 12,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,19 +384,24 @@
                     9
                 ],
                 ""virtual"": [
-                    0,
-                    1,
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9
-                ],
-                ""uses"": 1
             }
         }
     }
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 1
         }
     },
     ""2"": {
         ""process"": {
+            ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
             ""deadline"": 3,
             ""pages"": 10,
+            ""already_exec"": 2
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
+                ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -76,8 +79,10 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -89,35 +94,46 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0
         }
     },
     ""3"": {
         ""process"": {
+            ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
+            ""deadline"": 7,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
+        ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -128,8 +144,15 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -141,40 +164,48 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0
         }
     },
     ""4"": {
         ""process"": {
+            ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 2
         },
+        ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
+            ""D"",
+            ""A""
         ],
+        ""done_in_this_cicle"": false,
+        ""time"": 10,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -185,8 +216,20 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -198,44 +241,46 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 1
         }
     },
     ""5"": {
         ""process"": {
+            ""name"": ""A"",
+            ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 3,
             ""pages"": 10,
+            ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
+        ""done_in_this_cicle"": true,
+        ""time"": 13,
+        ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
@@ -259,9 +304,39 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
                 ""uses"": 0
+            },
+            ""D"": {
+                ""real"": null,
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 0
         }
     },
     ""6"": {
@@ -274,11 +349,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 15,
+        ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,19 +384,24 @@
                     9
                 ],
                 ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 0
         }
     }
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
 
 scalonator_translate = {
     ""FIFO"": fifo,
@@ -12,7 +14,8 @@
 }
 
 swap_translate = {
-    ""FIFO"": swap_fifo
 }
 
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
+from cpu.memory.swap_algorithm.swap_lru import swap_lru
+
 
 scalonator_translate = {
     ""FIFO"": fifo,
@@ -12,7 +14,8 @@
 }
 
 swap_translate = {
+    ""FIFO"": swap_fifo,
+    ""LRU"":swap_lru
 }
 
 "
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
-        [
-            ""A"",
-            4,
-            0
-        ],
         [
             ""B"",
-            7,
             2
         ],
         [
             ""C"",
-            9,
             4
         ],
         [
-            ""D"",
             13,
             6
         ]
     ],
-    ""0"": 5.25
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
         [
             ""B"",
+            5,
             2
         ],
         [
             ""C"",
+            7,
             4
         ],
         [
+            ""A"",
             13,
+            0
+        ],
+        [
+            ""D"",
+            15,
             6
         ]
     ],
+    ""0"": 7.0
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
-from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
@@ -24,6 +24,7 @@ def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
 
 
     def initialize(self, queue:List[ProcessIn]):
@@ -55,16 +56,15 @@ def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
-        #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
-            self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return False #Tudo certo! o processo jÃ¡ estÃ¡ carregado na memoria
             #NÃ£o precisa de OVERHEAD
         
@@ -76,7 +76,6 @@ def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
-                self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
@@ -93,15 +92,13 @@ def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
-                self.real_virtual_map[process.name][""uses""] = 0
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
-                self.real_virtual_map[process.name][""uses""] = 0 
                 self.swap(process)
                 return True
 
@@ -122,23 +119,23 @@ def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto nÃ£o tiver espaÃ§o, fazer  o swap para ter espaÃ§o!
         
-        #TODO Must teste this approach! It is the correct one!
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
-                self.p_order)
-        # old_p_name= self.swap_algorithm(
-            # self.p_order)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
-            self.real_virtual_map[old_p_name][""uses""] = 0
 
-        #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
-        self.real_virtual_map[process.name][""uses""] = 0 
 
         return True
          
@@ -175,9 +172,14 @@ def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
-    "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
+from collections import deque, Counter
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
@@ -24,6 +24,7 @@ def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
+        self.counter = Counter()
 
 
     def initialize(self, queue:List[ProcessIn]):
@@ -55,16 +56,15 @@ def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
+        self.counter[process.name] +=1
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return False #Tudo certo! o processo jÃ¡ estÃ¡ carregado na memoria
             #NÃ£o precisa de OVERHEAD
         
@@ -76,7 +76,6 @@ def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
@@ -93,15 +92,13 @@ def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.swap(process)
                 return True
 
@@ -122,23 +119,23 @@ def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto nÃ£o tiver espaÃ§o, fazer  o swap para ter espaÃ§o!
         
+        removed_p_count = 1
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
+                self.p_order, self.counter,removed_p_count)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
+            self.counter[old_p_name] = 0
 
+            removed_p_count+=1
+
+        #cadastra o novo processo na memoria real
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
+        self.counter[process.name] = 0 
 
         return True
          
@@ -175,9 +172,14 @@ def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
+
+            self.counter[p_name] = 0
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
+
+    def show_counter(self):
+        copy = json.dumps(dict(self.counter))
+        return copy"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"-from collections import deque
 
 def swap_fifo(
-    p_order:deque
 ):
     old_process_name = p_order[-1] #pega a primeira posiÃ§Ã£o
     p_order.rotate()"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import deque, Counter
 
 def swap_fifo(
+    p_order:deque,
+    counter:Counter,
+    removed_p_count:int
 ):
     old_process_name = p_order[-1] #pega a primeira posiÃ§Ã£o
     p_order.rotate()"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import deque,Counter
+
+def swap_lru(
+    p_order:deque,
+    counter:Counter,
+    removed_p_count:int
+):
+    least_used = counter.most_common()[:-removed_p_count-1:-1] 
+
+    return least_used[removed_p_count-1][0] #retorna o nome do processo menos usado
+
+"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"-from collections import deque
 
-d = deque()
 
-d.appendleft(""a"")
-d.appendleft(""b"")
-d.appendleft(""c"")
-d.appendleft(""d"")
 
 print()
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import Counter
 
+d = Counter()
 
+d[0]+=1
+d[0]+=1
+d[1]+=1
+d[1]+=1
+d[1]+=1
 
 print()
 "
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
@@ -83,19 +84,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
-        # if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
-        #     result = mmu.load_context(p)
-            
-        #     is_overhead = True
-        #     if not first:
-        #         sleep(overhead)
-        #         time_count+=overhead
-        #     first = False
-            
-        # else:
-        #     is_overhead = False
-        # is_process_done = False
-        # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -107,7 +96,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
-
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -120,14 +109,16 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
-            real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -172,7 +163,8 @@ def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
-    real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -183,6 +175,7 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
@@ -192,7 +185,8 @@ def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
-                ""real_virtual_map"": memory_map
             }
     
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
+    mmu_counter = """"
     print(""enters main loop!"")
 
     while True:
@@ -83,19 +84,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
+
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -107,7 +96,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
+                
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -120,14 +109,16 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
+        mmu_counter = mmu.show_counter()
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
+            real_virtual_map,
+            mmu_counter
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -172,7 +163,8 @@ def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
+    real_virtual_map: str,
+    mmu_counter:str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -183,6 +175,7 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
+    memory_counter=json.loads(mmu_counter)
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
@@ -192,7 +185,8 @@ def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
+                ""real_virtual_map"": memory_map,
+                ""memory_counter"":memory_counter
             }
     
 "
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 4
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -77,7 +78,10 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -89,35 +93,43 @@
                     8,
                     9
                 ],
-                ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 2,
-            ""execution_time"": 2,
-            ""deadline"": 5,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""C""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 6,
-        ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -129,7 +141,15 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -141,40 +161,44 @@
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
-            ""name"": ""C"",
-            ""arrival_time"": 4,
-            ""execution_time"": 1,
-            ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 1
         },
-        ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""D""
         ],
-        ""done_in_this_cicle"": true,
-        ""time"": 7,
-        ""started_time"": 6,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -186,7 +210,20 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -198,44 +235,41 @@
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
-            ""name"": ""D"",
-            ""arrival_time"": 6,
-            ""execution_time"": 3,
-            ""deadline"": 10,
             ""pages"": 10,
-            ""already_exec"": 2
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
-        ""done_in_this_cicle"": false,
-        ""time"": 9,
-        ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""D"": {
                 ""real"": [
                     0,
                     1,
@@ -261,6 +295,32 @@
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
@@ -274,11 +334,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 10,
-        ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,16 +369,16 @@
                     9
                 ],
                 ""virtual"": [
-                    0,
-                    1,
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9
                 ],
                 ""uses"": 1
             }"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
+            ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
+            ""deadline"": 5,
             ""pages"": 10,
+            ""already_exec"": 2
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
+                ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -77,7 +78,10 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -89,35 +93,43 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
+                ""uses"": 0
             }
         }
     },
     ""3"": {
         ""process"": {
+            ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
+            ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 7,
+        ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -129,7 +141,15 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -141,40 +161,44 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
+            ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
+            ""already_exec"": 2
         },
+        ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""D"",
+            ""A""
         ],
+        ""done_in_this_cicle"": false,
+        ""time"": 10,
+        ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -186,7 +210,20 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -198,44 +235,41 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
+            ""name"": ""A"",
+            ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
+            ""already_exec"": 4
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
+        ""done_in_this_cicle"": true,
+        ""time"": 13,
+        ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
@@ -261,6 +295,32 @@
                     9
                 ],
                 ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": null,
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
+                ""uses"": 0
             }
         }
     },
@@ -274,11 +334,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 15,
+        ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,16 +369,16 @@
                     9
                 ],
                 ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
                 ],
                 ""uses"": 1
             }"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;\ No newline at end of file
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"+{
+    ""config"":{
+        ""scale_algorithm"": ""RR"",
+        ""page_algorithm"": ""FIFO"",
+        ""quantum"": 10,
+        ""overhead"":0
+    },
+    ""processes"":[
+        {
+            ""name"":""p5"",
+            ""arrival_time"":35,
+            ""execution_time"":5,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p4"",
+            ""arrival_time"":21,
+            ""execution_time"":13,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p3"",
+            ""arrival_time"":19,
+            ""execution_time"":10,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p2"",
+            ""arrival_time"":13,
+            ""execution_time"":75,
+            ""pages"":10,
+            ""deadline"":8
+        },
+        {
+            ""name"":""p1"",
+            ""arrival_time"":5,
+            ""execution_time"":17,
+            ""pages"":10,
+            ""deadline"":5
+        },
+        {
+            ""name"":""p0"",
+            ""arrival_time"":0,
+            ""execution_time"":25,
+            ""pages"":10,
+            ""deadline"":7
+        }
+    ]
+}
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
-        [
-            ""A"",
-            4,
-            0
-        ],
         [
             ""B"",
-            6,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""D"",
-            10,
             6
         ]
     ],
-    ""0"": 3.75
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
         [
             ""B"",
+            5,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
+        [
+            ""A"",
+            13,
+            0
+        ],
         [
             ""D"",
+            15,
             6
         ]
     ],
+    ""0"": 7.0
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto nÃ£o tiver espaÃ§o, fazer  o swap para ter espaÃ§o!
         
         #TODO Must teste this approach! It is the correct one!
-        # while not self.memory_real.does_it_fit(process.pages): 
-        #     old_p_name= self.swap_algorithm(
-        #         self.p_order)
-        old_p_name= self.swap_algorithm(
-            self.p_order)
-
-
-        #Remove o index do processo antigo da memoria real
-        list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
-        self.memory_real.remove(list_index_to_remove)
-        self.real_virtual_map[old_p_name][""real""] = None
-        self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto nÃ£o tiver espaÃ§o, fazer  o swap para ter espaÃ§o!
         
         #TODO Must teste this approach! It is the correct one!
+        while not self.memory_real.does_it_fit(process.pages): 
+            old_p_name= self.swap_algorithm(
+                self.p_order)
+        # old_p_name= self.swap_algorithm(
+            # self.p_order)
+
+            #Remove o index do processo antigo da memoria real
+            list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
+            self.memory_real.remove(list_index_to_remove)
+            self.real_virtual_map[old_p_name][""real""] = None
+            self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
@@ -15,3 +16,20 @@ def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
+from typing import Union, List
 
 
 
@@ -15,3 +16,20 @@ def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
+def fifo_dont_use(
+    process_list:list[ProcessIn],
+    add_p:Union[list[ProcessIn],ProcessIn],
+    time_count:int=None,
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo fifo')
+    for x in process_list:
+        d.append(x)
+
+    if type(add_p) is list:
+        for p in add_p:
+            d.appendleft(p)
+    elif type(add_p) is ProcessIn:
+        d.append(add_p)
+
+    return d
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
-def rr(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
+from typing import Union, List
 
 
 
+def rr_v1(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
+
+def rr_v2(
+    process_list:list[ProcessIn],
+    time_count:int=None,
+    right:bool=False
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo rr')
+
+    for x in process_list:
+        d.append(x)
+
+
+    if right:
+        d.rotate()#rodar para a direita!
+
+    return d
+
+def rr_v3(
+    process_list:list[ProcessIn],
+    add_p:Union[list[ProcessIn],ProcessIn],
+    time_count:int=None,
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo rr')
+
+    for x in process_list:
+        d.append(x)
+
+    if type(add_p) is list:
+        for p in add_p:
+            d.append(p)
+    elif type(add_p) is ProcessIn:
+        d.appendleft(add_p)
+   
+    return d
+
+def rr(process_list:list[ProcessIn],time_count:int=None):
+    d = deque()
+    lesser = 9999
+    for x in process_list:
+        if x.already_exec < lesser:
+            lesser = x.already_exec
+    normalized = all([p.already_exec==lesser if p.already_exec!=0 else False for p in process_list])
+    if not normalized:        
+        process_list.sort(key=lambda x: x.already_exec - lesser, reverse=True)
+    else:
+        process_list=reversed(process_list)
+
+
+    for x in process_list:
+        d.append(x)
+    return d
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
-            queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
@@ -74,8 +74,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
-            is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
@@ -101,7 +101,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
-            sleep(1)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
@@ -117,7 +117,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
-            queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
+            queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count=time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
@@ -74,8 +74,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
             if not first:
+                is_overhead = True
                 sleep(overhead)
                 time_count+=overhead
             first = False
@@ -101,7 +101,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
+            sleep(0)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
@@ -117,7 +117,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
+            queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
-        ""started_time"": 0
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 4,
-        ""started_time"": 2
-    },
-    ""3"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 6,
-        ""started_time"": 4
-    },
-    ""4"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 8
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 8,
-        ""started_time"": 6
-    },
-    ""5"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 10
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
             ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 10,
-        ""started_time"": 8
     },
-    ""6"": {
         ""process"": {
             ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 12,
-        ""started_time"": 10
-    },
-    ""7"": {
-        ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 14,
-        ""started_time"": 12
-    },
-    ""8"": {
-        ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
             ""C""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 16,
-        ""started_time"": 14
     },
-    ""9"": {
         ""process"": {
             ""name"": ""C"",
-            ""arrival_time"": 0,
-            ""execution_time"": 2,
             ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 18,
-        ""started_time"": 16
     },
-    ""10"": {
         ""process"": {
             ""name"": ""D"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": false,
-        ""time"": 20,
-        ""started_time"": 18
     },
-    ""11"": {
         ""process"": {
             ""name"": ""D"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 5,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": true,
-        ""time"": 22,
-        ""started_time"": 20
-    },
-    ""12"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 2
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 24,
-        ""started_time"": 22
-    },
-    ""13"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 26,
-        ""started_time"": 24
-    },
-    ""14"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 28,
-        ""started_time"": 26
-    },
-    ""15"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 8
         },
-        ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 30,
-        ""started_time"": 28
     }
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
+        ""started_time"": 0,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""B""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 4,
+        ""started_time"": 2,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 1
+            }
+        }
     },
+    ""3"": {
         ""process"": {
             ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
+            ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""C""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 6,
+        ""started_time"": 4,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""4"": {
         ""process"": {
             ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
             ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 7,
+        ""started_time"": 6,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""5"": {
         ""process"": {
             ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": false,
+        ""time"": 9,
+        ""started_time"": 7,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""6"": {
         ""process"": {
             ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
+            ""already_exec"": 3
         },
+        ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 10,
+        ""started_time"": 9,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 1
+            }
+        }
     }
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
-            10,
             0
         ],
         [
             ""B"",
-            16,
-            0
         ],
         [
             ""C"",
-            18,
-            0
         ],
         [
             ""D"",
-            22,
-            0
-        ],
-        [
-            ""E"",
-            30,
-            0
         ]
     ],
-    ""0"": 19.2
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
+            4,
             0
         ],
         [
             ""B"",
+            6,
+            2
         ],
         [
             ""C"",
+            7,
+            4
         ],
         [
             ""D"",
+            10,
+            6
         ]
     ],
+    ""0"": 3.75
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
-
 
 class MMU:
     
@@ -120,7 +120,12 @@ def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
-
         old_p_name= self.swap_algorithm(
             self.p_order)
 
@@ -173,5 +178,7 @@ def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
-
     "
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
+import json
 
 class MMU:
     
@@ -120,7 +120,12 @@ def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
+        #Enquanto nÃ£o tiver espaÃ§o, fazer  o swap para ter espaÃ§o!
+        
+        #TODO Must teste this approach! It is the correct one!
+        # while not self.memory_real.does_it_fit(process.pages): 
+        #     old_p_name= self.swap_algorithm(
+        #         self.p_order)
         old_p_name= self.swap_algorithm(
             self.p_order)
 
@@ -173,5 +178,7 @@ def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
+    def show_real_virtual_map(self):
+        copy = json.dumps(self.real_virtual_map)
+        return copy
     "
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
-from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -45,6 +45,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
     print(""enters main loop!"")
 
     while True:
@@ -102,6 +103,9 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -110,6 +114,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
@@ -119,7 +124,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
-            started_time
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -163,7 +169,8 @@ def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
-    started_time:int
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -174,14 +181,16 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
-                ""started_time"":started_time
             }
     
 "
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
+from typing import Callable, List, Tuple, Dict
 from collections import deque
 
 from cpu.models import config_model
@@ -45,6 +45,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
+    real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
@@ -102,6 +103,9 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
+                real_virtual_map = mmu.show_real_virtual_map()
+
+                # real_virtual_map = mmu.show_real_virtual_map()
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -110,6 +114,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
+            real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
@@ -119,7 +124,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
+            started_time,
+            real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -163,7 +169,8 @@ def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
+    started_time:int,
+    real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -174,14 +181,16 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
+    memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
+                ""started_time"":started_time,
+                ""real_virtual_map"": memory_map
             }
     
 "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
-    ""FIFO"": swap_fifo,
-
 }
 
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
+    ""FIFO"": swap_fifo
 }
 
+
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;\ No newline at end of file
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+{
+    ""config"":{
+        ""scale_algorithm"": ""FIFO"",
+        ""page_algorithm"": ""FIFO"",
+        ""quantum"": 2,
+        ""overhead"":0
+    },
+    ""processes"":[
+       
+        {
+            ""name"":""A"",
+            ""arrival_time"":0,
+            ""execution_time"":10,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""B"",
+            ""arrival_time"":0,
+            ""execution_time"":6,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""C"",
+            ""arrival_time"":0,
+            ""execution_time"":2,
+            ""pages"":10,
+            ""deadline"":8
+        },
+        {
+            ""name"":""D"",
+            ""arrival_time"":0,
+            ""execution_time"":4,
+            ""pages"":10,
+            ""deadline"":5
+        },
+        {
+            ""name"":""E"",
+            ""arrival_time"":0,
+            ""execution_time"":8,
+            ""pages"":10,
+            ""deadline"":7
+        }
+    ]
+}
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"-from typing import Dict, List, Union, Callable, Tuple
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
@@ -9,23 +9,27 @@ class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
-    page_algorithm: Callable
-    
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
-        page_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
-        self.page_algorithm = page_algorithm
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
-            if not self.memory_real.is_memory_full(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
@@ -35,7 +39,7 @@ def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
-            elif not self.memory_virtual.is_memory_full(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
@@ -49,22 +53,26 @@ def initialize(self, queue:List[ProcessIn]):
                 
 
 
-    def load_context(self, process: ProcessIn):
         real_virtual_map = self.real_virtual_map
-        self.memory_real.add_stack(process)
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
-            return True #Tudo certo! o processo jÃ¡ estÃ¡ carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo nÃ£o estÃ¡ na memo_real, mas estÃ¡ na memo_virtual
-            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
@@ -78,14 +86,15 @@ def load_context(self, process: ProcessIn):
 
 
         else:#O processo nÃ£o ta nem na memoria real nem na virtual
-            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
@@ -112,8 +121,8 @@ def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
-        old_p_name= self.page_algorithm(
-            self.memory_real)
 
 
         #Remove o index do processo antigo da memoria real
@@ -132,7 +141,7 @@ def swap(self, process: ProcessIn):
 
 
 
-    def garbage_collector(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
@@ -148,8 +157,21 @@ def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
 
-    def init_memories(self):
-        pass
 
     "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+from typing import Dict, List, Union, Callable, Tuple, Set
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
@@ -9,23 +9,27 @@ class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
+    swap_algorithm: Callable
+    p_count: any
+    p_order: deque
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
+        swap_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
+        self.swap_algorithm = swap_algorithm
+        self.p_count = None
+        self.p_order = deque()
+
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
+            if self.memory_real.does_it_fit(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
@@ -35,7 +39,7 @@ def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
+            elif self.memory_virtual.does_it_fit(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
@@ -49,22 +53,26 @@ def initialize(self, queue:List[ProcessIn]):
                 
 
 
+    def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
+        if not process.name in real_virtual_map:
+            self.p_order.appendleft(process.name)
+        
+        #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
+            return False #Tudo certo! o processo jÃ¡ estÃ¡ carregado na memoria
+            #NÃ£o precisa de OVERHEAD
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo nÃ£o estÃ¡ na memo_real, mas estÃ¡ na memo_virtual
+            if self.memory_real.does_it_fit(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
@@ -78,14 +86,15 @@ def load_context(self, process: ProcessIn):
 
 
         else:#O processo nÃ£o ta nem na memoria real nem na virtual
+            if self.memory_real.does_it_fit(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
+                return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
@@ -112,8 +121,8 @@ def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
+        old_p_name= self.swap_algorithm(
+            self.p_order)
 
 
         #Remove o index do processo antigo da memoria real
@@ -132,7 +141,7 @@ def swap(self, process: ProcessIn):
 
 
 
+    def garbage_collector_all(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
@@ -148,8 +157,21 @@ def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
+    def garbage_collector(self,process:ProcessIn):
+            p_name = process.name
+            real_virtual_map = self.real_virtual_map
+            free_real_indexes = real_virtual_map[p_name][""real""]
+            free_virtual_indexes = real_virtual_map[p_name][""virtual""]
+
+            self.memory_real.remove(free_real_indexes)
+            self.memory_virtual.remove(free_virtual_indexes)
+
+
+            real_virtual_map[p_name][""real""] = None
+            real_virtual_map[p_name][""virtual""] = None
+            real_virtual_map[p_name][""uses""] = 0
+            self.real_virtual_map = real_virtual_map 
+            print(f""Removed processes = {p_name}"")
 
 
     "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
-    current_memory_space: int
-    space_graph: Dict[str, bool]
     process_stack: deque
     type_name:str
 
@@ -22,8 +22,9 @@ def space_initializer(self):
             space_graph[i] = False # lugar da memoria comeÃ§a vazio
         return space_graph
 
-    def is_memory_full(self, number_of_page_in: int):
-        if number_of_page_in < self.current_space_occupied:
             return True
         return False
     
@@ -48,10 +49,13 @@ def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
-        for index in index_list:
-            self.space_graph[index] = False
-            self.current_space_occupied -= 1
-            print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
+    current_space_occupied: int
+    space_graph: Dict[int, bool]
     process_stack: deque
     type_name:str
 
@@ -22,8 +22,9 @@ def space_initializer(self):
             space_graph[i] = False # lugar da memoria comeÃ§a vazio
         return space_graph
 
+    def does_it_fit(self, number_of_page_in: int):
+        free_space = self.total_memory_pages - self.current_space_occupied 
+        if free_space >= number_of_page_in:
             return True
         return False
     
@@ -48,10 +49,13 @@ def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
+        if index_list is None:
+            print(""Process is not in memory"")
+        else:
+            for index in index_list:
+                self.space_graph[index] = False
+                self.current_space_occupied -= 1
+                print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"-
 
 def swap_fifo(
-    memory_real
 ):
-    old_process_name = memory_real.process_stack[-1] #pega a primeira posiÃ§Ã£o
-    memory_real.process_stack.rotate()
-    return old_process_name
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+from collections import deque
 
 def swap_fifo(
+    p_order:deque
 ):
\ No newline at end of file
+    old_process_name = p_order[-1] #pega a primeira posiÃ§Ã£o
+    p_order.rotate()
+    return old_process_name"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
-from typing import List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -25,23 +25,21 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
-    scalonator_engine =  scalonator_translate[config.scale_algorithm] 
-    page_algorithm = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    # real_memory = Memory(""real"",total_memory_pages=20)
-    # virtual_memory = Memory(""virtual"",total_memory_pages=100)
-    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
-    # MMU.initialize(process_list)
-
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
-    cache_name = False
     is_process_done = False
     time_count = 0
     first = True
@@ -64,24 +62,37 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
-        if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
-            # result = mmu.load_context(p)
-            
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
-            
-        else:
             is_overhead = False
         is_process_done = False
-        cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -91,13 +102,15 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
-            queue.appendleft(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
@@ -177,10 +190,15 @@ def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
-    for index,p in enumerate(process_list):
         if time_count >= p.arrival_time:
             result.append(p)
-            process_list.pop(index)
     return result
         
 "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
+from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -25,23 +25,21 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
+    scalonator_engine: Callable =  scalonator_translate[config.scale_algorithm] 
+    swap_algorithm: Callable = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    real_memory = Memory(""real"",total_memory_pages=10)
+    virtual_memory = Memory(""virtual"",total_memory_pages=100)
+    mmu = MMU(real_memory,virtual_memory,swap_algorithm)
+    # mmu.initialize(process_list)
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
     is_process_done = False
     time_count = 0
     first = True
@@ -64,24 +62,37 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
+            is_process_done = False
+
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
+        #Retorna True caso precise trocar de contexto!
+        if mmu.load_context(p):
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
+        else: #Retorna False caso o contexto jÃ¡ estava carregado
             is_overhead = False
         is_process_done = False
+        
+        # if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
+        #     result = mmu.load_context(p)
+            
+        #     is_overhead = True
+        #     if not first:
+        #         sleep(overhead)
+        #         time_count+=overhead
+        #     first = False
+            
+        # else:
+        #     is_overhead = False
+        # is_process_done = False
+        # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -91,13 +102,15 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
+                mmu.garbage_collector(p)
+
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
+            queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
@@ -177,10 +190,15 @@ def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
+
+    process_copy = process_list.copy()
+
+    for p in process_copy:
         if time_count >= p.arrival_time:
             result.append(p)
+            process_list.remove(p)
+
+            # process_list.pop(index)
     return result
         
 "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
-#TODO: Need delete cicle_log data!
 
 app = FastAPI()
 "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
 
 app = FastAPI()
 "
KO;4;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    real_memory = MemoryReal(total_memory_pages=50)
-    virtual_memory = MemoryVirtual(total_memory_frames=100)
-    mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,7 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
-            mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -67,11 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
-            mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
-            result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    # real_memory = MemoryReal(total_memory_pages=50)
+    # virtual_memory = MemoryVirtual(total_memory_frames=100)
+    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,7 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
+            # mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -67,11 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
+            # mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
+            # result = mmu.load_context(p)
             
             is_overhead = True
             if not first:"
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return True #Tudo certo! o processo jÃ¡ estÃ¡ carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo nÃ£o estÃ¡ na memo_real, mas estÃ¡ na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
-                real_used_index = self.add_to_memory(process.page,self.memory_real)
-                self.real_virtual_map[process.name][""real""] = real_used_index #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
-                real_virtual_map = self.swap(real_virtual_map)
-                pass
 
 
-        else:#O processo nÃ£o tinha sido cadastrado antes!
-            pass
 
         
         
@@ -84,31 +97,55 @@ def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
-        if not memory.is_memory_full(pages):
-            for _ in range(pages): 
-               used_index = memory.add(used_index)
-            return used_index
-        else:
-            print(""memory full!"")
 
 
     def swap(self, process: ProcessIn):
-        for k in self.real_virtual_map.keys():
-            self.real_virtual_map[k]
 
-        self.page_algorithm()
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
-    def clean_done_process():
-        pass
 
-    def init_memory(self, process_list: List[ProcessIn]):
-        pass
 
-    def init_disk(self, process_list: List[ProcessIn]):
         pass
 
     "
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
+            self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return True #Tudo certo! o processo jÃ¡ estÃ¡ carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo nÃ£o estÃ¡ na memo_real, mas estÃ¡ na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
+                real_used_indexes = self.add_to_memory(process.page,self.memory_real)
+                self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
+                self.swap(process)
+                
+
+                return True
+
+
+        else:#O processo nÃ£o ta nem na memoria real nem na virtual
+            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃƒO esteja cheia, alocar o processo
+                self.memory_real.add_stack(process)
+                real_used_indexes = self.add_to_memory(process.page,self.memory_real)
+                virtual_used_indexes = self.add_to_memory(process.page,self.memory_virtual)
+
+                self.real_virtual_map[process.name][""real""] = real_used_indexes
+                self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
+            else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
+                self.swap(process)
 
 
 
         
         
@@ -84,31 +97,55 @@ def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
+        for _ in range(pages): 
+            used_index = memory.add(used_index)
+        return used_index
 
 
     def swap(self, process: ProcessIn):
 
+        new_p_real_index, old_p_name= self.page_algorithm(
+            self.memory_real,
+            process,
+            self.real_virtual_map)
+
+        #Remove o index do processo antigo da memoria real
+        list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
+        self.memory_real.remove(list_index_to_remove)
+
+        #Atualiza a remoÃ§Ã£o na hash table
+        self.real_virtual_map[old_p_name][""real""] = None
+        self.real_virtual_map[old_p_name][""uses""] = 0 #Fazer update da tabela hash
+
+        #
+        self.real_virtual_map[process.name][""real""] = new_p_real_index #Fazer update da tabela hash
+        self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
+
+        return True
+         
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
+    def garbage_collector(self,process_done:List[ProcessIn]):
+        real_virtual_map = self.real_virtual_map
+        for p in process_done:
+            free_real_index = real_virtual_map[p.name][""real""]
+            free_virtual_index = real_virtual_map[p.name][""virtual""]
+
+            self.memory_real.remove(free_real_index)
+            self.memory_virtual.remove(free_virtual_index)
+
+
+            real_virtual_map[p.name][""real""] = None
+            real_virtual_map[p.name][""virtual""] = None
+            real_virtual_map[p.name][""uses""] = None
+        self.real_virtual_map = real_virtual_map 
+        print(""Removed unused processes"")
 
 
+    def init_memories(self):
         pass
 
     "
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria comeÃ§a vazio
         return space_graph
 
-    def is_memory_full(self, page_in: int):
-        if page_in > self.current_memory_space:
             return True
         return False
     
@@ -29,7 +29,7 @@ def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
-    def add(self,used_index: List[any])-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
@@ -38,8 +38,9 @@ def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
-    def remove(self,index:int):
-        self.space_graph[str(index)] = False
-        self.current_space_occupied -= 1
-
 "
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria comeÃ§a vazio
         return space_graph
 
+    def is_memory_full(self, number_of_page_in: int):
+        if number_of_page_in > self.current_memory_space:
             return True
         return False
     
@@ -29,7 +29,7 @@ def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
+    def add(self,used_index:List)-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
@@ -38,8 +38,9 @@ def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
+    def remove(self,index_list:List[int]):
+        for index in index_list:
+            self.space_graph[index] = False
+            self.current_space_occupied -= 1
+            print(f""Removed {index} from real memory"")
 "
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    # real_memory = MemoryReal(total_memory_pages=50)
-    # virtual_memory = MemoryVirtual(total_memory_frames=100)
-    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,6 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -66,10 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             continue
 
         if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
-            # result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    real_memory = MemoryReal(total_memory_pages=50)
+    virtual_memory = MemoryVirtual(total_memory_frames=100)
+    mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,6 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
+            mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -66,10 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
+            mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process nÃ£o esteja carregado na cache
+            result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[gp]
+components = 100
+batch_size = 100"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.random_projection import GaussianRandomProjection
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""gauss_rp.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""gp"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""gp"", ""components"")
+transformer = GaussianRandomProjection(n_components=n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
+    # fit grp
+    embedding = transformer.fit_transform(
+        batch_of_images.reshape(batch_size, -1)
+    )
+    ###########################################################################
+    np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
+
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[pca]
+components = 50
+batch_size = 50"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.decomposition import IncrementalPCA
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""ipca.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""pca"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""pca"", ""components"")
+assert n_components <= batch_size
+transformer = IncrementalPCA(n_components = n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""IPCA of batch {batch:02d}"", end=""\n"")
+    # fit pca
+    transformer.fit(batch_of_images.reshape(batch_size, -1))
+###############################################################################
+with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
+
+    pickle.dump(transformer, file)
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
-        # get magnitude scale to better distinguis objects
-        # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
-    image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
+    image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[gp]
+components = 100
+batch_size = 100"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.random_projection import GaussianRandomProjection
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""gauss_rp.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""gp"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""gp"", ""components"")
+transformer = GaussianRandomProjection(n_components=n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
+    # fit grp
+    embedding = transformer.fit_transform(
+        batch_of_images.reshape(batch_size, -1)
+    )
+    ###########################################################################
+    np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
+
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
+
+[pca]
+components = 50
+batch_size = 50"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.decomposition import IncrementalPCA
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""ipca.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""pca"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""pca"", ""components"")
+assert n_components <= batch_size
+transformer = IncrementalPCA(n_components = n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""IPCA of batch {batch:02d}"", end=""\n"")
+    # fit pca
+    transformer.fit(batch_of_images.reshape(batch_size, -1))
+###############################################################################
+with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
+
+    pickle.dump(transformer, file)
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"-""""""Prepare raw images for Fourier Analysis""""""
-
-###############################################################################
-from configparser import ConfigParser, ExtendedInterpolation
-import glob
-import time
-
-from astropy.io import fits as pyfits
-import numpy as np
-
-from leosAi.utils.managefiles import FileDirectory
-###############################################################################
-start_time = time.time()
-###############################################################################
-parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""ir_pca.ini""
-parser.read(f""{config_file_name}"")
-# Check files and directory
-check = FileDirectory()
-# Handle configuration file
-# configuration = ConfigurationFile()
-###############################################################################
-# location of data
-data_directory = parser.get(""directory"", ""data"")
-data_type = parser.get(""common"", ""type"")
-path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
-
-for idx, path_to_file in enumerate(path_to_files):
-
-    file_name = path_to_file.split(""/"")[-1].split(""."")[0]
-    print(f""{idx:04d}: {file_name}"", end=""\r"")
-
-    image = np.load(path_to_file)
-###############################################################################
-# with open(
-#     f""{save_to}/{config_file_name}"",
-#     ""w"", encoding=""utf8""
-# ) as config_file:
-#
-#     parser.write(config_file)
-###############################################################################
-finish_time = time.time()
-print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
-        # get magnitude scale to better distinguis objects
-        # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
-    image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
+    image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5;BinFlush;scripts;f6b8b28ef6c4ca975f9ee38e9cef9830b74032e0;added memory sanitation and clipboard option;"def main():
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()"
OK;5;BinFlush;scripts;f6b8b28ef6c4ca975f9ee38e9cef9830b74032e0;added memory sanitation and clipboard option;"def main():
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
+    parser.add_argument('-f', action='store_true', help=""Require password to include every specified type of characters"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()"
KO;5;BinFlush;scripts;610dc5bb1e7fbed110ac160f2034b26df18ce21f;added option to copy password to clipboard, and some memory sanitation of sensitive data;" import secrets
 import string
 import argparse
-
 
 def main():
     # Defining command line arguments
@@ -12,8 +13,9 @@ def main():
     parser.add_argument('-u', action='store_true', help=""use uppercase"")
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
-    parser.add_argument('--charset', default="""", help=""custom character set \""in quotes\"""")
     args = parser.parse_args()
 
     # Checking how many sets of characters are to be used
@@ -25,7 +27,8 @@ def main():
 
     # Add sets to superset
     chars = """"
-    if args.l:    chars += string.ascii_lowercase
     if args.u:
         chars += string.ascii_uppercase
     if args.d:
@@ -40,13 +43,22 @@ def main():
 
     # Build and print actual password
     password = ''.join(secrets.choice(chars) for i in range(args.n))
-    print(password)
 
 def count_arguments(args) -> int:
     """""" Counts valid command line arguments except -n""""""
     n: int = 0
     for arg in vars(args):
-        if not arg == ""n"":
             n += bool(getattr(args, arg))
     return n
 "
OK;5;BinFlush;scripts;610dc5bb1e7fbed110ac160f2034b26df18ce21f;added option to copy password to clipboard, and some memory sanitation of sensitive data;" import secrets
 import string
 import argparse
+import gc
+import pyperclip # remember to pip3 install pyperclip
 
 def main():
     # Defining command line arguments
@@ -12,8 +13,9 @@ def main():
     parser.add_argument('-u', action='store_true', help=""use uppercase"")
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
+    parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
+    parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()
 
     # Checking how many sets of characters are to be used
@@ -25,7 +27,8 @@ def main():
 
     # Add sets to superset
     chars = """"
+    if args.l:    
+        chars += string.ascii_lowercase
     if args.u:
         chars += string.ascii_uppercase
     if args.d:
@@ -40,13 +43,22 @@ def main():
 
     # Build and print actual password
     password = ''.join(secrets.choice(chars) for i in range(args.n))
+    if args.c:
+        pyperclip.copy(password)
+    else:
+        print(password)
+
+    
+    # Clean up sensitive data
+    del(password)
+    del(chars)
+    gc.collect()
 
 def count_arguments(args) -> int:
     """""" Counts valid command line arguments except -n""""""
     n: int = 0
     for arg in vars(args):
+        if arg in [""u"", ""l"", ""d"", ""charset""]:
             n += bool(getattr(args, arg))
     return n
 "
KO;6;NTT123;a0-jax;df6a898f5b2d72b537c3c31d25ffbc7e4138a935;reduce memory usage;"def train(
         buffer.extend(data)
         data = list(buffer)
         shuffler.shuffle(data)
-        data = jax.tree_map(lambda *xs: np.stack(xs), *data)
-        N = data.state.shape[0]
         losses = []
         old_agent = jax.tree_map(lambda x: jnp.copy(x), agent)
         agent = agent.train()
         with click.progressbar(
             range(0, N - batch_size, batch_size), label=""  train agent""
         ) as bar:
             for i in bar:
-                batch = jax.tree_map(lambda x: x[i : (i + batch_size)], data)
                 agent, optim, loss = train_step(agent, optim, batch)
                 losses.append(loss)
 "
OK;6;NTT123;a0-jax;df6a898f5b2d72b537c3c31d25ffbc7e4138a935;reduce memory usage;"def train(
         buffer.extend(data)
         data = list(buffer)
         shuffler.shuffle(data)
+        N = len(data)
         losses = []
         old_agent = jax.tree_map(lambda x: jnp.copy(x), agent)
         agent = agent.train()
         with click.progressbar(
             range(0, N - batch_size, batch_size), label=""  train agent""
         ) as bar:
             for i in bar:
+                batch = data[i : (i + batch_size)]
+                batch = jax.tree_map(lambda *xs: np.stack(xs), *batch)
                 agent, optim, loss = train_step(agent, optim, batch)
                 losses.append(loss)
 "
KO;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
   </component>
 </module>
\ No newline at end of file"
OK;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
+    <orderEntry type=""module"" module-name=""state_machine"" />
   </component>
 </module>
\ No newline at end of file"
KO;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
OK;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
+      <module fileurl=""file://$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"def __init__(self, normalization, local_dataset, batch_size=32 ,epochs=1):
         self.epochs=epochs
 
 
-    def train(self):
-
-        optimizer = SGD(self.model.parameters(), lr=1e-3, weight_decay=5e-4)
-
-        criterion = CrossEntropyLoss()
-        criterion.cuda()
-
-        self.model.cuda()
-        self.model.train()
-
-        for epoch in range(self.epochs):
-            # training loop
-            for i, data in enumerate(self.dataset):
-                imgs, labels = data
-                imgs, labels = imgs.cuda(), labels.cuda()
-
-                optimizer.zero_grad()
-                pred = self.model(imgs)
-                pred = pred.cuda()
-                
-                loss = criterion(pred, labels)
-                loss.backward()
-                optimizer.step()
-
-        self.model = self.model.to('cpu')
-
-        self.model_dict = self.model.state_dict()
-        torch.cuda.empty_cache()
-
-
-    def get_data(self, key):
-        return self.model_dict[key]
-
-    def set_model(self, model_dict):
-        self.model = ResNet(self.normalization)
-        self.model.load_state_dict(model_dict)
-
     
\ No newline at end of file"
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"def __init__(self, normalization, local_dataset, batch_size=32 ,epochs=1):
         self.epochs=epochs
 
 
     
\ No newline at end of file"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"+from torch.optim import SGD
+from torch.nn import CrossEntropyLoss
+from resnet50 import ResNet
+import torch
+
+class ClientSimulation:
+
+    def __init__(self, n_clients, normalization):
+        self.n_clients = n_clients
+        self.normalization = normalization
+        
+
+    def train(self, clients, client_subset, server_model_dict):
+
+        cl_data = dict()
+        
+        for index in client_subset:
+            
+            cl = _Client(self.normalization, clients[index].dataset, clients[index].epochs, server_model_dict)
+            print(f""Training client {index}"")
+            cl.train()
+            print(""Done"")
+            cl_data[index] = cl
+
+        return cl_data
+
+
+class _Client:
+
+    def __init__(self, normalization, local_dataset, epochs, model_dict):
+        self.model = ResNet(normalization)
+        self.model.load_state_dict(model_dict)
+        self.dataset = local_dataset
+        self.epochs = epochs
+
+    def train(self):
+
+        optimizer = SGD(self.model.parameters(), lr=1e-3, weight_decay=5e-4)
+
+        criterion = CrossEntropyLoss()
+        criterion.cuda()
+
+        self.model.cuda()
+        self.model.train()
+
+        for epoch in range(self.epochs):
+            # training loop
+            for i, data in enumerate(self.dataset):
+                imgs, labels = data
+                imgs, labels = imgs.cuda(), labels.cuda()
+
+                optimizer.zero_grad()
+                pred = self.model(imgs)
+                pred = pred.cuda()
+                
+                loss = criterion(pred, labels)
+                loss.backward()
+                optimizer.step()
+
+        self.model = self.model.to('cpu')
+
+        self.model_dict = self.model.state_dict()
+        torch.cuda.empty_cache()
+
+    def get_data(self, key):
+        return self.model_dict[key]
+"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from test import test_accuracy
 import copy
 import gc
 
 
 # global parameters : number of epochs locally, normalization type
@@ -34,6 +35,10 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
     dataset = get_dataset(transform)
     subdatasets = get_iid_split(dataset)
 
     # create clients
 
     clients = dict()
@@ -54,17 +59,12 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
 
         print(f""##### ROUND {round}"")
 
-        client_subset = sample(range(N_CLIENTS), int(client_proportion*N_CLIENTS))
-
-        for index in client_subset:
-            print(f""Training client  {index}"")
-            server_model_dict = server.get_model_dict()
 
-            clients[index].set_model(server_model_dict)
-            clients[index].train()
-            print(""Done"")
 
-        model_dict = average(clients, normalization, client_subset)
 
         server.update_model(model_dict)
 "
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from test import test_accuracy
 import copy
 import gc
+from client_simulation import ClientSimulation
 
 
 # global parameters : number of epochs locally, normalization type
@@ -34,6 +35,10 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
     dataset = get_dataset(transform)
     subdatasets = get_iid_split(dataset)
 
+    n_clients_each_round = int(client_proportion*N_CLIENTS)
+
+    sim = ClientSimulation(n_clients_each_round, normalization)
+
     # create clients
 
     clients = dict()
@@ -54,17 +59,12 @@ def main(epochs, normalization, rounds, client_proportion, batch_size):
 
         print(f""##### ROUND {round}"")
 
+        client_subset = sample(range(N_CLIENTS), n_clients_each_round)
 
+        server_model_dict = server.get_model_dict()
+        trained_models = sim.train(clients, client_subset, server_model_dict)
 
+        model_dict = average(trained_models, normalization, client_subset)
 
         server.update_model(model_dict)
 "
KO;8;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el nÃºmero de taps que realice el usuario
 
-    if taps==64:
        up()
        goto(0, 0)
        color (""red"")"
OK;8;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el nÃºmero de taps que realice el usuario
 
+    if taps==300:
        up()
        goto(0, 0)
        color (""red"")"
KO;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
-        self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
OK;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
+        # self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
KO;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
-        print(cache.actual_data)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
@@ -65,14 +64,10 @@ def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
-        cache.actual_data['w'] = keys[pygame.K_w] * 6
-        cache.actual_data['s'] = keys[pygame.K_s] * 6
-        cache.actual_data['a'] = keys[pygame.K_a] * 6
-        cache.actual_data['d'] = keys[pygame.K_d] * 6
-        for key in cache.actual_data:
-            if cache.actual_data[key] == 0:
-                continue
-            cache.actual_data[key] = cache.actual_data[key] - 1
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
@@ -83,9 +78,6 @@ def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
-            if e.type == pygame.KEYDOWN:
-                if e.unicode in cache.actual_data:
-                    cache.actual_data[e.unicode] = 5
         
         screen.fill('black')
         data = cache.get_last_data()"
OK;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
@@ -65,14 +64,10 @@ def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
+        cache.actual_data['w'] = keys[pygame.K_w]
+        cache.actual_data['s'] = keys[pygame.K_s]
+        cache.actual_data['a'] = keys[pygame.K_a]
+        cache.actual_data['d'] = keys[pygame.K_d]
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
@@ -83,9 +78,6 @@ def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
         
         screen.fill('black')
         data = cache.get_last_data()"
KO;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
     simulation_id = request['data'].get(""id"")
-    if not uid:
-        return
-    elif not simulation_id:
-        return
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
@@ -93,6 +99,8 @@ async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
OK;9;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
+    old_simulation_id = request['cookie'].get(""id"")
     simulation_id = request['data'].get(""id"")
+    
+    if not uid: return
+    if not simulation_id: return
+    if not old_simulation_id: return
+    
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
+    old_simulation = storage.get_unit(""simulations"", simulation_id=old_simulation_id)
+    if not old_simulation:
+        return
+    old_simulation[""users""].remove(user['id'])
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
@@ -93,6 +99,8 @@ async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
+    for key in request['data']['keys']:
+        request['data']['keys'][key] *= 6
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
KO;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"Displays stock information relative to now, in years. Replace the 1 with any num
 ## Up and Down Command:
 
 `!up three_letter_stock_name value_to_reach`
 `!down three_letter_stock_name value_to_reach`
 
 Sets up an automatic alert for when the specified stock value exceeds or falls under a specified value.
 
 ## Buy and Sell Command:
 
 `!buy money_to_buy_shares_with three_letter_stock_name`
@@ -95,6 +102,22 @@ Deletes all pending alerts for the provided stock ticker that are also from the
 
 Deletes any pending alert that matches the stock ticker, `!up` and `!down` command, and also the value.
 
 # Administration Commands:
 ## Stop Command:
 "
OK;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"Displays stock information relative to now, in years. Replace the 1 with any num
 ## Up and Down Command:
 
 `!up three_letter_stock_name value_to_reach`
+
 `!down three_letter_stock_name value_to_reach`
 
 Sets up an automatic alert for when the specified stock value exceeds or falls under a specified value.
 
+`!up three_letter_stock_name percentage %`
+
+`!down three_letter_stock_name percentage %`
+
+Sets a relative pricing alert to it's current price.
+
 ## Buy and Sell Command:
 
 `!buy money_to_buy_shares_with three_letter_stock_name`
@@ -95,6 +102,22 @@ Deletes all pending alerts for the provided stock ticker that are also from the
 
 Deletes any pending alert that matches the stock ticker, `!up` and `!down` command, and also the value.
 
+## Portfolio:
+
+### This command only works in Direct Messages with the bot. It will send you a DM if you execute this command in any server.
+
+`!portfolio torn_api_key`
+
+Lists all stocks that you own with all transactions with their change in price relative to now. All transactions for that stock are ordered newest first to oldest last.
+
+`!portfolio torn_api_key stock_ticker`
+
+Lists all transactions for the specified ticker.
+
+`!portfolio torn_api_key stock_ticker number_of_transactions`
+
+Lists `number_of_transactions` of the selected stock before stopping. If you have two or more transactions, and use a it'll show the most recent transaction.
+
 # Administration Commands:
 ## Stop Command:
 "
KO;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							break
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)
@@ -538,10 +542,14 @@ async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							break
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)"
OK;9;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
+					found_stock = False
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
+							found_stock = True
 							break
+					if not found_stock:	
+						userdata[""value""].append(0)
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)
@@ -538,10 +542,14 @@ async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
+					found_stock = False
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
+							found_stock = True
 							break
+					if not found_stock:	
+						userdata[""value""].append(0)
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)"
KO;12;js0522;tgn_review;00428db9374655b3ecf0ce9806b2d8e1254101c0;Add vanilla RNN memory updater;"def update_memory(self, unique_node_ids, unique_messages, timestamps):
     pass
 
 
-class GRUMemoryUpdater(MemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
-    super(GRUMemoryUpdater, self).__init__()
     self.memory = memory
     self.layer_norm = torch.nn.LayerNorm(memory_dimension)
     self.message_dimension = message_dimension
     self.device = device
 
-    self.memory_updater = nn.GRUCell(input_size=message_dimension,
-                                     hidden_size=memory_dimension)
-
   def update_memory(self, unique_node_ids, unique_messages, timestamps):
     if len(unique_node_ids) <= 0:
       return
@@ -46,3 +43,26 @@ def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):
     updated_last_update[unique_node_ids] = timestamps
 
     return updated_memory, updated_last_update"
OK;12;js0522;tgn_review;00428db9374655b3ecf0ce9806b2d8e1254101c0;Add vanilla RNN memory updater;"def update_memory(self, unique_node_ids, unique_messages, timestamps):
     pass
 
 
+class SequenceMemoryUpdater(MemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
+    super(SequenceMemoryUpdater, self).__init__()
     self.memory = memory
     self.layer_norm = torch.nn.LayerNorm(memory_dimension)
     self.message_dimension = message_dimension
     self.device = device
 
   def update_memory(self, unique_node_ids, unique_messages, timestamps):
     if len(unique_node_ids) <= 0:
       return
@@ -46,3 +43,26 @@ def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):
     updated_last_update[unique_node_ids] = timestamps
 
     return updated_memory, updated_last_update
+
+
+class GRUMemoryUpdater(SequenceMemoryUpdater):
+  def __init__(self, memory, message_dimension, memory_dimension, device):
+    super(GRUMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)
+
+    self.memory_updater = nn.GRUCell(input_size=message_dimension,
+                                     hidden_size=memory_dimension)
+
+
+class RNNMemoryUpdater(SequenceMemoryUpdater):
+  def __init__(self, memory, message_dimension, memory_dimension, device):
+    super(RNNMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)
+
+    self.memory_updater = nn.RNNCell(input_size=message_dimension,
+                                     hidden_size=memory_dimension)
+
+
+def get_memory_updater(module_type, memory, message_dimension, memory_dimension, device):
+  if module_type == ""gru"":
+    return GRUMemoryUpdater(memory, message_dimension, memory_dimension, device)
+  elif module_type == ""rnn"":
+    return RNNMemoryUpdater(memory, message_dimension, memory_dimension, device)"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<component name=""InspectionProjectProfileManager"">
+  <settings>
+    <option name=""USE_PROJECT_PROFILE"" value=""false"" />
+    <version value=""1.0"" />
+  </settings>
+</component>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project version=""4"">
+  <component name=""ProjectModuleManager"">
+    <modules>
+      <module fileurl=""file://$PROJECT_DIR$/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/.idea/state_machine.iml"" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project version=""4"">
+  <component name=""RSettings"" path="""" />
+</project>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project version=""4"">
+  <component name=""ProjectId"" id=""29QNMxx7EeoT8aAdnSWICPXeq0h"" />
+</project>
\ No newline at end of file"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" from resources.state_machine.states.s03_test_state_2 import Test2StateBody
 
 class Initialization(InitializationBody):
-    def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
-            self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         else:
-            info = "">>> Info: transition error in {} state"".format(self)
-            return CloseProgram(info)
 
 class CloseProgram(CloseProgramBody):
-    def on_event(self, event):
         self.action()
 
 class Test1State(Test1StateBody):
-    def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
-            self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
-            info = "">>> Info: transition error in {} state"".format(self)
-            return CloseProgram(info)
 
 class Test2State(Test2StateBody):
-    def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
-            self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
-            info = "">>> Info: transition error in {} state"".format(self)
-            return CloseProgram(info)"
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" from resources.state_machine.states.s03_test_state_2 import Test2StateBody
 
 class Initialization(InitializationBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.Initialization
+
+        '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
+            states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
+            return CloseProgram()
 
+        '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             return Test1State()
         else:
+            states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
+            return CloseProgram()
 
 class CloseProgram(CloseProgramBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.CloseProgram
+
         self.action()
 
 class Test1State(Test1StateBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.Test1State
+
+        '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
+            states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
+            return CloseProgram()
 
+        '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
+            states_data.Test1State = Test1State()
             return Test2State()
         else:
+            states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
+            return CloseProgram()
 
 class Test2State(Test2StateBody):
+    def on_event(self, event, states_data):
+        '''import memory from States class'''
+        self = states_data.Test2State
+
+        '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
+            states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
+            return CloseProgram()
 
+        '''transition conditions'''
         if self.status == ""GO TO TEST1"":
+            '''clear data before transition'''
+            states_data.Test2State = Test2State()
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
+            states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
+            return CloseProgram()"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 
 '''import all your states here'''
-from resources.state_machine.my_states import Initialization
 
 
 class StateLoader(object): #in Karen project is a SimpleDevice class
@@ -18,7 +25,8 @@ def __init__(self):
         """""" Initialize the components. """"""
 
         # Start with a default state.
-        self.state = Initialization()
 
     def on_event(self, event):
         """"""
@@ -29,7 +37,7 @@ def on_event(self, event):
         #
 
         # The next state will be the result of the on_event function.
-        self.state = self.state.on_event(event)
 
 
 "
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 
 '''import all your states here'''
+from resources.state_machine.my_states import Initialization, CloseProgram, Test1State, Test2State
+
+class States():
+    def __init__(self):
+        self.Initialization = Initialization()
+        self.CloseProgram = CloseProgram()
+        self.Test1State = Test1State()
+        self.Test2State = Test2State()
 
 
 class StateLoader(object): #in Karen project is a SimpleDevice class
@@ -18,7 +25,8 @@ def __init__(self):
         """""" Initialize the components. """"""
 
         # Start with a default state.
+        self.states_data = States()
+        self.state = self.states_data.Initialization
 
     def on_event(self, event):
         """"""
@@ -29,7 +37,7 @@ def on_event(self, event):
         #
 
         # The next state will be the result of the on_event function.
+        self.state = self.state.on_event(event=event, states_data=self.states_data)
 
 
 "
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" class InitializationBody(object):
-
     def __init__(self,):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
         self.status = None
 
     def action(self):"
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" class InitializationBody(object):
     def __init__(self,):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
+
         self.status = None
 
     def action(self):"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 class CloseProgramBody(object):
 
-    def __init__(self, info):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
-        self.info = info
 
     def action(self):
         print(self)"
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 class CloseProgramBody(object):
 
+    def __init__(self):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
+        self.info = ""info""
 
     def action(self):
         print(self)"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
-        self.status = None
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
-        while self.counter < 5:
-            self.counter += 1
-            print(self.counter)
-            sleep(0.5)
-        self.status = ""GO TO TEST2""
 
     def __repr__(self):
         """"""
@@ -34,3 +34,4 @@ def __str__(self):
         return self.__class__.__name__
 
 "
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
+        self.status = ""GO TO TEST1""
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
+        self.counter += 1
+        print(self.counter)
+        sleep(0.5)
+        if self.counter >= 3:
+            self.status = ""GO TO TEST2""
 
     def __repr__(self):
         """"""
@@ -34,3 +34,4 @@ def __str__(self):
         return self.__class__.__name__
 
 
+"
KO;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
-        self.status = None
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
-        while self.counter < 5:
-            self.counter += 1
-            print(self.counter)
-            sleep(0.5)
-        self.status = ""GO TO TEST1""
 
     def __repr__(self):
         """""""
OK;12;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
+        self.status = ""GO TO TEST2""
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
+        self.counter += 1
+        print(self.counter)
+        sleep(0.5)
+        if self.counter >= 3:
+            self.status = ""GO TO TEST1""
 
     def __repr__(self):
         """""""
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
-				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
-				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
 				if(starget < etarget){
 					max_token=mid;
 				}"
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
+				tx=x+list_x[max_token];
+				ty=y+list_y[max_token];
+				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+					starget=data[ty*width+tx];
+				}
+				else starget=0;
+				
+				tx=x+list_x[mid];
+				ty=y+list_y[mid];
+				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+					etarget=data[ty*width+tx];
+				}
+				else etarget=0;
+				
 				if(starget < etarget){
 					max_token=mid;
 				}"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"+
+__kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
+						__global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+
+	if(x<height && y<width){
+		//init variable
+		float pos_avg=0,neg_avg=0;
+		int pos_count=0,neg_count=0,weight=0,start,end,max_weight=0,max_token;
+		int tx,ty;
+		float current,target;
+		current=data[y*width+x];
+		start=list_len*3/4;
+		end=list_len/4;
+		
+		//init direct weight
+		for(int i=0;i<end;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current) ++weight;
+			}
+		}
+		for(int i=start;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current) ++weight;
+			}
+		}
+		
+		//calc direct
+		for(int i=0;i<list_len;++i){
+			start=(start+1)%list_len;
+			end=(end+1)%list_len;
+			tx=x+list_x[start];
+			ty=y+list_y[start];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){ --weight; }
+			}
+			tx=x+list_x[end];
+			ty=y+list_y[end];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){ ++weight; }
+			}
+			//if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				if(weight>max_weight){
+					max_token=i;
+					max_weight=weight;
+				}
+				else if(weight==max_weight){
+					target=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
+					if(target < current){
+						max_token=i;
+					}
+				}
+			//}
+			
+		}
+		
+		//calc diff
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){
+					pos_avg+=target;
+					++pos_count;
+				}
+				else{
+					neg_avg+=target;
+					++neg_count;
+				}
+			}
+		}
+		
+		//diff finish
+		if(pos_count){ pos_avg/=(float)pos_count; }
+		else{ pos_avg=current; }
+		if(neg_count){ neg_avg/=(float)neg_count; }
+		else{ neg_avg=current; }
+		diff[y*width+x]=pos_avg-neg_avg;
+	
+		//direct finish
+		direct[y*width+x]=list_deg[max_token];
+	}
+}
+
+
+__kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
+                        __global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+	
+	if(x<height && y<width){
+		int tx,ty;
+		result[y*width+x]=0;
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				result[y*width+x]-=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
+			}
+		}
+	}
+}
+"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"+
+__kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
+						__global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+
+	if(x<width && y<height){
+		//init variable
+		float pos_avg=0,neg_avg=0;
+		int pos_count=0,neg_count=0,weight=0,start,end,mid,max_weight=0,max_token,dist_token;
+		int tx,ty,otx,oty,stx,sty,etx,ety;
+		float current,target,otarget,starget,etarget;
+		
+		//mid point
+		current=data[y*width+x];
+
+		//init direction (sliding windows, init weight)
+		start=0;
+		end=list_len/2+1;
+		mid=end/2+1;
+		weight=0;
+		
+		max_token=mid;
+		max_weight=weight;	//weight can be negative, just searching for largest weight
+		
+		//calc direct
+		for(int i=0;i<list_len;++i){
+			stx=x+list_x[start];
+			sty=y+list_y[start];
+			etx=x+list_x[end];
+			ety=y+list_y[end];
+			
+			//check start target of sliding window
+			if( (stx>=0 && stx<width) && (sty>=0 && sty<height) ){
+				starget=data[sty*width+stx];
+			}
+			else starget=-1;
+			
+			//check end target of sliding window (assume it is opposite of start target)
+			if( (etx>=0 && etx<width) && (ety>=0 && ety<height) ){
+				etarget=data[ety*width+etx];
+			}
+			else etarget=-1;
+			
+			//compare start and end to the middle point
+			if(starget>current && etarget>current){
+				//both is larger than middle point
+				if(starget>etarget) --weight;
+				else ++weight;
+			}
+			else{
+				if(starget>current) --weight;
+				if(etarget>current) ++weight;
+			}
+			
+			//update max_weight
+			if(weight>max_weight){
+				max_token=mid;
+				max_weight=weight;
+			}
+			else if(weight==max_weight){
+				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
+				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
+				if(starget < etarget){
+					max_token=mid;
+				}
+			}
+			
+			//move sliding window
+			start=(start+1)%list_len;
+			end=(end+1)%list_len;
+			mid=(mid+1)%list_len;
+		}
+		
+		//calculate diff (magnitude)
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+				target=data[ty*width+tx];
+				if(i>max_token) dist_token=i-max_token;
+				else dist_token=max_token-i;
+				if(dist_token>list_len/2) dist_token=list_len-dist_token;
+				
+				if(dist_token>list_len/4){
+					pos_avg+=target;
+					++pos_count;
+				}
+				else{
+					neg_avg+=target;
+					++neg_count;
+				}
+			}
+		}
+		
+		//finish diff (magnitude)
+		if(pos_count){ pos_avg/=(float)pos_count; }
+		else{ pos_avg=current; }
+		if(neg_count){ neg_avg/=(float)neg_count; }
+		else{ neg_avg=current; }
+		diff[y*width+x]=pos_avg-neg_avg;
+		
+		//direct finish
+		direct[y*width+x]=list_deg[max_token];
+	}
+}
+
+
+__kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
+                        __global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+	
+	if(x<width && y<height){
+		int tx,ty;
+		result[y*width+x]=0;
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+				result[y*width+x]+=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
+			}
+		}
+	}
+}
+"
KO;13;Boavizta;cloud-bill;cd5b941be98275fd0549d8abc4a5a460f713c4cc;fix meminfo total memory reader #1;" from typing import List
 
 from .model import MemoryDevice
@@ -24,7 +26,7 @@ def get_total_memory_in_kb() -> int:
             if 'MemTotal' in line:
                 mem_total_line = line.strip()
                 break
-    total_size_kb = int(mem_total_line.split()[0])
     return total_size_kb
 
 "
OK;13;Boavizta;cloud-bill;cd5b941be98275fd0549d8abc4a5a460f713c4cc;fix meminfo total memory reader #1;"+import re
+
 from typing import List
 
 from .model import MemoryDevice
@@ -24,7 +26,7 @@ def get_total_memory_in_kb() -> int:
             if 'MemTotal' in line:
                 mem_total_line = line.strip()
                 break
+    total_size_kb = int(re.search(r'[0-9]+', mem_total_line)[0])
     return total_size_kb
 
 "
KO;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
-        self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
OK;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
+        self.seg_head = SegHead(classes, 1024, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
KO;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
-# tf.keras.mixed_precision.set_global_policy('mixed_float16')
-physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
-for gpu in physical_devices:
-    tf.config.experimental.set_memory_growth(gpu, True)
-mirrored_strategy = tf.distribute.MirroredStrategy()
-
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",
@@ -44,7 +38,7 @@
 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
-                                                                                   ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,
@@ -57,25 +51,35 @@
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
-args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
-args.add_argument(""--random_crop_height"", type=int, default=None,
-                  help=""Height of random crop, random_crop_width must be given with this"")
-args.add_argument(""--random_crop_width"", type=int, default=None,
-                  help=""Width of random crop, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
 args = args.parse_args()
 
 tf.random.set_seed(args.random_seed)
-random_crop_size = (args.random_crop_width, args.random_crop_height) \
-    if args.random_crop_width is not None and args.random_crop_height is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset
@@ -93,13 +97,14 @@
 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
-logdir = os.path.join(args.save_dir, ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
-                                                                                      batch_size,
-                                                                                      optimizer_name, lr,
-                                                                                      args.lr_scheduler,
-                                                                                      backbone,
-                                                                                      model_name,
-                                                                                      time))
 
 # =========== Load Dataset ============ #
 
@@ -117,6 +122,14 @@
 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,
@@ -136,12 +149,13 @@
 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
-processed_train = dataset_train.map(get_images_processed)
-processed_train = processed_train.map(augmentor)
 processed_val = dataset_validation.map(get_images_processed)
-processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).prefetch(
     tf.data.experimental.AUTOTUNE)
-processed_val = processed_val.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)
@@ -167,7 +181,7 @@
         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
-    model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True) if random_crop_size is None else model(tf.random.uniform((1, random_crop_size[0], random_crop_size[1], 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):
@@ -182,7 +196,7 @@
 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
-        train_logits = model((mini_batch[0] / 127.5) - 1, training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
@@ -201,14 +215,14 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
-    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
-    val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
-    if random_crop_size is not None:
-        val_labs = tf.image.resize(val_labs, random_crop_size)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
@@ -220,7 +234,8 @@ def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
-    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -232,8 +247,6 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
-               # tf.concat(train_labs.values, axis=0), \
-               # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -274,7 +287,7 @@ def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
-        processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
@@ -306,35 +319,16 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
-                            tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
-for epoch in range(START_EPOCH, EPOCHS):
-    print(""\n ----------- Epoch {} --------------\n"".format(epoch))
-    step = 0
-    if epoch % args.save_interval == 0:
-        model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
-        print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
-    for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
-        c_step = (epoch * total_samples // args.batch_size) + step
-        loss, train_labs, train_logits = distributed_train_step(mini_batch)
-        step += 1
-
-        # ======== mIoU calculation ==========
-        mIoU.reset_states()
-        gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
-        pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
-        mIoU.update_state(gt, pred)
-        # ====================================
-        # print(""Epoch {}: {}/{}, Loss: {}, mIoU: {}"".format(epoch, step * batch_size, total_samples,
-        #                                                    loss.numpy(), mIoU.result().numpy()))
-        write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
-
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
@@ -357,7 +351,33 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
-                        tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))"
OK;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",
@@ -44,7 +38,7 @@
 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
+                                                                                    ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,
@@ -57,25 +51,35 @@
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
+args.add_argument(""--random_seed"", type=int, default=512, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
+args.add_argument(""--fp16"", action=""store_true"", default=False, help=""Give to enable mixed precision training."")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
+args.add_argument(""--random_crop_min"", type=float, default=None,
+                  help=""minimum value for crop height/width relative to original image"")
+args.add_argument(""--random_crop_max"", type=float, default=None,
+                  help=""Width of random crop as ratio of original width, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
+args.add_argument(""--all_augs"", action=""store_true"", default=False, help=""Add all augmentations except flip_up_down"")
 args = args.parse_args()
 
+if args.fp16:
+    tf.keras.mixed_precision.set_global_policy('mixed_float16')
+
+physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
+for gpu in physical_devices:
+    tf.config.experimental.set_memory_growth(gpu, True)
+mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())
+
 tf.random.set_seed(args.random_seed)
+random_crop_size = (args.random_crop_min, args.random_crop_max) \
+    if args.random_crop_max is not None and args.random_crop_min is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset
@@ -93,13 +97,14 @@
 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
+logdir = os.path.join(args.save_dir,
+                      ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
+                                                                          batch_size,
+                                                                          optimizer_name, lr,
+                                                                          args.lr_scheduler,
+                                                                          backbone,
+                                                                          model_name,
+                                                                          time))
 
 # =========== Load Dataset ============ #
 
@@ -117,6 +122,14 @@
 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
+if args.all_augs:
+    args.flip_left_right = True
+    random_crop_size = (0.5, 0.95)
+    args.random_hue = True
+    args.random_saturation = True
+    args.random_brightness = True
+    args.random_contrast = True
+
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,
@@ -136,12 +149,13 @@
 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
+processed_train = dataset_train.map(augmentor)
+processed_train = processed_train.map(get_images_processed)
 processed_val = dataset_validation.map(get_images_processed)
+processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).repeat(
+    EPOCHS).prefetch(
     tf.data.experimental.AUTOTUNE)
+processed_val = processed_val.batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)
@@ -167,7 +181,7 @@
         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
+    model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):
@@ -182,7 +196,7 @@
 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
+        train_logits = model(tf.image.per_image_standardization(mini_batch[0]), training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
@@ -201,14 +215,14 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
+    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3],
+                                             method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
+    val_logits = model(tf.image.per_image_standardization(mini_batch[0]),
+                       training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
@@ -220,7 +234,8 @@ def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
+    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3],
+                                               method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -232,8 +247,6 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -274,7 +287,7 @@ def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
+        processed_labs = tf.concat(batch[1].values[0], axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
@@ -306,35 +319,16 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
+                        tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
+def evaluate():
+    global val_writer
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
@@ -357,7 +351,33 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
+                    tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))
+
+
+epoch = 0
+while epoch < EPOCHS:
+    print(""\n ----------- Epoch {} --------------\n"".format(epoch))
+    step = 0
+    if epoch % args.save_interval == 0:
+        model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
+        print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
+    for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
+        c_step = (epoch * total_samples // args.batch_size) + step
+        loss, train_labs, train_logits = distributed_train_step(mini_batch)
+        step += 1
+
+        # ======== mIoU calculation ==========
+        mIoU.reset_states()
+        gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
+        pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
+        mIoU.update_state(gt, pred)
+        # ====================================
+        write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
+        if step == total_samples // args.batch_size:
+            epoch += 1
+            break
+
+    evaluate()"
KO;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
-                crop=(256, 256),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
@@ -16,9 +16,12 @@ def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
-    if crop is not None:
-        image_crop = list(crop) + [image.shape[-1]]
-        label_crop = list(crop) + [label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
OK;14;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
+                crop_scale=(0.05, 0.95),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
@@ -16,9 +16,12 @@ def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
+    if crop_scale is not None:
+        img_shp = tf.cast(tf.shape(image), tf.float32)
+        h_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[0]
+        w_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[1]
+        image_crop = [h_scale, w_scale, image.shape[-1]]
+        label_crop = [h_scale, w_scale, label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
KO;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
-        # b = tf.image.resize(y_b * x_b, tf.shape(a)[1:3])
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
-    def __init__(self, mid_channels, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
 
 
 class BiSeNetv2(K.Model):
-    def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=64, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
@@ -144,15 +162,15 @@ def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
-        self.seg_head = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head1 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head2 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head3 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
-    def call(self, inputs, training=None, mask=None):
-        original_size = tf.shape(inputs)[1:3]
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
@@ -161,28 +179,38 @@ def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
-        x2_s5 = self.ge3(x2_s4)                     # Stride /32
         x2_ce = self.ce(x2_s5)
 
-        final_feat = self.seg_head(tf.image.resize(self.aggregator((x1_s3, x2_ce)), original_size))
         if training:
-            out_s3 = tf.image.resize(self.seg_head1(x2_s3), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            out_s4 = tf.image.resize(self.seg_head2(x2_s4), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            out_s5 = tf.image.resize(self.seg_head3(x2_s5), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            return final_feat, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
-    bs = 7
-    x = tf.random.normal((bs, 512, 1024, 3))
     bisenet = BiSeNetv2(classes=19)
     optimizer = K.optimizers.SGD()
-    for _ in tqdm.tqdm(range(1000)):
-        with tf.GradientTape() as tape:
-            final, z1, z2, z3 = bisenet(x, True)
-            # final = bisenet(x, False)
-            loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
-        vars = bisenet.trainable_variables
-        optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file
\ No newline at end of file"
OK;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
+    def __init__(self, n_classes=19, mid_channels=64, upfactor=8, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
+        self.upfactor = upfactor
+        self.aux = aux
+        if self.aux:
+            self.conv_aux = K.Sequential([
+                K.layers.UpSampling2D(2),
+                ConvBlock(upfactor * upfactor, 3, 1, padding='same'),
+            ])
+            upfactor //= 2
+        self.conv_final = K.layers.Conv2D(n_classes, 1, 1)
+        # self.upsample = K.layers.UpSampling2D(upfactor, interpolation=""bilinear"")
+
+    def call(self, inputs, **kwargs):
+        x = self.conv(inputs)
+        x = self.drop(x)
+        if self.aux:
+            x = self.conv_aux(x)
+        return self.conv_final(x)
+        # else:
+        #     return self.upsample(self.conv_final(x))
 
 
 class BiSeNetv2(K.Model):
+    def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=128, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
@@ -144,15 +162,15 @@ def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
+        self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
+        self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
+        self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
+        self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)
+        self.aux_head4 = SegHead(classes, seg_channels, 32, aux=True)
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
+    def call(self, inputs, training=True, mask=None):
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
@@ -161,28 +179,38 @@ def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
+        x2_s5 = self.ge3(x2_s4)                     # Stride /32z
         x2_ce = self.ce(x2_s5)
 
+        final_feat = self.aggregator((x1_s3, x2_ce))
+        final_feat = self.seg_head(final_feat)
         if training:
+            out_s2 = self.aux_head1(x2_s2)
+            out_s3 = self.aux_head2(x2_s3)
+            out_s4 = self.aux_head3(x2_s3)
+            out_s5 = self.aux_head4(x2_s5)
+            return final_feat, out_s2, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
+
+    physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
+    for gpu in physical_devices:
+        tf.config.experimental.set_memory_growth(gpu, True)
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
+    bs = 14
+    x = tf.random.normal((bs, 1024, 2048, 3))
     bisenet = BiSeNetv2(classes=19)
+    # bisenet.build((bs, 1024, 2048, 3))
     optimizer = K.optimizers.SGD()
\ No newline at end of file
+    final, z1, z2, z3 = bisenet(x, True)
+    bisenet.summary()
+    # for _ in tqdm.tqdm(range(1000)):
+    #     final, z1, z2, z3 = bisenet(x, True)
+        # with tf.GradientTape() as tape:
+        #     final, z1, z2, z3 = bisenet(x, True)
+        #     # final = bisenet(x, False)
+        #     loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
+        # vars = bisenet.trainable_variables
+        # optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file"
KO;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
-                 sn=False,
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
@@ -48,13 +47,9 @@ def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
-        if sn:
-            self.conv2d = tfa.layers.SpectralNormalization(self.conv2d)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
-        elif norm_layer == 'instance':
-            self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 
@@ -91,8 +86,6 @@ def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
-        elif norm_layer == 'instance':
-            self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 "
OK;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
@@ -48,13 +47,9 @@ def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 
@@ -91,8 +86,6 @@ def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 "
KO;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
@@ -33,7 +34,7 @@
 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
-args.add_argument(""--lr"", type=float, default=1e-5, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",
@@ -55,7 +56,7 @@
 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
-args.add_argument(""--aux_weight"", type=float, default=0.25, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
@@ -200,26 +201,26 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
-    return loss, train_labs, train_logits
 
 
 def val_step(mini_batch, aux=False):
-    val_logits = model((mini_batch[0] / 127.5) - 1, training=False) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=False)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
-    # if aux:
-    #     losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
-    #                                                                                    1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
-    #         calc_loss(
-    #             val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
-    #               enumerate(val_logits)]
-    #     val_loss = tf.reduce_sum(losses)
-    #     val_logits = val_logits[0]
-    # else:
-    val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
-    return val_loss, val_labs, val_logits
 
 
 @tf.function
@@ -231,6 +232,8 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -268,8 +271,10 @@ def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
-        tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
-        processed_labs = tf.concat(batch[1].values, axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
OK;14;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
+# tf.keras.mixed_precision.set_global_policy('mixed_float16')
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
@@ -33,7 +34,7 @@
 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
+args.add_argument(""--lr"", type=float, default=1e-3, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",
@@ -55,7 +56,7 @@
 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
+args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
@@ -200,26 +201,26 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
+    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
+    val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
+    if aux:
+        losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
+                                                                                       1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
+            calc_loss(
+                val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
+                  enumerate(val_logits)]
+        val_loss = tf.reduce_sum(losses)
+        val_logits = val_logits[0]
+    else:
+        val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
+    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -231,6 +232,8 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
+               # tf.concat(train_labs.values, axis=0), \
+               # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -268,8 +271,10 @@ def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
+        # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
+        # processed_labs = tf.concat(batch[1].values, axis=0)
+        tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
+        processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
-systemdan/__main__.spec
\ No newline at end of file
\ No newline at end of file"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
\ No newline at end of file
+systemdan/__main__.spec
+systemdan/command
+systemdan/commandlinux
+systemdan/distlinux/
+__main__.spex
\ No newline at end of file"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
+    typer.echo(typer.style(f'\nMemory\n', fg=typer.colors.BLUE))
+    mem_info = system_info.get_memory_info()
+    typer.echo(f'Total memory: {mem_info[""total""]}')
+    typer.echo(f'Available memory: {mem_info[""available""]}')
+    typer.echo(f'Used memory: {mem_info[""used""]}')
+
+    typer.echo(f'Swap Percentage: {mem_info[""percentage""]}')
+    typer.echo(f'Swap total: {mem_info[""swap_total""]}')
+    typer.echo(f'Swap free: {mem_info[""swap_free""]}')
+    typer.echo(f'Swap used: {mem_info[""swap_used""]}')
+    typer.echo(f'Swap percentage: {mem_info[""swap_percentage""]}')
+
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
@@ -60,3 +72,19 @@ def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
+def get_size(bytes, suffix=""B""):
+    """"""
+    e.g:
+        1253656 => '1.20MB'
+        1253656678 => '1.17GB'
+    """"""
+    factor = 1024
+    for unit in ["""", ""K"", ""M"", ""G"", ""T"", ""P""]:
+        if bytes < factor:
+            return f""{bytes:.2f}{unit}{suffix}""
+        bytes /= factor
+
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
@@ -60,3 +72,19 @@ def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result
+
+def get_memory_info() -> dict:
+    """"""Get the memory information""""""
+    result = {}
+    svmem = psutil.virtual_memory()
+    result['total'] = get_size(svmem.total)
+    result['available'] = get_size(svmem.available)
+    result['used'] = get_size(svmem.used)
+    result['percentage'] = get_size(svmem.free)
+    swap = psutil.swap_memory()
+    result['swap_total'] = get_size(swap.total)
+    result['swap_free'] = get_size(swap.free)
+    result['swap_used'] = get_size(swap.used)
+    result['swap_percentage'] = swap.percent
+
+    return result"
KO;15;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
OK;15;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
+MEMORY = ""${var.PULSAR_MEMORY}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
         self.state = None
         self.alert_level = ""false""
         self.owners = None
@@ -59,6 +60,7 @@ def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
@@ -168,6 +170,7 @@ def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
@@ -358,6 +361,7 @@ def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
+        self.memory = deployment_context.MEMORY
         self.state = None
         self.alert_level = ""false""
         self.owners = None
@@ -59,6 +60,7 @@ def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
+            ""memory"": str(self.memory) if self.memory else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
@@ -168,6 +170,7 @@ def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
+            self.task.memory = deployment_context.MEMORY
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
@@ -358,6 +361,7 @@ def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
+            self.task.memory = deployment_context.MEMORY
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
-PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
+PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,memory:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
+  echo ""MEMORY = \""$PULSAR_MEMORY\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
KO;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
OK;15;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
+  {
+    ""name"": ""memory"",
+    ""type"": ""STRING"",
+    ""mode"": ""NULLABLE""
+  },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
KO;17;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;"-import create from 'zustand'
-
 
 export interface Tokens {
   access: string;
@@ -11,9 +10,19 @@ interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
-const useStore = create<State>(set => ({
-    tokens: {access: """", refresh: """"},
-    setTokens: (tokens) => set({tokens})
-}))
 
-export default useStore;
\ No newline at end of file"
OK;17;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;"+import create from ""zustand"";
 
 export interface Tokens {
   access: string;
@@ -11,9 +10,19 @@ interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
+const getLocalStorage = (key: string) =>
+  JSON.parse(window.localStorage.getItem(key) as string);
+const setLocalStorage = (key: string, value: any) =>
+  window.localStorage.setItem(key, JSON.stringify(value));
+
+const useStore = create<State>((set) => ({
+  tokens: getLocalStorage(""tokens"") || { access: """", refresh: """" },
+  setTokens: (tokens) =>
+    set(() => {
+      setLocalStorage(""tokens"", tokens);
+      return { tokens };
+    }),
+}));
+
 
\ No newline at end of file
+export default useStore;"
KO;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
+
+.idea/
\ No newline at end of file"
KO;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;18;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"+import pygame
+from pygame.locals import (
+    K_UP,
+    K_DOWN,
+    K_LEFT,
+    K_RIGHT,
+    K_ESCAPE,
+    KEYDOWN,
+    QUIT,
+)
+mat = [
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0]
+]
+def placematrix(i,j):
+  #insx = round(x//100)
+  #insy = round(y//100)
+  mat[i][j] = 1
+
+def print_matrix(matrix):
+  for i in matrix:
+    print(*i)
+  print()
+pygame.init()
+
+def isStop(i,j):
+    return i >= 5 or mat[i + 1][j] == 1
+def contgame(i,j):
+    placematrix(i,j)
+    print_matrix(mat)
+    #shuld summon new block
+
+screen = pygame.display.set_mode([600, 600])
+i = 0
+j = 2
+running = True
+x = 200
+y = 100
+z = 0
+while running:
+    for event in pygame.event.get():
+        if event.type == KEYDOWN:
+            if event.key == K_ESCAPE:
+                running = False
+            if event.key == K_LEFT:
+                if j >= 0:
+                    #x = x - 100
+                    j = j - 1
+            if event.key == K_RIGHT:
+                if j <= 4:
+                    #x = x + 100
+                    j = j + 1
+    screen.fill((0, 0, 0))
+
+    pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
+    if i < 9:
+        if z == 500:
+            if isStop(i,j):
+                contgame(i,j)
+                running = False
+            #y = y + 100
+            i = i + 1
+            z = 0
+        z = z + 1
+
+    pygame.display.flip()
+
+pygame.quit()
\ No newline at end of file"
KO;18;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
-        return self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
-            single_layer_size = (json_data['layer_width'], json_data['layer_height'])
-            asset_image_path = str(self.data_folder / artboard['layerassets'])
-            asset_image_rgb = Image.open(asset_image_path).convert('RGB')
-            asset_image_tensor = self.img_transform(asset_image_rgb)
-            images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
-            data.append((images, names, bboxes, colors, classes, labels))
         return data
 
 "
OK;18;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
+from typing import Any, Dict, List
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
+        self.artboard_detail: List[Dict[str, Any]] = []
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
+        artboard = self.index_json[idx]
+        json_data = self.artboard_detail[idx]
+        single_layer_size = (json_data['layer_width'], json_data['layer_height'])
+        asset_image_path = str(self.data_folder / artboard['layerassets'])
+        asset_image_rgb = Image.open(asset_image_path).convert('RGB')
+        asset_image_tensor = self.img_transform(asset_image_rgb)
+        images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
+        return images, *self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
+            self.artboard_detail.append(json_data)
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
+            data.append((names, bboxes, colors, classes, labels))
         return data
 
 "
KO;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
+
+.idea/
\ No newline at end of file"
KO;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"+import pygame
+from pygame.locals import (
+    K_UP,
+    K_DOWN,
+    K_LEFT,
+    K_RIGHT,
+    K_ESCAPE,
+    KEYDOWN,
+    QUIT,
+)
+mat = [
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0]
+]
+def placematrix(i,j):
+  #insx = round(x//100)
+  #insy = round(y//100)
+  mat[i][j] = 1
+
+def print_matrix(matrix):
+  for i in matrix:
+    print(*i)
+  print()
+pygame.init()
+
+def isStop(i,j):
+    return i >= 5 or mat[i + 1][j] == 1
+def contgame(i,j):
+    placematrix(i,j)
+    print_matrix(mat)
+    #shuld summon new block
+
+screen = pygame.display.set_mode([600, 600])
+i = 0
+j = 2
+running = True
+x = 200
+y = 100
+z = 0
+while running:
+    for event in pygame.event.get():
+        if event.type == KEYDOWN:
+            if event.key == K_ESCAPE:
+                running = False
+            if event.key == K_LEFT:
+                if j >= 0:
+                    #x = x - 100
+                    j = j - 1
+            if event.key == K_RIGHT:
+                if j <= 4:
+                    #x = x + 100
+                    j = j + 1
+    screen.fill((0, 0, 0))
+
+    pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
+    if i < 9:
+        if z == 500:
+            if isStop(i,j):
+                contgame(i,j)
+                running = False
+            #y = y + 100
+            i = i + 1
+            z = 0
+        z = z + 1
+
+    pygame.display.flip()
+
+pygame.quit()
\ No newline at end of file"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
+from .netserialize import *
 from .dockernet import *
 from .dockershm import *
+from .dockernetserialize import *"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+import os
+
+def get_data():
+   testfile = ""airbnb.csv""
+   datadir = ""/data/""
+   currdir = os.path.dirname(os.path.abspath(__file__)) 
+   f = open(currdir + datadir + testfile, ""r"")
+   data = f.read()
+   return data
+
+if __name__ == ""__main__"":
+    get_data()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
-print(f""recv reading from {__file__}..."")
-print(f""recv data: {contents}"")
 f.close()"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
+print(f""recv: read from {__file__}"")
 f.close()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
-print(f""send writing to shared mem from {__file__}..."")
-f.write(""a very simple message for recv"")
 f.close()"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+from data import get_data
+
 f = open(""test.txt"", ""w+"")
+print(f""send: writing to shared mem from {__file__}..."")
+f.write(get_data())
 f.close()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
+def copy_data_to_volume(volume): 
+    data_script = ""data.py""
+    curr_dir = os.path.dirname(os.path.abspath(__file__))
+    data_dir = curr_dir + ""/data""
+    dst_dir = volume[""Mountpoint""]+f""/data""
+    if os.path.exists(dst_dir):
+        shutil.rmtree(dst_dir)
+    shutil.copytree(data_dir, dst_dir)
+    shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
+
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
+    copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
-def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
-    data = sendsock.recv(1024)
-    print(f""send {data}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
-    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
+from data import get_data
+
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
+def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
+
     sendsock.sendall(b""my simple message"")
+    
+    # wait for confirmation
+    recvdata = sendsock.recv(1024)
+    print(f""send: {recvdata}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
+    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
-def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
-    print(""send writing to shared mem..."")
-    f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
-    print(f""recv data: {contents}"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
-    sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;20;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
+from data import get_data
+
+def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
+    print(""send: writing to shared mem..."")
+    f.write(data)
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
+    print(f""recv: received data"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
+    data = get_data()
+    sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
-    FLAG_STRATEGY_FILE_PATH
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_timestep,
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_freqstep,
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
-                    ""check_function"": self._check_compress,
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
-                    ""check_function"": self._check_flag_strategy,
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
-                    step_dict[key_lower][""value""] = value
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
+    FLAG_STRATEGY_FILE_PATH,
+    DEFAULT_FLAG_RFI,
+    DEFAULT_FLAG_MEMORYPERC
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_timestep
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_freqstep
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
+                    ""check_function"": self._check_compress
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
+                    ""check_function"": self._check_flag_strategy
+                },
+                ""flag_rfi"": {
+                    ""value"": None,
+                    ""default"": DEFAULT_FLAG_RFI,
+                    ""parsing_function"": (lambda x: bool(x)),
+                    ""check_function"": self._check_flag_rfi
+                },
+                ""flag_memoryperc"": {
+                    ""value"": DEFAULT_FLAG_MEMORYPERC, # This prevents any update from the parameter field
+                    ""default"": DEFAULT_FLAG_MEMORYPERC,
+                    ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_flag_memoryperc
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
+                    if step_dict[key_lower][""value""] is None:
+                        step_dict[key_lower][""value""] = value
+                    else:
+                        # The parameter has a fixed value that cannot be set
+                        break
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
+                # If the loop has not broken, it means the parameter is not expected
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
+    @staticmethod
+    def _check_flag_rfi(flag_rfi: bool) -> bool:
+        """""" """"""
+        return isinstance(flag_rfi, bool)
+
+
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
+    @staticmethod
+    def _check_flag_memoryperc(flag_memoryperc: int) -> bool:
+        """""" """"""
+        is_int = isinstance(flag_memoryperc, int)
+        is_percent = 0 <= flag_memoryperc <= 100
+        return is_int & is_percent
+
+
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
+from .netserialize import *
 from .dockernet import *
 from .dockershm import *
+from .dockernetserialize import *"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+import os
+
+def get_data():
+   testfile = ""airbnb.csv""
+   datadir = ""/data/""
+   currdir = os.path.dirname(os.path.abspath(__file__)) 
+   f = open(currdir + datadir + testfile, ""r"")
+   data = f.read()
+   return data
+
+if __name__ == ""__main__"":
+    get_data()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
-print(f""recv reading from {__file__}..."")
-print(f""recv data: {contents}"")
 f.close()"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
+print(f""recv: read from {__file__}"")
 f.close()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
-print(f""send writing to shared mem from {__file__}..."")
-f.write(""a very simple message for recv"")
 f.close()"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+from data import get_data
+
 f = open(""test.txt"", ""w+"")
+print(f""send: writing to shared mem from {__file__}..."")
+f.write(get_data())
 f.close()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
+def copy_data_to_volume(volume): 
+    data_script = ""data.py""
+    curr_dir = os.path.dirname(os.path.abspath(__file__))
+    data_dir = curr_dir + ""/data""
+    dst_dir = volume[""Mountpoint""]+f""/data""
+    if os.path.exists(dst_dir):
+        shutil.rmtree(dst_dir)
+    shutil.copytree(data_dir, dst_dir)
+    shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
+
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
+    copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
-def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
-    data = sendsock.recv(1024)
-    print(f""send {data}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
-    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
+from data import get_data
+
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
+def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
+
     sendsock.sendall(b""my simple message"")
+    
+    # wait for confirmation
+    recvdata = sendsock.recv(1024)
+    print(f""send: {recvdata}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
+    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
-def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
-    print(""send writing to shared mem..."")
-    f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
-    print(f""recv data: {contents}"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
-    sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;28;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
+from data import get_data
+
+def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
+    print(""send: writing to shared mem..."")
+    f.write(data)
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
+    print(f""recv: received data"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
+    data = get_data()
+    sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
+  VQSR: ""8g""
   OTHER: ""4g""
 
 ##############################"
KO;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['OTHER']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
-    resources: cpus=1, mem_mb=4000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;32;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
+        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['VQSR']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
+    resources: cpus=1, mem_mb=10000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
-# Specify type of NGS sequencing ('WES' or 'WGS'). Use for VQSR filtering
-DATA: ""WES""
-
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
-  MARK_DUP: ""40g""
-  HC: ""20g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
+# Specify type of NGS sequencing ('WES' or 'WGS'). Used for choosing memory. Also used for VQSR filtering.
+DATA: ""WES""
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
+  MARK_DUP_WES: ""35g""
+  MARK_DUP_WGS: ""70g""
+  HC_WES: ""9g""
+  HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 #### Set up report #####
 
 report: ""report/workflow.rst"""
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
+def get_bwa_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 15000
+    if config['DATA'] == ""WGS"":
+        return 50000
+    else:
+        return 15000
+
+def get_mkdup_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 40000
+    if config['DATA'] == ""WGS"":
+        return 80000
+    else:
+        return 40000
+
+def get_mkdup_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+
+def get_HC_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 10000
+    if config['DATA'] == ""WGS"":
+        return 20000
+    else:
+        return 10000
+
+def get_HC_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+        
 #### Set up report #####
 
 report: ""report/workflow.rst"""
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
-    resources: cpus=28, mem_mb=15000, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
+    resources: cpus=28, mem_mb=get_bwa_memory, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP']),
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
-    resources: cpus=28, mem_mb=40000, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
+        maxmemory = get_mkdup_xmx,
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
+    resources: cpus=28, mem_mb=get_mkdup_memory, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
KO;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC']),
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
-    resources: cpus=1, mem_mb=20000, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
OK;32;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
+        maxmemory = get_HC_xmx,
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
+    resources: cpus=1, mem_mb=get_HC_memory, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
KO;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
\ No newline at end of file"
OK;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
+Computer Architecture project to simulate a CPU, including memory and cache 
+
+1. What does the program do? 
+   Program will prompt user to point the program to two input files, the cpu instructions file as well as data input 
+
+2. What data do you need?
+   Program needs CPU instructions as well as data input to initialize memory bus 
+
+3. What aspects of a CPU can you simulate using Python?
+   The program can simulate 
+
+4. How will your CPU handle incoming instructions?
+   The cpu will receive the insructions from the cpu instructions file and run the appropriate ISA instructions
+
+5. How will your CPU output instructions?
+   CPU will output instructions to the terminal
+ 
+6. How will your CPU store data?
+   CPU will store the data in memory using the Memory class. 
\ No newline at end of file"
KO;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;
OK;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;"+MEMORY_BUS_SIZE = 128
+
+# This class implements a memory bus
+class Memory:
+    def __init__(self):
+        self.memory_bus = {}
+        self.initialize_memory_bus()
+
+    # Method to initialize memory bus with size based on MEMORY_BUS_SIZE variable
+    # Memory bus is setup as a dict with binary address being key, and value being value 
+    def initialize_memory_bus(self):
+        for i in range(MEMORY_BUS_SIZE):
+            self.memory_bus[f'{i:08b}'] = 0
+    
+    # Method to search for address in memory bus and return value if available 
+    def search_memory_bus(self, address):
+        if self.memory_bus.get(address) != None:
+            return self.memory_bus.get(address)
+        return None 
+    
+    # Method to write in memory bus only if address is within the bus size 
+    def write_memory_bus(self, address, value):
+        if self.memory_bus.get(address) != None:
+            self.memory_bus[address] = value"
KO;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
+  VQSR: ""8g""
   OTHER: ""4g""
 
 ##############################"
KO;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['OTHER']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
-    resources: cpus=1, mem_mb=4000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;40;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
+        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['VQSR']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
@@ -22,7 +22,7 @@ rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
+    resources: cpus=1, mem_mb=10000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
-# Specify type of NGS sequencing ('WES' or 'WGS'). Use for VQSR filtering
-DATA: ""WES""
-
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
-  MARK_DUP: ""40g""
-  HC: ""20g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
+# Specify type of NGS sequencing ('WES' or 'WGS'). Used for choosing memory. Also used for VQSR filtering.
+DATA: ""WES""
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
@@ -39,9 +41,6 @@ GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
@@ -58,8 +57,10 @@ WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
+  MARK_DUP_WES: ""35g""
+  MARK_DUP_WGS: ""70g""
+  HC_WES: ""9g""
+  HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 #### Set up report #####
 
 report: ""report/workflow.rst"""
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
+def get_bwa_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 15000
+    if config['DATA'] == ""WGS"":
+        return 50000
+    else:
+        return 15000
+
+def get_mkdup_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 40000
+    if config['DATA'] == ""WGS"":
+        return 80000
+    else:
+        return 40000
+
+def get_mkdup_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
+
+def get_HC_memory(resource):
+    if config['DATA'] == ""WES"":
+        return 10000
+    if config['DATA'] == ""WGS"":
+        return 20000
+    else:
+        return 10000
+
+def get_HC_xmx(resource):
+    if config['DATA'] == ""WES"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+    if config['DATA'] == ""WGS"":
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WGS'])
+    else:
+        expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
+        
 #### Set up report #####
 
 report: ""report/workflow.rst"""
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
-    resources: cpus=28, mem_mb=15000, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
+    resources: cpus=28, mem_mb=get_bwa_memory, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | """
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP']),
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
-    resources: cpus=28, mem_mb=40000, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
+        maxmemory = get_mkdup_xmx,
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
@@ -15,7 +15,7 @@ rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
+    resources: cpus=28, mem_mb=get_mkdup_memory, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \"
KO;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
-        maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC']),
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
-    resources: cpus=1, mem_mb=20000, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
OK;40;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
+        maxmemory = get_HC_xmx,
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
@@ -20,7 +20,7 @@ rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
+    resources: cpus=1, mem_mb=get_HC_memory, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \"
KO;42;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
-  # sage.interfaces.gap.gap_cmd = 'gap -r -o 24G '
-  sage.interfaces.gap.gap_cmd = 'gap -r -o 50G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
-    set_gap_memory_pool_size(50* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
OK;42;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
+  sage.interfaces.gap.gap_cmd = 'gap -r -o 256G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
+    set_gap_memory_pool_size(256* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
KO;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"def evaluate_samples(self,p,model,forwarding='head',knn=100):
             ri, ci = assign_classes_hungarian(C)
             accuracy = accuracy_from_assignment(C,ri,ci)
             print('Accuracy: ',accuracy)
-            # Â§------------------------------------------------
 
             feature_tensor = torch.nn.functional.normalize(feature_tensor, dim = 1)
-            similarity_matrix = torch.einsum('nd,cd->nc', [feature_tensor, feature_tensor]) # removed .cpu()
 
             #self.knn = knn
-            scores, idx_k = similarity_matrix.topk(k=knn, dim=1)
             #self.proximity = torch.mean(scores_k,dim=1)
             #self.kNN_indices = idx_k
             labels_topk = torch.zeros_like(idx_k)"
OK;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"def evaluate_samples(self,p,model,forwarding='head',knn=100):
             ri, ci = assign_classes_hungarian(C)
             accuracy = accuracy_from_assignment(C,ri,ci)
             print('Accuracy: ',accuracy)
+        # Â§------------------------------------------------
 
             feature_tensor = torch.nn.functional.normalize(feature_tensor, dim = 1)
+
+            idx_list = []
+            for i in range(len(feature_tensor)):
+                feature = torch.unsqueeze(feature_tensor[i],dim=0)
+                similarities = torch.mm(feature,feature_tensor.t())
+                scores, idx_ = similarities.topk(k=knn, dim=1)
+                idx_list.append(idx_)
+            idx_k = torch.cat(idx_list)
+            #similarity_matrix = torch.einsum('nd,cd->nc', [feature_tensor, feature_tensor]) # removed .cpu()
 
             #self.knn = knn
+            #scores, idx_k = similarity_matrix.topk(k=knn, dim=1)
             #self.proximity = torch.mean(scores_k,dim=1)
             #self.kNN_indices = idx_k
             labels_topk = torch.zeros_like(idx_k)"
KO;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);
OK;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""afd3e0db"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = tv_res18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""dcd1c27d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=1000, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 1,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torchvision\n"",
+    ""\n"",
+    ""resnet18 = torchvision.models.resnet18()\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""c9ecd56d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=10, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 2,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch.nn as nn\n"",
+    ""resnet18.fc = nn.Linear(512,10)\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""511ae48e"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""demoBatch = torch.rand([4,3,32,32])\n"",
+    ""modelSample = resnet18(demoBatch)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""13b59406"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([4, 10])""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""modelSample.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""f545a7ad"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = singleSoftmaxUse""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""19283bac"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""soft = torch.nn.Softmax(dim = 1) # create function class instance <callable>""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""928045a9"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test = soft(modelSample) # softmax""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""70fb3bbe"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor(1.0000, grad_fn=<AddBackward0>)""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""sum(test[0])""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""0911bd73"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = matrixC""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""915c9184"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([128])""
+      ]
+     },
+     ""execution_count"": 12,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""dummy_features = torch.rand([105000,128])\n"",
+    ""dummy_features = torch.nn.functional.normalize(dummy_features, dim = 1)\n"",
+    ""single_feature = dummy_features[0]\n"",
+    ""single_feature.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 16,
+   ""id"": ""063902c3"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""105000""
+      ]
+     },
+     ""execution_count"": 16,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(dummy_features)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""ad491fdd"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 105000])""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""matmul = torch.mm(torch.unsqueeze(single_feature,dim=0),dummy_features.t())\n"",
+    ""matmul.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""4f6da2a1"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 100])""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores, idx_k = matmul.topk(k=100, dim=1)\n"",
+    ""idx_k.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 15,
+   ""id"": ""397dd28a"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor([[1.0000, 0.8648, 0.8608, 0.8602, 0.8599, 0.8583, 0.8579, 0.8547, 0.8541,\n"",
+       ""         0.8538, 0.8535, 0.8535, 0.8532, 0.8530, 0.8523, 0.8522, 0.8518, 0.8518,\n"",
+       ""         0.8514, 0.8513, 0.8507, 0.8501, 0.8499, 0.8495, 0.8494, 0.8491, 0.8486,\n"",
+       ""         0.8485, 0.8482, 0.8478, 0.8462, 0.8459, 0.8459, 0.8458, 0.8458, 0.8455,\n"",
+       ""         0.8451, 0.8449, 0.8449, 0.8448, 0.8448, 0.8446, 0.8446, 0.8443, 0.8442,\n"",
+       ""         0.8441, 0.8441, 0.8441, 0.8440, 0.8440, 0.8437, 0.8437, 0.8436, 0.8435,\n"",
+       ""         0.8433, 0.8433, 0.8432, 0.8432, 0.8432, 0.8431, 0.8430, 0.8430, 0.8429,\n"",
+       ""         0.8428, 0.8426, 0.8423, 0.8422, 0.8422, 0.8422, 0.8422, 0.8421, 0.8421,\n"",
+       ""         0.8421, 0.8420, 0.8420, 0.8418, 0.8418, 0.8417, 0.8415, 0.8415, 0.8414,\n"",
+       ""         0.8414, 0.8414, 0.8413, 0.8413, 0.8413, 0.8411, 0.8411, 0.8410, 0.8408,\n"",
+       ""         0.8408, 0.8408, 0.8407, 0.8407, 0.8407, 0.8407, 0.8406, 0.8405, 0.8405,\n"",
+       ""         0.8405]])""
+      ]
+     },
+     ""execution_count"": 15,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5b5e42ed"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python 3 (ipykernel)"",
+   ""language"": ""python"",
+   ""name"": ""python3""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.7.10""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);
OK;45;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""afd3e0db"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = tv_res18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""dcd1c27d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=1000, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 1,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torchvision\n"",
+    ""\n"",
+    ""resnet18 = torchvision.models.resnet18()\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""c9ecd56d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""ResNet(\n"",
+       ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
+       ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""  (relu): ReLU(inplace=True)\n"",
+       ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
+       ""  (layer1): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer2): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer3): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (layer4): Sequential(\n"",
+       ""    (0): BasicBlock(\n"",
+       ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (downsample): Sequential(\n"",
+       ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
+       ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      )\n"",
+       ""    )\n"",
+       ""    (1): BasicBlock(\n"",
+       ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""      (relu): ReLU(inplace=True)\n"",
+       ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
+       ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
+       ""    )\n"",
+       ""  )\n"",
+       ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
+       ""  (fc): Linear(in_features=512, out_features=10, bias=True)\n"",
+       "")""
+      ]
+     },
+     ""execution_count"": 2,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch.nn as nn\n"",
+    ""resnet18.fc = nn.Linear(512,10)\n"",
+    ""resnet18""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""511ae48e"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""demoBatch = torch.rand([4,3,32,32])\n"",
+    ""modelSample = resnet18(demoBatch)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""13b59406"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([4, 10])""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""modelSample.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""f545a7ad"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = singleSoftmaxUse""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""19283bac"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""soft = torch.nn.Softmax(dim = 1) # create function class instance <callable>""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""928045a9"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test = soft(modelSample) # softmax""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""70fb3bbe"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor(1.0000, grad_fn=<AddBackward0>)""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""sum(test[0])""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""0911bd73"",
+   ""metadata"": {},
+   ""source"": [
+    ""## @ref = matrixC""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""915c9184"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([128])""
+      ]
+     },
+     ""execution_count"": 12,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import torch\n"",
+    ""\n"",
+    ""dummy_features = torch.rand([105000,128])\n"",
+    ""dummy_features = torch.nn.functional.normalize(dummy_features, dim = 1)\n"",
+    ""single_feature = dummy_features[0]\n"",
+    ""single_feature.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 16,
+   ""id"": ""063902c3"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""105000""
+      ]
+     },
+     ""execution_count"": 16,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(dummy_features)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""ad491fdd"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 105000])""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""matmul = torch.mm(torch.unsqueeze(single_feature,dim=0),dummy_features.t())\n"",
+    ""matmul.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""4f6da2a1"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""torch.Size([1, 100])""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores, idx_k = matmul.topk(k=100, dim=1)\n"",
+    ""idx_k.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 15,
+   ""id"": ""397dd28a"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""tensor([[1.0000, 0.8648, 0.8608, 0.8602, 0.8599, 0.8583, 0.8579, 0.8547, 0.8541,\n"",
+       ""         0.8538, 0.8535, 0.8535, 0.8532, 0.8530, 0.8523, 0.8522, 0.8518, 0.8518,\n"",
+       ""         0.8514, 0.8513, 0.8507, 0.8501, 0.8499, 0.8495, 0.8494, 0.8491, 0.8486,\n"",
+       ""         0.8485, 0.8482, 0.8478, 0.8462, 0.8459, 0.8459, 0.8458, 0.8458, 0.8455,\n"",
+       ""         0.8451, 0.8449, 0.8449, 0.8448, 0.8448, 0.8446, 0.8446, 0.8443, 0.8442,\n"",
+       ""         0.8441, 0.8441, 0.8441, 0.8440, 0.8440, 0.8437, 0.8437, 0.8436, 0.8435,\n"",
+       ""         0.8433, 0.8433, 0.8432, 0.8432, 0.8432, 0.8431, 0.8430, 0.8430, 0.8429,\n"",
+       ""         0.8428, 0.8426, 0.8423, 0.8422, 0.8422, 0.8422, 0.8422, 0.8421, 0.8421,\n"",
+       ""         0.8421, 0.8420, 0.8420, 0.8418, 0.8418, 0.8417, 0.8415, 0.8415, 0.8414,\n"",
+       ""         0.8414, 0.8414, 0.8413, 0.8413, 0.8413, 0.8411, 0.8411, 0.8410, 0.8408,\n"",
+       ""         0.8408, 0.8408, 0.8407, 0.8407, 0.8407, 0.8407, 0.8406, 0.8405, 0.8405,\n"",
+       ""         0.8405]])""
+      ]
+     },
+     ""execution_count"": 15,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""scores""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5b5e42ed"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python 3 (ipykernel)"",
+   ""language"": ""python"",
+   ""name"": ""python3""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.7.10""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
