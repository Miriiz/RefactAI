Label;Page;Username;Repo;Commit;Bug;Code
KO;1;CorentinJ;Real-Time-Voice-Cloning;ded7b37234e229d9bde0a9a506f7c65605803731;"Low memory inference fix (#536)

* For low_mem, use spawned workers instead of forked workers (resolves #36)
Used implementation from @lilydjwg: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/36#issuecomment-529380190

* Different method of passing the seed for low_mem inference
Resolves #491, #529, #535";" from synthesizer.hparams import hparams
 from multiprocess.pool import Pool  # You're free to use either one
 #from multiprocessing import Pool   # 
 from synthesizer import audio
 from pathlib import Path
 from typing import Union, List
@@ -97,16 +98,16 @@ def synthesize_spectrograms(self, texts: List[str],
             # Low memory inference mode: load the model upon every request. The model has to be 
             # loaded in a separate process to be able to release GPU memory (a simple workaround 
             # to tensorflow's intricacies)
-            specs, alignments = Pool(1).starmap(Synthesizer._one_shot_synthesize_spectrograms, 
-                                                [(self.checkpoint_fpath, embeddings, texts)])[0]
     
         return (specs, alignments) if return_alignments else specs
 
     @staticmethod
-    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts):
         # Load the model and forward the inputs
         tf.compat.v1.reset_default_graph()
-        model = Tacotron2(checkpoint_fpath, hparams, seed=self._seed)
         specs, alignments = model.my_synthesize(embeddings, texts)
         
         # Detach the outputs (not doing so will cause the process to hang)"
OK;1;CorentinJ;Real-Time-Voice-Cloning;ded7b37234e229d9bde0a9a506f7c65605803731;"Low memory inference fix (#536)

* For low_mem, use spawned workers instead of forked workers (resolves #36)
Used implementation from @lilydjwg: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/36#issuecomment-529380190

* Different method of passing the seed for low_mem inference
Resolves #491, #529, #535";" from synthesizer.hparams import hparams
 from multiprocess.pool import Pool  # You're free to use either one
 #from multiprocessing import Pool   # 
+from multiprocess.context import SpawnContext
 from synthesizer import audio
 from pathlib import Path
 from typing import Union, List
@@ -97,16 +98,16 @@ def synthesize_spectrograms(self, texts: List[str],
             # Low memory inference mode: load the model upon every request. The model has to be 
             # loaded in a separate process to be able to release GPU memory (a simple workaround 
             # to tensorflow's intricacies)
+            specs, alignments = Pool(1, context=SpawnContext()).starmap(Synthesizer._one_shot_synthesize_spectrograms,
+                                                [(self.checkpoint_fpath, embeddings, texts, self._seed)])[0]
     
         return (specs, alignments) if return_alignments else specs
 
     @staticmethod
+    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts, seed):
         # Load the model and forward the inputs
         tf.compat.v1.reset_default_graph()
+        model = Tacotron2(checkpoint_fpath, hparams, seed=seed)
         specs, alignments = model.my_synthesize(embeddings, texts)
         
         # Detach the outputs (not doing so will cause the process to hang)"
KO;2;google;jax;7098088f4eb15cf750398889e4341dbc15cda1b3;"Add jax.config.jax_default_device to jax in-memory cache key

This fixes a case where we'd get a cache hit when evaluating a
primitive (e.g. jnp.ones) even if the default device was changed,
causing the default device to not take effect.

PiperOrigin-RevId: 454986939";"def _trace_context(self):
     the C++ JIT state, which is handled separately.""""""
     return (self.x64_enabled, self.jax_numpy_rank_promotion,
             self.jax_default_matmul_precision, self.jax_dynamic_shapes,
-            self.jax_numpy_dtype_promotion)
 
 class NoDefault: pass
 no_default = NoDefault()"
OK;2;google;jax;7098088f4eb15cf750398889e4341dbc15cda1b3;"Add jax.config.jax_default_device to jax in-memory cache key

This fixes a case where we'd get a cache hit when evaluating a
primitive (e.g. jnp.ones) even if the default device was changed,
causing the default device to not take effect.

PiperOrigin-RevId: 454986939";"def _trace_context(self):
     the C++ JIT state, which is handled separately.""""""
     return (self.x64_enabled, self.jax_numpy_rank_promotion,
             self.jax_default_matmul_precision, self.jax_dynamic_shapes,
+            self.jax_numpy_dtype_promotion, self.jax_default_device)
 
 class NoDefault: pass
 no_default = NoDefault()"
KO;2;google;jax;7098088f4eb15cf750398889e4341dbc15cda1b3;"Add jax.config.jax_default_device to jax in-memory cache key

This fixes a case where we'd get a cache hit when evaluating a
primitive (e.g. jnp.ones) even if the default device was changed,
causing the default device to not take effect.

PiperOrigin-RevId: 454986939";"def test_default_backend(self):
     first_local_device = api.local_devices()[0]
     self.assertEqual(first_local_device.platform, api.default_backend())
 
   def test_dunder_jax_array(self):
     # https://github.com/google/jax/pull/4725
 "
OK;2;google;jax;7098088f4eb15cf750398889e4341dbc15cda1b3;"Add jax.config.jax_default_device to jax in-memory cache key

This fixes a case where we'd get a cache hit when evaluating a
primitive (e.g. jnp.ones) even if the default device was changed,
causing the default device to not take effect.

PiperOrigin-RevId: 454986939";"def test_default_backend(self):
     first_local_device = api.local_devices()[0]
     self.assertEqual(first_local_device.platform, api.default_backend())
 
+  @jtu.skip_on_devices(""cpu"")
+  def test_default_device(self):
+    system_default_device = jnp.zeros(2).device()
+    test_device = jax.devices(""cpu"")[-1]
+
+    # Sanity check creating array using system default device
+    self.assertEqual(jnp.ones(1).device(), system_default_device)
+
+    # Create array with default_device set
+    with jax.default_device(test_device):
+      # Hits cached primitive path
+      self.assertEqual(jnp.ones(1).device(), test_device)
+      # Uncached
+      self.assertEqual(jnp.zeros((1, 2)).device(), test_device)
+
+    # Test that we can reset to system default device
+    self.assertEqual(jnp.ones(1).device(), system_default_device)
+
   def test_dunder_jax_array(self):
     # https://github.com/google/jax/pull/4725
 "
KO;2;facebookresearch;fairseq;eb2d7862c29990e5be35ee227a6952ae21d621a1;"fix ema memory leak (#3384)

Summary:
fixes memory leak in ema module by making sure the update happens in no_grad regime

X-link: https://github.com/fairinternal/fairseq-py/pull/3384

Reviewed By: arbabu123

Differential Revision: D36352890

Pulled By: alexeib

fbshipit-source-id: 0f3575ac356a13483e00ed431375b2c798621a3a";"def _step_internal(self, new_model):
         ema_params = (
             self.fp32_params if self.config.ema_fp32 else self.model.state_dict()
         )
-        for key, param in new_model.state_dict().items():
             if isinstance(param, dict):
                 continue
             try:
@@ -107,6 +107,7 @@ def _step_internal(self, new_model):
                 ema_param = (
                     param.float().clone() if param.ndim == 1 else copy.deepcopy(param)
                 )
 
             if param.shape != ema_param.shape:
                 raise ValueError(
@@ -118,15 +119,21 @@ def _step_internal(self, new_model):
                 # Do not decay a model.version pytorch param
                 continue
 
-            if key in self.skip_keys:
-                ema_param = param.to(dtype=ema_param.dtype).clone()
-                ema_params[key].copy_(ema_param)
             else:
                 ema_param.mul_(decay)
-                ema_param.add_(param.to(dtype=ema_param.dtype), alpha=1 - decay)
             ema_state_dict[key] = ema_param
         self.restore(ema_state_dict, build_fp32_params=False)
 
     def step(self, new_model):
         self._step_internal(new_model)
 "
OK;2;facebookresearch;fairseq;eb2d7862c29990e5be35ee227a6952ae21d621a1;"fix ema memory leak (#3384)

Summary:
fixes memory leak in ema module by making sure the update happens in no_grad regime

X-link: https://github.com/fairinternal/fairseq-py/pull/3384

Reviewed By: arbabu123

Differential Revision: D36352890

Pulled By: alexeib

fbshipit-source-id: 0f3575ac356a13483e00ed431375b2c798621a3a";"def _step_internal(self, new_model):
         ema_params = (
             self.fp32_params if self.config.ema_fp32 else self.model.state_dict()
         )
+        for key, param in new_model.named_parameters():
             if isinstance(param, dict):
                 continue
             try:
@@ -107,6 +107,7 @@ def _step_internal(self, new_model):
                 ema_param = (
                     param.float().clone() if param.ndim == 1 else copy.deepcopy(param)
                 )
+                ema_params[key] = ema_param
 
             if param.shape != ema_param.shape:
                 raise ValueError(
@@ -118,15 +119,21 @@ def _step_internal(self, new_model):
                 # Do not decay a model.version pytorch param
                 continue
 
+            if key in self.skip_keys or not param.requires_grad:
+                ema_params[key].copy_(param.to(dtype=ema_param.dtype).data)
+                ema_param = ema_params[key]
             else:
                 ema_param.mul_(decay)
+                ema_param.add_(param.data.to(dtype=ema_param.dtype), alpha=1 - decay)
+
             ema_state_dict[key] = ema_param
+
+        for key, param in new_model.named_buffers():
+            ema_state_dict[key] = param
+
         self.restore(ema_state_dict, build_fp32_params=False)
 
+    @torch.no_grad()
     def step(self, new_model):
         self._step_internal(new_model)
 "
KO;3;Z4nzu;hackingtool;6db6a615edf084d248b7d266fda4321980cd0ca5;"Do not use bare `except:`

Do not use bare `except:`, it also catches unexpected events like memory errors, interrupts, system exit, and so on.  Prefer `except Exception:`.  If you're sure what you're doing, be explicit and write `except BaseException:`.";" 
 
 def clear_screen():
-    if system() == ""Linux"":
-        os.system(""clear"")
-    if system() == ""Windows"":
-        os.system(""cls"")
 
 
 def validate_input(ip, val_range):
     try:
         ip = int(ip)
         if ip in val_range:
             return ip
-        else:
-            return None
-    except:
-        return None
 
 
 class HackingTool(object):
@@ -46,8 +43,7 @@ class HackingTool(object):
 
     def __init__(self, options = None, installable: bool = True,
                  runnable: bool = True):
-        if options is None:
-            options = []
         if isinstance(options, list):
             self.OPTIONS = []
             if installable:
@@ -176,7 +172,7 @@ def show_options(self, parent = None):
         except (TypeError, ValueError):
             print(""Please enter a valid option"")
             input(""\n\nPress ENTER to continue:"")
-        except Exception as e:
             print_exc()
             input(""\n\nPress ENTER to continue:"")
         return self.show_options(parent = parent)"
OK;3;Z4nzu;hackingtool;6db6a615edf084d248b7d266fda4321980cd0ca5;"Do not use bare `except:`

Do not use bare `except:`, it also catches unexpected events like memory errors, interrupts, system exit, and so on.  Prefer `except Exception:`.  If you're sure what you're doing, be explicit and write `except BaseException:`.";" 
 
 def clear_screen():
+    os.system(""cls"" if system() == ""Windows"" else ""clear"")
 
 
 def validate_input(ip, val_range):
+    val_range = val_range or []
     try:
         ip = int(ip)
         if ip in val_range:
             return ip
+    except Exception:
+        pass
+    return None
 
 
 class HackingTool(object):
@@ -46,8 +43,7 @@ class HackingTool(object):
 
     def __init__(self, options = None, installable: bool = True,
                  runnable: bool = True):
+        options  = options or []
         if isinstance(options, list):
             self.OPTIONS = []
             if installable:
@@ -176,7 +172,7 @@ def show_options(self, parent = None):
         except (TypeError, ValueError):
             print(""Please enter a valid option"")
             input(""\n\nPress ENTER to continue:"")
+        except Exception:
             print_exc()
             input(""\n\nPress ENTER to continue:"")
         return self.show_options(parent = parent)"
KO;5;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"def assemble(self):
         if is_win:
             # Set checksum to appease antiviral software. Also set build timestamp to current time to increase entropy
             # (but honor SOURCE_DATE_EPOCH environment variable for reproducible builds).
             build_timestamp = int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))
-            winutils.fixup_exe_headers(build_name, build_timestamp)
         elif is_darwin:
             # If the version of macOS SDK used to build bootloader exceeds that of macOS SDK used to built Python
             # library (and, by extension, bundled Tcl/Tk libraries), force the version declared by the frozen executable"
OK;5;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"def assemble(self):
         if is_win:
             # Set checksum to appease antiviral software. Also set build timestamp to current time to increase entropy
             # (but honor SOURCE_DATE_EPOCH environment variable for reproducible builds).
+            logger.info(""Fixing EXE headers"")
             build_timestamp = int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))
+            winutils.set_exe_build_timestamp(build_name, build_timestamp)
+            winutils.update_exe_pe_checksum(build_name)
         elif is_darwin:
             # If the version of macOS SDK used to build bootloader exceeds that of macOS SDK used to built Python
             # library (and, by extension, bundled Tcl/Tk libraries), force the version declared by the frozen executable"
KO;5;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"def convert_dll_name_to_str(dll_name):
         return dll_name
 
 
-def fixup_exe_headers(exe_path, timestamp=None):
     """"""
-    Set executable's checksum and build timestamp in its headers.
-
-    This optional checksum is supposed to protect the executable against corruption but some anti-viral software have
-    taken to flagging anything without it set correctly as malware. See issue #5579.
     """"""
     import pefile
-    pe = pefile.PE(exe_path, fast_load=False)  # full load because we need all headers
-    # Set build timestamp.
-    # See: https://0xc0decafe.com/malware-analyst-guide-to-pe-timestamps
-    if timestamp is not None:
         timestamp = int(timestamp)
         # Set timestamp field in FILE_HEADER
         pe.FILE_HEADER.TimeDateStamp = timestamp
@@ -169,7 +170,189 @@ def fixup_exe_headers(exe_path, timestamp=None):
         for debug_entry in debug_entries:
             if debug_entry.struct.TimeDateStamp:
                 debug_entry.struct.TimeDateStamp = timestamp
-    # Set PE checksum
-    pe.OPTIONAL_HEADER.CheckSum = pe.generate_checksum()
-    pe.close()
-    pe.write(exe_path)"
OK;5;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"def convert_dll_name_to_str(dll_name):
         return dll_name
 
 
+def set_exe_build_timestamp(exe_path, timestamp):
     """"""
+    Modifies the executable's build timestamp by updating values in the corresponding PE headers.
     """"""
     import pefile
+
+    with pefile.PE(exe_path, fast_load=True) as pe:
+        # Manually perform a full load. We need it to load all headers, but specifying it in the constructor triggers
+        # byte statistics gathering that takes forever with large files. So we try to go around that...
+        pe.full_load()
+
+        # Set build timestamp.
+        # See: https://0xc0decafe.com/malware-analyst-guide-to-pe-timestamps
         timestamp = int(timestamp)
         # Set timestamp field in FILE_HEADER
         pe.FILE_HEADER.TimeDateStamp = timestamp
@@ -169,7 +170,189 @@ def fixup_exe_headers(exe_path, timestamp=None):
         for debug_entry in debug_entries:
             if debug_entry.struct.TimeDateStamp:
                 debug_entry.struct.TimeDateStamp = timestamp
+
+        # Generate updated EXE data
+        data = pe.write()
+
+    # Rewrite the exe
+    with open(exe_path, 'wb') as fp:
+        fp.write(data)
+
+
+def update_exe_pe_checksum(exe_path):
+    """"""
+    Compute the executable's PE checksum, and write it to PE headers.
+
+    This optional checksum is supposed to protect the executable against corruption but some anti-viral software have
+    taken to flagging anything without it set correctly as malware. See issue #5579.
+    """"""
+    import pefile
+
+    # Compute checksum using our equivalent of the MapFileAndCheckSumW - for large files, it is significantly faster
+    # than pure-pyton pefile.PE.generate_checksum(). However, it requires the file to be on disk (i.e., cannot operate
+    # on a memory buffer).
+    try:
+        checksum = compute_exe_pe_checksum(exe_path)
+    except Exception as e:
+        raise RuntimeError(""Failed to compute PE checksum!"") from e
+
+    # Update the checksum
+    with pefile.PE(exe_path, fast_load=True) as pe:
+        pe.OPTIONAL_HEADER.CheckSum = checksum
+
+        # Generate updated EXE data
+        data = pe.write()
+
+    # Rewrite the exe
+    with open(exe_path, 'wb') as fp:
+        fp.write(data)
+
+
+def compute_exe_pe_checksum(exe_path):
+    """"""
+    This is a replacement for the MapFileAndCheckSumW function. As noted in MSDN documentation, the Microsoft's
+    implementation of MapFileAndCheckSumW internally calls its ASCII variant (MapFileAndCheckSumA), and therefore
+    cannot handle paths that contain characters that are not representable in the current code page.
+    See: https://docs.microsoft.com/en-us/windows/win32/api/imagehlp/nf-imagehlp-mapfileandchecksumw
+
+    This function is based on Wine's implementation of MapFileAndCheckSumW, and due to being based entirely on
+    the pure widechar-API functions, it is not limited by the current code page.
+    """"""
+    # ctypes bindings for relevant win32 API functions
+    import ctypes
+    from ctypes import windll, wintypes
+
+    INVALID_HANDLE = wintypes.HANDLE(-1).value
+
+    GetLastError = ctypes.windll.kernel32.GetLastError
+    GetLastError.argtypes = ()
+    GetLastError.restype = wintypes.DWORD
+
+    CloseHandle = windll.kernel32.CloseHandle
+    CloseHandle.argtypes = (
+        wintypes.HANDLE,  # hObject
+    )
+    CloseHandle.restype = wintypes.BOOL
+
+    CreateFileW = windll.kernel32.CreateFileW
+    CreateFileW.argtypes = (
+        wintypes.LPCWSTR,  # lpFileName
+        wintypes.DWORD,  # dwDesiredAccess
+        wintypes.DWORD,  # dwShareMode
+        wintypes.LPVOID,  # lpSecurityAttributes
+        wintypes.DWORD,  # dwCreationDisposition
+        wintypes.DWORD,  # dwFlagsAndAttributes
+        wintypes.HANDLE,  # hTemplateFile
+    )
+    CreateFileW.restype = wintypes.HANDLE
+
+    CreateFileMappingW = windll.kernel32.CreateFileMappingW
+    CreateFileMappingW.argtypes = (
+        wintypes.HANDLE,  # hFile
+        wintypes.LPVOID,  # lpSecurityAttributes
+        wintypes.DWORD,  # flProtect
+        wintypes.DWORD,  # dwMaximumSizeHigh
+        wintypes.DWORD,  # dwMaximumSizeLow
+        wintypes.LPCWSTR,  # lpName
+    )
+    CreateFileMappingW.restype = wintypes.HANDLE
+
+    MapViewOfFile = windll.kernel32.MapViewOfFile
+    MapViewOfFile.argtypes = (
+        wintypes.HANDLE,  # hFileMappingObject
+        wintypes.DWORD,  # dwDesiredAccess
+        wintypes.DWORD,  # dwFileOffsetHigh
+        wintypes.DWORD,  # dwFileOffsetLow
+        wintypes.DWORD,  # dwNumberOfBytesToMap
+    )
+    MapViewOfFile.restype = wintypes.LPVOID
+
+    UnmapViewOfFile = windll.kernel32.UnmapViewOfFile
+    UnmapViewOfFile.argtypes = (
+        wintypes.LPCVOID,  # lpBaseAddress
+    )
+    UnmapViewOfFile.restype = wintypes.BOOL
+
+    GetFileSizeEx = windll.kernel32.GetFileSizeEx
+    GetFileSizeEx.argtypes = (
+        wintypes.HANDLE,  # hFile
+        wintypes.PLARGE_INTEGER,  # lpFileSize
+    )
+
+    CheckSumMappedFile = windll.imagehlp.CheckSumMappedFile
+    CheckSumMappedFile.argtypes = (
+        wintypes.LPVOID,  # BaseAddress
+        wintypes.DWORD,  # FileLength
+        wintypes.PDWORD,  # HeaderSum
+        wintypes.PDWORD,  # CheckSum
+    )
+    CheckSumMappedFile.restype = wintypes.LPVOID
+
+    # Open file
+    hFile = CreateFileW(
+        ctypes.c_wchar_p(exe_path),
+        0x80000000,  # dwDesiredAccess = GENERIC_READ
+        0x00000001 | 0x00000002,  # dwShareMode = FILE_SHARE_READ | FILE_SHARE_WRITE,
+        None,  # lpSecurityAttributes = NULL
+        3,  # dwCreationDisposition = OPEN_EXISTING
+        0x80,  # dwFlagsAndAttributes = FILE_ATTRIBUTE_NORMAL
+        None  # hTemplateFile = NULL
+    )
+    if hFile == INVALID_HANDLE:
+        err = GetLastError()
+        raise RuntimeError(f""Failed to open file {exe_path}! Error code: {err}"")
+
+    # Query file size
+    fileLength = wintypes.LARGE_INTEGER(0)
+    if GetFileSizeEx(hFile, fileLength) == 0:
+        err = GetLastError()
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to query file size file! Error code: {err}"")
+    fileLength = fileLength.value
+    if fileLength > (2**32 - 1):
+        raise RuntimeError(""Executable size exceeds maximum allowed executable size on Windows (4 GiB)!"")
+
+    # Map the file
+    hMapping = CreateFileMappingW(
+        hFile,
+        None,  # lpFileMappingAttributes = NULL
+        0x02,  # flProtect = PAGE_READONLY
+        0,  # dwMaximumSizeHigh = 0
+        0,  # dwMaximumSizeLow = 0
+        None  # lpName = NULL
+    )
+    if not hMapping:
+        err = GetLastError()
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to map file! Error code: {err}"")
+
+    # Create map view
+    baseAddress = MapViewOfFile(
+        hMapping,
+        4,  # dwDesiredAccess = FILE_MAP_READ
+        0,  # dwFileOffsetHigh = 0
+        0,  # dwFileOffsetLow = 0
+        0  # dwNumberOfBytesToMap = 0
+    )
+    if baseAddress == 0:
+        err = GetLastError()
+        CloseHandle(hMapping)
+        CloseHandle(hFile)
+        raise RuntimeError(f""Failed to create map view! Error code: {err}"")
+
+    # Finally, compute the checksum
+    headerSum = wintypes.DWORD(0)
+    checkSum = wintypes.DWORD(0)
+    ret = CheckSumMappedFile(baseAddress, fileLength, ctypes.byref(headerSum), ctypes.byref(checkSum))
+    if ret is None:
+        err = GetLastError()
+
+    # Cleanup
+    UnmapViewOfFile(baseAddress)
+    CloseHandle(hMapping)
+    CloseHandle(hFile)
+
+    if ret is None:
+        raise RuntimeError(f""CheckSumMappedFile failed! Error code: {err}"")
+
+    return checkSum.value"
KO;5;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";
OK;5;pyinstaller;pyinstaller;41483cb9e6d5086416c8fea6ad6781782c091c60;"winutils: optimize PE headers fixup

Attempt to optimize PE headers fix-up from both time- and memory-
intensity perspective.

First, avoid specifying `fast_load=False` in `pefile.PE` constructor,
because that triggers the bytes statistics collection
https://github.com/erocarrera/pefile/blob/v2022.5.30/pefile.py#L2862-L2876
which takes a long time for large files. Instead, we can obtain
full headers (required for build timestamp modification) by
calling `pe.full_load()` ourselves.

Second, use (an equivalent of) `MapFileAndCheckSumW` to compute
the PE checksum. For large files, it is orders of magnitude
faster than its pure-python `pefile.PE.generate_checksum`
counterpart.

The downside is that `MapFileAndCheckSumW` requires an on-disk
file as opposed to a memory buffer, so we need to split the
PE headers fixup into two separate steps, with each modifying
the corresponding PE headers and (re)writing the whole file.
Even so, this brings the fix-up process for a 700MB executable
down to seconds instead of minutes.

In addition, as noted on MSDN, `MapFileAndCheckSumW` internally
calls its ASCII variant (`MapFileAndCheckSumA`), so it cannot
handle file paths that contain characters that are not representable
in the current code page. Therefore, we implement our own equivalent
using `ctypes` and pure widechar-based win32 API functions.";"+(Windows) Optimize EXE PE headers fix-up process in an attempt to reduce
+the processing time and the memory footprint with large onefile builds."
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";+Add a line showing the heap size over time to the memory plot in the html-based reporters (which already showed the resident size over time).
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" from ._memray import Destination
 from ._memray import FileDestination
 from ._memray import FileReader
-from ._memray import MemoryRecord
 from ._memray import SocketDestination
 from ._memray import SocketReader
 from ._memray import Tracker
@@ -16,7 +16,7 @@
 __all__ = [
     ""AllocationRecord"",
     ""AllocatorType"",
-    ""MemoryRecord"",
     ""dump_all_records"",
     ""start_thread_trace"",
     ""Tracker"","
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" from ._memray import Destination
 from ._memray import FileDestination
 from ._memray import FileReader
+from ._memray import MemorySnapshot
 from ._memray import SocketDestination
 from ._memray import SocketReader
 from ._memray import Tracker
@@ -16,7 +16,7 @@
 __all__ = [
     ""AllocationRecord"",
     ""AllocatorType"",
+    ""MemorySnapshot"",
     ""dump_all_records"",
     ""start_thread_trace"",
     ""Tracker"","
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"from memray._metadata import Metadata as Metadata
 from ._memray import AllocationRecord as AllocationRecord
 from ._memray import AllocatorType as AllocatorType
 from ._memray import FileReader as FileReader
-from ._memray import MemoryRecord as MemoryRecord
 from ._memray import SocketReader as SocketReader
 from ._memray import Tracker as Tracker
 from ._memray import dump_all_records as dump_all_records"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"from memray._metadata import Metadata as Metadata
 from ._memray import AllocationRecord as AllocationRecord
 from ._memray import AllocatorType as AllocatorType
 from ._memray import FileReader as FileReader
+from ._memray import MemorySnapshot as MemorySnapshot
 from ._memray import SocketReader as SocketReader
 from ._memray import Tracker as Tracker
 from ._memray import dump_all_records as dump_all_records"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"from . import Destination
 
 PythonStackElement = Tuple[str, str, int]
 NativeStackElement = Tuple[str, str, int]
-MemoryRecord = NamedTuple(""MemoryRecord"", [(""time"", int), (""rss"", int)])
 
 def set_log_level(level: int) -> None: ...
 
@@ -95,7 +97,7 @@ class FileReader:
     def get_all_allocation_records_aggregated(
         self, merge_threads: bool
     ) -> Iterable[AllocationRecord]: ...
-    def get_memory_records(self) -> Iterable[MemoryRecord]: ...
     def __enter__(self) -> Any: ...
     def __exit__(
         self,"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"from . import Destination
 
 PythonStackElement = Tuple[str, str, int]
 NativeStackElement = Tuple[str, str, int]
+MemorySnapshot = NamedTuple(
+    ""MemorySnapshot"", [(""time"", int), (""rss"", int), (""heap"", int)]
+)
 
 def set_log_level(level: int) -> None: ...
 
@@ -95,7 +97,7 @@ class FileReader:
     def get_all_allocation_records_aggregated(
         self, merge_threads: bool
     ) -> Iterable[AllocationRecord]: ...
+    def get_memory_snapshots(self) -> Iterable[MemorySnapshot]: ...
     def __enter__(self) -> Any: ...
     def __exit__(
         self,"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"from _memray.record_reader cimport RecordReader
 from _memray.record_reader cimport RecordResult
 from _memray.record_writer cimport RecordWriter
 from _memray.records cimport Allocation as _Allocation
-from _memray.records cimport MemoryRecord as _MemoryRecord
 from _memray.sink cimport FileSink
 from _memray.sink cimport NullSink
 from _memray.sink cimport Sink
@@ -213,7 +214,7 @@ cdef class AllocationRecord:
                 f""allocations={self.n_allocations}>"")
 
 
-MemoryRecord = collections.namedtuple(""MemoryRecord"", ""time rss"")
 
 cdef class Tracker:
     """"""Context manager for tracking memory allocations in a Python script.
@@ -436,7 +437,7 @@ cdef class FileReader:
     cdef cppstring _path
 
     cdef object _file
-    cdef vector[_MemoryRecord] _memory_records
     cdef HighWatermark _high_watermark
     cdef object _header
     cdef bool _report_progress
@@ -450,7 +451,7 @@ cdef class FileReader:
         self._path = ""/proc/self/fd/"" + str(self._file.fileno())
         self._report_progress = report_progress
 
-        # Initial pass to populate _header, _high_watermark, and _memory_records.
         cdef shared_ptr[RecordReader] reader_sp = make_shared[RecordReader](
             unique_ptr[FileSource](new FileSource(self._path)),
             False
@@ -460,10 +461,10 @@ cdef class FileReader:
         self._header = reader.getHeader()
         stats = self._header[""stats""]
 
-        n_memory_records_approx = 2048
         if 0 < stats[""start_time""] < stats[""end_time""]:
-            n_memory_records_approx = (stats[""end_time""] - stats[""start_time""]) / 10
-        self._memory_records.reserve(n_memory_records_approx)
 
         cdef object total = stats['n_allocations'] or None
         cdef HighWatermarkFinder finder
@@ -473,6 +474,7 @@ cdef class FileReader:
             total=total,
             report_progress=self._report_progress
         )
         with progress_indicator:
             while True:
                 PyErr_CheckSignals()
@@ -481,7 +483,14 @@ cdef class FileReader:
                     finder.processAllocation(reader.getLatestAllocation())
                     progress_indicator.update(1)
                 elif ret == RecordResult.RecordResultMemoryRecord:
-                    self._memory_records.push_back(reader.getLatestMemoryRecord())
                 else:
                     break
         self._high_watermark = finder.getHighWatermark()
@@ -596,9 +605,9 @@ cdef class FileReader:
 
         reader.close()
 
-    def get_memory_records(self):
-        for record in self._memory_records:
-            yield MemoryRecord(record.ms_since_epoch, record.rss)
 
     @property
     def metadata(self):"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"from _memray.record_reader cimport RecordReader
 from _memray.record_reader cimport RecordResult
 from _memray.record_writer cimport RecordWriter
 from _memray.records cimport Allocation as _Allocation
+from _memray.records cimport MemoryRecord
+from _memray.records cimport MemorySnapshot as _MemorySnapshot
 from _memray.sink cimport FileSink
 from _memray.sink cimport NullSink
 from _memray.sink cimport Sink
@@ -213,7 +214,7 @@ cdef class AllocationRecord:
                 f""allocations={self.n_allocations}>"")
 
 
+MemorySnapshot = collections.namedtuple(""MemorySnapshot"", ""time rss heap"")
 
 cdef class Tracker:
     """"""Context manager for tracking memory allocations in a Python script.
@@ -436,7 +437,7 @@ cdef class FileReader:
     cdef cppstring _path
 
     cdef object _file
+    cdef vector[_MemorySnapshot] _memory_snapshots
     cdef HighWatermark _high_watermark
     cdef object _header
     cdef bool _report_progress
@@ -450,7 +451,7 @@ cdef class FileReader:
         self._path = ""/proc/self/fd/"" + str(self._file.fileno())
         self._report_progress = report_progress
 
+        # Initial pass to populate _header, _high_watermark, and _memory_snapshots.
         cdef shared_ptr[RecordReader] reader_sp = make_shared[RecordReader](
             unique_ptr[FileSource](new FileSource(self._path)),
             False
@@ -460,10 +461,10 @@ cdef class FileReader:
         self._header = reader.getHeader()
         stats = self._header[""stats""]
 
+        n_memory_snapshots_approx = 2048
         if 0 < stats[""start_time""] < stats[""end_time""]:
+            n_memory_snapshots_approx = (stats[""end_time""] - stats[""start_time""]) / 10
+        self._memory_snapshots.reserve(n_memory_snapshots_approx)
 
         cdef object total = stats['n_allocations'] or None
         cdef HighWatermarkFinder finder
@@ -473,6 +474,7 @@ cdef class FileReader:
             total=total,
             report_progress=self._report_progress
         )
+        cdef MemoryRecord memory_record
         with progress_indicator:
             while True:
                 PyErr_CheckSignals()
@@ -481,7 +483,14 @@ cdef class FileReader:
                     finder.processAllocation(reader.getLatestAllocation())
                     progress_indicator.update(1)
                 elif ret == RecordResult.RecordResultMemoryRecord:
+                    memory_record = reader.getLatestMemoryRecord()
+                    self._memory_snapshots.push_back(
+                        _MemorySnapshot(
+                            memory_record.ms_since_epoch,
+                            memory_record.rss,
+                            finder.getCurrentWatermark(),
+                        )
+                    )
                 else:
                     break
         self._high_watermark = finder.getHighWatermark()
@@ -596,9 +605,9 @@ cdef class FileReader:
 
         reader.close()
 
+    def get_memory_snapshots(self):
+        for record in self._memory_snapshots:
+            yield MemorySnapshot(record.ms_since_epoch, record.rss, record.heap)
 
     @property
     def metadata(self):"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"struct MemoryRecord
     size_t rss;
 };
 
 struct AllocationRecord
 {
     uintptr_t address;"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"struct MemoryRecord
     size_t rss;
 };
 
+struct MemorySnapshot
+{
+    unsigned long int ms_since_epoch;
+    size_t rss;
+    size_t heap;
+};
+
 struct AllocationRecord
 {
     uintptr_t address;"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"cdef extern from ""records.h"" namespace ""memray::tracking_api"":
    struct MemoryRecord:
        unsigned long int ms_since_epoch
        size_t rss"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"cdef extern from ""records.h"" namespace ""memray::tracking_api"":
    struct MemoryRecord:
        unsigned long int ms_since_epoch
        size_t rss
+
+   struct MemorySnapshot:
+       unsigned long int ms_since_epoch
+       size_t rss
+       size_t heap
+"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"HighWatermarkFinder::getHighWatermark() const noexcept
     return d_last_high_water_mark;
 }
 
 PyObject*
 Py_ListFromSnapshotAllocationRecords(const reduced_snapshot_map_t& stack_to_allocation)
 {"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"HighWatermarkFinder::getHighWatermark() const noexcept
     return d_last_high_water_mark;
 }
 
+size_t
+HighWatermarkFinder::getCurrentWatermark() const noexcept
+{
+    return d_current_memory;
+}
+
 PyObject*
 Py_ListFromSnapshotAllocationRecords(const reduced_snapshot_map_t& stack_to_allocation)
 {"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"class HighWatermarkFinder
     HighWatermarkFinder() = default;
     void processAllocation(const Allocation& allocation);
     HighWatermark getHighWatermark() const noexcept;
 
   private:
     HighWatermarkFinder(const HighWatermarkFinder&) = delete;"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"class HighWatermarkFinder
     HighWatermarkFinder() = default;
     void processAllocation(const Allocation& allocation);
     HighWatermark getHighWatermark() const noexcept;
+    size_t getCurrentWatermark() const noexcept;
 
   private:
     HighWatermarkFinder(const HighWatermarkFinder&) = delete;"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"cdef extern from ""snapshot.h"" namespace ""memray::api"":
     cdef cppclass HighWatermarkFinder:
         void processAllocation(const Allocation&) except+
         HighWatermark getHighWatermark()
 
     cdef cppclass reduced_snapshot_map_t:
         pass"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"cdef extern from ""snapshot.h"" namespace ""memray::api"":
     cdef cppclass HighWatermarkFinder:
         void processAllocation(const Allocation&) except+
         HighWatermark getHighWatermark()
+        size_t getCurrentWatermark()
 
     cdef cppclass reduced_snapshot_map_t:
         pass"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" 
 from memray import AllocationRecord
 from memray import FileReader
-from memray import MemoryRecord
 from memray._errors import MemrayCommandError
 from memray.reporters import BaseReporter
 
@@ -23,7 +23,7 @@ def __call__(
         self,
         allocations: Iterable[AllocationRecord],
         *,
-        memory_records: Iterable[MemoryRecord],
         native_traces: bool,
     ) -> BaseReporter:
         ...
@@ -82,7 +82,7 @@ def write_report(
                 snapshot = reader.get_high_watermark_allocation_records(
                     merge_threads=merge_threads if merge_threads is not None else True
                 )
-            memory_records = tuple(reader.get_memory_records())
             reporter = self.reporter_factory(
                 snapshot,
                 memory_records=memory_records,"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" 
 from memray import AllocationRecord
 from memray import FileReader
+from memray import MemorySnapshot
 from memray._errors import MemrayCommandError
 from memray.reporters import BaseReporter
 
@@ -23,7 +23,7 @@ def __call__(
         self,
         allocations: Iterable[AllocationRecord],
         *,
+        memory_records: Iterable[MemorySnapshot],
         native_traces: bool,
     ) -> BaseReporter:
         ...
@@ -82,7 +82,7 @@ def write_report(
                 snapshot = reader.get_high_watermark_allocation_records(
                     merge_threads=merge_threads if merge_threads is not None else True
                 )
+            memory_records = tuple(reader.get_memory_snapshots())
             reporter = self.reporter_factory(
                 snapshot,
                 memory_records=memory_records,"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" import _ from ""lodash"";
 
 export function initMemoryGraph(memory_records) {
-  const x = memory_records.map((a) => new Date(a[0]));
-  const y = memory_records.map((a) => a[1]);
 
-  var trace = {
-    x,
-    y,
     mode: ""lines"",
   };
 
-  var data = [trace];
 
   var layout = {
     xaxis: {
@@ -20,7 +29,7 @@ export function initMemoryGraph(memory_records) {
     },
     yaxis: {
       title: {
-        text: ""Resident Size"",
       },
       tickformat: "".4~s"",
       exponentformat: ""B"",
@@ -43,6 +52,7 @@ export function initMemoryGraph(memory_records) {
       exponentformat: ""B"",
       ticksuffix: ""B"",
     },
   };
   var config = {
     responsive: true,"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" import _ from ""lodash"";
 
 export function initMemoryGraph(memory_records) {
+  const time = memory_records.map((a) => new Date(a[0]));
+  const resident_size = memory_records.map((a) => a[1]);
+  const heap_size = memory_records.map((a) => a[2]);
 
+  var resident_size_plot = {
+    x: time,
+    y: resident_size,
     mode: ""lines"",
+    name: ""Resident size"",
   };
 
+  var heap_size_plot = {
+    x: time,
+    y: heap_size,
+    mode: ""lines"",
+    name: ""Heap size"",
+  };
+
+  var data = [resident_size_plot, heap_size_plot];
 
   var layout = {
     xaxis: {
@@ -20,7 +29,7 @@ export function initMemoryGraph(memory_records) {
     },
     yaxis: {
       title: {
+        text: ""Memory Size"",
       },
       tickformat: "".4~s"",
       exponentformat: ""B"",
@@ -43,6 +52,7 @@ export function initMemoryGraph(memory_records) {
       exponentformat: ""B"",
       ticksuffix: ""B"",
     },
+    showlegend: false,
   };
   var config = {
     responsive: true,"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" from typing import TextIO
 
 from memray import AllocationRecord
-from memray import MemoryRecord
 from memray import Metadata
 from memray.reporters.frame_tools import StackFrame
 from memray.reporters.frame_tools import is_cpython_internal
@@ -58,7 +58,7 @@ def __init__(
         self,
         data: Dict[str, Any],
         *,
-        memory_records: Iterable[MemoryRecord],
     ) -> None:
         super().__init__()
         self.data = data
@@ -69,7 +69,7 @@ def from_snapshot(
         cls,
         allocations: Iterator[AllocationRecord],
         *,
-        memory_records: Iterable[MemoryRecord],
         native_traces: bool,
     ) -> ""FlameGraphReporter"":
         data: Dict[str, Any] = {"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" from typing import TextIO
 
 from memray import AllocationRecord
+from memray import MemorySnapshot
 from memray import Metadata
 from memray.reporters.frame_tools import StackFrame
 from memray.reporters.frame_tools import is_cpython_internal
@@ -58,7 +58,7 @@ def __init__(
         self,
         data: Dict[str, Any],
         *,
+        memory_records: Iterable[MemorySnapshot],
     ) -> None:
         super().__init__()
         self.data = data
@@ -69,7 +69,7 @@ def from_snapshot(
         cls,
         allocations: Iterator[AllocationRecord],
         *,
+        memory_records: Iterable[MemorySnapshot],
         native_traces: bool,
     ) -> ""FlameGraphReporter"":
         data: Dict[str, Any] = {"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" 
 from memray import AllocationRecord
 from memray import AllocatorType
-from memray import MemoryRecord
 from memray import Metadata
 from memray.reporters.templates import render_report
 
@@ -18,7 +18,7 @@ def __init__(
         self,
         data: List[Dict[str, Any]],
         *,
-        memory_records: Iterable[MemoryRecord],
     ):
         super().__init__()
         self.data = data
@@ -29,7 +29,7 @@ def from_snapshot(
         cls,
         allocations: Iterator[AllocationRecord],
         *,
-        memory_records: Iterable[MemoryRecord],
         native_traces: bool,
     ) -> ""TableReporter"":
 "
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" 
 from memray import AllocationRecord
 from memray import AllocatorType
+from memray import MemorySnapshot
 from memray import Metadata
 from memray.reporters.templates import render_report
 
@@ -18,7 +18,7 @@ def __init__(
         self,
         data: List[Dict[str, Any]],
         *,
+        memory_records: Iterable[MemorySnapshot],
     ):
         super().__init__()
         self.data = data
@@ -29,7 +29,7 @@ def from_snapshot(
         cls,
         allocations: Iterator[AllocationRecord],
         *,
+        memory_records: Iterable[MemorySnapshot],
         native_traces: bool,
     ) -> ""TableReporter"":
 "
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" 
 import jinja2
 
-from memray import MemoryRecord
 from memray import Metadata
 
 
@@ -29,7 +29,7 @@ def render_report(
     kind: str,
     data: Union[Dict[str, Any], Iterable[Dict[str, Any]]],
     metadata: Metadata,
-    memory_records: Iterable[MemoryRecord],
     show_memory_leaks: bool,
     merge_threads: bool,
 ) -> str:"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";" 
 import jinja2
 
+from memray import MemorySnapshot
 from memray import Metadata
 
 
@@ -29,7 +29,7 @@ def render_report(
     kind: str,
     data: Union[Dict[str, Any], Iterable[Dict[str, Any]]],
     metadata: Metadata,
+    memory_records: Iterable[MemorySnapshot],
     show_memory_leaks: bool,
     merge_threads: bool,
 ) -> str:"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"def test_header_allocator(self, allocator, allocator_name, tmpdir):
         assert metadata.python_allocator == allocator_name
 
 
-class TestMemoryRecords:
     @pytest.mark.valgrind
-    def test_memory_records_are_written(self, tmp_path):
         # GIVEN
         allocator = MemoryAllocator()
         output = tmp_path / ""test.bin""
@@ -1112,17 +1112,18 @@ def test_memory_records_are_written(self, tmp_path):
             time.sleep(0.11)
             allocator.free()
 
-        memory_records = list(FileReader(output).get_memory_records())
 
-        assert memory_records
-        assert all(record.rss > 0 for record in memory_records)
-        assert sorted(memory_records, key=lambda r: r.time) == memory_records
         assert all(
             _next.time - prev.time >= 10
-            for prev, _next in zip(memory_records, memory_records[1:])
         )
 
-    def test_memory_records_tick_interval(self, tmp_path):
         # GIVEN
         allocator = MemoryAllocator()
         output = tmp_path / ""test.bin""
@@ -1132,12 +1133,13 @@ def test_memory_records_tick_interval(self, tmp_path):
             allocator.valloc(1234)
             time.sleep(0.11)
 
-        memory_records = list(FileReader(output).get_memory_records())
 
-        assert len(memory_records)
-        assert all(record.rss > 0 for record in memory_records)
-        assert sorted(memory_records, key=lambda r: r.time) == memory_records
         assert all(
             _next.time - prev.time >= 20
-            for prev, _next in zip(memory_records, memory_records[1:])
         )"
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"def test_header_allocator(self, allocator, allocator_name, tmpdir):
         assert metadata.python_allocator == allocator_name
 
 
+class TestMemorySnapshots:
     @pytest.mark.valgrind
+    def test_memory_snapshots_are_written(self, tmp_path):
         # GIVEN
         allocator = MemoryAllocator()
         output = tmp_path / ""test.bin""
@@ -1112,17 +1112,18 @@ def test_memory_records_are_written(self, tmp_path):
             time.sleep(0.11)
             allocator.free()
 
+        memory_snapshots = list(FileReader(output).get_memory_snapshots())
 
+        assert memory_snapshots
+        assert all(record.rss > 0 for record in memory_snapshots)
+        assert any(record.heap >= 1234 for record in memory_snapshots)
+        assert sorted(memory_snapshots, key=lambda r: r.time) == memory_snapshots
         assert all(
             _next.time - prev.time >= 10
+            for prev, _next in zip(memory_snapshots, memory_snapshots[1:])
         )
 
+    def test_memory_snapshots_tick_interval(self, tmp_path):
         # GIVEN
         allocator = MemoryAllocator()
         output = tmp_path / ""test.bin""
@@ -1132,12 +1133,13 @@ def test_memory_records_tick_interval(self, tmp_path):
             allocator.valloc(1234)
             time.sleep(0.11)
 
+        memory_snapshots = list(FileReader(output).get_memory_snapshots())
 
+        assert len(memory_snapshots)
+        assert all(record.rss > 0 for record in memory_snapshots)
+        assert any(record.heap >= 1234 for record in memory_snapshots)
+        assert sorted(memory_snapshots, key=lambda r: r.time) == memory_snapshots
         assert all(
             _next.time - prev.time >= 20
+            for prev, _next in zip(memory_snapshots, memory_snapshots[1:])
         )"
KO;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"def test_tracker_and_reporter_interactions_for_peak(self, tmp_path, merge_thread
         calls = [
             call(os.fspath(result_path), report_progress=True),
             call().get_high_watermark_allocation_records(merge_threads=merge_threads),
-            call().get_memory_records(),
         ]
         reader_mock.assert_has_calls(calls)
 
@@ -192,7 +192,7 @@ def test_tracker_and_reporter_interactions_for_leak(self, tmp_path, merge_thread
         calls = [
             call(os.fspath(result_path), report_progress=True),
             call().get_leaked_allocation_records(merge_threads=merge_threads),
-            call().get_memory_records(),
         ]
         reader_mock.assert_has_calls(calls)
 "
OK;5;bloomberg;memray;066274134de86197580bf6c372fdb279d60c5808;"Add plot with heap size to the flamegraph/table memory plot

To allow users to compare resident size and allocated size over time,
add a new plot line to the graphs that we show in the different reporter
that shows also allocated memory size.

This commit also changes the memory records to include the heap size as
well as the resident size.

Signed-off-by: Pablo Galindo <pablogsal@gmail.com>";"def test_tracker_and_reporter_interactions_for_peak(self, tmp_path, merge_thread
         calls = [
             call(os.fspath(result_path), report_progress=True),
             call().get_high_watermark_allocation_records(merge_threads=merge_threads),
+            call().get_memory_snapshots(),
         ]
         reader_mock.assert_has_calls(calls)
 
@@ -192,7 +192,7 @@ def test_tracker_and_reporter_interactions_for_leak(self, tmp_path, merge_thread
         calls = [
             call(os.fspath(result_path), report_progress=True),
             call().get_leaked_allocation_records(merge_threads=merge_threads),
+            call().get_memory_snapshots(),
         ]
         reader_mock.assert_has_calls(calls)
 "
KO;5;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";
OK;5;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";+Fix a crash with SIGBUS when the file system fills up while ``memray run`` is writing a capture file.
KO;5;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"FileSink::grow(size_t needed)
     new_size = (new_size / 4096 + 1) * 4096;
     assert(new_size > d_fileSize);  // check for overflow
 
-    // Seek to 1 byte before the new size
-    off_t offset = lseek(d_fd, new_size - 1, SEEK_SET);
-    if (offset == -1) {
-        return false;
-    }
-
-    // Then write 1 byte.
-    ssize_t rc;
     do {
-        rc = write(d_fd, ""\0"", 1);
-    } while (rc < 0 && errno == EINTR);
 
-    if (rc < 0) {
         return false;
     }
 "
OK;5;bloomberg;memray;8623bfb1f329250db8b9f0acf6d6f564e22ebbb9;"Grow capture files with `posix_fallocate`

Previously we were using lseek + write to grow our capture files, but
that resulted in SIGBUS being raised when a disk had filled up, rather
than an error code being returned (since it created sparse files that
would actually grow to require more disk space only as we `memcpy`'d
data into our memory-mapped array.

Instead, use `posix_fallocate` to grow our capture file. As long as this
call succeeds, it is guaranteed that future copies into our memory
mapped array will succeed.

Signed-off-by: Matt Wozniski <mwozniski@bloomberg.net>";"FileSink::grow(size_t needed)
     new_size = (new_size / 4096 + 1) * 4096;
     assert(new_size > d_fileSize);  // check for overflow
 
+    off_t delta = new_size - d_fileSize;
+    int rc;
     do {
+        // posix_fallocate returns an error number instead of setting errno
+        rc = posix_fallocate(d_fd, d_fileSize, delta);
+    } while (rc == EINTR);
 
+    if (rc != 0) {
+        errno = rc;
         return false;
     }
 "
