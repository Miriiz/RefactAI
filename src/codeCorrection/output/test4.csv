Label;Page;Username;Repo;Commit;Bug;Code
KO;1;bloomberg;pytest-memray;16b256ca2b793adbaed489512265318aee264287;"Report memory limit and allocated memory in longrepr (#5)

Refactor limit_memory to return a schema-based type, which
can be converted to a PytestSectio and a longrepr string.

Remove item.nodeid from longrepr since longrepr is only used by
junit-xml parser and the structure of the xml document
makes the item.nodeid in the error message redundant.

Add a test that parses the resulting xml file to make sure
the expected string was written.

Co-authored-by: Bernát Gábor <bgabor8@bloomberg.net>";" from __future__ import annotations
 
 from memray import AllocationRecord
 
 from .utils import parse_memory_string
 from .utils import sizeof_fmt
 
 
 def limit_memory(
     limit: str, *, _allocations: list[AllocationRecord]
-) -> tuple[str, str] | None:
-    """"""Limit memory used by the test""""""
     max_memory = parse_memory_string(limit)
     total_allocated_memory = sum(record.size for record in _allocations)
     if total_allocated_memory < max_memory:
         return None
-    total_memory_str = sizeof_fmt(total_allocated_memory)
-    max_memory_str = sizeof_fmt(max_memory)
-    text_lines = [
-        f""Test is using {total_memory_str} out of limit of {max_memory_str}"",
-        ""List of allocations: "",
-    ]
-    for record in _allocations:
-        size = record.size
-        stack_trace = record.stack_trace()
-        if not stack_trace:
-            continue
-        (function, file, line), *_ = stack_trace
-        text_lines.append(f""\t- {function}:{file}:{line} -> {sizeof_fmt(size)}"")
-
-    return ""memray-max-memory"", ""\n"".join(text_lines)
 
 
 __all__ = ["
OK;1;bloomberg;pytest-memray;16b256ca2b793adbaed489512265318aee264287;"Report memory limit and allocated memory in longrepr (#5)

Refactor limit_memory to return a schema-based type, which
can be converted to a PytestSectio and a longrepr string.

Remove item.nodeid from longrepr since longrepr is only used by
junit-xml parser and the structure of the xml document
makes the item.nodeid in the error message redundant.

Add a test that parses the resulting xml file to make sure
the expected string was written.

Co-authored-by: Bernát Gábor <bgabor8@bloomberg.net>";" from __future__ import annotations
 
+from dataclasses import dataclass
+from typing import Optional
+from typing import Tuple
+
 from memray import AllocationRecord
 
 from .utils import parse_memory_string
 from .utils import sizeof_fmt
 
+PytestSection = Tuple[str, str]
+
+
+@dataclass
+class _MemoryInfo:
+    """"""Type that holds all memray-related info for a failed test.""""""
+
+    max_memory: float
+    total_allocated_memory: int
+    allocations: list[AllocationRecord]
+
+    @property
+    def section(self) -> PytestSection:
+        """"""Return a tuple in the format expected by section reporters.""""""
+        total_memory_str = sizeof_fmt(self.total_allocated_memory)
+        max_memory_str = sizeof_fmt(self.max_memory)
+        text_lines = [
+            f""Test is using {total_memory_str} out of limit of {max_memory_str}"",
+            ""List of allocations: "",
+        ]
+        for record in self.allocations:
+            size = record.size
+            stack_trace = record.stack_trace()
+            if not stack_trace:
+                continue
+            (function, file, line), *_ = stack_trace
+            text_lines.append(f""\t- {function}:{file}:{line} -> {sizeof_fmt(size)}"")
+        return ""memray-max-memory"", ""\n"".join(text_lines)
+
+    @property
+    def long_repr(self) -> str:
+        """"""Generate a longrepr user-facing error message.""""""
+        total_memory_str = sizeof_fmt(self.total_allocated_memory)
+        max_memory_str = sizeof_fmt(self.max_memory)
+        return f""Test was limited to {max_memory_str} but allocated {total_memory_str}""
+
 
 def limit_memory(
     limit: str, *, _allocations: list[AllocationRecord]
+) -> Optional[_MemoryInfo]:
+    """"""Limit memory used by the test.""""""
     max_memory = parse_memory_string(limit)
     total_allocated_memory = sum(record.size for record in _allocations)
     if total_allocated_memory < max_memory:
         return None
+    return _MemoryInfo(max_memory, total_allocated_memory, _allocations)
 
 
 __all__ = ["
KO;1;bloomberg;pytest-memray;16b256ca2b793adbaed489512265318aee264287;"Report memory limit and allocated memory in longrepr (#5)

Refactor limit_memory to return a schema-based type, which
can be converted to a PytestSectio and a longrepr string.

Remove item.nodeid from longrepr since longrepr is only used by
junit-xml parser and the structure of the xml document
makes the item.nodeid in the error message redundant.

Add a test that parses the resulting xml file to make sure
the expected string was written.

Co-authored-by: Bernát Gábor <bgabor8@bloomberg.net>";"def pytest_runtest_makereport(
             res = marker_fn(*marker.args, **marker.kwargs, _allocations=allocations)
             if res:
                 report.outcome = ""failed""
-                report.longrepr = f""Memray detected problems with test {item.nodeid}""
-                report.sections.append(res)
                 outcome.force_result(report)
         return None
 "
OK;1;bloomberg;pytest-memray;16b256ca2b793adbaed489512265318aee264287;"Report memory limit and allocated memory in longrepr (#5)

Refactor limit_memory to return a schema-based type, which
can be converted to a PytestSectio and a longrepr string.

Remove item.nodeid from longrepr since longrepr is only used by
junit-xml parser and the structure of the xml document
makes the item.nodeid in the error message redundant.

Add a test that parses the resulting xml file to make sure
the expected string was written.

Co-authored-by: Bernát Gábor <bgabor8@bloomberg.net>";"def pytest_runtest_makereport(
             res = marker_fn(*marker.args, **marker.kwargs, _allocations=allocations)
             if res:
                 report.outcome = ""failed""
+                report.longrepr = res.long_repr
+                report.sections.append(res.section)
                 outcome.force_result(report)
         return None
 "
KO;1;bloomberg;pytest-memray;16b256ca2b793adbaed489512265318aee264287;"Report memory limit and allocated memory in longrepr (#5)

Refactor limit_memory to return a schema-based type, which
can be converted to a PytestSectio and a longrepr string.

Remove item.nodeid from longrepr since longrepr is only used by
junit-xml parser and the structure of the xml document
makes the item.nodeid in the error message redundant.

Add a test that parses the resulting xml file to make sure
the expected string was written.

Co-authored-by: Bernát Gábor <bgabor8@bloomberg.net>";" from __future__ import annotations
 
 from unittest.mock import patch
 
 import pytest
@@ -98,6 +99,36 @@ def test_memory_alloc_fails():
     assert result.ret == ExitCode.OK
 
 
 def test_memray_with_junit_xml(pytester: Pytester) -> None:
     pytester.makepyfile(
         """""""
OK;1;bloomberg;pytest-memray;16b256ca2b793adbaed489512265318aee264287;"Report memory limit and allocated memory in longrepr (#5)

Refactor limit_memory to return a schema-based type, which
can be converted to a PytestSectio and a longrepr string.

Remove item.nodeid from longrepr since longrepr is only used by
junit-xml parser and the structure of the xml document
makes the item.nodeid in the error message redundant.

Add a test that parses the resulting xml file to make sure
the expected string was written.

Co-authored-by: Bernát Gábor <bgabor8@bloomberg.net>";" from __future__ import annotations
 
+import xml.etree.ElementTree as ET
 from unittest.mock import patch
 
 import pytest
@@ -98,6 +99,36 @@ def test_memory_alloc_fails():
     assert result.ret == ExitCode.OK
 
 
+@pytest.mark.parametrize(
+    ""memlimit, mem_to_alloc"",
+    [(5, 100), (10, 200)],
+)
+def test_memray_with_junit_xml_error_msg(
+    pytester: Pytester, memlimit: int, mem_to_alloc: int
+):
+    xml_output_file = pytester.makefile("".xml"", """")
+    pytester.makepyfile(
+        f""""""
+        import pytest
+        from memray._test import MemoryAllocator
+        allocator = MemoryAllocator()
+
+        @pytest.mark.limit_memory(""{memlimit}B"")
+        def test_memory_alloc_fails():
+            allocator.valloc({mem_to_alloc})
+            allocator.free()
+        """"""
+    )
+    result = pytester.runpytest(""--memray"", ""--junit-xml"", xml_output_file)
+    assert result.ret == ExitCode.TESTS_FAILED
+
+    expected = f""Test was limited to {memlimit}.0B but allocated {mem_to_alloc}.0B""
+    root = ET.parse(str(xml_output_file)).getroot()
+    for testcase in root.iter(""testcase""):
+        failure = testcase.find(""failure"")
+        assert expected in failure.text
+
+
 def test_memray_with_junit_xml(pytester: Pytester) -> None:
     pytester.makepyfile(
         """""""
KO;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"Jupyter Notebook will be available on port 3100 (http://localhost:3100).
 
 ## Examples
 
 * Connecting to the first LiteServer in mainnet config:
 ```python
 import requests
@@ -48,7 +50,7 @@ client = TonlibClient(ls_index=0, # choose LiteServer index to connect
                       loop=loop)
 
 # init tonlibjson
-await client.init(max_restarts=None)
 ```
 
 * Reading blocks info:
@@ -83,9 +85,22 @@ async def main():
                           loop=loop)
     
     # init tonlibjson
-    await client.init(max_restarts=None)
 
 
 if __name__ == '__main__':
     asyncio.run(main())
-```
\ No newline at end of file"
OK;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"Jupyter Notebook will be available on port 3100 (http://localhost:3100).
 
 ## Examples
 
+We recommend to use IPython or Jupyter Notebook for prototyping because they allow to run `async` code. An example of running `async` code from script could be found in the end of this section.
+
 * Connecting to the first LiteServer in mainnet config:
 ```python
 import requests
@@ -48,7 +50,7 @@ client = TonlibClient(ls_index=0, # choose LiteServer index to connect
                       loop=loop)
 
 # init tonlibjson
+await client.init()
 ```
 
 * Reading blocks info:
@@ -83,9 +85,22 @@ async def main():
                           loop=loop)
     
     # init tonlibjson
+    await client.init()
+    
+    # reading masterchain info
+    masterchain_info = await client.get_masterchain_info()
+
+    # closing session
+    await client.close()
 
 
 if __name__ == '__main__':
     asyncio.run(main())
\ No newline at end of file
+```
+
+## Running tests
+
+To run tests in *asyncio* mode use the following command: 
+```bash
+PYTHONPATH=./ pytest --asyncio-mode=strict tests/
+```"
KO;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""ed3cdccf"",
    ""metadata"": {},
    ""outputs"": [],
@@ -39,7 +39,7 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""7e898620"",
    ""metadata"": {},
    ""outputs"": [],
@@ -58,18 +58,33 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""22e67d8d"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""loop = asyncio.get_running_loop()\n"",
     ""client = TonlibClient(ls_index=0, # choose LiteServer to connect\n"",
     ""                      config=ton_config,\n"",
     ""                      keystore='/tmp/ton_keystore',\n"",
     ""                      loop=loop)\n"",
     ""\n"",
-    ""await client.init(max_restarts=None)""
    ]
   },
   {
@@ -82,10 +97,35 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""a0c3eaf0"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""masterchain_info = await client.get_masterchain_info()\n"",
     ""masterchain_info""
@@ -101,10 +141,51 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""3da9bd64"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""block_header = await client.get_block_header(**masterchain_info['last'])\n"",
     ""block_header""
@@ -120,10 +201,28 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""e5470d37"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""shards = await client.get_shards(master_seqno=masterchain_info['last']['seqno'])\n"",
     ""shards""
@@ -139,10 +238,19 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""d09bc06b"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""txs = await client.get_block_transactions(**masterchain_info['last'], count=10)\n"",
     ""\n"",
@@ -160,29 +268,78 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""e8397a5b"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""tx = txs['transactions'][0]\n"",
     ""tx""
    ]
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""b65b4586"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""await client.get_transactions(**tx, limit=1)""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": null,
-   ""id"": ""af4da2e1"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": []"
OK;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 1,
    ""id"": ""ed3cdccf"",
    ""metadata"": {},
    ""outputs"": [],
@@ -39,7 +39,7 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 2,
    ""id"": ""7e898620"",
    ""metadata"": {},
    ""outputs"": [],
@@ -58,18 +58,33 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 3,
    ""id"": ""22e67d8d"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""[ 4][t 0][2022-05-06 20:36:00.314293800][Client.cpp:78][&tonlib_requests]\tBegin to wait for updates with timeout 1.000000\u001b[0m\n"",
+      ""[ 4][t 1][2022-05-06 20:36:00.320407500][TonlibClient.cpp:1477][!Tonlib][&tonlib_query]\tTonlib got query [id:1] setLogVerbosityLevel {\n"",
+      ""  new_verbosity_level = 0\n"",
+      ""}\u001b[0m\n"",
+      ""[ 4][t 1][2022-05-06 20:36:00.321866900][TonlibClient.cpp:1516][!Tonlib][&tonlib_query]\tTonlib got static query setLogVerbosityLevel {\n"",
+      ""  new_verbosity_level = 0\n"",
+      ""}\u001b[0m\n"",
+      ""2022-05-06 20:36:00,331 client          TonLib #000 inited successfully\n""
+     ]
+    }
+   ],
    ""source"": [
     ""loop = asyncio.get_running_loop()\n"",
     ""client = TonlibClient(ls_index=0, # choose LiteServer to connect\n"",
     ""                      config=ton_config,\n"",
     ""                      keystore='/tmp/ton_keystore',\n"",
     ""                      loop=loop)\n"",
     ""\n"",
+    ""await client.init()""
    ]
   },
   {
@@ -82,10 +97,35 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 4,
    ""id"": ""a0c3eaf0"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.masterchainInfo',\n"",
+       "" 'last': {'@type': 'ton.blockIdExt',\n"",
+       ""  'workchain': -1,\n"",
+       ""  'shard': '-9223372036854775808',\n"",
+       ""  'seqno': 20361208,\n"",
+       ""  'root_hash': 'FDEyd4nZgolZ9ryWCK32u/5MsPCDTo6qJHT3XGFF8XA=',\n"",
+       ""  'file_hash': 'Jn9q+fqs6dF5wc+3DzjCsQdGL4g8IfIr9RpYnS1QwsA='},\n"",
+       "" 'state_root_hash': 'xQvdczD+gdaTamtLDIo9rwxLG3w4fchSiEcnaPsHuN0=',\n"",
+       "" 'init': {'@type': 'ton.blockIdExt',\n"",
+       ""  'workchain': -1,\n"",
+       ""  'shard': '0',\n"",
+       ""  'seqno': 0,\n"",
+       ""  'root_hash': 'F6OpKZKqvqeFp6CQmFomXNMfMj2EnaUSOXN+Mh+wVWk=',\n"",
+       ""  'file_hash': 'XplPz01CXAps5qeSWUtxcyBfdAo5zVb1N979KLSKD24='},\n"",
+       "" '@extra': '1651869370.3371992:0:0.507966611324214'}""
+      ]
+     },
+     ""execution_count"": 4,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""masterchain_info = await client.get_masterchain_info()\n"",
     ""masterchain_info""
@@ -101,10 +141,51 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 5,
    ""id"": ""3da9bd64"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.header',\n"",
+       "" 'id': {'@type': 'ton.blockIdExt',\n"",
+       ""  'workchain': -1,\n"",
+       ""  'shard': '-9223372036854775808',\n"",
+       ""  'seqno': 20361208,\n"",
+       ""  'root_hash': 'FDEyd4nZgolZ9ryWCK32u/5MsPCDTo6qJHT3XGFF8XA=',\n"",
+       ""  'file_hash': 'Jn9q+fqs6dF5wc+3DzjCsQdGL4g8IfIr9RpYnS1QwsA='},\n"",
+       "" 'global_id': -239,\n"",
+       "" 'version': 0,\n"",
+       "" 'flags': 1,\n"",
+       "" 'after_merge': False,\n"",
+       "" 'after_split': False,\n"",
+       "" 'before_split': False,\n"",
+       "" 'want_merge': True,\n"",
+       "" 'want_split': False,\n"",
+       "" 'validator_list_hash_short': 122883420,\n"",
+       "" 'catchain_seqno': 306940,\n"",
+       "" 'min_ref_mc_seqno': 20361205,\n"",
+       "" 'is_key_block': False,\n"",
+       "" 'prev_key_block_seqno': 20351399,\n"",
+       "" 'start_lt': '27699743000000',\n"",
+       "" 'end_lt': '27699743000004',\n"",
+       "" 'gen_utime': 1651869354,\n"",
+       "" 'vert_seqno': 1,\n"",
+       "" 'prev_blocks': [{'@type': 'ton.blockIdExt',\n"",
+       ""   'workchain': -1,\n"",
+       ""   'shard': '-9223372036854775808',\n"",
+       ""   'seqno': 20361207,\n"",
+       ""   'root_hash': 'AM+LcJOyfGSm1dpbaQDXpOgY7bcMcEDawtCAUwcJLJw=',\n"",
+       ""   'file_hash': 'AyW+wJSoJZfbBB3Y1JrgJ19SKlpC7WvitkeVk/yplYI='}],\n"",
+       "" '@extra': '1651869370.6080866:0:0.17246900748102634'}""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""block_header = await client.get_block_header(**masterchain_info['last'])\n"",
     ""block_header""
@@ -120,10 +201,28 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 6,
    ""id"": ""e5470d37"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.shards',\n"",
+       "" 'shards': [{'@type': 'ton.blockIdExt',\n"",
+       ""   'workchain': 0,\n"",
+       ""   'shard': '-9223372036854775808',\n"",
+       ""   'seqno': 25532997,\n"",
+       ""   'root_hash': 'JI8s3H5c7g4Vexlezl6V+xhvKYjIZKsA8ItgZHJdtQU=',\n"",
+       ""   'file_hash': 'cMglRNmDveIczi8SjzIsBGHDT0baUa+bwe1ba5Qh7CI='}],\n"",
+       "" '@extra': '1651869370.8817048:0:0.3841969484094021'}""
+      ]
+     },
+     ""execution_count"": 6,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""shards = await client.get_shards(master_seqno=masterchain_info['last']['seqno'])\n"",
     ""shards""
@@ -139,10 +238,19 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 7,
    ""id"": ""d09bc06b"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Is incomplete: False\n"",
+      ""Num txs: 5\n""
+     ]
+    }
+   ],
    ""source"": [
     ""txs = await client.get_block_transactions(**masterchain_info['last'], count=10)\n"",
     ""\n"",
@@ -160,29 +268,78 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 8,
    ""id"": ""e8397a5b"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.shortTxId',\n"",
+       "" 'mode': 135,\n"",
+       "" 'account': '-1:3333333333333333333333333333333333333333333333333333333333333333',\n"",
+       "" 'lt': '27699743000001',\n"",
+       "" 'hash': 'jOIVA+yIkoPywpjrFj9VYTohez2au0ooow5uPLYWFAc='}""
+      ]
+     },
+     ""execution_count"": 8,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""tx = txs['transactions'][0]\n"",
     ""tx""
    ]
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 9,
    ""id"": ""b65b4586"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""[{'@type': 'raw.transaction',\n"",
+       ""  'address': {'@type': 'accountAddress',\n"",
+       ""   'account_address': 'Ef8zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzM0vF'},\n"",
+       ""  'utime': 1651869357,\n"",
+       ""  'data': 'te6cckECBwEAAYkAA69zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzAAAZMVlmyAIoy6aZP4NZkz72hGeQDNHm2NpTPKWaFnx4LB9YizUu2gAAGTFZZsgBYnWGrQABQIAQIDAQGgBACCco+Ec6Mj7xtio/Duyx50cd7VDh/dyMx2l/MOpAqkYJIU5/6pVPLSgialzCVl4c6EkwC2lWh3d1HX8y6moQPwkl4CDwQJKU/jnFgRBQYAq2n+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAE/zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzSlP45xAAAADJiss2QAMTrDVpAAJ5CYUwQ6+AAAAAAAAAAAGQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFvAAAAAAAAAAAAAAAABLUUtpEnlC4z33SeGHxRhIq/htUa7i3D8ghbwxhQTn44EwPyXiw==',\n"",
+       ""  'transaction_id': {'@type': 'internal.transactionId',\n"",
+       ""   'lt': '27699744000002',\n"",
+       ""   'hash': 'AnL1aJnySYYOG+Godrlio8NhYRZO3avoLzfkdXpnnOo='},\n"",
+       ""  'fee': '0',\n"",
+       ""  'storage_fee': '0',\n"",
+       ""  'other_fee': '0',\n"",
+       ""  'in_msg': {'@type': 'raw.message',\n"",
+       ""   'source': 'Ef8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAU',\n"",
+       ""   'destination': 'Ef8zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzM0vF',\n"",
+       ""   'value': '2772405873',\n"",
+       ""   'fwd_fee': '0',\n"",
+       ""   'ihr_fee': '0',\n"",
+       ""   'created_lt': '27699744000000',\n"",
+       ""   'body_hash': 'lqKW0iTyhcZ77pPDD4owkVfw2qNdxbh+QQt4YwoJz8c=',\n"",
+       ""   'msg_data': {'@type': 'msg.dataRaw',\n"",
+       ""    'body': 'te6cckEBAQEAAgAAAEysuc0=',\n"",
+       ""    'init_state': ''},\n"",
+       ""   'message': ''},\n"",
+       ""  'out_msgs': []}]""
+      ]
+     },
+     ""execution_count"": 9,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""await client.get_transactions(**tx, limit=1)""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": null,
+   ""id"": ""68dc43a6"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": []"
KO;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def local_config(self):
         local['liteservers'] = [local['liteservers'][self.ls_index]]
         return local
 
-    async def reconnect(self, max_restarts=None):
-        if max_restarts is not None:
-            max_restarts -= 1
-        if max_restarts is None or max_restarts >= 0:
-            await self.init(max_restarts)
-            logger.info(f'Client #{self.ls_index:03d} reconnected (max_restarts: {max_restarts})')
-        else:
-            logger.info('Client #{self.ls_index:03d} has no reconnect attempts left')
-            self.tonlib_wrapper = None
-
-    async def init(self, max_restarts=None):
         """"""
         TL Spec
             init options:options = options.Info;
@@ -69,41 +59,58 @@ async def init(self, max_restarts=None):
         :param key: base64 pub key of liteserver node
         :return: None
         """"""
-        self.semaphore = asyncio.Semaphore(self.max_parallel_requests)
-        
-        self.loaded_contracts_num = 0
-        wrapper = TonLib(self.loop, self.ls_index, self.cdll_path)
-        keystore_obj = {
-            '@type': 'keyStoreTypeDirectory',
-            'directory': self.keystore
-        }
-        # create keystore
-        Path(self.keystore).mkdir(parents=True, exist_ok=True)
-
-        request = {
-            '@type': 'init',
-            'options': {
-                '@type': 'options',
-                'config': {
-                    '@type': 'config',
-                    'config': json.dumps(self.local_config),
-                    'use_callbacks_for_network': False,
-                    'blockchain_name': '',
-                    'ignore_cache': False
-                },
-                'keystore_type': keystore_obj
             }
-        }
-        self.tonlib_wrapper = wrapper
 
-        # set verbosity level
-        await self.set_verbosity_level(self.verbosity_level)
-        
-        # set confog
-        await self.tonlib_wrapper.execute(request)
-        self.tonlib_wrapper.set_restart_hook(hook=self.reconnect, max_requests=1024, max_restarts=max_restarts)
 
-        logger.info(F""TonLib #{self.ls_index:03d} inited successfully"")
 
     async def set_verbosity_level(self, level):
         request = {"
OK;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def local_config(self):
         local['liteservers'] = [local['liteservers'][self.ls_index]]
         return local
 
+    async def init(self):
         """"""
         TL Spec
             init options:options = options.Info;
@@ -69,41 +59,58 @@ async def init(self, max_restarts=None):
         :param key: base64 pub key of liteserver node
         :return: None
         """"""
+        if self.tonlib_wrapper is None:            
+            self.loaded_contracts_num = 0
+            wrapper = TonLib(self.loop, self.ls_index, self.cdll_path)
+            keystore_obj = {
+                '@type': 'keyStoreTypeDirectory',
+                'directory': self.keystore
             }
+            # create keystore
+            Path(self.keystore).mkdir(parents=True, exist_ok=True)
+
+            request = {
+                '@type': 'init',
+                'options': {
+                    '@type': 'options',
+                    'config': {
+                        '@type': 'config',
+                        'config': json.dumps(self.local_config),
+                        'use_callbacks_for_network': False,
+                        'blockchain_name': '',
+                        'ignore_cache': False
+                    },
+                    'keystore_type': keystore_obj
+                }
+            }
+            self.tonlib_wrapper = wrapper
+
+            # set verbosity level
+            await self.set_verbosity_level(self.verbosity_level)
+            
+            # set confog
+            await self.tonlib_wrapper.execute(request)
+
+            # set semaphore
+            self.semaphore = asyncio.Semaphore(self.max_parallel_requests)
+            
+            logger.info(F""TonLib #{self.ls_index:03d} inited successfully"")
+        else:
+            logger.warning(f'init is already done')
+
+    async def close(self):
+        if self.tonlib_wrapper is not None:
+            await self.tonlib_wrapper.close()
+            del self.tonlib_wrapper
+
+    async def __aenter__(self):
+        await self.init()
 
+    async def __aexit__(self, *args):
+        await self.close()
 
+    def __await__(self):
+        return self.init()
 
     async def set_verbosity_level(self, level):
         request = {"
KO;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         tonlib_json_client_create.argtypes = []
         try:
             self._client = tonlib_json_client_create()
-        except Exception:
-            asyncio.ensure_future(self.restart(), loop=loop)
 
         tonlib_json_client_receive = tonlib.tonlib_client_json_receive
         tonlib_json_client_receive.restype = c_char_p
@@ -75,110 +75,103 @@ def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         self.futures = {}
         self.loop = loop
         self.ls_index = ls_index
-        self.read_results_task = asyncio.ensure_future(self.read_results(), loop=self.loop)
-        self.del_expired_futures_task = asyncio.ensure_future(self.del_expired_futures_loop(), loop=self.loop)
-        self.shutdown_state = False  # False, ""started"", ""finished""
-        self.request_num = 0
         self.verbose = verbose
 
-        self.max_requests = None
-        self.max_restarts = None
-
     def __del__(self):
         try:
             self._tonlib_json_client_destroy(self._client)
-        except Exception:
-            logger.error(f'Traceback: {traceback.format_exc()}')
-            asyncio.ensure_future(self.restart(), loop=self.loop)
 
     def send(self, query):
         query = json.dumps(query).encode('utf-8')
         try:
             self._tonlib_json_client_send(self._client, query)
-        except Exception:
-            asyncio.ensure_future(self.restart(), loop=self.loop)
-
-    async def restart(self):
-        if not self.shutdown_state:
-            self.shutdown_state = ""started""
-            asyncio.ensure_future(self.restart_hook(self.max_restarts), loop=self.loop)
 
     def receive(self, timeout=10):
         result = None
         try:
             result = self._tonlib_json_client_receive(self._client, timeout)  # time.sleep # asyncio.sleep
-        except Exception:
-            asyncio.ensure_future(self.restart(), loop=self.loop)
         if result:
             result = json.loads(result.decode('utf-8'))
         return result
 
-    def set_restart_hook(self, hook, max_requests=None, max_restarts=None):
-        self.max_requests = max_requests
-        self.max_restarts = max_restarts
-        self.restart_hook = hook
-
     def execute(self, query, timeout=10):
-        query_type = query.get('@type', '?')
-        # logger.debug(f'Tonlib #{self.ls_index:03d}. Executing query with timeout={timeout}: {query_type}')
-        extra_id = ""%s:%s:%s"" % (time.time()+timeout, self.ls_index, random.random())
         query[""@extra""] = extra_id
         
-        self.loop.run_in_executor(None, lambda: self.send(query))
-        
         future_result = self.loop.create_future()
         self.futures[extra_id] = future_result
 
-        self.request_num += 1
-
-        if self.max_requests and self.max_requests < self.request_num:
-            asyncio.ensure_future(self.restart(), loop=self.loop)
-
         return future_result
-
     @property
-    def _is_finishing(self):
-        return (not len(self.futures)) and (self.shutdown_state in [""started"", ""finished""])
 
     async def read_results(self):
-        timeout = 3
         delta = 5
         receive_func = functools.partial(self.receive, timeout)
 
-        while not self._is_finishing:
             result = None
             try:
                 result = await asyncio.wait_for(self.loop.run_in_executor(None, receive_func), timeout=timeout + delta)
             except asyncio.TimeoutError:
-                logger.critical(f""Tonlib #{self.ls_index:03d} Stuck!"")
-                asyncio.ensure_future(self.restart(), loop=self.loop)
-                await asyncio.sleep(2)
             except:
                 logger.critical(f""Tonlib #{self.ls_index:03d} crashed: {traceback.format_exc()}"")
-                asyncio.ensure_future(self.restart(), loop=self.loop)
-                await asyncio.sleep(2)
             
-            # return result
             if result and isinstance(result, dict) and (""@extra"" in result) and (result[""@extra""] in self.futures):
                 try:
                     if not self.futures[result[""@extra""]].done():
                         self.futures[result[""@extra""]].set_result(result)
                         self.futures.pop(result[""@extra""])
                 except Exception as e:
                     logger.error(f'Tonlib #{self.ls_index:03d} receiving result exception: {e}')
-        self.shutdown_state = ""finished""
 
     async def del_expired_futures_loop(self):
-        while not self._is_finishing:
-            await self.cancel_futures()
             await asyncio.sleep(1)
 
-    async def cancel_futures(self, cancel_all=False):
-        now = time.time()
-        to_del = []
-        for i in self.futures:
-            if float(i.split("":"")[0]) <= now or cancel_all:
-                to_del.append(i)
-        for i in to_del:
-            i.cancel()
-            self.futures.pop(i)"
OK;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         tonlib_json_client_create.argtypes = []
         try:
             self._client = tonlib_json_client_create()
+        except Exception as ee:
+            raise RuntimeError(f""Failed to create tonlibjson client: {ee}"")
 
         tonlib_json_client_receive = tonlib.tonlib_client_json_receive
         tonlib_json_client_receive.restype = c_char_p
@@ -75,110 +75,103 @@ def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         self.futures = {}
         self.loop = loop
         self.ls_index = ls_index
+        self._state = None  # None, ""finished"", ""crashed"", ""stuck""
         self.verbose = verbose
 
+        # creating tasks
+        self.read_results_task = asyncio.ensure_future(self.read_results(), loop=self.loop)
+        self.del_expired_futures_task = asyncio.ensure_future(self.del_expired_futures_loop(), loop=self.loop)
+    
     def __del__(self):
         try:
             self._tonlib_json_client_destroy(self._client)
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.__del__: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.__del__: {ee}')
 
     def send(self, query):
         query = json.dumps(query).encode('utf-8')
         try:
             self._tonlib_json_client_send(self._client, query)
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.send: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.send: {ee}')
 
     def receive(self, timeout=10):
         result = None
         try:
             result = self._tonlib_json_client_receive(self._client, timeout)  # time.sleep # asyncio.sleep
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.receive: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.receive: {ee}')
         if result:
             result = json.loads(result.decode('utf-8'))
         return result
 
     def execute(self, query, timeout=10):
+        extra_id = ""%s:%s:%s"" % (time.time() + timeout, self.ls_index, random.random())
         query[""@extra""] = extra_id
         
         future_result = self.loop.create_future()
         self.futures[extra_id] = future_result
 
+        self.loop.run_in_executor(None, lambda: self.send(query))
         return future_result
+    
     @property
+    def _is_working(self):
+        return self._state not in ('crashed', 'stuck', 'finished')
 
+    async def close(self):
+        try:
+            self._state = 'finished'
+            await self.read_results_task
+            await self.del_expired_futures_task
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.close: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.close: {ee}')
+
+    def cancel_futures(self, cancel_all=False):
+        now = time.time()
+        to_del = []
+        for i in self.futures:
+            if float(i.split("":"")[0]) <= now or cancel_all:
+                to_del.append(i)
+        logger.debug(f'Pruning {len(to_del)} tasks')
+        for i in to_del:
+            self.futures[i].cancel()
+            self.futures.pop(i)
+
+    # tasks
     async def read_results(self):
+        timeout = 1
         delta = 5
         receive_func = functools.partial(self.receive, timeout)
 
+        while self._is_working:
+            # return reading result
             result = None
             try:
                 result = await asyncio.wait_for(self.loop.run_in_executor(None, receive_func), timeout=timeout + delta)
             except asyncio.TimeoutError:
+                logger.critical(f""Tonlib #{self.ls_index:03d} stuck (timeout error)"")
+                self._state = ""stuck""
             except:
                 logger.critical(f""Tonlib #{self.ls_index:03d} crashed: {traceback.format_exc()}"")
+                self._state = ""crashed""
             
             if result and isinstance(result, dict) and (""@extra"" in result) and (result[""@extra""] in self.futures):
                 try:
                     if not self.futures[result[""@extra""]].done():
                         self.futures[result[""@extra""]].set_result(result)
                         self.futures.pop(result[""@extra""])
                 except Exception as e:
                     logger.error(f'Tonlib #{self.ls_index:03d} receiving result exception: {e}')
 
     async def del_expired_futures_loop(self):
+        while self._is_working:
+            self.cancel_futures()
             await asyncio.sleep(1)
 
+        # finished
+        self.cancel_futures(cancel_all=True)"
KO;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"     author='K-Dimentional Tree',
     author_email='kdimentionaltree@gmail.com',
     name='pytonlib',
-    version='0.0.4',
     packages=find_packages('.', exclude=['tests']),
     install_requires=[
         'crc16==0.1.1',"
OK;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"     author='K-Dimentional Tree',
     author_email='kdimentionaltree@gmail.com',
     name='pytonlib',
+    version='0.0.5',
     packages=find_packages('.', exclude=['tests']),
     install_requires=[
         'crc16==0.1.1',"
KO;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"async def test_get_masterchain_info(tonlib_client: TonlibClient):
     res = await tonlib_client.get_masterchain_info()
     assert res['@type'] == 'blocks.masterchainInfo'
 
 
 @pytest.mark.asyncio
 async def test_get_block_header(tonlib_client: TonlibClient):
     masterchain_block = await tonlib_client.get_masterchain_info()
     res = await tonlib_client.get_block_header(**masterchain_block['last'])
     assert res['@type'] == 'blocks.header'
 
 
 @pytest.mark.asyncio
 async def test_get_shards(tonlib_client: TonlibClient):
     masterchain_info = await tonlib_client.get_masterchain_info()
     shards = await tonlib_client.get_shards(master_seqno=masterchain_info['last']['seqno'])
     assert shards['@type'] == 'blocks.shards'
 
 
 @pytest.mark.asyncio
 async def test_get_transactions(tonlib_client: TonlibClient):
@@ -72,6 +78,8 @@ async def test_get_transactions(tonlib_client: TonlibClient):
 
     tx = await tonlib_client.get_transactions(**txs['transactions'][0], limit=1)
     assert tx[0]['@type'] == 'raw.transaction'
 
 
 def test_sync_code(tonlib_config, ton_keystore, ls_index):
@@ -84,6 +92,6 @@ async def main():
                               loop=loop,
                               verbosity_level=0)
         await client.init()
-        return client
 
     asyncio.run(main())"
OK;1;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"async def test_get_masterchain_info(tonlib_client: TonlibClient):
     res = await tonlib_client.get_masterchain_info()
     assert res['@type'] == 'blocks.masterchainInfo'
 
+    await tonlib_client.close()
+
 
 @pytest.mark.asyncio
 async def test_get_block_header(tonlib_client: TonlibClient):
     masterchain_block = await tonlib_client.get_masterchain_info()
     res = await tonlib_client.get_block_header(**masterchain_block['last'])
     assert res['@type'] == 'blocks.header'
 
+    await tonlib_client.close()
+
 
 @pytest.mark.asyncio
 async def test_get_shards(tonlib_client: TonlibClient):
     masterchain_info = await tonlib_client.get_masterchain_info()
     shards = await tonlib_client.get_shards(master_seqno=masterchain_info['last']['seqno'])
     assert shards['@type'] == 'blocks.shards'
 
+    await tonlib_client.close()
+
 
 @pytest.mark.asyncio
 async def test_get_transactions(tonlib_client: TonlibClient):
@@ -72,6 +78,8 @@ async def test_get_transactions(tonlib_client: TonlibClient):
 
     tx = await tonlib_client.get_transactions(**txs['transactions'][0], limit=1)
     assert tx[0]['@type'] == 'raw.transaction'
+    
+    await tonlib_client.close()
 
 
 def test_sync_code(tonlib_config, ton_keystore, ls_index):
@@ -84,6 +92,6 @@ async def main():
                               loop=loop,
                               verbosity_level=0)
         await client.init()
+        await client.close()
 
     asyncio.run(main())"
KO;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
OK;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+memoryhammer.com
\ No newline at end of file"
KO;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
OK;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+theme: jekyll-theme-minimal
\ No newline at end of file"
KO;1;abishekmuthian;memory-hammer;1d88f9e750a7570d2d52a849aec7daa0920315bb;Updated README.md with link to needgap on the problem statement and highlighted the memory_hammer.py file name.;"An always-on Anki review system.
 Click the above image for a video demo.
 
 ### Why
-To solve Anki review accumulation by making the cards available for review when its due using always-on display system.
 
 ### How
 Using e-paper display attached to a raspberry pi and memory-hammer software.
@@ -28,7 +28,7 @@ Using e-paper display attached to a raspberry pi and memory-hammer software.
 5. pip3.10 install -r requirements.txt
 
 ### Usage
-1. Edit the **Config** section of the memory_hammer.py with the IP address of your Anki Desktop and port for Anki Connect.
 2. python3.10 memory_hammer.py
 
 #### Download the Decks using Get Decks"
OK;1;abishekmuthian;memory-hammer;1d88f9e750a7570d2d52a849aec7daa0920315bb;Updated README.md with link to needgap on the problem statement and highlighted the memory_hammer.py file name.;"An always-on Anki review system.
 Click the above image for a video demo.
 
 ### Why
+To solve Anki review accumulation by making the cards available for review when its due using always-on display system. By extension addressing [Human Memory, lack of thereof](https://needgap.com/problems/41-human-memory-lack-of-thereof-psychology-neuroscience).
 
 ### How
 Using e-paper display attached to a raspberry pi and memory-hammer software.
@@ -28,7 +28,7 @@ Using e-paper display attached to a raspberry pi and memory-hammer software.
 5. pip3.10 install -r requirements.txt
 
 ### Usage
+1. Edit the **Config** section of the **memory_hammer.py** with the IP address of your Anki Desktop and port for Anki Connect.
 2. python3.10 memory_hammer.py
 
 #### Download the Decks using Get Decks"
KO;1;abishekmuthian;memory-hammer;9f099d913697c50d8fd2cb3c8311d60f0658a903;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;
OK;1;abishekmuthian;memory-hammer;9f099d913697c50d8fd2cb3c8311d60f0658a903;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+                    GNU AFFERO GENERAL PUBLIC LICENSE
+                       Version 3, 19 November 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU Affero General Public License is a free, copyleft license for
+software and other kinds of works, specifically designed to ensure
+cooperation with the community in the case of network server software.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+our General Public Licenses are intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  Developers that use our General Public Licenses protect your rights
+with two steps: (1) assert copyright on the software, and (2) offer
+you this License which gives you legal permission to copy, distribute
+and/or modify the software.
+
+  A secondary benefit of defending all users' freedom is that
+improvements made in alternate versions of the program, if they
+receive widespread use, become available for other developers to
+incorporate.  Many developers of free software are heartened and
+encouraged by the resulting cooperation.  However, in the case of
+software used on network servers, this result may fail to come about.
+The GNU General Public License permits making a modified version and
+letting the public access it on a server without ever releasing its
+source code to the public.
+
+  The GNU Affero General Public License is designed specifically to
+ensure that, in such cases, the modified source code becomes available
+to the community.  It requires the operator of a network server to
+provide the source code of the modified version running there to the
+users of that server.  Therefore, public use of a modified version, on
+a publicly accessible server, gives the public access to the source
+code of the modified version.
+
+  An older license, called the Affero General Public License and
+published by Affero, was designed to accomplish similar goals.  This is
+a different license, not a version of the Affero GPL, but Affero has
+released a new version of the Affero GPL which permits relicensing under
+this license.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  ""This License"" refers to version 3 of the GNU Affero General Public License.
+
+  ""Copyright"" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  ""The Program"" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as ""you"".  ""Licensees"" and
+""recipients"" may be individuals or organizations.
+
+  To ""modify"" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a ""modified version"" of the
+earlier work or a work ""based on"" the earlier work.
+
+  A ""covered work"" means either the unmodified Program or a work based
+on the Program.
+
+  To ""propagate"" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To ""convey"" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays ""Appropriate Legal Notices""
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The ""source code"" for a work means the preferred form of the work
+for making modifications to it.  ""Object code"" means any non-source
+form of a work.
+
+  A ""Standard Interface"" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The ""System Libraries"" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+""Major Component"", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The ""Corresponding Source"" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    ""keep intact all notices"".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+""aggregate"" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A ""User Product"" is either (1) a ""consumer product"", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, ""normally used"" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  ""Installation Information"" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  ""Additional permissions"" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered ""further
+restrictions"" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An ""entity transaction"" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A ""contributor"" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's ""contributor version"".
+
+  A contributor's ""essential patent claims"" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, ""control"" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a ""patent license"" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To ""grant"" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  ""Knowingly relying"" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is ""discriminatory"" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Remote Network Interaction; Use with the GNU General Public License.
+
+  Notwithstanding any other provision of this License, if you modify the
+Program, your modified version must prominently offer all users
+interacting with it remotely through a computer network (if your version
+supports such interaction) an opportunity to receive the Corresponding
+Source of your version by providing access to the Corresponding Source
+from a network server at no charge, through some standard or customary
+means of facilitating copying of software.  This Corresponding Source
+shall include the Corresponding Source for any work covered by version 3
+of the GNU General Public License that is incorporated pursuant to the
+following paragraph.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the work with which it is combined will remain governed by version
+3 of the GNU General Public License.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU Affero General Public License from time to time.  Such new versions
+will be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU Affero General
+Public License ""or any later version"" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU Affero General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU Affero General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ""AS IS"" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the ""copyright"" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU Affero General Public License as published
+    by the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU Affero General Public License for more details.
+
+    You should have received a copy of the GNU Affero General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If your software can interact with users remotely through a computer
+network, you should also make sure that it provides a way for users to
+get its source.  For example, if your program is a web application, its
+interface could display a ""Source"" link that leads users to an archive
+of the code.  There are many ways you could offer source, and different
+solutions will be better for different programs; see section 13 for the
+specific requirements.
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a ""copyright disclaimer"" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU AGPL, see
+<https://www.gnu.org/licenses/>."
KO;1;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
-        M, G, T to specify MiB, GiB, or TiB, respectively.
-        Values without suffix are interpreted as bytes.
         Values are rounded to the closest MiB.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
-def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
@@ -60,29 +60,84 @@ def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
-def max_memory(max_memory):
-    """"""Interpret the config parameter --max-memory.""""""
     try:
-        matches = re.match(r""(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[%MGT])?"", max_memory)
         value = float(matches[""value""])
         unit = matches[""unit""]
-        if unit == ""%"":
-            max_memory = share_of_ram(share=(value / 100.0))
-        else:
-            # convert to MiB
-            if unit is None:
-                value *= 2**-10
-                if value < 1:
-                    value = 1
-            # elif unit == ""M"":
-            #    value *= 2 ** 1
-            elif unit == ""G"":
-                value *= 2**10
-            elif unit == ""T"":
-                value *= 2**20
-            max_memory = round(value)
     except TypeError:
-        raise ValueError(f""Could not interpret --max-memory: {max_memory}"")
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
@@ -95,4 +150,4 @@ def max_memory(max_memory):
     return max_memory
 
 
-MAX_JVM_MEMORY = max_memory(arguments.max_memory)"
OK;1;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
+        K, M, G, T to specify KiB, MiB, GiB, or TiB, respectively.
         Values are rounded to the closest MiB.
+        Values without suffix are interpreted as bytes.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
+def _share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
@@ -60,29 +60,84 @@ def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
+def _parse_max_memory_string(max_memory):
+    """"""
+    Extract maximum memory value and unit from text input.
+
+    Arguments
+    ---------
+    max_memory : str
+        Input text from the config parameter --max-memory.
+
+    Returns
+    -------
+    tuple: a tuple containing
+        - value (float): Amount of memory to be allocated in a given unit.
+        - unit (str): The unit of memory.
+    """"""
     try:
+        matches = re.match(
+            r""^(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[^0-9])?$"", max_memory
+        )
         value = float(matches[""value""])
         unit = matches[""unit""]
+
+        if unit is not None and unit not in ""%KMGT"":
+            raise ValueError(
+                ""Could not interpret the memory unit from --max-memory.""
+                ""The suffix for --max-memory should be '%', 'K', 'M', 'G' or 'T'.""
+                ""For example to allocate five gigabytes of memory, use: '5G'""
+            )
+        return value, unit
     except TypeError:
+        raise ValueError(
+            f""Could not interpret --max-memory: {max_memory}.""
+            f""To allocate memory, use e.g. '5G' for five gigabytes of memory.""
+        )
+
+
+def _get_max_memory(max_memory):
+    """"""
+    Interpret the config parameter --max-memory.
+
+    Arguments
+    ---------
+
+    max_memory : str
+        Memory limit for the JVM running R5.
+
+        Use % as a suffix to specify a share of total RAM;
+        K, M, G, T suffix specify KiB, MiB, GiB, or TiB, respectively.
+        Values are rounded to the closest MiB.
+        Values without suffix are interpreted as bytes.
+
+    Returns
+    -------
+    float
+        Maximum amount of memory allocated for R5 in MiB.
+    """"""
+
+    value, unit = _parse_max_memory_string(max_memory)
+
+    if unit == ""%"":
+        max_memory = _share_of_ram(share=(value / 100.0))
+    else:
+        # convert to MiB
+        if unit is None:
+            value *= 2**-20
+        elif unit == ""K"":
+            value *= 2**-10
+        elif unit == ""M"":
+            value *= 2**1
+        elif unit == ""G"":
+            value *= 2**10
+        elif unit == ""T"":
+            value *= 2**20
+
+        if value < 1:
+            value = 1
+
+        max_memory = round(value)
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
@@ -95,4 +150,4 @@ def max_memory(max_memory):
     return max_memory
 
 
+MAX_JVM_MEMORY = _get_max_memory(arguments.max_memory)"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.identity_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -84,11 +84,11 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.random_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -140,12 +140,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.level_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            randomness=self.randomness,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -204,12 +206,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.circular_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            randomness=self.randomness,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()"
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.identity_hv(
+                self.num_embeddings, self.embedding_dim, **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -84,11 +84,11 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.random_hv(
+                self.num_embeddings, self.embedding_dim, **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -140,12 +140,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.level_hv(
+                self.num_embeddings,
+                self.embedding_dim,
+                randomness=self.randomness,
+                **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -204,12 +206,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.circular_hv(
+                self.num_embeddings,
+                self.embedding_dim,
+                randomness=self.randomness,
+                **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
-    def __init__(self, threshold=0.0):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
@@ -82,7 +82,7 @@ def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
-            raise IndexError()
 
         return index
 
@@ -241,7 +241,7 @@ def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
-        """"""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
@@ -273,7 +273,7 @@ def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
-        value = functional.multiset(input, dim=-2)
         return cls(value, size=input.size(-2))
 
 
@@ -434,7 +434,7 @@ def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
-        return cls(value, size=input.size(-2))
 
 
 class Sequence:
@@ -663,7 +663,9 @@ def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
-            self.value = torch.zeros(dim_or_input, dtype=dtype, device=device)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
@@ -766,7 +768,7 @@ def clear(self) -> None:
             >>> DS.clear()
 
         """"""
-        self.value.fill_(0.0)
         self.size = 0
 
     @classmethod"
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
+    def __init__(self, threshold=0.5):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
@@ -82,7 +82,7 @@ def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
+            raise IndexError(""No elements in memory"")
 
         return index
 
@@ -241,7 +241,7 @@ def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
+        r""""""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
@@ -273,7 +273,7 @@ def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
+        value = functional.multiset(input)
         return cls(value, size=input.size(-2))
 
 
@@ -434,7 +434,7 @@ def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
+        return cls(value, size=keys.size(-2))
 
 
 class Sequence:
@@ -663,7 +663,9 @@ def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
+            self.value = functional.identity_hv(
+                1, dim_or_input, dtype=dtype, device=device
+            ).squeeze(0)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
@@ -766,7 +768,7 @@ def clear(self) -> None:
             >>> DS.clear()
 
         """"""
+        self.value.fill_(1.0)
         self.size = 0
 
     @classmethod"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestDistinctSequence:
+    def test_creation_dim(self):
+        S = structures.DistinctSequence(10000)
+        assert torch.equal(S.value, torch.ones(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        S = structures.DistinctSequence(hv[0])
+        assert torch.equal(S.value, hv[0])
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_append(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+
+    def test_appendleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.appendleft(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+
+    def test_pop(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[1])
+        S.pop(hv[1])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.pop(hv[0])
+        S.append(hv[2])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+        S.append(hv[3])
+        S.pop(hv[3])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+
+    def test_popleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.appendleft(hv[0])
+        S.appendleft(hv[1])
+        S.popleft(hv[1])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.popleft(hv[0])
+        S.appendleft(hv[2])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+        S.appendleft(hv[3])
+        S.popleft(hv[3])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+
+    def test_replace(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.replace(0, hv[0], hv[1])
+        assert functional.cosine_similarity(S.value, hv)[1] > 0.5
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        assert len(S) == 4
+        S.pop(hv[0])
+        S.pop(hv[0])
+        S.pop(hv[0])
+        assert len(S) == 1
+        S.pop(hv[0])
+        assert len(S) == 0
+        S.append(hv[0])
+        assert len(S) == 1
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        assert len(S) == 4
+        S.clear()
+        assert len(S) == 0"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+seed1 = 2147483643
+letters = list(string.ascii_lowercase)
+
+
+class TestFSA:
+    def test_creation_dim(self):
+        F = structures.FiniteStateAutomata(10000)
+        assert torch.equal(F.value, torch.zeros(10000))
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_transition(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        actions = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], actions[1], actions[2])
+        assert torch.equal(
+            F.value,
+            torch.tensor([1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0]),
+        )
+        F.add_transition(tokens[1], actions[1], actions[3])
+        assert torch.equal(
+            F.value, torch.tensor([0.0, 0.0, -2.0, 2.0, 0.0, 2.0, 0.0, -2.0, -2.0, 0.0])
+        )
+        F.add_transition(tokens[2], actions[1], actions[3])
+        assert torch.equal(
+            F.value,
+            torch.tensor([1.0, 1.0, -3.0, 1.0, 1.0, 3.0, -1.0, -1.0, -1.0, 1.0]),
+        )
+
+    def test_transition(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        states = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], states[1], states[2])
+        F.add_transition(tokens[1], states[1], states[3])
+        F.add_transition(tokens[2], states[1], states[5])
+
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[0]), states)
+            ).item()
+            == 2
+        )
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[1]), states)
+            ).item()
+            == 3
+        )
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[2]), states)
+            ).item()
+            == 5
+        )
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        states = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], states[1], states[2])
+        F.add_transition(tokens[1], states[1], states[3])
+        F.add_transition(tokens[2], states[1], states[5])
+
+        F.clear()
+        assert torch.equal(
+            F.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestGraph:
+    def test_creation_dim(self):
+        G = structures.Graph(10000, directed=True)
+        assert torch.equal(G.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        g = functional.bind(hv[0], hv[1])
+        G = structures.Graph(g)
+        assert torch.equal(G.value, g)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_edge(self):
+        G = structures.Graph(8)
+        hv = torch.tensor(
+            [
+                [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
+                [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
+                [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
+                [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
+            ]
+        )
+
+        G.add_edge(hv[0], hv[1])
+        assert torch.equal(
+            G.value, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        G.add_edge(hv[2], hv[3])
+        assert torch.equal(
+            G.value, torch.tensor([-2.0, -2.0, 0.0, 2.0, -2.0, 0.0, 2.0, -2.0])
+        )
+
+        GD = structures.Graph(8, directed=True)
+
+        GD.add_edge(hv[0], hv[1])
+        assert torch.equal(
+            GD.value, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        GD.add_edge(hv[2], hv[3])
+        assert torch.equal(
+            GD.value, torch.tensor([0.0, 0.0, 0.0, -2.0, 0.0, -2.0, 2.0, -2.0])
+        )
+
+    def test_encode_edge(self):
+        G = structures.Graph(8)
+        hv = torch.tensor(
+            [
+                [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
+                [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
+                [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
+                [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
+            ]
+        )
+
+        e1 = G.encode_edge(hv[0], hv[1])
+        assert torch.equal(
+            e1, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        e2 = G.encode_edge(hv[2], hv[3])
+        assert torch.equal(
+            e2, torch.tensor([-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0])
+        )
+
+        GD = structures.Graph(8, directed=True)
+
+        e1 = GD.encode_edge(hv[0], hv[1])
+        assert torch.equal(
+            e1, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        e2 = GD.encode_edge(hv[2], hv[3])
+        print(e2)
+        assert torch.equal(
+            e2, torch.tensor([1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0])
+        )
+
+    def test_node_neighbors(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(10, 10000, generator=generator)
+        G = structures.Graph(10000, directed=True)
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(G.node_neighbors(hv[1]), hv)
+            ).item()
+            == 2
+        )
+        assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[1] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[1] < 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[0] < 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[0] < 0.5
+
+        G1 = structures.Graph(10000, directed=False)
+
+        G1.add_edge(hv[0], hv[1])
+        G1.add_edge(hv[0], hv[2])
+        G1.add_edge(hv[1], hv[2])
+        assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[0] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[1] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[0] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[1] > 0.5
+
+    def test_contains(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(4, 8, generator=generator)
+        G = structures.Graph(8)
+
+        e1 = G.encode_edge(hv[0], hv[1])
+        e2 = G.encode_edge(hv[0], hv[2])
+        e3 = G.encode_edge(hv[2], hv[3])
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        assert G.contains(e1) > torch.tensor(0.6)
+        assert G.contains(e2) > torch.tensor([0.6])
+        assert G.contains(e3) < torch.tensor(0.6)
+
+        GD = structures.Graph(8, directed=True)
+
+        ee1 = GD.encode_edge(hv[0], hv[1])
+        ee2 = GD.encode_edge(hv[0], hv[2])
+        ee3 = GD.encode_edge(hv[2], hv[3])
+        ee4 = GD.encode_edge(hv[1], hv[0])
+
+        GD.add_edge(hv[0], hv[1])
+        GD.add_edge(hv[0], hv[2])
+        GD.add_edge(hv[3], hv[1])
+
+        assert GD.contains(ee1) > torch.tensor(0.6)
+        assert GD.contains(ee2) > torch.tensor(0.6)
+        assert GD.contains(ee3) < torch.tensor(0.6)
+        assert GD.contains(ee4) < torch.tensor(0.6)
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(4, 8, generator=generator)
+        G = structures.Graph(8)
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        G.clear()
+
+        assert torch.equal(
+            G.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed_key = 2147483644
+seed_value = 2147483622
+letters = list(string.ascii_lowercase)
+
+
+class TestHashtable:
+    def test_creation_dim(self):
+        H = structures.HashTable(10000)
+        assert torch.equal(H.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        hash_v1 = functional.bind(keys_hv[0], values_hv[0])
+        hash_v2 = functional.bind(keys_hv[1], values_hv[1])
+        hasht = functional.bundle(hash_v1, hash_v2)
+
+        H = structures.HashTable(hasht)
+        assert torch.equal(H.value, hasht)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed_key)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed_key)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+
+    def test_remove(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+
+        H.remove(keys_hv[0], values_hv[0])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) < 0.2)[0],
+            torch.tensor(True),
+        )
+
+    def test_get(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[0]), values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[1]), values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[2]), values_hv) > 0.5)[2],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            torch.all(
+                (functional.cosine_similarity(H.get(values_hv[2]), values_hv) > 0.5)
+                == False
+            ),
+            torch.tensor(True),
+        )
+
+    def test_getitem(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[2]], values_hv) > 0.5)[2],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            torch.all(
+                (functional.cosine_similarity(H[values_hv[2]], values_hv) > 0.5)
+                == False
+            ),
+            torch.tensor(True),
+        )
+
+    def test_replace(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        H.replace(keys_hv[0], values_hv[0], values_hv[1])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+
+    def test_length(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert len(H) == 3
+        H.remove(keys_hv[0], values_hv[0])
+
+        assert len(H) == 2
+
+    def test_clear(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+        assert len(H) == 3
+        H.clear()
+        assert len(H) == 0
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(False),
+        )
+        H.add(keys_hv[0], values_hv[0])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+
+    def test_from_tensor(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(2, 3, generator=generator_key)
+
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(2, 3, generator=generator_value)
+
+        H = structures.HashTable.from_tensors(keys_hv, values_hv)
+        assert torch.equal(H.value, torch.tensor([2.0, 0.0, 0.0]))"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestMemory:
+    def test_creation(self):
+        M = structures.Memory()
+
+        assert M.keys == []
+        assert M.values == []
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert torch.equal(M.keys[0], keys_hv[0])
+        assert torch.equal(M.keys[1], keys_hv[1])
+        assert torch.equal(M.keys[2], keys_hv[2])
+        assert M.values[0] == letters[0]
+        assert M.values[1] == letters[1]
+        assert M.values[2] == letters[2]
+
+    def test_index(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert M.index(keys_hv[0]) == 0
+        assert M.index(keys_hv[1]) == 1
+        assert M.index(keys_hv[2]) == 2
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        del M[keys_hv[0]]
+
+        assert len(M) == 2
+
+        M.add(keys_hv[0], letters[0])
+        assert len(M) == 3
+
+    def test_getitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+    def test_setitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+        M[keys_hv[0]] = letters[3]
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[3]
+
+    def test_delitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+        del M[keys_hv[0]]
+        try:
+            M[keys_hv[0]]
+        except IndexError:
+            assert True
+
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+        assert len(M) == 2"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestMultiset:
+    def test_creation_dim(self):
+        M = structures.Multiset(10000)
+        assert torch.equal(M.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+        multiset = functional.multiset(keys_hv)
+
+        M = structures.Multiset(multiset)
+        assert torch.equal(M.value, multiset)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        assert torch.equal(M.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        M.add(keys_hv[1])
+        assert torch.equal(M.value, torch.tensor([2.0, 0.0, 0.0, 2.0]))
+
+        M.add(keys_hv[2])
+        assert torch.equal(M.value, torch.tensor([3.0, 1.0, 1.0, 1.0]))
+
+    def test_remove(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        assert M.contains(keys_hv[0]) > torch.tensor([0.5])
+
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        assert M.contains(keys_hv[1]) > torch.tensor([0.5])
+        assert M.remove(keys_hv[0]) is None
+
+    def test_contains(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.7])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        M.remove(keys_hv[1])
+        assert M.contains(keys_hv[1]) < torch.tensor([0.1])
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        assert len(M) == 3
+        M.remove(keys_hv[0])
+
+        assert len(M) == 2
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        M.clear()
+
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        assert M.contains(keys_hv[1]) < torch.tensor([0.1])
+
+        M.add(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+
+    def test_from_ngrams(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 3, generator=generator)
+        M = structures.Multiset.from_ngrams(keys_hv)
+
+        assert torch.equal(M.value, torch.tensor([0.0, 4.0, 0.0]))
+
+    def test_from_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset.from_tensor(keys_hv)
+        assert torch.equal(M.value, torch.tensor([2.0, 10.0, 4.0, 2.0]))"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestSequence:
+    def test_creation_dim(self):
+        S = structures.Sequence(10000)
+        assert torch.equal(S.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        seq = functional.bundle(hv[1], functional.permute(hv[0], shifts=1))
+
+        S = structures.Sequence(seq)
+        assert torch.equal(S.value, seq)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_append(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.append(hv[0])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.append(hv[1])
+        assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
+
+        S.append(hv[2])
+        assert torch.equal(S.value, torch.tensor([3.0, 3.0, 3.0, -3.0]))
+
+    def test_appendleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.appendleft(hv[0])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.appendleft(hv[1])
+        assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
+
+        S.appendleft(hv[2])
+        assert torch.equal(S.value, torch.tensor([3.0, -1.0, 3.0, 1.0]))
+
+    def test_pop(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        S.pop(hv[2])
+        assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
+
+        S.pop(hv[1])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.pop(hv[0])
+        assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
+
+    def test_popleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.appendleft(hv[0])
+        S.appendleft(hv[1])
+        S.appendleft(hv[2])
+
+        S.popleft(hv[2])
+        assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
+
+        S.popleft(hv[1])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.popleft(hv[0])
+        assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
+
+    def test_replace(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.Sequence(10000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+        S.append(hv[3])
+        S.append(hv[4])
+        S.append(hv[5])
+        S.append(hv[6])
+
+        assert functional.cosine_similarity(S[2], hv)[2] > 0.35
+        S.replace(2, hv[2], hv[6])
+        assert functional.cosine_similarity(S[2], hv)[2] < 0.35
+        assert functional.cosine_similarity(S[2], hv)[6] > 0.35
+
+        hv1 = functional.random_hv(10, 10000)
+        S2 = structures.Sequence.from_tensor(hv1)
+        assert functional.cosine_similarity(S2[2], hv1)[2] > 0.3
+        S2.replace(2, hv1[2], hv1[6])
+        assert functional.cosine_similarity(S2[2], hv1)[2] < 0.3
+        assert functional.cosine_similarity(S2[2], hv1)[6] > 0.3
+
+    def test_concat(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        S2 = structures.Sequence(1000)
+        S2.append(hv[0])
+        S2.append(hv[1])
+        S2.append(hv[2])
+
+        assert len(S) == 3
+        assert len(S2) == 3
+        S = S.concat(S2)
+        assert len(S) == 6
+
+        assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(S[2], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(S[4], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 2
+
+        SS = structures.Sequence(1000)
+
+        SS.appendleft(hv[0])
+        SS.appendleft(hv[1])
+        SS.appendleft(hv[2])
+
+        SS2 = structures.Sequence(1000)
+        SS2.appendleft(hv[0])
+        SS2.appendleft(hv[1])
+        SS2.appendleft(hv[2])
+
+        SS = SS.concat(SS2)
+
+        assert torch.argmax(functional.cosine_similarity(SS[0], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(SS[1], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(SS[2], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(SS[3], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(SS[4], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(SS[5], hv)).item() == 0
+
+    def test_getitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert len(S) == 3
+        S.pop(hv[2])
+
+        assert len(S) == 2
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert len(S) == 3
+        S.clear()
+        assert len(S) == 0
+        S.append(hv[0])
+        assert len(S) == 1
+
+    def test_from_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.Sequence.from_tensor(hv)
+
+        assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 3
+        assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 5
+        assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestTree:
+    def test_creation_dim(self):
+        T = structures.Tree(10000)
+        assert torch.equal(T.value, torch.zeros(10000))
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_leaf(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        T = structures.Tree(10000)
+        T.add_leaf(hv[0], [""l"", ""l""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
+            ).item()
+            == 0
+        )
+        T.add_leaf(hv[1], [""l"", ""r""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
+            ).item()
+            == 1
+        )
+
+    def test_get_leaf(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        T = structures.Tree(10000)
+        T.add_leaf(hv[0], [""l"", ""l""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
+            ).item()
+            == 0
+        )
+        T.add_leaf(hv[1], [""l"", ""r""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
+            ).item()
+            == 1
+        )
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 10, generator=generator)
+        T = structures.Tree(10)
+
+        T.add_leaf(hv[0], [""l"", ""l""])
+        T.add_leaf(hv[1], [""l"", ""r""])
+
+        T.clear()
+        assert torch.equal(
+            T.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
-  in_original: Indicates whether a parameter is expected to be present in the
-    saved checkpoint. Will raise an error if the parameter was expected,
-    but is not present.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
@@ -66,14 +67,14 @@ class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
-  in_original: bool = True
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
-    return set(x.keys()) >= {'original_key', 'in_original', 'value_fn'}
   return False
 
 
@@ -86,8 +87,10 @@ def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
-def apply_transformations(original_tree: PyTree, transformations: PyTree,
-                          new_tree: PyTree) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
@@ -162,6 +165,9 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
@@ -187,8 +193,15 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
-        if not transform.in_original:
-          continue  # do not override existing value of key in new
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
@@ -199,8 +212,7 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
-                f'Transformation key {original_key} not found in origin tree (in_original=True)'
-            )
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
@@ -209,9 +221,11 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
-      # carry over directly from original, otherwise use value from new
-      if key in original:
-        new[key] = original[key]
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
+  use_fallback: if True, takes the value from the fallback tree. If
+    `default_to_original=True` in `apply_transformations`, the fallback tree is
+    `new_tree`. If `default_to_original=False` in `apply_transformations`, the
+    fallback tree is `original_tree`.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
@@ -66,14 +67,14 @@ class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
+  use_fallback: bool = False
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
+    return set(x.keys()) >= {'original_key', 'value_fn', 'multi_value_fn'}
   return False
 
 
@@ -86,8 +87,10 @@ def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
+def apply_transformations(original_tree: PyTree,
+                          transformations: PyTree,
+                          new_tree: PyTree,
+                          default_to_original: Optional[bool] = True) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
@@ -162,6 +165,9 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
+    default_to_original: If True, the values of keys unspecified in
+      transformations will be taken from `original_tree`. If False, they will be
+      taken from `new_tree`.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
@@ -187,8 +193,15 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
+        if transform.use_fallback:
+          if not default_to_original:
+            if key not in original:
+              raise ValueError(
+                  f'{key} not found in origin tree (`use_fallback` requested).'
+              )
+            new[key] = original[key]
+          # else simply retain new[key]
+          continue
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
@@ -199,8 +212,7 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
+                f'Transformation key {original_key} not found in origin tree.')
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
@@ -209,9 +221,11 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
+      if default_to_original:
+        # carry over directly from original, otherwise use value from new
+        if key in original:
+          new[key] = original[key]
+      # if default_to_new, do not carry over key from original
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
-        'f': Transform(in_original=False),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
@@ -140,6 +140,60 @@ def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
   def test_regex(self):
     original = {
         'a1': 1,
@@ -268,7 +322,7 @@ class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
-        d=Transform(in_original=False),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
@@ -340,8 +394,8 @@ def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
-        # LargeModel layer 0 is a newly inserted layer, thus in_original=False.
-        r'(.*)Dense_0(.*)': Transform(in_original=False),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
@@ -371,6 +425,50 @@ def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
 
 if __name__ == '__main__':
   absltest.main()"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
+        'f': Transform(use_fallback=True),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
@@ -140,6 +140,60 @@ def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
+  def test_default_new(self):
+    transforms = {
+        'a': Transform(use_fallback=True),  # use value from original
+        # implicit drop ""b""
+        # implicit retain ""c/a"", ""c/a""
+        'b1': Transform(original_key='b'),
+        # implicit add ""f"" and ""g""
+    }
+    new = {
+        'a': ...,
+        'c': {
+            'a': 10,
+            'e': 11,
+        },
+        'b1': ...,
+        'f': None,
+        'g': 2,
+    }
+    expected = {
+        'a': 0,
+        'c': {
+            'a': 10,
+            'e': 11,
+        },
+        'b1': 1,
+        'f': None,
+        'g': 2,
+    }
+    self.assertDictEqual(
+        expected,
+        apply_transformations(
+            self.original, transforms, new, default_to_original=False))
+
+  def test_missing_key_default(self):
+    transforms = {'f': Transform(use_fallback=True)}
+    new = {
+        'a': 2,
+        'b': 3,
+        'c': {
+            'a': 7,
+            'e': 8,
+        },
+        'f': 20,
+    }
+    with self.assertRaises(ValueError):
+      apply_transformations(
+          self.original, transforms, new, default_to_original=False)
+
+    expected = {'a': 0, 'b': 1, 'c': {'a': 2, 'e': 3}, 'f': 20}
+    self.assertDictEqual(
+        expected,
+        apply_transformations(
+            self.original, transforms, new, default_to_original=True))
+
   def test_regex(self):
     original = {
         'a1': 1,
@@ -268,7 +322,7 @@ class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
+        d=Transform(use_fallback=True),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
@@ -340,8 +394,8 @@ def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
+        # LargeModel layer 0 is a newly inserted layer, thus use_fallback=True.
+        r'(.*)Dense_0(.*)': Transform(use_fallback=True),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
@@ -371,6 +425,50 @@ def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
+  def test_flax_train_state_default_new(self):
+
+    class Model(nn.Module):
+
+      @nn.compact
+      def __call__(self, x):
+        x = x.reshape((x.shape[0], -1))  # flatten
+        x = nn.Dense(features=16)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=8)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=8)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=4)(x)
+        return x
+
+    old_state = test_utils.init_flax_model(Model())
+    new_state = test_utils.init_flax_model(Model())
+
+    transformations = {
+        # values default to new_state, use_fallback=True instructs the Transform
+        # to fall back on old_state for this key.
+        r'(.*)Dense_1(.*)': Transform(use_fallback=True),
+    }
+    restored_state = apply_transformations(
+        old_state, transformations, new_state, default_to_original=False)
+
+    # Construct expected tree
+    old_state_dict = traverse_util.flatten_dict(
+        serialization.to_state_dict(old_state), keep_empty_nodes=True, sep='/')
+    new_state_dict = traverse_util.flatten_dict(
+        serialization.to_state_dict(new_state), keep_empty_nodes=True, sep='/')
+    expected_state_dict = {}
+    for k, v in new_state_dict.items():
+      if 'Dense_1' in k:
+        expected_state_dict[k] = old_state_dict[k]
+      else:
+        expected_state_dict[k] = v
+
+    expected_state = serialization.from_state_dict(
+        new_state, traverse_util.unflatten_dict(expected_state_dict, sep='/'))
+
+    test_utils.assert_tree_equal(self, expected_state, restored_state)
+
 
 if __name__ == '__main__':
   absltest.main()"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
+        'pyyaml',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
KO;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"Jupyter Notebook will be available on port 3100 (http://localhost:3100).
 
 ## Examples
 
 * Connecting to the first LiteServer in mainnet config:
 ```python
 import requests
@@ -48,7 +50,7 @@ client = TonlibClient(ls_index=0, # choose LiteServer index to connect
                       loop=loop)
 
 # init tonlibjson
-await client.init(max_restarts=None)
 ```
 
 * Reading blocks info:
@@ -83,9 +85,22 @@ async def main():
                           loop=loop)
     
     # init tonlibjson
-    await client.init(max_restarts=None)
 
 
 if __name__ == '__main__':
     asyncio.run(main())
-```
\ No newline at end of file"
OK;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"Jupyter Notebook will be available on port 3100 (http://localhost:3100).
 
 ## Examples
 
+We recommend to use IPython or Jupyter Notebook for prototyping because they allow to run `async` code. An example of running `async` code from script could be found in the end of this section.
+
 * Connecting to the first LiteServer in mainnet config:
 ```python
 import requests
@@ -48,7 +50,7 @@ client = TonlibClient(ls_index=0, # choose LiteServer index to connect
                       loop=loop)
 
 # init tonlibjson
+await client.init()
 ```
 
 * Reading blocks info:
@@ -83,9 +85,22 @@ async def main():
                           loop=loop)
     
     # init tonlibjson
+    await client.init()
+    
+    # reading masterchain info
+    masterchain_info = await client.get_masterchain_info()
+
+    # closing session
+    await client.close()
 
 
 if __name__ == '__main__':
     asyncio.run(main())
\ No newline at end of file
+```
+
+## Running tests
+
+To run tests in *asyncio* mode use the following command: 
+```bash
+PYTHONPATH=./ pytest --asyncio-mode=strict tests/
+```"
KO;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""ed3cdccf"",
    ""metadata"": {},
    ""outputs"": [],
@@ -39,7 +39,7 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""7e898620"",
    ""metadata"": {},
    ""outputs"": [],
@@ -58,18 +58,33 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""22e67d8d"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""loop = asyncio.get_running_loop()\n"",
     ""client = TonlibClient(ls_index=0, # choose LiteServer to connect\n"",
     ""                      config=ton_config,\n"",
     ""                      keystore='/tmp/ton_keystore',\n"",
     ""                      loop=loop)\n"",
     ""\n"",
-    ""await client.init(max_restarts=None)""
    ]
   },
   {
@@ -82,10 +97,35 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""a0c3eaf0"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""masterchain_info = await client.get_masterchain_info()\n"",
     ""masterchain_info""
@@ -101,10 +141,51 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""3da9bd64"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""block_header = await client.get_block_header(**masterchain_info['last'])\n"",
     ""block_header""
@@ -120,10 +201,28 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""e5470d37"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""shards = await client.get_shards(master_seqno=masterchain_info['last']['seqno'])\n"",
     ""shards""
@@ -139,10 +238,19 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""d09bc06b"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""txs = await client.get_block_transactions(**masterchain_info['last'], count=10)\n"",
     ""\n"",
@@ -160,29 +268,78 @@
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""e8397a5b"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""tx = txs['transactions'][0]\n"",
     ""tx""
    ]
   },
   {
    ""cell_type"": ""code"",
-   ""execution_count"": null,
    ""id"": ""b65b4586"",
    ""metadata"": {},
-   ""outputs"": [],
    ""source"": [
     ""await client.get_transactions(**tx, limit=1)""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": null,
-   ""id"": ""af4da2e1"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": []"
OK;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 1,
    ""id"": ""ed3cdccf"",
    ""metadata"": {},
    ""outputs"": [],
@@ -39,7 +39,7 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 2,
    ""id"": ""7e898620"",
    ""metadata"": {},
    ""outputs"": [],
@@ -58,18 +58,33 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 3,
    ""id"": ""22e67d8d"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""[ 4][t 0][2022-05-06 20:36:00.314293800][Client.cpp:78][&tonlib_requests]\tBegin to wait for updates with timeout 1.000000\u001b[0m\n"",
+      ""[ 4][t 1][2022-05-06 20:36:00.320407500][TonlibClient.cpp:1477][!Tonlib][&tonlib_query]\tTonlib got query [id:1] setLogVerbosityLevel {\n"",
+      ""  new_verbosity_level = 0\n"",
+      ""}\u001b[0m\n"",
+      ""[ 4][t 1][2022-05-06 20:36:00.321866900][TonlibClient.cpp:1516][!Tonlib][&tonlib_query]\tTonlib got static query setLogVerbosityLevel {\n"",
+      ""  new_verbosity_level = 0\n"",
+      ""}\u001b[0m\n"",
+      ""2022-05-06 20:36:00,331 client          TonLib #000 inited successfully\n""
+     ]
+    }
+   ],
    ""source"": [
     ""loop = asyncio.get_running_loop()\n"",
     ""client = TonlibClient(ls_index=0, # choose LiteServer to connect\n"",
     ""                      config=ton_config,\n"",
     ""                      keystore='/tmp/ton_keystore',\n"",
     ""                      loop=loop)\n"",
     ""\n"",
+    ""await client.init()""
    ]
   },
   {
@@ -82,10 +97,35 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 4,
    ""id"": ""a0c3eaf0"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.masterchainInfo',\n"",
+       "" 'last': {'@type': 'ton.blockIdExt',\n"",
+       ""  'workchain': -1,\n"",
+       ""  'shard': '-9223372036854775808',\n"",
+       ""  'seqno': 20361208,\n"",
+       ""  'root_hash': 'FDEyd4nZgolZ9ryWCK32u/5MsPCDTo6qJHT3XGFF8XA=',\n"",
+       ""  'file_hash': 'Jn9q+fqs6dF5wc+3DzjCsQdGL4g8IfIr9RpYnS1QwsA='},\n"",
+       "" 'state_root_hash': 'xQvdczD+gdaTamtLDIo9rwxLG3w4fchSiEcnaPsHuN0=',\n"",
+       "" 'init': {'@type': 'ton.blockIdExt',\n"",
+       ""  'workchain': -1,\n"",
+       ""  'shard': '0',\n"",
+       ""  'seqno': 0,\n"",
+       ""  'root_hash': 'F6OpKZKqvqeFp6CQmFomXNMfMj2EnaUSOXN+Mh+wVWk=',\n"",
+       ""  'file_hash': 'XplPz01CXAps5qeSWUtxcyBfdAo5zVb1N979KLSKD24='},\n"",
+       "" '@extra': '1651869370.3371992:0:0.507966611324214'}""
+      ]
+     },
+     ""execution_count"": 4,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""masterchain_info = await client.get_masterchain_info()\n"",
     ""masterchain_info""
@@ -101,10 +141,51 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 5,
    ""id"": ""3da9bd64"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.header',\n"",
+       "" 'id': {'@type': 'ton.blockIdExt',\n"",
+       ""  'workchain': -1,\n"",
+       ""  'shard': '-9223372036854775808',\n"",
+       ""  'seqno': 20361208,\n"",
+       ""  'root_hash': 'FDEyd4nZgolZ9ryWCK32u/5MsPCDTo6qJHT3XGFF8XA=',\n"",
+       ""  'file_hash': 'Jn9q+fqs6dF5wc+3DzjCsQdGL4g8IfIr9RpYnS1QwsA='},\n"",
+       "" 'global_id': -239,\n"",
+       "" 'version': 0,\n"",
+       "" 'flags': 1,\n"",
+       "" 'after_merge': False,\n"",
+       "" 'after_split': False,\n"",
+       "" 'before_split': False,\n"",
+       "" 'want_merge': True,\n"",
+       "" 'want_split': False,\n"",
+       "" 'validator_list_hash_short': 122883420,\n"",
+       "" 'catchain_seqno': 306940,\n"",
+       "" 'min_ref_mc_seqno': 20361205,\n"",
+       "" 'is_key_block': False,\n"",
+       "" 'prev_key_block_seqno': 20351399,\n"",
+       "" 'start_lt': '27699743000000',\n"",
+       "" 'end_lt': '27699743000004',\n"",
+       "" 'gen_utime': 1651869354,\n"",
+       "" 'vert_seqno': 1,\n"",
+       "" 'prev_blocks': [{'@type': 'ton.blockIdExt',\n"",
+       ""   'workchain': -1,\n"",
+       ""   'shard': '-9223372036854775808',\n"",
+       ""   'seqno': 20361207,\n"",
+       ""   'root_hash': 'AM+LcJOyfGSm1dpbaQDXpOgY7bcMcEDawtCAUwcJLJw=',\n"",
+       ""   'file_hash': 'AyW+wJSoJZfbBB3Y1JrgJ19SKlpC7WvitkeVk/yplYI='}],\n"",
+       "" '@extra': '1651869370.6080866:0:0.17246900748102634'}""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""block_header = await client.get_block_header(**masterchain_info['last'])\n"",
     ""block_header""
@@ -120,10 +201,28 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 6,
    ""id"": ""e5470d37"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.shards',\n"",
+       "" 'shards': [{'@type': 'ton.blockIdExt',\n"",
+       ""   'workchain': 0,\n"",
+       ""   'shard': '-9223372036854775808',\n"",
+       ""   'seqno': 25532997,\n"",
+       ""   'root_hash': 'JI8s3H5c7g4Vexlezl6V+xhvKYjIZKsA8ItgZHJdtQU=',\n"",
+       ""   'file_hash': 'cMglRNmDveIczi8SjzIsBGHDT0baUa+bwe1ba5Qh7CI='}],\n"",
+       "" '@extra': '1651869370.8817048:0:0.3841969484094021'}""
+      ]
+     },
+     ""execution_count"": 6,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""shards = await client.get_shards(master_seqno=masterchain_info['last']['seqno'])\n"",
     ""shards""
@@ -139,10 +238,19 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 7,
    ""id"": ""d09bc06b"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Is incomplete: False\n"",
+      ""Num txs: 5\n""
+     ]
+    }
+   ],
    ""source"": [
     ""txs = await client.get_block_transactions(**masterchain_info['last'], count=10)\n"",
     ""\n"",
@@ -160,29 +268,78 @@
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 8,
    ""id"": ""e8397a5b"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""{'@type': 'blocks.shortTxId',\n"",
+       "" 'mode': 135,\n"",
+       "" 'account': '-1:3333333333333333333333333333333333333333333333333333333333333333',\n"",
+       "" 'lt': '27699743000001',\n"",
+       "" 'hash': 'jOIVA+yIkoPywpjrFj9VYTohez2au0ooow5uPLYWFAc='}""
+      ]
+     },
+     ""execution_count"": 8,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""tx = txs['transactions'][0]\n"",
     ""tx""
    ]
   },
   {
    ""cell_type"": ""code"",
+   ""execution_count"": 9,
    ""id"": ""b65b4586"",
    ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""[{'@type': 'raw.transaction',\n"",
+       ""  'address': {'@type': 'accountAddress',\n"",
+       ""   'account_address': 'Ef8zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzM0vF'},\n"",
+       ""  'utime': 1651869357,\n"",
+       ""  'data': 'te6cckECBwEAAYkAA69zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzAAAZMVlmyAIoy6aZP4NZkz72hGeQDNHm2NpTPKWaFnx4LB9YizUu2gAAGTFZZsgBYnWGrQABQIAQIDAQGgBACCco+Ec6Mj7xtio/Duyx50cd7VDh/dyMx2l/MOpAqkYJIU5/6pVPLSgialzCVl4c6EkwC2lWh3d1HX8y6moQPwkl4CDwQJKU/jnFgRBQYAq2n+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAE/zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzSlP45xAAAADJiss2QAMTrDVpAAJ5CYUwQ6+AAAAAAAAAAAGQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFvAAAAAAAAAAAAAAAABLUUtpEnlC4z33SeGHxRhIq/htUa7i3D8ghbwxhQTn44EwPyXiw==',\n"",
+       ""  'transaction_id': {'@type': 'internal.transactionId',\n"",
+       ""   'lt': '27699744000002',\n"",
+       ""   'hash': 'AnL1aJnySYYOG+Godrlio8NhYRZO3avoLzfkdXpnnOo='},\n"",
+       ""  'fee': '0',\n"",
+       ""  'storage_fee': '0',\n"",
+       ""  'other_fee': '0',\n"",
+       ""  'in_msg': {'@type': 'raw.message',\n"",
+       ""   'source': 'Ef8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAU',\n"",
+       ""   'destination': 'Ef8zMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzMzM0vF',\n"",
+       ""   'value': '2772405873',\n"",
+       ""   'fwd_fee': '0',\n"",
+       ""   'ihr_fee': '0',\n"",
+       ""   'created_lt': '27699744000000',\n"",
+       ""   'body_hash': 'lqKW0iTyhcZ77pPDD4owkVfw2qNdxbh+QQt4YwoJz8c=',\n"",
+       ""   'msg_data': {'@type': 'msg.dataRaw',\n"",
+       ""    'body': 'te6cckEBAQEAAgAAAEysuc0=',\n"",
+       ""    'init_state': ''},\n"",
+       ""   'message': ''},\n"",
+       ""  'out_msgs': []}]""
+      ]
+     },
+     ""execution_count"": 9,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
    ""source"": [
     ""await client.get_transactions(**tx, limit=1)""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": null,
+   ""id"": ""68dc43a6"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": []"
KO;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def local_config(self):
         local['liteservers'] = [local['liteservers'][self.ls_index]]
         return local
 
-    async def reconnect(self, max_restarts=None):
-        if max_restarts is not None:
-            max_restarts -= 1
-        if max_restarts is None or max_restarts >= 0:
-            await self.init(max_restarts)
-            logger.info(f'Client #{self.ls_index:03d} reconnected (max_restarts: {max_restarts})')
-        else:
-            logger.info('Client #{self.ls_index:03d} has no reconnect attempts left')
-            self.tonlib_wrapper = None
-
-    async def init(self, max_restarts=None):
         """"""
         TL Spec
             init options:options = options.Info;
@@ -69,41 +59,58 @@ async def init(self, max_restarts=None):
         :param key: base64 pub key of liteserver node
         :return: None
         """"""
-        self.semaphore = asyncio.Semaphore(self.max_parallel_requests)
-        
-        self.loaded_contracts_num = 0
-        wrapper = TonLib(self.loop, self.ls_index, self.cdll_path)
-        keystore_obj = {
-            '@type': 'keyStoreTypeDirectory',
-            'directory': self.keystore
-        }
-        # create keystore
-        Path(self.keystore).mkdir(parents=True, exist_ok=True)
-
-        request = {
-            '@type': 'init',
-            'options': {
-                '@type': 'options',
-                'config': {
-                    '@type': 'config',
-                    'config': json.dumps(self.local_config),
-                    'use_callbacks_for_network': False,
-                    'blockchain_name': '',
-                    'ignore_cache': False
-                },
-                'keystore_type': keystore_obj
             }
-        }
-        self.tonlib_wrapper = wrapper
 
-        # set verbosity level
-        await self.set_verbosity_level(self.verbosity_level)
-        
-        # set confog
-        await self.tonlib_wrapper.execute(request)
-        self.tonlib_wrapper.set_restart_hook(hook=self.reconnect, max_requests=1024, max_restarts=max_restarts)
 
-        logger.info(F""TonLib #{self.ls_index:03d} inited successfully"")
 
     async def set_verbosity_level(self, level):
         request = {"
OK;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def local_config(self):
         local['liteservers'] = [local['liteservers'][self.ls_index]]
         return local
 
+    async def init(self):
         """"""
         TL Spec
             init options:options = options.Info;
@@ -69,41 +59,58 @@ async def init(self, max_restarts=None):
         :param key: base64 pub key of liteserver node
         :return: None
         """"""
+        if self.tonlib_wrapper is None:            
+            self.loaded_contracts_num = 0
+            wrapper = TonLib(self.loop, self.ls_index, self.cdll_path)
+            keystore_obj = {
+                '@type': 'keyStoreTypeDirectory',
+                'directory': self.keystore
             }
+            # create keystore
+            Path(self.keystore).mkdir(parents=True, exist_ok=True)
+
+            request = {
+                '@type': 'init',
+                'options': {
+                    '@type': 'options',
+                    'config': {
+                        '@type': 'config',
+                        'config': json.dumps(self.local_config),
+                        'use_callbacks_for_network': False,
+                        'blockchain_name': '',
+                        'ignore_cache': False
+                    },
+                    'keystore_type': keystore_obj
+                }
+            }
+            self.tonlib_wrapper = wrapper
+
+            # set verbosity level
+            await self.set_verbosity_level(self.verbosity_level)
+            
+            # set confog
+            await self.tonlib_wrapper.execute(request)
+
+            # set semaphore
+            self.semaphore = asyncio.Semaphore(self.max_parallel_requests)
+            
+            logger.info(F""TonLib #{self.ls_index:03d} inited successfully"")
+        else:
+            logger.warning(f'init is already done')
+
+    async def close(self):
+        if self.tonlib_wrapper is not None:
+            await self.tonlib_wrapper.close()
+            del self.tonlib_wrapper
+
+    async def __aenter__(self):
+        await self.init()
 
+    async def __aexit__(self, *args):
+        await self.close()
 
+    def __await__(self):
+        return self.init()
 
     async def set_verbosity_level(self, level):
         request = {"
KO;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         tonlib_json_client_create.argtypes = []
         try:
             self._client = tonlib_json_client_create()
-        except Exception:
-            asyncio.ensure_future(self.restart(), loop=loop)
 
         tonlib_json_client_receive = tonlib.tonlib_client_json_receive
         tonlib_json_client_receive.restype = c_char_p
@@ -75,110 +75,103 @@ def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         self.futures = {}
         self.loop = loop
         self.ls_index = ls_index
-        self.read_results_task = asyncio.ensure_future(self.read_results(), loop=self.loop)
-        self.del_expired_futures_task = asyncio.ensure_future(self.del_expired_futures_loop(), loop=self.loop)
-        self.shutdown_state = False  # False, ""started"", ""finished""
-        self.request_num = 0
         self.verbose = verbose
 
-        self.max_requests = None
-        self.max_restarts = None
-
     def __del__(self):
         try:
             self._tonlib_json_client_destroy(self._client)
-        except Exception:
-            logger.error(f'Traceback: {traceback.format_exc()}')
-            asyncio.ensure_future(self.restart(), loop=self.loop)
 
     def send(self, query):
         query = json.dumps(query).encode('utf-8')
         try:
             self._tonlib_json_client_send(self._client, query)
-        except Exception:
-            asyncio.ensure_future(self.restart(), loop=self.loop)
-
-    async def restart(self):
-        if not self.shutdown_state:
-            self.shutdown_state = ""started""
-            asyncio.ensure_future(self.restart_hook(self.max_restarts), loop=self.loop)
 
     def receive(self, timeout=10):
         result = None
         try:
             result = self._tonlib_json_client_receive(self._client, timeout)  # time.sleep # asyncio.sleep
-        except Exception:
-            asyncio.ensure_future(self.restart(), loop=self.loop)
         if result:
             result = json.loads(result.decode('utf-8'))
         return result
 
-    def set_restart_hook(self, hook, max_requests=None, max_restarts=None):
-        self.max_requests = max_requests
-        self.max_restarts = max_restarts
-        self.restart_hook = hook
-
     def execute(self, query, timeout=10):
-        query_type = query.get('@type', '?')
-        # logger.debug(f'Tonlib #{self.ls_index:03d}. Executing query with timeout={timeout}: {query_type}')
-        extra_id = ""%s:%s:%s"" % (time.time()+timeout, self.ls_index, random.random())
         query[""@extra""] = extra_id
         
-        self.loop.run_in_executor(None, lambda: self.send(query))
-        
         future_result = self.loop.create_future()
         self.futures[extra_id] = future_result
 
-        self.request_num += 1
-
-        if self.max_requests and self.max_requests < self.request_num:
-            asyncio.ensure_future(self.restart(), loop=self.loop)
-
         return future_result
-
     @property
-    def _is_finishing(self):
-        return (not len(self.futures)) and (self.shutdown_state in [""started"", ""finished""])
 
     async def read_results(self):
-        timeout = 3
         delta = 5
         receive_func = functools.partial(self.receive, timeout)
 
-        while not self._is_finishing:
             result = None
             try:
                 result = await asyncio.wait_for(self.loop.run_in_executor(None, receive_func), timeout=timeout + delta)
             except asyncio.TimeoutError:
-                logger.critical(f""Tonlib #{self.ls_index:03d} Stuck!"")
-                asyncio.ensure_future(self.restart(), loop=self.loop)
-                await asyncio.sleep(2)
             except:
                 logger.critical(f""Tonlib #{self.ls_index:03d} crashed: {traceback.format_exc()}"")
-                asyncio.ensure_future(self.restart(), loop=self.loop)
-                await asyncio.sleep(2)
             
-            # return result
             if result and isinstance(result, dict) and (""@extra"" in result) and (result[""@extra""] in self.futures):
                 try:
                     if not self.futures[result[""@extra""]].done():
                         self.futures[result[""@extra""]].set_result(result)
                         self.futures.pop(result[""@extra""])
                 except Exception as e:
                     logger.error(f'Tonlib #{self.ls_index:03d} receiving result exception: {e}')
-        self.shutdown_state = ""finished""
 
     async def del_expired_futures_loop(self):
-        while not self._is_finishing:
-            await self.cancel_futures()
             await asyncio.sleep(1)
 
-    async def cancel_futures(self, cancel_all=False):
-        now = time.time()
-        to_del = []
-        for i in self.futures:
-            if float(i.split("":"")[0]) <= now or cancel_all:
-                to_del.append(i)
-        for i in to_del:
-            i.cancel()
-            self.futures.pop(i)"
OK;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         tonlib_json_client_create.argtypes = []
         try:
             self._client = tonlib_json_client_create()
+        except Exception as ee:
+            raise RuntimeError(f""Failed to create tonlibjson client: {ee}"")
 
         tonlib_json_client_receive = tonlib.tonlib_client_json_receive
         tonlib_json_client_receive.restype = c_char_p
@@ -75,110 +75,103 @@ def __init__(self, loop, ls_index, cdll_path=None, verbose=0):
         self.futures = {}
         self.loop = loop
         self.ls_index = ls_index
+        self._state = None  # None, ""finished"", ""crashed"", ""stuck""
         self.verbose = verbose
 
+        # creating tasks
+        self.read_results_task = asyncio.ensure_future(self.read_results(), loop=self.loop)
+        self.del_expired_futures_task = asyncio.ensure_future(self.del_expired_futures_loop(), loop=self.loop)
+    
     def __del__(self):
         try:
             self._tonlib_json_client_destroy(self._client)
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.__del__: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.__del__: {ee}')
 
     def send(self, query):
         query = json.dumps(query).encode('utf-8')
         try:
             self._tonlib_json_client_send(self._client, query)
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.send: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.send: {ee}')
 
     def receive(self, timeout=10):
         result = None
         try:
             result = self._tonlib_json_client_receive(self._client, timeout)  # time.sleep # asyncio.sleep
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.receive: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.receive: {ee}')
         if result:
             result = json.loads(result.decode('utf-8'))
         return result
 
     def execute(self, query, timeout=10):
+        extra_id = ""%s:%s:%s"" % (time.time() + timeout, self.ls_index, random.random())
         query[""@extra""] = extra_id
         
         future_result = self.loop.create_future()
         self.futures[extra_id] = future_result
 
+        self.loop.run_in_executor(None, lambda: self.send(query))
         return future_result
+    
     @property
+    def _is_working(self):
+        return self._state not in ('crashed', 'stuck', 'finished')
 
+    async def close(self):
+        try:
+            self._state = 'finished'
+            await self.read_results_task
+            await self.del_expired_futures_task
+        except Exception as ee:
+            logger.error(f""Exception in tonlibjson.close: {traceback.format_exc()}"")
+            raise RuntimeError(f'Error in tonlibjson.close: {ee}')
+
+    def cancel_futures(self, cancel_all=False):
+        now = time.time()
+        to_del = []
+        for i in self.futures:
+            if float(i.split("":"")[0]) <= now or cancel_all:
+                to_del.append(i)
+        logger.debug(f'Pruning {len(to_del)} tasks')
+        for i in to_del:
+            self.futures[i].cancel()
+            self.futures.pop(i)
+
+    # tasks
     async def read_results(self):
+        timeout = 1
         delta = 5
         receive_func = functools.partial(self.receive, timeout)
 
+        while self._is_working:
+            # return reading result
             result = None
             try:
                 result = await asyncio.wait_for(self.loop.run_in_executor(None, receive_func), timeout=timeout + delta)
             except asyncio.TimeoutError:
+                logger.critical(f""Tonlib #{self.ls_index:03d} stuck (timeout error)"")
+                self._state = ""stuck""
             except:
                 logger.critical(f""Tonlib #{self.ls_index:03d} crashed: {traceback.format_exc()}"")
+                self._state = ""crashed""
             
             if result and isinstance(result, dict) and (""@extra"" in result) and (result[""@extra""] in self.futures):
                 try:
                     if not self.futures[result[""@extra""]].done():
                         self.futures[result[""@extra""]].set_result(result)
                         self.futures.pop(result[""@extra""])
                 except Exception as e:
                     logger.error(f'Tonlib #{self.ls_index:03d} receiving result exception: {e}')
 
     async def del_expired_futures_loop(self):
+        while self._is_working:
+            self.cancel_futures()
             await asyncio.sleep(1)
 
+        # finished
+        self.cancel_futures(cancel_all=True)"
KO;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"     author='K-Dimentional Tree',
     author_email='kdimentionaltree@gmail.com',
     name='pytonlib',
-    version='0.0.4',
     packages=find_packages('.', exclude=['tests']),
     install_requires=[
         'crc16==0.1.1',"
OK;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"     author='K-Dimentional Tree',
     author_email='kdimentionaltree@gmail.com',
     name='pytonlib',
+    version='0.0.5',
     packages=find_packages('.', exclude=['tests']),
     install_requires=[
         'crc16==0.1.1',"
KO;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"async def test_get_masterchain_info(tonlib_client: TonlibClient):
     res = await tonlib_client.get_masterchain_info()
     assert res['@type'] == 'blocks.masterchainInfo'
 
 
 @pytest.mark.asyncio
 async def test_get_block_header(tonlib_client: TonlibClient):
     masterchain_block = await tonlib_client.get_masterchain_info()
     res = await tonlib_client.get_block_header(**masterchain_block['last'])
     assert res['@type'] == 'blocks.header'
 
 
 @pytest.mark.asyncio
 async def test_get_shards(tonlib_client: TonlibClient):
     masterchain_info = await tonlib_client.get_masterchain_info()
     shards = await tonlib_client.get_shards(master_seqno=masterchain_info['last']['seqno'])
     assert shards['@type'] == 'blocks.shards'
 
 
 @pytest.mark.asyncio
 async def test_get_transactions(tonlib_client: TonlibClient):
@@ -72,6 +78,8 @@ async def test_get_transactions(tonlib_client: TonlibClient):
 
     tx = await tonlib_client.get_transactions(**txs['transactions'][0], limit=1)
     assert tx[0]['@type'] == 'raw.transaction'
 
 
 def test_sync_code(tonlib_config, ton_keystore, ls_index):
@@ -84,6 +92,6 @@ async def main():
                               loop=loop,
                               verbosity_level=0)
         await client.init()
-        return client
 
     asyncio.run(main())"
OK;2;toncenter;pytonlib;7942c21b7e9b3b584645230030b618f110e4141b;"Removed autorestarts and fixed crashes on exit (#3)

* Autorestarts removed

* PyTONLib v0.0.5

- Removed autorestart hooks for crashes (they probably had memory leaks).
- Fixed crash on exit from script.";"async def test_get_masterchain_info(tonlib_client: TonlibClient):
     res = await tonlib_client.get_masterchain_info()
     assert res['@type'] == 'blocks.masterchainInfo'
 
+    await tonlib_client.close()
+
 
 @pytest.mark.asyncio
 async def test_get_block_header(tonlib_client: TonlibClient):
     masterchain_block = await tonlib_client.get_masterchain_info()
     res = await tonlib_client.get_block_header(**masterchain_block['last'])
     assert res['@type'] == 'blocks.header'
 
+    await tonlib_client.close()
+
 
 @pytest.mark.asyncio
 async def test_get_shards(tonlib_client: TonlibClient):
     masterchain_info = await tonlib_client.get_masterchain_info()
     shards = await tonlib_client.get_shards(master_seqno=masterchain_info['last']['seqno'])
     assert shards['@type'] == 'blocks.shards'
 
+    await tonlib_client.close()
+
 
 @pytest.mark.asyncio
 async def test_get_transactions(tonlib_client: TonlibClient):
@@ -72,6 +78,8 @@ async def test_get_transactions(tonlib_client: TonlibClient):
 
     tx = await tonlib_client.get_transactions(**txs['transactions'][0], limit=1)
     assert tx[0]['@type'] == 'raw.transaction'
+    
+    await tonlib_client.close()
 
 
 def test_sync_code(tonlib_config, ton_keystore, ls_index):
@@ -84,6 +92,6 @@ async def main():
                               loop=loop,
                               verbosity_level=0)
         await client.init()
+        await client.close()
 
     asyncio.run(main())"
KO;2;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
-def init(S=13, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
OK;2;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
+def init(S=19, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
KO;2;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
-		no_obj_iou_1, idx_1 = internal_function(yhat[0:int(N / 2)], y[0:int(N / 2)])
-		no_obj_iou_2, idx_2 = internal_function(yhat[int(N / 2):], y[int(N / 2):])
-		no_obj_iou = torch.cat([no_obj_iou_1, no_obj_iou_2], dim=0)
-		idx = torch.cat([idx_1, idx_2], dim=0)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
OK;2;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
+		def obtain_by_crop(crop) -> list[torch.Tensor]:
+			""""""Obtain no_obj_iou by cropping down batch, used to enable large batch training
+
+			Args:
+				crop (int): crop count
+
+			Returns:
+				list[torch.Tensor]: no_obj_iou and idx
+			""""""
+			no_obj_iou = torch.tensor([], dtype=torch.bool).to(yhat.device)
+			idx = torch.tensor([], dtype=torch.int64).to(yhat.device)
+			for i in range(crop):
+				no_obj_iou_i, idx_i = internal_function(yhat[int(i * N / crop):int((i + 1) * N / crop)], 
+														y[int(i * N / crop):int((i + 1) * N / crop)])
+				no_obj_iou = torch.cat([no_obj_iou, no_obj_iou_i], dim=0)
+				idx = torch.cat([idx, idx_i], dim=0)
+			
+			return no_obj_iou, idx
+
+		if S == 19:
+			crop = 3
+		else:
+			crop = 1
+		
+		no_obj_iou, idx = obtain_by_crop(crop)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
-Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/scheduler_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
+Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/cron_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
 
 app = Flask(__name__)
 
@@ -19,12 +20,15 @@
 def run():
     # TODO - allow only POST
     # TODO - add authentication
-    now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
     sheets_service = SheetsService()
-    entries = retrieve_entries_from_db(sheets_service, now)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
         return Response(status=200)
 
     for i, row in entries:
@@ -43,12 +47,9 @@ def run():
         )
         sheets_service.update_entry(updated_entry)
 
-    return Response(status=200)
-
 
-def retrieve_entries_from_db(sheets_service, nextrun_ts):
-    parsed_time = parse_time(nextrun_ts)
-    return sheets_service.get_entries_by_nextrun(parsed_time)
 
 
 def send_message(chat_id, content):"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
+import gc
 
 app = Flask(__name__)
 
@@ -19,12 +20,15 @@
 def run():
     # TODO - allow only POST
     # TODO - add authentication
     sheets_service = SheetsService()
+    now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
+    parsed_time = parse_time(now)
+    entries = sheets_service.get_entries_by_nextrun(parsed_time)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
+        gc.collect()
+
         return Response(status=200)
 
     for i, row in entries:
@@ -43,12 +47,9 @@ def run():
         )
         sheets_service.update_entry(updated_entry)
 
+    gc.collect()  # https://github.com/googleapis/google-api-python-client/issues/535
 
+    return Response(status=200)
 
 
 def send_message(chat_id, content):"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
-    update.message.reply_text(config.confirm_message, parse_mode=""MarkdownV2"")
 
 
 def remove_job(update):"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
+    update.message.reply_text(config.confirm_message)
 
 
 def remove_job(update):"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
-confirm_message = ""Ok\. Done\. Added\. Your message will be sent when the time comes\.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
+confirm_message = ""Ok. Done. Added. Your message will be sent when the time comes. Check /list to make sure that your job is added correctly.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -71,6 +78,13 @@ def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -147,6 +161,14 @@ def add_chat_data(
             ],
             inherit=True,
         )
         return
 
     def add_user(self, user_id, username, first_name):
@@ -155,6 +177,12 @@ def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -181,6 +209,12 @@ def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
@@ -203,12 +237,6 @@ def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
-            logger.info(
-                ""New user added, username=%s, user_id=%s"",
-                update.message.from_user.username,
-                update.message.from_user.id,
-            )
-
             return
 
         # check that username hasn't changed
@@ -222,7 +250,7 @@ def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
-                ""username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
@@ -238,7 +266,7 @@ def sync_user_data(self, update):
             )
 
             logger.info(
-                ""first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
+        logger.info(
+            'New job entry ""%s"" added by user ""%s"", chat_id=%s',
+            jobname,
+            username,
+            str(chat_id),
+        )
+
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -71,6 +78,13 @@ def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
+        logger.info(
+            'Job entry ""%s"" updated by user ""%s"", chat_id=%s',
+            get_value(entry, ""jobname""),
+            get_value(entry, ""last_updated_by""),
+            str(get_value(entry, ""chat_id"")),
+        )
+
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -147,6 +161,14 @@ def add_chat_data(
             ],
             inherit=True,
         )
+
+        logger.info(
+            'New chat entry created by user ""%s"", chat_id=%s, chat_title=%s',
+            created_by_username,
+            str(chat_id),
+            chat_title,
+        )
+
         return
 
     def add_user(self, user_id, username, first_name):
@@ -155,6 +177,12 @@ def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
+        logger.info(
+            'New user created, user_id=%s, username=""%s""',
+            str(user_id),
+            username,
+        )
+
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -181,6 +209,12 @@ def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
+        logger.info(
+            'User superseded, user_id=%s, field_changed=""%s""',
+            get_value(entry, ""user_id""),
+            field_changed,
+        )
+
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
@@ -203,12 +237,6 @@ def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
             return
 
         # check that username hasn't changed
@@ -222,7 +250,7 @@ def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
+                ""User's username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
@@ -238,7 +266,7 @@ def sync_user_data(self, update):
             )
 
             logger.info(
+                ""User's first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
KO;2;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
-the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of current user and software\microsoft\windows\currentVersion\run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
OK;2;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
+the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of HKCU\Software\Microsoft\Windows\CurrentVersion\Run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
KO;3;dshadoff;PCE_TurboEverdrive_USB;8425b34f8850f72ec4cef51a71261efa299ea42b;"Updates for headers, 3Mb games, mappers

Update to fix:
- games with headers
- 3Mb games to align properly to memory
- Recognition of SF2 and send of special mapper flag in protocol
- Recognition of Populous and send of special mapper flag in protocol";" # Grab data from binary file to send
 #
 f = open(sys.argv[2], ""rb"") 
-data = f.read()
 f.close()
 
-# now for Turbo Everdrive protocol:
 #
 
 # This seems to query the port for an Everdrive
 #
 ser.write(b'\x2A') 
@@ -52,7 +85,6 @@
 ser.write(b'\x67') 
 ser.flush()
 
-block_start = 0
 file_end = len(data)
 blocks_sent = 0
 
@@ -71,10 +103,25 @@
         ser.write(b'\x2B')
 
 if (file_end > block_start):
-    data += ""\0"" * (8192 - (file_end - block_start) )
     ser.write(data[block_start:block_start+8192])
 
 ser.write(b'\x2D') 
 ser.write(b'\x2D') 
 ser.flush()
 "
OK;3;dshadoff;PCE_TurboEverdrive_USB;8425b34f8850f72ec4cef51a71261efa299ea42b;"Updates for headers, 3Mb games, mappers

Update to fix:
- games with headers
- 3Mb games to align properly to memory
- Recognition of SF2 and send of special mapper flag in protocol
- Recognition of Populous and send of special mapper flag in protocol";" # Grab data from binary file to send
 #
 f = open(sys.argv[2], ""rb"") 
+inpdata = f.read()
 f.close()
 
+# flags for mappers for special games
 #
+sf2 = 0
+populous = 0
+pop_off = 0x1f26
+pop_chk = bytes(""POPULOUS"", 'utf-8')
 
+# Remove header if needed
+#
+block_start = 0
+if (len(inpdata) % 8192) == 0:
+    data = inpdata
+else:
+    if (len(inpdata) % 8192) == 512:
+        data = inpdata[512:]
+        print(""Removing header"")
+    else:
+        print(""This is an odd size for a ROM file... are you sure it's OK ?"")
+
+# Make 3Mbit games into linear address space if needed
+#
+if (len(data) == 393216):
+    tempdata = data
+    data = tempdata[0:262144] + tempdata[0:262144] + tempdata[262144:]
+
+# Set mapper flag for SF2 game
+#
+if (len(data) == 2621440):
+    sf2 = 1
+
+# Set mapper flag for Populous game
+#
+if (data[pop_off:pop_off+8] == pop_chk):
+    populous = 1
+
+# Now for Turbo Everdrive protocol:
 # This seems to query the port for an Everdrive
 #
 ser.write(b'\x2A') 
@@ -52,7 +85,6 @@
 ser.write(b'\x67') 
 ser.flush()
 
 file_end = len(data)
 blocks_sent = 0
 
@@ -71,10 +103,25 @@
         ser.write(b'\x2B')
 
 if (file_end > block_start):
+    print(""file end = "", file_end, "", block_start = "", block_start)
+    data += b'\0' * (8192 - (file_end - block_start) )
     ser.write(data[block_start:block_start+8192])
 
+# No banks of data to follow
+#
 ser.write(b'\x2D') 
+
+print()
+# Enable mappers (if any)
+#
+if (sf2 == 1):
+    ser.write(b'\x73')
+else:
+    if (populous == 1):
+        ser.write(b'\x70')
+
+# End transmission
+#
 ser.write(b'\x2D') 
 ser.flush()
 "
KO;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";" from dbt.adapters.starrocks.connections import StarRocksConnectionManager
 from dbt.adapters.starrocks.relation import StarRocksRelation
 
-
-class Engine(str, Enum):
-    olap = ""olap""
-    mysql = ""mysql""
-    elasticsearch = ""elasticsearch""
-    hive = ""hive""
-    iceberg = ""iceberg""
-
-
 class StarRocksConfig(AdapterConfig):
-    engine: Engine = Engine.olap
-    duplicate_key: Tuple[str]
-    partition_by: Tuple[str]
-    partition_by_init: List[str]
-    distributed_by: Tuple[str]
-    buckets: int
-    properties: Dict[str, str]
 
 
 class StarRocksAdapter(SQLAdapter):
@@ -171,3 +163,4 @@ def test(row: agate.Row) -> bool:
         return (table_database, table_schema.lower()) in schemas
 
     return test"
OK;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";" from dbt.adapters.starrocks.connections import StarRocksConnectionManager
 from dbt.adapters.starrocks.relation import StarRocksRelation
 
 class StarRocksConfig(AdapterConfig):
+    engine: Optional[str] = None
+    table_type: Optional[str] = None  # DUPLICATE/PRIMARY/UNIQUE/AGGREGATE
+    keys: Optional[List[str]] = None
+    partition_by: Optional[List[str]] = None
+    partition_by_init: Optional[List[str]] = None
+    distributed_by: Optional[List[str]] = None
+    buckets: Optional[int] = None
+    properties: Optional[Dict[str, str]] = None
 
 
 class StarRocksAdapter(SQLAdapter):
@@ -171,3 +163,4 @@ def test(row: agate.Row) -> bool:
         return (table_database, table_schema.lower()) in schemas
 
     return test
+"
KO;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"  * limitations under the License.
  */
 
-{% macro starrocks__engine() -%}
-    {% set label = 'ENGINE' %}
-    {% set engine = config.get('engine', validator=validation.any[basestring]) %}
-    {% if engine is not none %}
-    {{ label }} = {{ engine }}
-  {% else %}
-    {{ label }} = OLAP
-  {% endif %}
-{%- endmacro %}
-
-{% macro starrocks__partition_by() -%}
-  {% set cols = config.get('partition_by') %}
-  {% if cols is not none %}
-    PARTITION BY RANGE (
-      {% for col in cols %}
-        {{ col }}{% if not loop.last %},{% endif %}
-      {% endfor %}
-    )(
-        {% set init = config.get('partition_by_init',validator=validation.any[list]) %}
-        {% if init is not none %}
-          {% for row in init %}
-            {{ row }}{% if not loop.last %},{% endif %}
-          {% endfor %}
-        {% endif %}
-    )
-  {% endif %}
-{%- endmacro %}
-
-{% macro starrocks__duplicate_key() -%}
-  {% set cols = config.get('duplicate_key', validator=validation.any[list]) %}
-  {% if cols is not none %}
-    DUPLICATE KEY (
-      {% for item in cols %}
-        {{ item }}
-      {% if not loop.last %},{% endif %}
-      {% endfor %}
-    )
-  {% endif %}
-{%- endmacro %}
-
-{% macro starrocks__distributed_by(column_names) -%}
-  {% set label = 'DISTRIBUTED BY HASH' %}
-  {% set engine = config.get('engine', validator=validation.any[basestring]) %}
-  {% set cols = config.get('distributed_by', validator=validation.any[list]) %}
-  {% if cols is none and engine in [none,'OLAP'] %}
-    {% set cols = column_names %}
-  {% endif %}
-  {% if cols %}
-    {{ label }} (
-      {% for item in cols %}
-        {{ item }}{% if not loop.last %},{% endif %}
-      {% endfor %}
-    ) BUCKETS {{ config.get('buckets', validator=validation.any[int]) or 1 }}
-  {% endif %}
-{%- endmacro %}
-
-{% macro starrocks__properties() -%}
-  {% set properties = config.get('properties', validator=validation.any[dict]) or {""replication_num"":""1""} %}
-  {% if properties is not none %}
-    PROPERTIES (
-      {% for key, value in properties.items() %}
-        ""{{ key }}"" = ""{{ value }}""{% if not loop.last %},{% endif %}
-      {% endfor %}
-    )
-  {% endif %}
-{%- endmacro%}
-
 {% macro starrocks__drop_relation(relation) -%}
   {% call statement('drop_relation', auto_begin=False) %}
     drop {{ relation.type }} if exists {{ relation }}"
OK;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"  * limitations under the License.
  */
 
 {% macro starrocks__drop_relation(relation) -%}
   {% call statement('drop_relation', auto_begin=False) %}
     drop {{ relation.type }} if exists {{ relation }}"
KO;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";\ No newline at end of file
OK;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"+{% macro starrocks__olap_table(is_create_table_as) -%}
+
+  {% set is_create_table = is_create_table_as is none or not is_create_table_as %}
+
+  {% set table_type = config.get('table_type', 'DUPLICATE') %}
+  {% set keys = config.get('keys') %}
+  {% set partition_by = config.get('partition_by') %}
+  {% set partition_by_init = config.get('partition_by_init') %}
+  {% set buckets = config.get('buckets', 10) %}
+  {% set distributed_by = config.get('distributed_by') %}
+  {% if is_create_table %}
+    {% set properties = config.get('properties', {""replication_num"":""1""}) %}
+  {% else %}
+    {% set properties = config.get('properties') %}
+  {% endif %}
+
+  {# 1. SET ENGINE #}
+  {% if is_create_table %}
+    ENGINE = OLAP
+  {% endif %}
+
+  {# 2. SET KEYS #}
+  {% if is_create_table %}
+    {% if keys is not none %}
+      {% if table_type == ""DUPLICATE"" %}
+        DUPLICATE KEY (
+          {% for item in keys %}
+            {{ item }}{% if not loop.last %},{% endif %}
+          {% endfor %}
+        )
+      {% elif table_type == ""PRIMARY"" %}
+        PRIMARY KEY (
+          {% for item in keys %}
+            {{ item }}{% if not loop.last %},{% endif %}
+          {% endfor %}
+        )
+      {% elif table_type == ""UNIQUE"" %}
+        UNIQUE KEY (
+          {% for item in keys %}
+            {{ item }}{% if not loop.last %},{% endif %}
+          {% endfor %}
+        )
+      {% else %}
+        {% set msg -%}
+          ""{{ table_type }}"" is not support
+        {%- endset %}
+        {{ exceptions.raise_compiler_error(msg) }}
+      {% endif %}
+    {% else %}
+      {% if table_type != ""DUPLICATE"" %}
+        {% set msg -%}
+          ""{{ table_type }}"" is must set ""keys""
+        {%- endset %}
+        {{ exceptions.raise_compiler_error(msg) }}
+      {% endif %}
+    {% endif %}
+  {% endif %}
+
+  {# 3. SET PARTITION #}
+  {{ starrocks__partition_by(partition_by, partition_by_init) }}
+
+  {# 4. SET DISTRIBUTED #}
+  {% if distributed_by is not none %}
+    DISTRIBUTED BY HASH (
+      {% for item in distributed_by %}
+        {{ item }}{% if not loop.last %},{% endif %}
+      {% endfor %}
+    ) BUCKETS {{ buckets }}
+  {% else %}
+    {% set msg -%}
+      [distributed_by] is not set
+    {%- endset %}
+    {{ exceptions.raise_compiler_error(msg) }}
+  {% endif %}
+
+  {# 4. SET PROPERTIES #}
+  {% if properties is not none %}
+    PROPERTIES (
+      {% for key, value in properties.items() %}
+        ""{{ key }}"" = ""{{ value }}""{% if not loop.last %},{% endif %}
+      {% endfor %}
+    )
+  {% endif %}
+{%- endmacro %}
+
+{% macro starrocks__other_table() -%}
+  {% set engine = config.get('engine') %}
+  {% set properties = config.get('properties') %}
+
+  ENGINE = {{ engine }}
+  {% if properties is not none %}
+    PROPERTIES (
+      {% for key, value in properties.items() %}
+        ""{{ key }}"" = ""{{ value }}""{% if not loop.last %},{% endif %}
+      {% endfor %}
+    )
+  {% endif %}
+{%- endmacro %}
+
+{% macro starrocks__partition_by(cols, init) -%}
+  {% if cols is not none %}
+    PARTITION BY RANGE (
+      {% for col in cols %}
+        {{ col }}{% if not loop.last %},{% endif %}
+      {% endfor %}
+    )(
+      {% if init is not none %}
+        {% for row in init %}
+          {{ row }}{% if not loop.last %},{% endif %}
+        {% endfor %}
+      {% endif %}
+    )
+  {% endif %}
+{%- endmacro %}
\ No newline at end of file"
KO;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";" 
 {% macro starrocks__create_table_as(temporary, relation, sql) -%}
   {% set sql_header = config.get('sql_header', none) %}
 
   {{ sql_header if sql_header is not none }}
   create table {{ relation.include(database=False) }}
-    {{ starrocks__partition_by() }}
-    {{ starrocks__distributed_by() }}
-    {{ starrocks__properties() }} as {{ sql }}
 {%- endmacro %}"
OK;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";" 
 {% macro starrocks__create_table_as(temporary, relation, sql) -%}
   {% set sql_header = config.get('sql_header', none) %}
+  {% set engine = config.get('engine', 'OLAP') %}
 
   {{ sql_header if sql_header is not none }}
+
   create table {{ relation.include(database=False) }}
+  {% if engine == 'OLAP' %}
+    {{ starrocks__olap_table(True) }}
+  {% else %}
+    {% set msg -%}
+      ""ENGINE = {{ engine }}"" is not ""CREATE TABLE ... AS ...""
+    {%- endset %}
+    {{ exceptions.raise_compiler_error(msg) }}
+  {% endif %}
+
+  as {{ sql }}
+
 {%- endmacro %}"
KO;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"  */
 
 {% macro starrocks__create_csv_table(model, agate_table) -%}
-    {% set column_override = model['config'].get('column_types', {}) %}
-    {% set quote_seed_column = model['config'].get('quote_columns', None) %}
 
-    {% set sql %}
     create table {{ this.render() }}
     (
         {% for col_name in agate_table.column_names %}
@@ -26,17 +27,17 @@
         {{ adapter.quote_seed_column(column_name, quote_seed_column) }} {{ type }}{% if not loop.last %},{% endif %}
         {% endfor %}
     )
-    {{ starrocks__engine() }}
-    {{ starrocks__duplicate_key() }}
-    {{ starrocks__partition_by() }}
-    {{ starrocks__distributed_by(agate_table.column_names) }}
-    {{ starrocks__properties() }}
-    {% endset %}
 
-    {% call statement('_') %}
     {{ sql }}
-    {% endcall %}
 
-    {{ return(sql) }}
 
 {%- endmacro %}"
OK;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"  */
 
 {% macro starrocks__create_csv_table(model, agate_table) -%}
+  {% set column_override = model['config'].get('column_types', {}) %}
+  {% set quote_seed_column = model['config'].get('quote_columns', None) %}
+  {% set engine = config.get('engine', 'OLAP') %}
 
+  {% set sql %}
     create table {{ this.render() }}
     (
         {% for col_name in agate_table.column_names %}
@@ -26,17 +27,17 @@
         {{ adapter.quote_seed_column(column_name, quote_seed_column) }} {{ type }}{% if not loop.last %},{% endif %}
         {% endfor %}
     )
+    {% if engine == 'OLAP' %}
+      {{ starrocks__olap_table(False) }}
+    {% else %}
+      {{ starrocks__other_table() }}
+    {% endif %}
+  {% endset %}
 
+  {% call statement('_') %}
     {{ sql }}
+  {% endcall %}
 
+  {{ return(sql) }}
 
 {%- endmacro %}"
KO;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"target:
   schema: dbt_test_{{ var('_dbt_random_suffix') }}
 sequences:
   test_dbt_empty: empty
-  test_dbt_base: base
   # test_dbt_ephemeral: ephemeral
-  test_dbt_incremental: incremental
   # test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp
   # test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols
   test_dbt_data_test: data_test
-  test_dbt_schema_test: schema_test
   # test_dbt_ephemeral_data_tests: data_test_ephemeral_models"
OK;3;StarRocks;dbt-starrocks;f88426342481aa8aaa05fa6c310c033ccfe11c99;"Table supports multiple data models (#10)

#4
1. Table supports selecting one from `Duplicate Key`/`Unique Key`/`Primary Key`
2. Table supports set `ENGINE`
3. Table supports set `keys`
4. Table supports set `DISTRIBUTED BY`
5. Table supports set `PROPERTIES`
6. Table supports set `PARTITION BY`

### Notice:
1. `Create table as` can only set engine='OLAP' and table_type='DUPLICATE'
2. distributed_by is must

### Test:
  test_dbt_empty: empty
  test_dbt_base: base      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral: ephemeral      `[ERROR: not support with ]`
  test_dbt_incremental: incremental      `[ERROR: distributed_by is must ]`
  test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp      `[ERROR: not support with ]`
  test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols     `[ERROR: not support with ]`
  test_dbt_data_test: data_test
  test_dbt_schema_test: schema_test      `[ERROR: distributed_by is must ]`
  test_dbt_ephemeral_data_tests: data_test_ephemeral_models      `[ERROR: not support with ]`

# example:

## dbt seed properties(whatever_you_want.yml):
### Minimum configuration:
```
config:
  distributed_by: ['id']
```

### Complete configuration:
```
config:
  engine: 'OLAP'
  keys: ['id', 'name', 'some_date']
  table_type: 'PRIMARY'     //PRIMARY or DUPLICATE or UNIQUE
  distributed_by: ['id']
  buckets: 3                //default 10
  partition_by: ['some_date']
  partition_by_init: [""PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))""]
  properties: {""replication_num"":""1"", ""in_memory"": ""true""}
```
  
## dbt run config(table/incremental):
### Minimum configuration:
```
{{ config(materialized=var(""materialized_var"", ""table""), distributed_by=['id'])}}
{{ config(materialized='incremental', distributed_by=['id']) }}
```

### Complete configuration:
```
{{ config(materialized='table', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
{{ config(materialized='incremental', engine='OLAP', buckets=32, distributed_by=['id'], properties={""in_memory"": ""true""}) }}
```";"target:
   schema: dbt_test_{{ var('_dbt_random_suffix') }}
 sequences:
   test_dbt_empty: empty
+  # test_dbt_base: base
   # test_dbt_ephemeral: ephemeral
+  # test_dbt_incremental: incremental
   # test_dbt_snapshot_strategy_timestamp: snapshot_strategy_timestamp
   # test_dbt_snapshot_strategy_check_cols: snapshot_strategy_check_cols
   test_dbt_data_test: data_test
+  # test_dbt_schema_test: schema_test
   # test_dbt_ephemeral_data_tests: data_test_ephemeral_models"
KO;3;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
-            fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
-            data = jarray.zeros(memory_block.getSize(), ""b"")
-            memory_block.getBytes(memory_block.getStart(), data)
-            fp.write(','.join((str(i) for i in data)))
-            fp.write("")),\n"")
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
OK;3;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
+            if memory_block.isInitialized():
+                fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
+                data = jarray.zeros(memory_block.getSize(), ""b"")
+                memory_block.getBytes(memory_block.getStart(), data)
+                fp.write(','.join((str(i) for i in data)))
+                fp.write("")),\n"")
+            else:
+                fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=None),\n"".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
KO;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
-        
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
+            start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
+            elapsed_time = time.time() - start_time
+            self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
+
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -847,80 +848,95 @@ def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
-        self.empty_cache()
-        img = cp.array(img, dtype=np.float32)
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
-
-        del img
-        # cp._default_memory_pool.free_all_blocks()
-
-        img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
-        bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
-        reconfactor_cp = cp.array(self._reconfactor)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
-
-        del bcarray
-        del reconfactor_cp
-        # cp._default_memory_pool.free_all_blocks()
-
-        img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
-        for offs in range(0, 2*self.N - blocksize, blocksize):
-            imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        del img2
-        del imf
-        # cp._default_memory_pool.free_all_blocks()
-
-        res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
-        del img3
-        cp._default_memory_pool.free_all_blocks()
 
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
-        self.empty_cache()
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
-        imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
-        del img1
-        img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
-        reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
-
-        img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        for offs in range(0, 2 * self.N - blocksize, blocksize):
-            imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        del img2
-        postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
-        res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):
@@ -963,12 +979,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
-                print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
-            cp._default_memory_pool.free_all_blocks()
             if self.debug:
-                print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
+        self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -847,80 +848,95 @@ def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
+    def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
+        try:
+            # Sometimes we are called from a new thread and then the plan_cache needs to be
+            # reset to 0 to avoid running out of GPU memory
+            cp.fft.config.get_plan_cache().set_size(0)
+            img1 = cp.array(img, dtype=np.float32)
+            nim = img1.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
+
+            del img1
+
+            img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
+            bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
+            reconfactor_cp = cp.array(self._reconfactor)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
+
+            del bcarray
+            del reconfactor_cp
+
+            img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
+            for offs in range(0, 2*self.N - blocksize, blocksize):
+                imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            del img2
+            del imf
+            res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
+            del img3
+        except Exception as e:
+            res = f'Exception in batchreconstruct_cupy: {e}'
+        return res
+
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
+        res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
+        cp.get_default_memory_pool().free_all_blocks()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
+        return res
 
+    def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
+        assert pytorch, ""No pytorch present""
+        try:
+            nim = img.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
+            imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
+            del img1
+            img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
+            reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
+
+            img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            for offs in range(0, 2 * self.N - blocksize, blocksize):
+                imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            del img2
+            postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
+            res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except Exception as e:
+            res = f'Exception in batchreconstruct_pytorch: {e}'
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
+        res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
+        if torch.has_cuda:
+            torch.cuda.empty_cache()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):
@@ -963,12 +979,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
+                print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+            cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
-                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
+                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
-    def batchreconstructcompact_cupy(self, img, blocksize=128):
-        assert cupy, ""No CuPy present""
         try:
-            # cp.get_default_memory_pool().free_all_blocks()
-            # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-            # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -893,24 +887,17 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
-            raise e
-            # Tidy up GPU memory
-        finally:
-            img1 = None
-            imf = None
-            bcarray = None
-            reconfactor_cp = None
-            img2 = None
-            img3 = None
-            print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-            cp.get_default_memory_pool().free_all_blocks()
-            print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-            print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
-    def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
@@ -941,25 +928,15 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
-            if torch.has_cuda:
-                # Tidy up gpu memory
-                if 'img1' in locals(): del img1
-                if 'imf' in locals(): del imf
-                if 'bcarray' in locals(): del bcarray
-                if 'reconfactor_pt' in locals(): del reconfactor_pt
-                if 'img2' in locals(): del img2
-                if 'img3' in locals(): del img3
-                torch.cuda.empty_cache()
-                print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-                raise e
         if torch.has_cuda:
-            del imf
-            del bcarray
-            del reconfactor_pt
-            del img3
             torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
+    def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -893,24 +887,17 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
+            res = f'Exception in batchreconstruct_cupy: {e}'
         return res
 
+    def batchreconstructcompact_cupy(self, img, blocksize=128):
+        assert cupy, ""No CuPy present""
+        res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
+        cp.get_default_memory_pool().free_all_blocks()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
+        return res
+
+    def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
@@ -941,25 +928,15 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
+            res = f'Exception in batchreconstruct_pytorch: {e}'
+        return res
+
+    def batchreconstructcompact_pytorch(self, img, blocksize=128):
+        assert pytorch, ""No pytorch present""
+        res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
         if torch.has_cuda:
             torch.cuda.empty_cache()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
-                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
+                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -885,23 +890,24 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
-
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             # Tidy up GPU memory
-            if 'img1' in locals(): del img1
-            if 'imf' in locals(): del imf
-            if 'bcarray' in locals(): del bcarray
-            if 'reconfactor_cp' in locals(): del reconfactor_cp
-            if 'img2' in locals(): del img2
-            if 'img3' in locals(): del img3
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-            raise e
-        cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
OK;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
+            # cp.get_default_memory_pool().free_all_blocks()
+            # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+            # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -885,23 +890,24 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
+            raise e
             # Tidy up GPU memory
+        finally:
+            img1 = None
+            imf = None
+            bcarray = None
+            reconfactor_cp = None
+            img2 = None
+            img3 = None
+            print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
KO;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
-        
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
+            start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
+            elapsed_time = time.time() - start_time
+            self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
+
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
-        except RuntimeError as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
-            cp._default_memory_pool.free_all_blocks()
             raise e
-        cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
@@ -931,7 +934,7 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
-        except RuntimeError as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
@@ -941,6 +944,9 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 raise e
         if torch.has_cuda:
             del imf
@@ -990,12 +996,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
-                print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
-            cp._default_memory_pool.free_all_blocks()
             if self.debug:
-                print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
+        except Exception as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
+            cp.get_default_memory_pool().free_all_blocks()
+            print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+            print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             raise e
+        cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
@@ -931,7 +934,7 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except Exception as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
@@ -941,6 +944,9 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
+                print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
                 raise e
         if torch.has_cuda:
             del imf
@@ -990,12 +996,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
+                print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+            cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -849,78 +850,104 @@ def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
-        self.empty_cache()
-        img = cp.array(img, dtype=np.float32)
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
-
-        del img
-        # cp._default_memory_pool.free_all_blocks()
-
-        img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
-        bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
-        reconfactor_cp = cp.array(self._reconfactor)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
-
-        del bcarray
-        del reconfactor_cp
-        # cp._default_memory_pool.free_all_blocks()
-
-        img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
-        for offs in range(0, 2*self.N - blocksize, blocksize):
-            imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        del img2
-        del imf
-        # cp._default_memory_pool.free_all_blocks()
-
-        res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
-        del img3
         cp._default_memory_pool.free_all_blocks()
-
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
-        self.empty_cache()
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
-        imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
-        del img1
-        img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
-        reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
-
-        img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        for offs in range(0, 2 * self.N - blocksize, blocksize):
-            imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        del img2
-        postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
-        res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
+        self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -849,78 +850,104 @@ def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
+        try:
+            # Sometimes we are called from a new thread and then the plan_cache needs to be
+            # reset to 0 to avoid running out of GPU memory
+            cp.fft.config.get_plan_cache().set_size(0)
+            img1 = cp.array(img, dtype=np.float32)
+            nim = img1.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
+
+            del img1
+
+            img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
+            bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
+            reconfactor_cp = cp.array(self._reconfactor)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
+
+            del bcarray
+            del reconfactor_cp
+
+            img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
+            for offs in range(0, 2*self.N - blocksize, blocksize):
+                imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            del img2
+            del imf
+
+            res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
+            del img3
+        except RuntimeError as e:
+            # Tidy up GPU memory
+            if 'img1' in locals(): del img1
+            if 'imf' in locals(): del imf
+            if 'bcarray' in locals(): del bcarray
+            if 'reconfactor_cp' in locals(): del reconfactor_cp
+            if 'img2' in locals(): del img2
+            if 'img3' in locals(): del img3
+            cp._default_memory_pool.free_all_blocks()
+            raise e
         cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
+        try:
+            nim = img.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
+            imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
+            del img1
+            img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
+            reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
+
+            img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            for offs in range(0, 2 * self.N - blocksize, blocksize):
+                imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            del img2
+            postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
+            res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except RuntimeError as e:
+            if torch.has_cuda:
+                # Tidy up gpu memory
+                if 'img1' in locals(): del img1
+                if 'imf' in locals(): del imf
+                if 'bcarray' in locals(): del bcarray
+                if 'reconfactor_pt' in locals(): del reconfactor_pt
+                if 'img2' in locals(): del img2
+                if 'img3' in locals(): del img3
+                torch.cuda.empty_cache()
+                raise e
+        if torch.has_cuda:
+            del imf
+            del bcarray
+            del reconfactor_pt
+            del img3
+            torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
 *.parquet
\ No newline at end of file"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
+*.ipc
 *.parquet
\ No newline at end of file"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
-A smol simulator framework built on top of PyGame.
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
+A smol simulator framework built on top of [PyGame](https://www.pygame.org/docs/).
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
-from vi import Agent, BaseConfig, Simulation, Snapshot, dataclass
 
 
-@dataclass
-class MySnapshot(Snapshot):  # 👈 inherit Snapshot to collect base metrics.
-    # We want to keep track of how many other agents were in our agent's radius,
-    # so we add an extra `in_radius` metric to our Snapshot!
-    in_radius: int
 
 
-class MyAgent(Agent):
-    def update(self):
         # If at least one agent is within our agent's radius, then we turn red!
-        if len(self.in_radius()) > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
-    def snapshot(self) -> MySnapshot:
-        return MySnapshot(
-            # Automatically fill-in all the Snapshot attributes such as agent, frame, x and y.
-            **super().snapshot().as_dict(),
-            # Then add our own metric: in_radius!
-            in_radius=len(self.in_radius()),
-        )
-
 
 print(
     # We're using a seed to collect the same data every time.
@@ -39,9 +32,7 @@ def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
-    # convert the output of the simulation into a Polars DataFrame
-    .to_polars()
-    .groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
+from vi import Agent, BaseConfig, Simulation
 
 
+class MyAgent(Agent):
+    def every_frame(self):
+        # As radius calculation is quite performance heavy,
+        # we only calculate it once per frame.
+        in_radius = len(self.in_radius())
 
+        # We want to keep track of how many other agents were in our agent's radius,
+        # so we add data to the `in_radius` column of our dataframe!
+        self.save_data(""in_radius"", in_radius)
 
         # If at least one agent is within our agent's radius, then we turn red!
+        if in_radius > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
 
 print(
     # We're using a seed to collect the same data every time.
@@ -39,9 +32,7 @@ def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
+    .snapshots.groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
-optional = true
 python-versions = "">=3.8""
 
-[[package]]
-name = ""pandas""
-version = ""1.4.2""
-description = ""Powerful data structures for data analysis, time series, and statistics""
-category = ""main""
-optional = true
-python-versions = "">=3.8""
-
-[package.dependencies]
-numpy = [
-    {version = "">=1.18.5"", markers = ""platform_machine != \""aarch64\"" and platform_machine != \""arm64\"" and python_version < \""3.10\""""},
-    {version = "">=1.19.2"", markers = ""platform_machine == \""aarch64\"" and python_version < \""3.10\""""},
-    {version = "">=1.20.0"", markers = ""platform_machine == \""arm64\"" and python_version < \""3.10\""""},
-    {version = "">=1.21.0"", markers = ""python_version >= \""3.10\""""},
-]
-python-dateutil = "">=2.8.1""
-pytz = "">=2020.1""
-
-[package.extras]
-test = [""hypothesis (>=5.5.3)"", ""pytest (>=6.0)"", ""pytest-xdist (>=1.31)""]
-
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
@@ -137,7 +116,7 @@ name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
-optional = true
 python-versions = "">=3.7""
 
 [package.dependencies]
@@ -179,33 +158,6 @@ toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
-[[package]]
-name = ""python-dateutil""
-version = ""2.8.2""
-description = ""Extensions to the standard Python datetime module""
-category = ""main""
-optional = true
-python-versions = ""!=3.0.*,!=3.1.*,!=3.2.*,>=2.7""
-
-[package.dependencies]
-six = "">=1.5""
-
-[[package]]
-name = ""pytz""
-version = ""2022.1""
-description = ""World timezone definitions, modern and historical""
-category = ""main""
-optional = true
-python-versions = ""*""
-
-[[package]]
-name = ""six""
-version = ""1.16.0""
-description = ""Python 2 and 3 compatibility utilities""
-category = ""main""
-optional = true
-python-versions = "">=2.7, !=3.0.*, !=3.1.*, !=3.2.*""
-
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
@@ -250,15 +202,10 @@ python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
-[extras]
-full = [""pandas"", ""polars""]
-pandas = [""pandas""]
-polars = [""polars""]
-
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
-content-hash = ""8fda347438c855d8fcb0e5b6319823d331690eba7e5f28f2a7cfaa4d69b2020d""
 
 [metadata.files]
 black = [
@@ -372,29 +319,6 @@ numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
-pandas = [
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_universal2.whl"", hash = ""sha256:be67c782c4f1b1f24c2f16a157e12c2693fd510f8df18e3287c77f33d124ed07""},
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_x86_64.whl"", hash = ""sha256:5a206afa84ed20e07603f50d22b5f0db3fb556486d8c2462d8bc364831a4b417""},
-    {file = ""pandas-1.4.2-cp310-cp310-macosx_11_0_arm64.whl"", hash = ""sha256:0010771bd9223f7afe5f051eb47c4a49534345dfa144f2f5470b27189a4dd3b5""},
-    {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:3228198333dd13c90b6434ddf61aa6d57deaca98cf7b654f4ad68a2db84f8cfe""},
-    {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:5b79af3a69e5175c6fa7b4e046b21a646c8b74e92c6581a9d825687d92071b51""},
-    {file = ""pandas-1.4.2-cp310-cp310-win_amd64.whl"", hash = ""sha256:5586cc95692564b441f4747c47c8a9746792e87b40a4680a2feb7794defb1ce3""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_universal2.whl"", hash = ""sha256:061609334a8182ab500a90fe66d46f6f387de62d3a9cb9aa7e62e3146c712167""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_x86_64.whl"", hash = ""sha256:b8134651258bce418cb79c71adeff0a44090c98d955f6953168ba16cc285d9f7""},
-    {file = ""pandas-1.4.2-cp38-cp38-macosx_11_0_arm64.whl"", hash = ""sha256:df82739e00bb6daf4bba4479a40f38c718b598a84654cbd8bb498fd6b0aa8c16""},
-    {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:385c52e85aaa8ea6a4c600a9b2821181a51f8be0aee3af6f2dcb41dafc4fc1d0""},
-    {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:295872bf1a09758aba199992c3ecde455f01caf32266d50abc1a073e828a7b9d""},
-    {file = ""pandas-1.4.2-cp38-cp38-win32.whl"", hash = ""sha256:95c1e422ced0199cf4a34385ff124b69412c4bc912011ce895582bee620dfcaa""},
-    {file = ""pandas-1.4.2-cp38-cp38-win_amd64.whl"", hash = ""sha256:5c54ea4ef3823108cd4ec7fb27ccba4c3a775e0f83e39c5e17f5094cb17748bc""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_universal2.whl"", hash = ""sha256:c072c7f06b9242c855ed8021ff970c0e8f8b10b35e2640c657d2a541c5950f59""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_x86_64.whl"", hash = ""sha256:f549097993744ff8c41b5e8f2f0d3cbfaabe89b4ae32c8c08ead6cc535b80139""},
-    {file = ""pandas-1.4.2-cp39-cp39-macosx_11_0_arm64.whl"", hash = ""sha256:ff08a14ef21d94cdf18eef7c569d66f2e24e0bc89350bcd7d243dd804e3b5eb2""},
-    {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:8c5bf555b6b0075294b73965adaafb39cf71c312e38c5935c93d78f41c19828a""},
-    {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:51649ef604a945f781105a6d2ecf88db7da0f4868ac5d45c51cb66081c4d9c73""},
-    {file = ""pandas-1.4.2-cp39-cp39-win32.whl"", hash = ""sha256:d0d4f13e4be7ce89d7057a786023c461dd9370040bdb5efa0a7fe76b556867a0""},
-    {file = ""pandas-1.4.2-cp39-cp39-win_amd64.whl"", hash = ""sha256:09d8be7dd9e1c4c98224c4dfe8abd60d145d934e9fc1f5f411266308ae683e6a""},
-    {file = ""pandas-1.4.2.tar.gz"", hash = ""sha256:92bc1fc585f1463ca827b45535957815b7deb218c549b7c18402c322c7549a12""},
-]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
@@ -475,18 +399,6 @@ pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
-python-dateutil = [
-    {file = ""python-dateutil-2.8.2.tar.gz"", hash = ""sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86""},
-    {file = ""python_dateutil-2.8.2-py2.py3-none-any.whl"", hash = ""sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9""},
-]
-pytz = [
-    {file = ""pytz-2022.1-py2.py3-none-any.whl"", hash = ""sha256:e68985985296d9a66a881eb3193b0906246245294a881e7c8afe623866ac6a5c""},
-    {file = ""pytz-2022.1.tar.gz"", hash = ""sha256:1e760e2fe6a8163bc0b3d9a19c4f84342afa0a2affebfaa84b01b978a02ecaa7""},
-]
-six = [
-    {file = ""six-1.16.0-py2.py3-none-any.whl"", hash = ""sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254""},
-    {file = ""six-1.16.0.tar.gz"", hash = ""sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926""},
-]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
+optional = false
 python-versions = "">=3.8""
 
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
@@ -137,7 +116,7 @@ name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
+optional = false
 python-versions = "">=3.7""
 
 [package.dependencies]
@@ -179,33 +158,6 @@ toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
@@ -250,15 +202,10 @@ python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
+content-hash = ""5509c339bc3154af4b5c349b84ad6a79801ec11bd63a83881a76c9c664791262""
 
 [metadata.files]
 black = [
@@ -372,29 +319,6 @@ numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
@@ -475,18 +399,6 @@ pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
-
-# Optional DataFrame libraries
-pandas = { version = ""^1.4.2"", optional = true }
-polars = { version = ""^0.13.38"", optional = true }
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
-[tool.poetry.extras]
-pandas = [""pandas""]
-polars = [""polars""]
-full = [""pandas"", ""polars""] # all features
-
 [tool.isort]
 profile = ""black""
 "
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
+polars = ""^0.13.38""
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
 [tool.isort]
 profile = ""black""
 "
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
-from .metrics import Snapshot
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
-from typing import TYPE_CHECKING, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask
@@ -10,7 +10,7 @@
 from pygame.surface import Surface
 
 from .config import BaseConfig
-from .metrics import Snapshot
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
@@ -48,19 +48,16 @@ class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
-    # Sites
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
-    # Proximity
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
-    # Config (shared with other agents too)
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
@@ -74,6 +71,9 @@ class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
@@ -86,12 +86,14 @@ def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
 
         self.__proximity = proximity
 
@@ -156,15 +158,19 @@ def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
-    def update(self):
         """"""Run your own agent logic at every tick of the simulation.
-        Every frame of the simulation, this update method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
@@ -202,8 +208,8 @@ def there_is_no_escape(self) -> bool:
 
         return changed
 
-    def update_position(self):
-        """"""Update the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
@@ -331,24 +337,42 @@ def change_image(self, index: int):
 
         self._image_index = index
 
-    def snapshot(self) -> Snapshot:
-        """"""Create a Snapshot of agent data that you're interested in.
 
-        By default the Agent will produce a Snapshot with the following data:
         - agent identifier
         - current frame
         - x and y coordinates
 
-        However, you can also add your own data by inheriting the Snapshot dataclass.
-        Add any fields that you like and then overwrite this method to produce your custom Snapshot.
 
-        Make sure to call `super().snapshot()` to collect the default Snapshot data.
         """"""
 
-        return Snapshot(
-            x=self.pos.x,
-            y=self.pos.y,
-            id=self.id,
-            frame=self.shared.counter,
-            image_index=self._image_index,
-        )"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
+from typing import TYPE_CHECKING, Any, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask
@@ -10,7 +10,7 @@
 from pygame.surface import Surface
 
 from .config import BaseConfig
+from .metrics import Metrics
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
@@ -48,19 +48,16 @@ class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
@@ -74,6 +71,9 @@ class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
+    __metrics: Metrics
+    """"""Data collection of the snapshots.""""""
+
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
@@ -86,12 +86,14 @@ def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
+        metrics: Metrics,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
+        self.__metrics = metrics
 
         self.__proximity = proximity
 
@@ -156,15 +158,19 @@ def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
+    def every_frame(self):
         """"""Run your own agent logic at every tick of the simulation.
+        Every frame of the simulation, this method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
+    def update(self):
+        self._collect_replay_data()
+        self.every_frame()
+
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
@@ -202,8 +208,8 @@ def there_is_no_escape(self) -> bool:
 
         return changed
 
+    def change_position(self):
+        """"""Change the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
@@ -331,24 +337,42 @@ def change_image(self, index: int):
 
         self._image_index = index
 
+    def save_data(self, column: str, value: Any):
+        """"""Add extra data to the simulation's metrics.
 
+        The following data is collected automatically:
         - agent identifier
         - current frame
         - x and y coordinates
 
+        Examples
+        --------
 
+        Saving the number of agents in radius:
+
+        >>> from vi import Agent
+        >>> class MyAgent(Agent):
+        ...     def every_frame(self):
+        ...         in_radius = len(self.in_radius())
+        ...         self.save_data(""in_radius"", in_radius)
         """"""
 
+        self.__metrics._temporary_snapshots[column].append(value)
+
+    def _collect_replay_data(self):
+        """"""Add the minimum data needed for the replay simulation to the dataframe.""""""
+
+        x, y = round_pos(self.pos)
+        snapshots = self.__metrics._temporary_snapshots
+
+        snapshots[""frame""].append(self.shared.counter)
+        snapshots[""id""].append(self.id)
+
+        snapshots[""x""].append(x)
+        snapshots[""y""].append(y)
+
+        snapshots[""image_index""].append(self._image_index)
+
+        if self.config.image_rotation:
+            angle = self.move.angle_to(Vector2((0, -1)))
+            snapshots[""angle""].append(round(angle))"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
-import dataclasses
 from dataclasses import dataclass, field
-from typing import TYPE_CHECKING, Any
 
-if TYPE_CHECKING:
-    from pandas import DataFrame as PandasDataFrame
-    from polars import DataFrame as PolarsDataFrame
-    from polars import Series as PolarsSeries
-
-
-@dataclass
-class Snapshot:
-    """"""Data that's collected for every agent in every frame of the simulation.""""""
-
-    frame: int
-    """"""The current frame of the simulation.""""""
-
-    id: int
-    """"""The identifier of the agent.""""""
-
-    x: float
-    """"""The x coordinate of the agent.""""""
-
-    y: float
-    """"""The y coordinate of the agent.""""""
-
-    image_index: int
-    """"""The current index of the image list.""""""
-
-    def as_dict(self) -> dict[str, Any]:
-        """"""Convert this Snapshot into a dictionary.""""""
-
-        return dataclasses.asdict(self)
 
 
 @dataclass
@@ -42,28 +14,29 @@ class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
-    def to_polars(self) -> PolarsSeries:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
-@dataclass
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
-    fps: Fps = field(default_factory=Fps)
     """"""The frames-per-second history to analyse performance.""""""
 
-    snapshots: list[dict[str, Any]] = field(default_factory=list)
-    """"""The most important data (snapshot) of every agent at every moment in time.""""""
 
-    def to_pandas(self) -> PandasDataFrame:
-        import pandas as pd
 
-        return pd.DataFrame(self.snapshots)
 
-    def to_polars(self) -> PolarsDataFrame:
-        import polars as pl
 
-        return pl.from_dicts(self.snapshots)"
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
+from collections import defaultdict
 from dataclasses import dataclass, field
+from typing import Any
 
+import polars as pl
 
 
 @dataclass
@@ -42,28 +14,29 @@ class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
+    def to_polars(self) -> pl.Series:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
+    fps: Fps
     """"""The frames-per-second history to analyse performance.""""""
 
+    _temporary_snapshots: defaultdict[str, list[Any]]
+    snapshots: pl.DataFrame
 
+    def __init__(self):
+        self.fps = Fps()
+        self._temporary_snapshots = defaultdict(list)
+        self.snapshots = pl.DataFrame()
 
+    def merge(self):
+        df = pl.from_dict(self._temporary_snapshots)
 
+        self.snapshots.vstack(df, in_place=True)
 
+        self._temporary_snapshots = defaultdict(list)"
KO;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
-from .util import load_image, load_images, round_pos
 
 if TYPE_CHECKING:
     from .agent import Agent
@@ -148,6 +148,7 @@ def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
             )
 
         return self
@@ -173,6 +174,7 @@ def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
         )
 
         return self
@@ -272,8 +274,8 @@ def tick(self):
         # Update all agents
         self._all.update()
 
-        # Snapshot marked agent data
-        self.__save_snapshots()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
@@ -306,16 +308,7 @@ def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
-            agent.update_position()
-
-    def __save_snapshots(self):
-        """"""Save a Snapshot of each agent and add it to Metrics.""""""
-
-        for sprite in self._agents.sprites():
-            agent: Agent = sprite  # type: ignore
-            snapshot = agent.snapshot()
-
-            self.__metrics.snapshots.append(snapshot.as_dict())
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
OK;3;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
+from .util import load_image, load_images
 
 if TYPE_CHECKING:
     from .agent import Agent
@@ -148,6 +148,7 @@ def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
+                metrics=self.__metrics,
             )
 
         return self
@@ -173,6 +174,7 @@ def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
+            metrics=self.__metrics,
         )
 
         return self
@@ -272,8 +274,8 @@ def tick(self):
         # Update all agents
         self._all.update()
 
+        # Merge the collected snapshots into the dataframe.
+        self.__metrics.merge()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
@@ -306,16 +308,7 @@ def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
+            agent.change_position()
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
KO;4;leonweber;biomuppet;685303a6ddb88d8850800e8efb0354a57831da47;garbo overlap tfidf - needs memory;"def compute_overlap(
     # The cosine_similarity fxn will fail here due to memory issues
     # we can home cook a cos-sim, but i wonder if a matrix math is sufficient
 
-
     # Compute the max overlap between the 2
     t0 = time()
-    #overlap = cosine_similarity(btok_mat, mtok_mat)
     overlap = btok_mat * mtok_mat.T 
     print(time() - t0, ""Elapsed time"")
 
     # TODO: LEON
     # I needed to write a loop as just a overlap.max(axis=0) will collapse due to size
     # Ideally here, you just need to ask if, for an entry in the machamp dataset
     # who the largest 
     overlap_sents = []
     for idx in tqdm(range(mtok_mat.shape[0])):
         if overlap[:, idx].max() >= thresh:"
OK;4;leonweber;biomuppet;685303a6ddb88d8850800e8efb0354a57831da47;garbo overlap tfidf - needs memory;"def compute_overlap(
     # The cosine_similarity fxn will fail here due to memory issues
     # we can home cook a cos-sim, but i wonder if a matrix math is sufficient
 
     # Compute the max overlap between the 2
     t0 = time()
+    #overlap = cosine_similarity(btok_mat, mtok_mat) 
     overlap = btok_mat * mtok_mat.T 
+    
     print(time() - t0, ""Elapsed time"")
 
     # TODO: LEON
     # I needed to write a loop as just a overlap.max(axis=0) will collapse due to size
     # Ideally here, you just need to ask if, for an entry in the machamp dataset
     # who the largest 
+
+    # Ideally you want to do:
+    #overlap.max(axis=1) > 0.8 (find any machamp entry with the largest BLURB overlap, if the blurb cos-sim is over let's say 0.8, then it has a high similarity to a sentence in BLURB
+    # Collapse over all blurb indices 
+    # The matrix overlap is N_examples_in_blurb x N_examples_in_machamp
     overlap_sents = []
     for idx in tqdm(range(mtok_mat.shape[0])):
         if overlap[:, idx].max() >= thresh:"
KO;4;JakubSzukala;car-control-reinforcement-learning;39ff5c13fedcdcef9075ac1f8ea7051ecf0844b2;add replay memory;"Some initial remarks:
 * The return function is blurry and unreadable so maybe mask it somehow? To not distract / create some random correlation
 * Other drive statistics are barely visible, will have to see when the car actually drives but maybe they can stay?
 * Rest of the image seems to have satisfying quality
-
 
 "
OK;4;JakubSzukala;car-control-reinforcement-learning;39ff5c13fedcdcef9075ac1f8ea7051ecf0844b2;add replay memory;"Some initial remarks:
 * The return function is blurry and unreadable so maybe mask it somehow? To not distract / create some random correlation
 * Other drive statistics are barely visible, will have to see when the car actually drives but maybe they can stay?
 * Rest of the image seems to have satisfying quality
+* Make it to Gray scale?
 
 "
KO;4;JakubSzukala;car-control-reinforcement-learning;39ff5c13fedcdcef9075ac1f8ea7051ecf0844b2;add replay memory;" device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
 
 env = gym.make('CarRacing-v1')
-#env = gym.make('LunarLander-v2')
 env.action_space.seed(42)
 
 observation, info = env.reset(seed=42, return_info=True)
 
 for _ in range(1000):
-
     screen = get_screen(env)
     plt.figure()
     plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),"
OK;4;JakubSzukala;car-control-reinforcement-learning;39ff5c13fedcdcef9075ac1f8ea7051ecf0844b2;add replay memory;" device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
 
 env = gym.make('CarRacing-v1')
+# For gray scale we can use wrapper:
+# env = GrayScaleObservation(gym.make('CarRacing-v1'))
+
 env.action_space.seed(42)
 
 observation, info = env.reset(seed=42, return_info=True)
 
 for _ in range(1000):
     screen = get_screen(env)
     plt.figure()
     plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),"
KO;4;JakubSzukala;car-control-reinforcement-learning;39ff5c13fedcdcef9075ac1f8ea7051ecf0844b2;add replay memory;" import matplotlib
 import matplotlib.pyplot as plt
 
 import torch
 import torch.nn as nn
 import torch.optim as optim
 import torch.nn.functional as F
 import torchvision.transforms as T"
OK;4;JakubSzukala;car-control-reinforcement-learning;39ff5c13fedcdcef9075ac1f8ea7051ecf0844b2;add replay memory;" import matplotlib
 import matplotlib.pyplot as plt
 
+from collections import namedtuple, deque
+
 import torch
 import torch.nn as nn
 import torch.optim as optim
 import torch.nn.functional as F
 import torchvision.transforms as T
+
+# same as tuple, but access by name, self documentation and neat __repr__
+Transition = namedtuple('Transition', 
+        ('state', 'action', 'next_state', 'reward'))
+
+class ReplayMemory(object):
+    def __init__(self, capacity):
+        # Deque for faster append / pop than list
+        self.memory = deque([], maxlen=capacity)
+
+    
+    # any num of args, * transforms into iterable
+    def push(self, *args):
+        self.memory.append(Transition(*args))
+
+
+    def sample(self, batch_size):
+        return random.sample(self.memory, batch_size)
+
+
+    def __len__(self):
+        return len(self.memory)
+"
KO;4;zxx2643;nn-pde-solver;211c4dbe827c230cf954ce504bc719c94af1772a;control gpu memory growing for tf;" import sys
 sys.path.append('../')
 from pde_system_diffusion_steady_state import WeakPDESteadyStateDiffusion as thisPDESystem
 
 if __name__ == '__main__':"
OK;4;zxx2643;nn-pde-solver;211c4dbe827c230cf954ce504bc719c94af1772a;control gpu memory growing for tf;" import sys
 sys.path.append('../')
+import tensorflow as tf
+physical_devices = tf.config.list_physical_devices('GPU')
+try:
+  tf.config.experimental.set_memory_growth(physical_devices[0], True)
+except:
+  # Invalid device or cannot modify virtual devices once initialized.
+  pass
 from pde_system_diffusion_steady_state import WeakPDESteadyStateDiffusion as thisPDESystem
 
 if __name__ == '__main__':"
KO;4;zxx2643;nn-pde-solver;211c4dbe827c230cf954ce504bc719c94af1772a;control gpu memory growing for tf;" DataPath = data/octagon-32x32/
 DataFolder = DNS/
 DataAugTimes = 10
-Epochs = 1000
 InitialEpoch = 10
 BatchSize = 256
 Optimizer = Nadam"
OK;4;zxx2643;nn-pde-solver;211c4dbe827c230cf954ce504bc719c94af1772a;control gpu memory growing for tf;" DataPath = data/octagon-32x32/
 DataFolder = DNS/
 DataAugTimes = 10
+Epochs = 20
 InitialEpoch = 10
 BatchSize = 256
 Optimizer = Nadam"
KO;4;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
-    partition_group = row.partition_group
     embeddings = row.embeddings
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
-    save_path = os.path.join(fs_base_path, f""img_emb_{shard_index}-{partition_group}.npy"")
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
@@ -127,7 +131,7 @@ def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
-    if working_fs.exists(output_folder_path) and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
@@ -138,7 +142,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
-    if working_fs.exists(meta_embed_folder_path) and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
@@ -149,7 +153,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
-    if working_fs.exists(empty_embed_folder_path) and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
@@ -171,6 +175,9 @@ def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
@@ -194,7 +201,7 @@ def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
-    example_embedding = np.array(data.first().embeddings)
 
     end_recall_timer()
 
@@ -204,6 +211,7 @@ def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
@@ -249,13 +257,20 @@ def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
         grouped = (
-            data.orderBy(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
OK;4;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
     embeddings = row.embeddings
+    if ""partition_group"" in row:
+        partition_group = row.partition_group
+        filename = f""img_emb_{shard_index}-{partition_group}.npy""
+    else:
+        filename = f""img_emb_{shard_index}.npy""
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
+    save_path = os.path.join(fs_base_path, filename)
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
@@ -127,7 +131,7 @@ def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
+    if working_fs.exists(output_folder_path) and len(working_fs.ls(output_folder_path)) > 0 and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
@@ -138,7 +142,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
+    if working_fs.exists(meta_embed_folder_path) and len(working_fs.ls(meta_embed_folder_path)) > 0 and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
@@ -149,7 +153,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
+    if working_fs.exists(empty_embed_folder_path) and len(working_fs.ls(empty_embed_folder_path)) > 0 and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
@@ -171,6 +175,9 @@ def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
+        .config(""spark.sql.shuffle.partitions"", ""1000000"")
+        .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
+        .config(""spark.kryoserializer.buffer"", ""1g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
@@ -194,7 +201,7 @@ def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
+    print(f""Data has {data.rdd.getNumPartitions()} partitions"")
 
     end_recall_timer()
 
@@ -204,6 +211,7 @@ def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
+        example_embedding = np.array(data.first().embeddings)
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
@@ -249,13 +257,20 @@ def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
+        print(""Number of partitions"", data.rdd.getNumPartitions())
+        # data = data.repartition(""img_shard"")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
+        data = data.repartition(""img_shard"", ""partition_group"")
+        print(""Number of partitions"", data.rdd.getNumPartitions())
         grouped = (
+            data
+            # .orderBy(""img_index"")
+            .sortWithinPartitions(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
+            # .groupBy(""img_shard"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
KO;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
 
 import findspark
 
@@ -152,6 +154,8 @@ def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
@@ -164,6 +168,8 @@ def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
@@ -190,12 +196,14 @@ def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
@@ -215,6 +223,8 @@ def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
@@ -225,21 +235,58 @@ def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
-        added_df = spark.createDataFrame(added_data, [""img_shard"", ""img_index"", ""embeddings""])
         data = data.union(added_df)
 
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
-    # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
-    # I now know what happens. It immediatly crashes.
 
-    # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
OK;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
+import time
+import plotext as plt
 
 import findspark
 
@@ -152,6 +154,8 @@ def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
+    start_time = time.perf_counter()
+
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
@@ -164,6 +168,8 @@ def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
+    start_embedding_load_time = time.perf_counter()
+
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
@@ -190,12 +196,14 @@ def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
+    start_recall_time = time.perf_counter()
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
+    start_missing_fill_time = time.perf_counter()
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
@@ -215,6 +223,8 @@ def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
+        current_amount = 0
+        added_files = 0
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
@@ -225,21 +235,58 @@ def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
+                current_amount += 1
+                if current_amount > 1000:
+                    df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
+                    with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
+                        df.to_parquet(f)
+                    added_data.clear()
+                    current_amount = 0
+                    added_files += 1
+        df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
+        with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
+            df.to_parquet(f)
+        empty_path = os.path.join(output_base_path, intermediate_folder, ""empty_*.parquet"")
+        added_df = spark.read.parquet(empty_path)
         data = data.union(added_df)
 
+    full_count = data.count()
+
+    start_export_time = time.perf_counter()
+
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
+    # # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
+    # # I now know what happens. It immediatly crashes.
 
+    # # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
+    end_time = time.perf_counter()
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
+
+    embed_reader_initialization_time = start_embedding_load_time - start_time
+    embedding_load_time = start_recall_time - start_embedding_load_time
+    recall_time = start_missing_fill_time - start_recall_time
+    missing_fill_time = start_export_time - start_missing_fill_time
+    export_time = end_time - start_export_time
+    total_time = end_time - start_time
+
+    print(f""Total Execution Time: {total_time:0.2f}s"")
+
+    tasks = list(reversed([""Initialize Embedding Reader"", ""Load Embeddings"", ""Recall Embeddings"", ""Insert Missing Embeddings"", ""Save Embeddings""]))
+    times = list(reversed([embed_reader_initialization_time, embedding_load_time, recall_time, missing_fill_time, export_time]))
+    plt.bar(tasks, times, orientation=""horizontal"", width = 0.3)
+    plt.clc()
+    plt.xlabel(""Execution Time (s)"")
+    plt.show()
+
+    
+
+
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
KO;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
-tqdm>=4.60.0
\ No newline at end of file
\ No newline at end of file"
OK;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
\ No newline at end of file
+tqdm>=4.60.0
+plotext>=5.0.2
\ No newline at end of file"
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"click-left = ~/.config/polybar/scripts/screenshot
 
 [module/recorder]
 type = custom/text
-content = 
 content-foreground = ${colors.magenta}
 content-padding = 1
 content-font = 3"
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"click-left = ~/.config/polybar/scripts/screenshot
 
 [module/recorder]
 type = custom/text
+content = 
 content-foreground = ${colors.magenta}
 content-padding = 1
 content-font = 3"
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"   },
   ""cpu"": {
     ""interval"": 2,
-    ""format"": ""CPU: {usage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""idle_inhibitor"": {
@@ -32,7 +32,7 @@
   },
   ""memory"": {
     ""interval"": 2,
-    ""format"": ""MEM: {percentage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""network"": {"
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"   },
   ""cpu"": {
     ""interval"": 2,
+    ""format"": ""<span size='14pt'> </span> {usage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""idle_inhibitor"": {
@@ -32,7 +32,7 @@
   },
   ""memory"": {
     ""interval"": 2,
+    ""format"": ""<span size='14pt'></span>  {percentage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""network"": {"
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"   },
   ""cpu"": {
     ""interval"": 2,
-    ""format"": ""CPU: {usage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""idle_inhibitor"": {
@@ -32,7 +32,7 @@
   },
   ""memory"": {
     ""interval"": 2,
-    ""format"": ""MEM: {percentage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""network"": {"
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"   },
   ""cpu"": {
     ""interval"": 2,
+    ""format"": ""<span size='14pt'> </span> {usage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""idle_inhibitor"": {
@@ -32,7 +32,7 @@
   },
   ""memory"": {
     ""interval"": 2,
+    ""format"": ""<span size='14pt'></span>  {percentage}%"",
     ""on-click"": ""alacritty -a btop -e btop""
   },
   ""network"": {"
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"set theme ~/.config/rofi/wayland/colorpicker.rasi
 
 function markup
   if test -e $marker
-    echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""colorpicker-active"" }'
   else
-    echo '{ ""text"": """", ""tooltip"": ""Color Picker"", ""class"": ""colorpicker"" }'
   end
 end
 "
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"set theme ~/.config/rofi/wayland/colorpicker.rasi
 
 function markup
   if test -e $marker
+    echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""colorpicker-active"" }'
   else
+    echo '{ ""text"": """", ""tooltip"": ""Color Picker"", ""class"": ""colorpicker"" }'
   end
 end
 "
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"function markup
   if test -e $marker
     echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""launcher-active"" }'
   else
-    echo '{ ""text"": """", ""tooltip"": ""Power Menu"", ""class"": ""launcher"" }'
   end
 end
 "
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"function markup
   if test -e $marker
     echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""launcher-active"" }'
   else
+    echo '{ ""text"": """", ""tooltip"": ""Power Menu"", ""class"": ""launcher"" }'
   end
 end
 "
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"mkdir -p $savepath
 
 function markup
   if test -e $marker
-    echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""recorder-active"" }'
   else
-    echo '{ ""text"": """", ""tooltip"": ""Recorder"", ""class"": ""recorder"" }'
   end
 end
 "
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"mkdir -p $savepath
 
 function markup
   if test -e $marker
+    echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""recorder-active"" }'
   else
+    echo '{ ""text"": """", ""tooltip"": ""Recorder"", ""class"": ""recorder"" }'
   end
 end
 "
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"mkdir -p $savepath
 
 function markup
   if test -e $marker
-    echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""screenshot-active"" }'
   else
-    echo '{ ""text"": """", ""tooltip"": ""Screenshot"", ""class"": ""screenshot"" }'
   end
 end
 "
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"mkdir -p $savepath
 
 function markup
   if test -e $marker
+    echo '{ ""text"": """", ""tooltip"": ""Close Overlay"", ""class"": ""screenshot-active"" }'
   else
+    echo '{ ""text"": """", ""tooltip"": ""Screenshot"", ""class"": ""screenshot"" }'
   end
 end
 "
KO;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"tooltip label {
 /* CPU & Memory */
 #cpu,
 #memory {
-  min-width: 90px;
-  padding-top: 6px;
   font-family: ""Iosevka Nerd Font"";
-  font-size: 15px;
   font-weight: 600;
 }
 
@@ -104,8 +104,8 @@ tooltip label {
   min-width: 60px;
   padding-top: 4px;
   font-family: ""Iosevka Nerd Font"";
-  font-size: 16px;
-  font-weight: 800;
   color: @fg3;
 }
 "
OK;4;jadnw;jetdots;d118ffb5ef2cf756523adcb559b7c784173667cc;:hammer: Change cpu & memory icons;"tooltip label {
 /* CPU & Memory */
 #cpu,
 #memory {
+  min-width: 80px;
+  padding-top: 4px;
   font-family: ""Iosevka Nerd Font"";
+  font-size: 16px;
   font-weight: 600;
 }
 
@@ -104,8 +104,8 @@ tooltip label {
   min-width: 60px;
   padding-top: 4px;
   font-family: ""Iosevka Nerd Font"";
+  font-size: 18px;
+  font-weight: 700;
   color: @fg3;
 }
 "
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
-            symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
-        fat_header = self.SDAT.read_struct(info.NNSSndArcFat, header.fatOffset)
-        fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
-        info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
+            symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
+        fat_header = self.SDAT.read_struct(info.NNSSndArcFat, offset=header.fatOffset)
+        fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, offset=header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
+        info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
-    def read_seqarc_strings(cls, offset: int, sdat: SdatIO):
         def inner():
-            for x in sdat.read_array(cls, offset):
                 symbol = sdat.get_string(offset, x.symbol)
-                table = NNSSndArcOffsetTable.read_strings(offset + x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
@@ -177,7 +180,9 @@ class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
-    def read_file(self, base: int, sdat: SdatIO) -> bytearray:
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
@@ -211,15 +216,21 @@ class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
-        return [sdat.read_struct(sbcls, base + x.offset) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
-        return [list_factory(sdat.read_array(sbcls, base + x.offset)) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
-        return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base + offset)]
 
 
 # Non-C-types
@@ -243,7 +254,7 @@ def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
-            NNSSndArcSeqArcOffset.read_seqarc_strings(offset + header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
@@ -297,6 +308,7 @@ def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
@@ -306,20 +318,27 @@ def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
-                    result[kind.name].append([dataclasses.asdict(x) for x in info])
                 else:
-                    result[kind.name].append(dataclasses.asdict(info))
-                result[kind.name][-1]['name'] = getattr(info, 'name', f'{kind.name}_{i:03d}')
-                if hasattr(info, 'arc_names'):
-                    result[kind.name][-1]['arc_names'] = info.arc_names
-                if hasattr(info, 'filename'):
-                    result[kind.name][-1]['filename'] = info.filename
         return result
 
     def dump_files(self, files, outdir):"
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
+    def read_seqarc_strings(cls, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+
         def inner():
+            for x in sdat.read_array(cls, base, offset):
                 symbol = sdat.get_string(offset, x.symbol)
+                table = NNSSndArcOffsetTable.read_strings(offset, x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
@@ -177,7 +180,9 @@ class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
+    def read_file(self, base: int, sdat: SdatIO) -> typing.ByteString:
+        if base == 0:
+            return b''
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
@@ -211,15 +216,21 @@ class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+        return [sdat.read_struct(sbcls, base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
+        if 0 in (base, offset):
+            return []
+        return [list_factory(sdat.read_array(sbcls, base, x.offset)) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+        return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
 
 # Non-C-types
@@ -243,7 +254,7 @@ def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
+            NNSSndArcSeqArcOffset.read_seqarc_strings(offset, header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
@@ -297,6 +308,7 @@ def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
+                    assert info.fileId < 65536
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
@@ -306,20 +318,27 @@ def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
+    @staticmethod
+    def single_to_dict(info, index, idx2=None):
+        if not dataclasses.is_dataclass(info):
+            return {}
+        ret = dataclasses.asdict(info)
+        ret['name'] = getattr(info, 'name', f'{info._kind.name}_{index:03d}' + '' if idx2 is None else f'_{idx2:03d}')
+        if hasattr(info, 'arc_names'):
+            ret['arc_names'] = info.arc_names
+        if hasattr(info, 'filename'):
+            ret['filename'] = info.filename
+        return ret
+
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
+                    result[kind.name].append([InfoData.single_to_dict(x, i, j) for j, x in enumerate(info)])
                 else:
+                    result[kind.name].append(InfoData.single_to_dict(info, i))
         return result
 
     def dump_files(self, files, outdir):"
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
-        if offset == 0:
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
@@ -154,17 +154,25 @@ def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
-    def read_struct(self, struct: NamedStruct, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
-    def read_array(self, struct: NamedStruct, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
@@ -184,6 +192,7 @@ def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
         if not objs:
             self.write_long(offset, 0)
         else:"
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
+        if 0 in (base, offset):
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
@@ -154,17 +154,25 @@ def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
+    def read_struct(self, struct: NamedStruct, base: int = None, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
+        if 0 in (base, offset):
+            return None
+        if base is not None and offset is not None:
+            offset += base
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
+    def read_array(self, struct: NamedStruct, base: int = None, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
+        if 0 in (base, offset):
+            return []
+        if base is not None and offset is not None:
+            offset += base
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
@@ -184,6 +192,7 @@ def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
+        assert offset != 0
         if not objs:
             self.write_long(offset, 0)
         else:"
KO;4;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
-                    def _spherical_func(viewdirs):
-                        raw_rgb, sigma = eval_fn(
-                            grid_chunk[:, None], dirs=viewdirs)
-                        return raw_rgb, sigma
-
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
OK;4;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
+            
+            def _spherical_func(viewdirs):
+                raw_rgb, sigma = eval_fn(
+                    grid_chunk[:, None], dirs=viewdirs)
+                return raw_rgb, sigma
+                    
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;4;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
-                    def _spherical_func(viewdirs):
-                        raw_rgb, sigma = eval_fn(
-                            grid_chunk[:, None], dirs=viewdirs)
-                        return raw_rgb, sigma
-
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
OK;4;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
+            
+            def _spherical_func(viewdirs):
+                raw_rgb, sigma = eval_fn(
+                    grid_chunk[:, None], dirs=viewdirs)
+                return raw_rgb, sigma
+                    
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
-                ],
-                ""uses"": 0
             }
         }
     },
     ""2"": {
         ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
             ""deadline"": 3,
             ""pages"": 10,
-            ""already_exec"": 4
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -76,8 +79,10 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -89,35 +94,46 @@
                     8,
                     9
                 ],
-                ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 2,
-            ""execution_time"": 2,
-            ""deadline"": 3,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
-            ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
-        ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -128,8 +144,15 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -141,40 +164,48 @@
                     8,
                     9
                 ],
-                ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
-            ""name"": ""C"",
-            ""arrival_time"": 4,
-            ""execution_time"": 1,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 1
         },
-        ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
-            ""D""
         ],
-        ""done_in_this_cicle"": true,
-        ""time"": 9,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -185,8 +216,20 @@
                     7,
                     8,
                     9
-                ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -198,44 +241,46 @@
                     8,
                     9
                 ],
-                ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
-            ""name"": ""D"",
-            ""arrival_time"": 6,
-            ""execution_time"": 3,
-            ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
-        ""done_in_this_cicle"": false,
-        ""time"": 12,
-        ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""D"": {
                 ""real"": [
                     0,
                     1,
@@ -259,9 +304,39 @@
                     7,
                     8,
                     9
-                ],
                 ""uses"": 0
             }
         }
     },
     ""6"": {
@@ -274,11 +349,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 13,
-        ""started_time"": 12,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,19 +384,24 @@
                     9
                 ],
                 ""virtual"": [
-                    0,
-                    1,
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9
-                ],
-                ""uses"": 1
             }
         }
     }
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 1
         }
     },
     ""2"": {
         ""process"": {
+            ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
             ""deadline"": 3,
             ""pages"": 10,
+            ""already_exec"": 2
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
+                ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -76,8 +79,10 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -89,35 +94,46 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0
         }
     },
     ""3"": {
         ""process"": {
+            ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
+            ""deadline"": 7,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
+        ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -128,8 +144,15 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -141,40 +164,48 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0
         }
     },
     ""4"": {
         ""process"": {
+            ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 2
         },
+        ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
+            ""D"",
+            ""A""
         ],
+        ""done_in_this_cicle"": false,
+        ""time"": 10,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -185,8 +216,20 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -198,44 +241,46 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 1
         }
     },
     ""5"": {
         ""process"": {
+            ""name"": ""A"",
+            ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 3,
             ""pages"": 10,
+            ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
+        ""done_in_this_cicle"": true,
+        ""time"": 13,
+        ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
@@ -259,9 +304,39 @@
                     7,
                     8,
                     9
+                ]
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
                 ""uses"": 0
+            },
+            ""D"": {
+                ""real"": null,
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 0
         }
     },
     ""6"": {
@@ -274,11 +349,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 15,
+        ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,19 +384,24 @@
                     9
                 ],
                 ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ]
             }
+        },
+        ""memory_counter"": {
+            ""A"": 0,
+            ""B"": 0,
+            ""C"": 0,
+            ""D"": 0
         }
     }
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
 
 scalonator_translate = {
     ""FIFO"": fifo,
@@ -12,7 +14,8 @@
 }
 
 swap_translate = {
-    ""FIFO"": swap_fifo
 }
 
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
+from cpu.memory.swap_algorithm.swap_lru import swap_lru
+
 
 scalonator_translate = {
     ""FIFO"": fifo,
@@ -12,7 +14,8 @@
 }
 
 swap_translate = {
+    ""FIFO"": swap_fifo,
+    ""LRU"":swap_lru
 }
 
 "
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
-        [
-            ""A"",
-            4,
-            0
-        ],
         [
             ""B"",
-            7,
             2
         ],
         [
             ""C"",
-            9,
             4
         ],
         [
-            ""D"",
             13,
             6
         ]
     ],
-    ""0"": 5.25
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
         [
             ""B"",
+            5,
             2
         ],
         [
             ""C"",
+            7,
             4
         ],
         [
+            ""A"",
             13,
+            0
+        ],
+        [
+            ""D"",
+            15,
             6
         ]
     ],
+    ""0"": 7.0
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
-from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
@@ -24,6 +24,7 @@ def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
 
 
     def initialize(self, queue:List[ProcessIn]):
@@ -55,16 +56,15 @@ def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
-        #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
-            self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return False #Tudo certo! o processo já está carregado na memoria
             #Não precisa de OVERHEAD
         
@@ -76,7 +76,6 @@ def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
-                self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
@@ -93,15 +92,13 @@ def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
-                self.real_virtual_map[process.name][""uses""] = 0
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
-                self.real_virtual_map[process.name][""uses""] = 0 
                 self.swap(process)
                 return True
 
@@ -122,23 +119,23 @@ def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
-        #TODO Must teste this approach! It is the correct one!
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
-                self.p_order)
-        # old_p_name= self.swap_algorithm(
-            # self.p_order)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
-            self.real_virtual_map[old_p_name][""uses""] = 0
 
-        #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
-        self.real_virtual_map[process.name][""uses""] = 0 
 
         return True
          
@@ -175,9 +172,14 @@ def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
-    "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
+from collections import deque, Counter
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
@@ -24,6 +24,7 @@ def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
+        self.counter = Counter()
 
 
     def initialize(self, queue:List[ProcessIn]):
@@ -55,16 +56,15 @@ def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
+        self.counter[process.name] +=1
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return False #Tudo certo! o processo já está carregado na memoria
             #Não precisa de OVERHEAD
         
@@ -76,7 +76,6 @@ def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
@@ -93,15 +92,13 @@ def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.swap(process)
                 return True
 
@@ -122,23 +119,23 @@ def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
+        removed_p_count = 1
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
+                self.p_order, self.counter,removed_p_count)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
+            self.counter[old_p_name] = 0
 
+            removed_p_count+=1
+
+        #cadastra o novo processo na memoria real
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
+        self.counter[process.name] = 0 
 
         return True
          
@@ -175,9 +172,14 @@ def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
+
+            self.counter[p_name] = 0
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
+
+    def show_counter(self):
+        copy = json.dumps(dict(self.counter))
+        return copy"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"-from collections import deque
 
 def swap_fifo(
-    p_order:deque
 ):
     old_process_name = p_order[-1] #pega a primeira posição
     p_order.rotate()"
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import deque, Counter
 
 def swap_fifo(
+    p_order:deque,
+    counter:Counter,
+    removed_p_count:int
 ):
     old_process_name = p_order[-1] #pega a primeira posição
     p_order.rotate()"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import deque,Counter
+
+def swap_lru(
+    p_order:deque,
+    counter:Counter,
+    removed_p_count:int
+):
+    least_used = counter.most_common()[:-removed_p_count-1:-1] 
+
+    return least_used[removed_p_count-1][0] #retorna o nome do processo menos usado
+
+"
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"-from collections import deque
 
-d = deque()
 
-d.appendleft(""a"")
-d.appendleft(""b"")
-d.appendleft(""c"")
-d.appendleft(""d"")
 
 print()
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"+from collections import Counter
 
+d = Counter()
 
+d[0]+=1
+d[0]+=1
+d[1]+=1
+d[1]+=1
+d[1]+=1
 
 print()
 "
KO;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
@@ -83,19 +84,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
-        # if p.name != cache_name: #Caso o process não esteja carregado na cache
-        #     result = mmu.load_context(p)
-            
-        #     is_overhead = True
-        #     if not first:
-        #         sleep(overhead)
-        #         time_count+=overhead
-        #     first = False
-            
-        # else:
-        #     is_overhead = False
-        # is_process_done = False
-        # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -107,7 +96,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
-
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -120,14 +109,16 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
-            real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -172,7 +163,8 @@ def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
-    real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -183,6 +175,7 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
@@ -192,7 +185,8 @@ def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
-                ""real_virtual_map"": memory_map
             }
     
 "
OK;4;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
+    mmu_counter = """"
     print(""enters main loop!"")
 
     while True:
@@ -83,19 +84,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
+
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -107,7 +96,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
+                
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -120,14 +109,16 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
+        mmu_counter = mmu.show_counter()
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
+            real_virtual_map,
+            mmu_counter
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -172,7 +163,8 @@ def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
+    real_virtual_map: str,
+    mmu_counter:str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -183,6 +175,7 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
+    memory_counter=json.loads(mmu_counter)
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
@@ -192,7 +185,8 @@ def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
+                ""real_virtual_map"": memory_map,
+                ""memory_counter"":memory_counter
             }
     
 "
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 4
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -77,7 +78,10 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -89,35 +93,43 @@
                     8,
                     9
                 ],
-                ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 2,
-            ""execution_time"": 2,
-            ""deadline"": 5,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""C""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 6,
-        ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -129,7 +141,15 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -141,40 +161,44 @@
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
-            ""name"": ""C"",
-            ""arrival_time"": 4,
-            ""execution_time"": 1,
-            ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 1
         },
-        ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [
-            ""D""
         ],
-        ""done_in_this_cicle"": true,
-        ""time"": 7,
-        ""started_time"": 6,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": [
                     0,
                     1,
                     2,
@@ -186,7 +210,20 @@
                     8,
                     9
                 ],
-                ""virtual"": [
                     0,
                     1,
                     2,
@@ -198,44 +235,41 @@
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
-            ""name"": ""D"",
-            ""arrival_time"": 6,
-            ""execution_time"": 3,
-            ""deadline"": 10,
             ""pages"": 10,
-            ""already_exec"": 2
         },
         ""quantum"": 2,
-        ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
-        ""done_in_this_cicle"": false,
-        ""time"": 9,
-        ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""B"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""C"": {
-                ""real"": null,
-                ""virtual"": null,
-                ""uses"": 0
-            },
-            ""D"": {
                 ""real"": [
                     0,
                     1,
@@ -261,6 +295,32 @@
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
@@ -274,11 +334,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
-        ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 10,
-        ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,16 +369,16 @@
                     9
                 ],
                 ""virtual"": [
-                    0,
-                    1,
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9
                 ],
                 ""uses"": 1
             }"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
+            ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
+            ""deadline"": 5,
             ""pages"": 10,
+            ""already_exec"": 2
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
+                ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -77,7 +78,10 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -89,35 +93,43 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
+                ""uses"": 0
             }
         }
     },
     ""3"": {
         ""process"": {
+            ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
+            ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""A""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 7,
+        ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -129,7 +141,15 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -141,40 +161,44 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
+            ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
+            ""already_exec"": 2
         },
+        ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
+            ""D"",
+            ""A""
         ],
+        ""done_in_this_cicle"": false,
+        ""time"": 10,
+        ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
+                ""virtual"": [
                     0,
                     1,
                     2,
@@ -186,7 +210,20 @@
                     8,
                     9
                 ],
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
                     0,
                     1,
                     2,
@@ -198,44 +235,41 @@
                     8,
                     9
                 ],
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
+            ""name"": ""A"",
+            ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
+            ""already_exec"": 4
         },
         ""quantum"": 2,
+        ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
+        ""done_in_this_cicle"": true,
+        ""time"": 13,
+        ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
@@ -261,6 +295,32 @@
                     9
                 ],
                 ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": null,
+                ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
+                ],
+                ""uses"": 0
             }
         }
     },
@@ -274,11 +334,11 @@
             ""already_exec"": 3
         },
         ""quantum"": 1,
+        ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 15,
+        ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
@@ -309,16 +369,16 @@
                     9
                 ],
                 ""virtual"": [
+                    10,
+                    11,
+                    12,
+                    13,
+                    14,
+                    15,
+                    16,
+                    17,
+                    18,
+                    19
                 ],
                 ""uses"": 1
             }"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;\ No newline at end of file
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"+{
+    ""config"":{
+        ""scale_algorithm"": ""RR"",
+        ""page_algorithm"": ""FIFO"",
+        ""quantum"": 10,
+        ""overhead"":0
+    },
+    ""processes"":[
+        {
+            ""name"":""p5"",
+            ""arrival_time"":35,
+            ""execution_time"":5,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p4"",
+            ""arrival_time"":21,
+            ""execution_time"":13,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p3"",
+            ""arrival_time"":19,
+            ""execution_time"":10,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""p2"",
+            ""arrival_time"":13,
+            ""execution_time"":75,
+            ""pages"":10,
+            ""deadline"":8
+        },
+        {
+            ""name"":""p1"",
+            ""arrival_time"":5,
+            ""execution_time"":17,
+            ""pages"":10,
+            ""deadline"":5
+        },
+        {
+            ""name"":""p0"",
+            ""arrival_time"":0,
+            ""execution_time"":25,
+            ""pages"":10,
+            ""deadline"":7
+        }
+    ]
+}
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
-        [
-            ""A"",
-            4,
-            0
-        ],
         [
             ""B"",
-            6,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""D"",
-            10,
             6
         ]
     ],
-    ""0"": 3.75
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
         [
             ""B"",
+            5,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
+        [
+            ""A"",
+            13,
+            0
+        ],
         [
             ""D"",
+            15,
             6
         ]
     ],
+    ""0"": 7.0
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         #TODO Must teste this approach! It is the correct one!
-        # while not self.memory_real.does_it_fit(process.pages): 
-        #     old_p_name= self.swap_algorithm(
-        #         self.p_order)
-        old_p_name= self.swap_algorithm(
-            self.p_order)
-
-
-        #Remove o index do processo antigo da memoria real
-        list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
-        self.memory_real.remove(list_index_to_remove)
-        self.real_virtual_map[old_p_name][""real""] = None
-        self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         #TODO Must teste this approach! It is the correct one!
+        while not self.memory_real.does_it_fit(process.pages): 
+            old_p_name= self.swap_algorithm(
+                self.p_order)
+        # old_p_name= self.swap_algorithm(
+            # self.p_order)
+
+            #Remove o index do processo antigo da memoria real
+            list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
+            self.memory_real.remove(list_index_to_remove)
+            self.real_virtual_map[old_p_name][""real""] = None
+            self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
@@ -15,3 +16,20 @@ def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
+from typing import Union, List
 
 
 
@@ -15,3 +16,20 @@ def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
+def fifo_dont_use(
+    process_list:list[ProcessIn],
+    add_p:Union[list[ProcessIn],ProcessIn],
+    time_count:int=None,
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo fifo')
+    for x in process_list:
+        d.append(x)
+
+    if type(add_p) is list:
+        for p in add_p:
+            d.appendleft(p)
+    elif type(add_p) is ProcessIn:
+        d.append(add_p)
+
+    return d
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
-def rr(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
+from typing import Union, List
 
 
 
+def rr_v1(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
+
+def rr_v2(
+    process_list:list[ProcessIn],
+    time_count:int=None,
+    right:bool=False
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo rr')
+
+    for x in process_list:
+        d.append(x)
+
+
+    if right:
+        d.rotate()#rodar para a direita!
+
+    return d
+
+def rr_v3(
+    process_list:list[ProcessIn],
+    add_p:Union[list[ProcessIn],ProcessIn],
+    time_count:int=None,
+)-> deque[ProcessIn]:
+    d = deque()
+    print('fazendo rr')
+
+    for x in process_list:
+        d.append(x)
+
+    if type(add_p) is list:
+        for p in add_p:
+            d.append(p)
+    elif type(add_p) is ProcessIn:
+        d.appendleft(add_p)
+   
+    return d
+
+def rr(process_list:list[ProcessIn],time_count:int=None):
+    d = deque()
+    lesser = 9999
+    for x in process_list:
+        if x.already_exec < lesser:
+            lesser = x.already_exec
+    normalized = all([p.already_exec==lesser if p.already_exec!=0 else False for p in process_list])
+    if not normalized:        
+        process_list.sort(key=lambda x: x.already_exec - lesser, reverse=True)
+    else:
+        process_list=reversed(process_list)
+
+
+    for x in process_list:
+        d.append(x)
+    return d
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
-            queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
@@ -74,8 +74,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
-            is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
@@ -101,7 +101,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
-            sleep(1)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
@@ -117,7 +117,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
-            queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
OK;4;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
+            queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count=time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
@@ -74,8 +74,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
             if not first:
+                is_overhead = True
                 sleep(overhead)
                 time_count+=overhead
             first = False
@@ -101,7 +101,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
+            sleep(0)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
@@ -117,7 +117,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
+            queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
-        ""started_time"": 0
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 4,
-        ""started_time"": 2
-    },
-    ""3"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 6,
-        ""started_time"": 4
-    },
-    ""4"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 8
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B"",
-            ""A""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 8,
-        ""started_time"": 6
-    },
-    ""5"": {
-        ""process"": {
-            ""name"": ""A"",
-            ""arrival_time"": 0,
-            ""execution_time"": 10,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 10
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
             ""B""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 10,
-        ""started_time"": 8
     },
-    ""6"": {
         ""process"": {
             ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 12,
-        ""started_time"": 10
-    },
-    ""7"": {
-        ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
-            ""C"",
-            ""B""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 14,
-        ""started_time"": 12
-    },
-    ""8"": {
-        ""process"": {
-            ""name"": ""B"",
-            ""arrival_time"": 0,
-            ""execution_time"": 6,
-            ""deadline"": 10,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E"",
-            ""D"",
             ""C""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 16,
-        ""started_time"": 14
     },
-    ""9"": {
         ""process"": {
             ""name"": ""C"",
-            ""arrival_time"": 0,
-            ""execution_time"": 2,
             ""deadline"": 8,
             ""pages"": 10,
-            ""already_exec"": 2
         },
-        ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": true,
-        ""time"": 18,
-        ""started_time"": 16
     },
-    ""10"": {
         ""process"": {
             ""name"": ""D"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
-            ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": false,
-        ""time"": 20,
-        ""started_time"": 18
     },
-    ""11"": {
         ""process"": {
             ""name"": ""D"",
-            ""arrival_time"": 0,
-            ""execution_time"": 4,
-            ""deadline"": 5,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": true,
-        ""time"": 22,
-        ""started_time"": 20
-    },
-    ""12"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 2
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 24,
-        ""started_time"": 22
-    },
-    ""13"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 4
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 26,
-        ""started_time"": 24
-    },
-    ""14"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
-            ""pages"": 10,
-            ""already_exec"": 6
-        },
-        ""quantum"": 2,
-        ""overhead"": 0,
-        ""next_processess"": [
-            ""E""
-        ],
-        ""done_in_this_cicle"": false,
-        ""time"": 28,
-        ""started_time"": 26
-    },
-    ""15"": {
-        ""process"": {
-            ""name"": ""E"",
-            ""arrival_time"": 0,
-            ""execution_time"": 8,
-            ""deadline"": 7,
             ""pages"": 10,
-            ""already_exec"": 8
         },
-        ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
-        ""time"": 30,
-        ""started_time"": 28
     }
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
+        ""started_time"": 0,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
+            ""execution_time"": 4,
+            ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""B""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 4,
+        ""started_time"": 2,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 1
+            }
+        }
     },
+    ""3"": {
         ""process"": {
             ""name"": ""B"",
+            ""arrival_time"": 2,
+            ""execution_time"": 2,
+            ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""C""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 6,
+        ""started_time"": 4,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""4"": {
         ""process"": {
             ""name"": ""C"",
+            ""arrival_time"": 4,
+            ""execution_time"": 1,
             ""deadline"": 8,
             ""pages"": 10,
+            ""already_exec"": 1
         },
+        ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
+        ""time"": 7,
+        ""started_time"": 6,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""5"": {
         ""process"": {
             ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": false,
+        ""time"": 9,
+        ""started_time"": 7,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 0
+            }
+        }
     },
+    ""6"": {
         ""process"": {
             ""name"": ""D"",
+            ""arrival_time"": 6,
+            ""execution_time"": 3,
+            ""deadline"": 10,
             ""pages"": 10,
+            ""already_exec"": 3
         },
+        ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
+        ""time"": 10,
+        ""started_time"": 9,
+        ""real_virtual_map"": {
+            ""A"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""B"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""C"": {
+                ""real"": null,
+                ""virtual"": null,
+                ""uses"": 0
+            },
+            ""D"": {
+                ""real"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""virtual"": [
+                    0,
+                    1,
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9
+                ],
+                ""uses"": 1
+            }
+        }
     }
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
-            10,
             0
         ],
         [
             ""B"",
-            16,
-            0
         ],
         [
             ""C"",
-            18,
-            0
         ],
         [
             ""D"",
-            22,
-            0
-        ],
-        [
-            ""E"",
-            30,
-            0
         ]
     ],
-    ""0"": 19.2
 }
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
+            4,
             0
         ],
         [
             ""B"",
+            6,
+            2
         ],
         [
             ""C"",
+            7,
+            4
         ],
         [
             ""D"",
+            10,
+            6
         ]
     ],
+    ""0"": 3.75
 }
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
-
 
 class MMU:
     
@@ -120,7 +120,12 @@ def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
-
         old_p_name= self.swap_algorithm(
             self.p_order)
 
@@ -173,5 +178,7 @@ def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
-
     "
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
+import json
 
 class MMU:
     
@@ -120,7 +120,12 @@ def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
+        #Enquanto não tiver espaço, fazer  o swap para ter espaço!
+        
+        #TODO Must teste this approach! It is the correct one!
+        # while not self.memory_real.does_it_fit(process.pages): 
+        #     old_p_name= self.swap_algorithm(
+        #         self.p_order)
         old_p_name= self.swap_algorithm(
             self.p_order)
 
@@ -173,5 +178,7 @@ def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
+    def show_real_virtual_map(self):
+        copy = json.dumps(self.real_virtual_map)
+        return copy
     "
KO;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
-from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -45,6 +45,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
     print(""enters main loop!"")
 
     while True:
@@ -102,6 +103,9 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -110,6 +114,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
@@ -119,7 +124,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
-            started_time
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -163,7 +169,8 @@ def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
-    started_time:int
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -174,14 +181,16 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
-                ""started_time"":started_time
             }
     
 "
OK;4;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
+from typing import Callable, List, Tuple, Dict
 from collections import deque
 
 from cpu.models import config_model
@@ -45,6 +45,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
+    real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
@@ -102,6 +103,9 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
+                real_virtual_map = mmu.show_real_virtual_map()
+
+                # real_virtual_map = mmu.show_real_virtual_map()
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
@@ -110,6 +114,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
+            real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
@@ -119,7 +124,8 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
+            started_time,
+            real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
@@ -163,7 +169,8 @@ def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
+    started_time:int,
+    real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
@@ -174,14 +181,16 @@ def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
+    memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
+                ""started_time"":started_time,
+                ""real_virtual_map"": memory_map
             }
     
 "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
-    ""FIFO"": swap_fifo,
-
 }
 
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
+    ""FIFO"": swap_fifo
 }
 
+
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;\ No newline at end of file
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+{
+    ""config"":{
+        ""scale_algorithm"": ""FIFO"",
+        ""page_algorithm"": ""FIFO"",
+        ""quantum"": 2,
+        ""overhead"":0
+    },
+    ""processes"":[
+       
+        {
+            ""name"":""A"",
+            ""arrival_time"":0,
+            ""execution_time"":10,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""B"",
+            ""arrival_time"":0,
+            ""execution_time"":6,
+            ""pages"":10,
+            ""deadline"":10
+        },
+        {
+            ""name"":""C"",
+            ""arrival_time"":0,
+            ""execution_time"":2,
+            ""pages"":10,
+            ""deadline"":8
+        },
+        {
+            ""name"":""D"",
+            ""arrival_time"":0,
+            ""execution_time"":4,
+            ""pages"":10,
+            ""deadline"":5
+        },
+        {
+            ""name"":""E"",
+            ""arrival_time"":0,
+            ""execution_time"":8,
+            ""pages"":10,
+            ""deadline"":7
+        }
+    ]
+}
\ No newline at end of file"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"-from typing import Dict, List, Union, Callable, Tuple
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
@@ -9,23 +9,27 @@ class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
-    page_algorithm: Callable
-    
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
-        page_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
-        self.page_algorithm = page_algorithm
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
-            if not self.memory_real.is_memory_full(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
@@ -35,7 +39,7 @@ def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
-            elif not self.memory_virtual.is_memory_full(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
@@ -49,22 +53,26 @@ def initialize(self, queue:List[ProcessIn]):
                 
 
 
-    def load_context(self, process: ProcessIn):
         real_virtual_map = self.real_virtual_map
-        self.memory_real.add_stack(process)
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
-            return True #Tudo certo! o processo já está carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
-            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
@@ -78,14 +86,15 @@ def load_context(self, process: ProcessIn):
 
 
         else:#O processo não ta nem na memoria real nem na virtual
-            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
@@ -112,8 +121,8 @@ def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
-        old_p_name= self.page_algorithm(
-            self.memory_real)
 
 
         #Remove o index do processo antigo da memoria real
@@ -132,7 +141,7 @@ def swap(self, process: ProcessIn):
 
 
 
-    def garbage_collector(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
@@ -148,8 +157,21 @@ def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
 
-    def init_memories(self):
-        pass
 
     "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+from typing import Dict, List, Union, Callable, Tuple, Set
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
@@ -9,23 +9,27 @@ class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
+    swap_algorithm: Callable
+    p_count: any
+    p_order: deque
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
+        swap_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
+        self.swap_algorithm = swap_algorithm
+        self.p_count = None
+        self.p_order = deque()
+
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
+            if self.memory_real.does_it_fit(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
@@ -35,7 +39,7 @@ def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
+            elif self.memory_virtual.does_it_fit(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
@@ -49,22 +53,26 @@ def initialize(self, queue:List[ProcessIn]):
                 
 
 
+    def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
+        if not process.name in real_virtual_map:
+            self.p_order.appendleft(process.name)
+        
+        #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
+            return False #Tudo certo! o processo já está carregado na memoria
+            #Não precisa de OVERHEAD
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
+            if self.memory_real.does_it_fit(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
@@ -78,14 +86,15 @@ def load_context(self, process: ProcessIn):
 
 
         else:#O processo não ta nem na memoria real nem na virtual
+            if self.memory_real.does_it_fit(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
+                return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
@@ -112,8 +121,8 @@ def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
+        old_p_name= self.swap_algorithm(
+            self.p_order)
 
 
         #Remove o index do processo antigo da memoria real
@@ -132,7 +141,7 @@ def swap(self, process: ProcessIn):
 
 
 
+    def garbage_collector_all(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
@@ -148,8 +157,21 @@ def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
+    def garbage_collector(self,process:ProcessIn):
+            p_name = process.name
+            real_virtual_map = self.real_virtual_map
+            free_real_indexes = real_virtual_map[p_name][""real""]
+            free_virtual_indexes = real_virtual_map[p_name][""virtual""]
+
+            self.memory_real.remove(free_real_indexes)
+            self.memory_virtual.remove(free_virtual_indexes)
+
+
+            real_virtual_map[p_name][""real""] = None
+            real_virtual_map[p_name][""virtual""] = None
+            real_virtual_map[p_name][""uses""] = 0
+            self.real_virtual_map = real_virtual_map 
+            print(f""Removed processes = {p_name}"")
 
 
     "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
-    current_memory_space: int
-    space_graph: Dict[str, bool]
     process_stack: deque
     type_name:str
 
@@ -22,8 +22,9 @@ def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
-    def is_memory_full(self, number_of_page_in: int):
-        if number_of_page_in < self.current_space_occupied:
             return True
         return False
     
@@ -48,10 +49,13 @@ def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
-        for index in index_list:
-            self.space_graph[index] = False
-            self.current_space_occupied -= 1
-            print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
+    current_space_occupied: int
+    space_graph: Dict[int, bool]
     process_stack: deque
     type_name:str
 
@@ -22,8 +22,9 @@ def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
+    def does_it_fit(self, number_of_page_in: int):
+        free_space = self.total_memory_pages - self.current_space_occupied 
+        if free_space >= number_of_page_in:
             return True
         return False
     
@@ -48,10 +49,13 @@ def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
+        if index_list is None:
+            print(""Process is not in memory"")
+        else:
+            for index in index_list:
+                self.space_graph[index] = False
+                self.current_space_occupied -= 1
+                print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"-
 
 def swap_fifo(
-    memory_real
 ):
-    old_process_name = memory_real.process_stack[-1] #pega a primeira posição
-    memory_real.process_stack.rotate()
-    return old_process_name
\ No newline at end of file"
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;"+from collections import deque
 
 def swap_fifo(
+    p_order:deque
 ):
\ No newline at end of file
+    old_process_name = p_order[-1] #pega a primeira posição
+    p_order.rotate()
+    return old_process_name"
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
-from typing import List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -25,23 +25,21 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
-    scalonator_engine =  scalonator_translate[config.scale_algorithm] 
-    page_algorithm = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    # real_memory = Memory(""real"",total_memory_pages=20)
-    # virtual_memory = Memory(""virtual"",total_memory_pages=100)
-    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
-    # MMU.initialize(process_list)
-
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
-    cache_name = False
     is_process_done = False
     time_count = 0
     first = True
@@ -64,24 +62,37 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
-        if p.name != cache_name: #Caso o process não esteja carregado na cache
-            # result = mmu.load_context(p)
-            
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
-            
-        else:
             is_overhead = False
         is_process_done = False
-        cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -91,13 +102,15 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
-            queue.appendleft(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
@@ -177,10 +190,15 @@ def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
-    for index,p in enumerate(process_list):
         if time_count >= p.arrival_time:
             result.append(p)
-            process_list.pop(index)
     return result
         
 "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
+from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
@@ -25,23 +25,21 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
+    scalonator_engine: Callable =  scalonator_translate[config.scale_algorithm] 
+    swap_algorithm: Callable = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    real_memory = Memory(""real"",total_memory_pages=10)
+    virtual_memory = Memory(""virtual"",total_memory_pages=100)
+    mmu = MMU(real_memory,virtual_memory,swap_algorithm)
+    # mmu.initialize(process_list)
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
     is_process_done = False
     time_count = 0
     first = True
@@ -64,24 +62,37 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
+            is_process_done = False
+
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
+        #Retorna True caso precise trocar de contexto!
+        if mmu.load_context(p):
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
+        else: #Retorna False caso o contexto já estava carregado
             is_overhead = False
         is_process_done = False
+        
+        # if p.name != cache_name: #Caso o process não esteja carregado na cache
+        #     result = mmu.load_context(p)
+            
+        #     is_overhead = True
+        #     if not first:
+        #         sleep(overhead)
+        #         time_count+=overhead
+        #     first = False
+            
+        # else:
+        #     is_overhead = False
+        # is_process_done = False
+        # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
@@ -91,13 +102,15 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
+                mmu.garbage_collector(p)
+
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
+            queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
@@ -177,10 +190,15 @@ def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
+
+    process_copy = process_list.copy()
+
+    for p in process_copy:
         if time_count >= p.arrival_time:
             result.append(p)
+            process_list.remove(p)
+
+            # process_list.pop(index)
     return result
         
 "
KO;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
-#TODO: Need delete cicle_log data!
 
 app = FastAPI()
 "
OK;4;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
 
 app = FastAPI()
 "
KO;4;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    real_memory = MemoryReal(total_memory_pages=50)
-    virtual_memory = MemoryVirtual(total_memory_frames=100)
-    mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,7 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
-            mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -67,11 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
-            mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
-            result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    # real_memory = MemoryReal(total_memory_pages=50)
+    # virtual_memory = MemoryVirtual(total_memory_frames=100)
+    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,7 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
+            # mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -67,11 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
+            # mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
+            # result = mmu.load_context(p)
             
             is_overhead = True
             if not first:"
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return True #Tudo certo! o processo já está carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
-                real_used_index = self.add_to_memory(process.page,self.memory_real)
-                self.real_virtual_map[process.name][""real""] = real_used_index #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
-                real_virtual_map = self.swap(real_virtual_map)
-                pass
 
 
-        else:#O processo não tinha sido cadastrado antes!
-            pass
 
         
         
@@ -84,31 +97,55 @@ def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
-        if not memory.is_memory_full(pages):
-            for _ in range(pages): 
-               used_index = memory.add(used_index)
-            return used_index
-        else:
-            print(""memory full!"")
 
 
     def swap(self, process: ProcessIn):
-        for k in self.real_virtual_map.keys():
-            self.real_virtual_map[k]
 
-        self.page_algorithm()
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
-    def clean_done_process():
-        pass
 
-    def init_memory(self, process_list: List[ProcessIn]):
-        pass
 
-    def init_disk(self, process_list: List[ProcessIn]):
         pass
 
     "
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
+            self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return True #Tudo certo! o processo já está carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
+                real_used_indexes = self.add_to_memory(process.page,self.memory_real)
+                self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
+                self.swap(process)
+                
+
+                return True
+
+
+        else:#O processo não ta nem na memoria real nem na virtual
+            if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
+                self.memory_real.add_stack(process)
+                real_used_indexes = self.add_to_memory(process.page,self.memory_real)
+                virtual_used_indexes = self.add_to_memory(process.page,self.memory_virtual)
+
+                self.real_virtual_map[process.name][""real""] = real_used_indexes
+                self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
+            else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
+                self.swap(process)
 
 
 
         
         
@@ -84,31 +97,55 @@ def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
+        for _ in range(pages): 
+            used_index = memory.add(used_index)
+        return used_index
 
 
     def swap(self, process: ProcessIn):
 
+        new_p_real_index, old_p_name= self.page_algorithm(
+            self.memory_real,
+            process,
+            self.real_virtual_map)
+
+        #Remove o index do processo antigo da memoria real
+        list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
+        self.memory_real.remove(list_index_to_remove)
+
+        #Atualiza a remoção na hash table
+        self.real_virtual_map[old_p_name][""real""] = None
+        self.real_virtual_map[old_p_name][""uses""] = 0 #Fazer update da tabela hash
+
+        #
+        self.real_virtual_map[process.name][""real""] = new_p_real_index #Fazer update da tabela hash
+        self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
+
+        return True
+         
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
+    def garbage_collector(self,process_done:List[ProcessIn]):
+        real_virtual_map = self.real_virtual_map
+        for p in process_done:
+            free_real_index = real_virtual_map[p.name][""real""]
+            free_virtual_index = real_virtual_map[p.name][""virtual""]
+
+            self.memory_real.remove(free_real_index)
+            self.memory_virtual.remove(free_virtual_index)
+
+
+            real_virtual_map[p.name][""real""] = None
+            real_virtual_map[p.name][""virtual""] = None
+            real_virtual_map[p.name][""uses""] = None
+        self.real_virtual_map = real_virtual_map 
+        print(""Removed unused processes"")
 
 
+    def init_memories(self):
         pass
 
     "
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
-    def is_memory_full(self, page_in: int):
-        if page_in > self.current_memory_space:
             return True
         return False
     
@@ -29,7 +29,7 @@ def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
-    def add(self,used_index: List[any])-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
@@ -38,8 +38,9 @@ def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
-    def remove(self,index:int):
-        self.space_graph[str(index)] = False
-        self.current_space_occupied -= 1
-
 "
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
+    def is_memory_full(self, number_of_page_in: int):
+        if number_of_page_in > self.current_memory_space:
             return True
         return False
     
@@ -29,7 +29,7 @@ def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
+    def add(self,used_index:List)-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
@@ -38,8 +38,9 @@ def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
+    def remove(self,index_list:List[int]):
+        for index in index_list:
+            self.space_graph[index] = False
+            self.current_space_occupied -= 1
+            print(f""Removed {index} from real memory"")
 "
KO;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
-    # real_memory = MemoryReal(total_memory_pages=50)
-    # virtual_memory = MemoryVirtual(total_memory_frames=100)
-    # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,6 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -66,10 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
-            # result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
+    real_memory = MemoryReal(total_memory_pages=50)
+    virtual_memory = MemoryVirtual(total_memory_frames=100)
+    mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
@@ -53,6 +53,7 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
+            mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
@@ -66,10 +67,11 @@ def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
+            mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
+            result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
KO;5;yk;apes-stylegan2-ada-pytorch;d3a616a9c86f9a72087caca088dec7d045f44a4b;"Specify --shm-size=2g and fix typo in code comments

Fix OOM crash in data loader workers caused by docker's small default
shared memory size.";"set -e
 #
 # Use it like:
 #
-# ./run_docker.sh python generate.py --help
 #
 # To override the default `stylegan2ada:latest` image, run:
 #
-# IMAGE=my_image:v1.0 ./run_docker.sh python generate.py --help
 #
 
 rest=$@
@@ -30,7 +30,7 @@ IMAGE=""${IMAGE:-sg2ada:latest}""
 
 CONTAINER_ID=$(docker inspect --format=""{{.Id}}"" ${IMAGE} 2> /dev/null)
 if [[ ""${CONTAINER_ID}"" ]]; then
-    docker run --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \
         --workdir=/scratch -e HOME=/scratch $IMAGE $@
 else
     echo ""Unknown container image: ${IMAGE}"""
OK;5;yk;apes-stylegan2-ada-pytorch;d3a616a9c86f9a72087caca088dec7d045f44a4b;"Specify --shm-size=2g and fix typo in code comments

Fix OOM crash in data loader workers caused by docker's small default
shared memory size.";"set -e
 #
 # Use it like:
 #
+# ./docker_run.sh python generate.py --help
 #
 # To override the default `stylegan2ada:latest` image, run:
 #
+# IMAGE=my_image:v1.0 ./docker_run.sh python generate.py --help
 #
 
 rest=$@
@@ -30,7 +30,7 @@ IMAGE=""${IMAGE:-sg2ada:latest}""
 
 CONTAINER_ID=$(docker inspect --format=""{{.Id}}"" ${IMAGE} 2> /dev/null)
 if [[ ""${CONTAINER_ID}"" ]]; then
+    docker run --shm-size=2g --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \
         --workdir=/scratch -e HOME=/scratch $IMAGE $@
 else
     echo ""Unknown container image: ${IMAGE}"""
KO;5;lleonart1984;rendertoy;99460f4bc2da456bd17676f7521455833d8da618;Work around for the dynamic memory accessing in OpenCL 1.1;" __ctx__ = cl.create_some_context()
 __queue__ = cl.CommandQueue(__ctx__)
 
 __code__ = """"""
 #define float4x4 float16
 
@@ -62,13 +87,13 @@
     return (float4)(dot(v, m.even.even), dot(v, m.odd.even), dot(v, m.even.odd), dot(v, m.odd.odd));    
 }
 
-
-__global char* memory_pool;
-
 
 #define wrap_coord(c) (fmod(fmod(c, 1.0f) + 1.0f, 1.0f))
 
-#define sample2D(texture, c) (((float4*)(memory_pool + (texture).offset + 16*((int)(wrap_coord((c).y) * (texture).height) * (texture).width + (int)(wrap_coord((c).x) * (texture).width))))[0]) 
 
 """"""
 
@@ -221,10 +246,10 @@ def build_kernel_function(name, arguments, return_type, body):
 
 def build_kernel_main(name, arguments, body):
     global __code__
-    signature = ', '.join([_get_annotation_as_cltype(annotation)+"" ""+ arg_name for arg_name, annotation in arguments.items()]+['__global char* memory_pool_arg'])
     __code__ += f""""""
     __kernel void {name}({signature}) {{
-    memory_pool = memory_pool_arg;
     int thread_id = get_global_id(0);
     {body}
     }}
@@ -290,8 +315,7 @@ class Texture2D:
     offset: np.int32
 
 
-def create_buffer(count: int, dtype: np.dtype):
-    return cla.zeros(__queue__, (count,), dtype)
 
 
 def create_buffer_from(ary: np.ndarray):
@@ -523,9 +547,9 @@ def perspective(fov = 3.141593 / 4, aspect_ratio = 1.0, znear = .01, zfar = 100.
 
 
 class MemoryPool:
-    def __init__(self, max_size=1024*1024*1024): # 1GB by default
-        self.max_size = max_size
-        self.buffer = create_buffer(max_size, np.uint8)
         self.malloc_ptr = 0
 
     def allocate_texture(self, width, height):"
OK;5;lleonart1984;rendertoy;99460f4bc2da456bd17676f7521455833d8da618;Work around for the dynamic memory accessing in OpenCL 1.1;" __ctx__ = cl.create_some_context()
 __queue__ = cl.CommandQueue(__ctx__)
 
+def create_buffer(count: int, dtype: np.dtype):
+    return cla.zeros(__queue__, (count,), dtype)
+
+__MAX_SIZE__ = 1024*1024*1024
+__MEMORY_POOL_BUFFER__ = create_buffer(__MAX_SIZE__, np.uint8)
+
+
+def get_buffer_ptr():
+    # Create kernel to retrieve buffer ptr
+    program = cl.Program(__ctx__,
+                         """"""
+    __kernel void get_ptr(__global char* ptr, __global unsigned long* ptr_store) {
+        ptr_store[0] = (unsigned long)ptr;
+    }
+                         """""").build()
+    kernel = program.get_ptr
+    ptr_store = cla.to_device(__queue__, np.zeros((1,), np.int64))
+    kernel(__queue__, (1,), None, __MEMORY_POOL_BUFFER__.data, ptr_store.data)
+    return int(ptr_store.map_to_host())
+
+
+test1 = get_buffer_ptr()
+test2 = get_buffer_ptr()
+
+
 __code__ = """"""
 #define float4x4 float16
 
@@ -62,13 +87,13 @@
     return (float4)(dot(v, m.even.even), dot(v, m.odd.even), dot(v, m.even.odd), dot(v, m.odd.odd));    
 }
 
+""""""+\
+f""__constant unsigned long memory_pool_ptr = {get_buffer_ptr()};""+\
+""""""
 
 #define wrap_coord(c) (fmod(fmod(c, 1.0f) + 1.0f, 1.0f))
 
+#define sample2D(texture, c) (((float4*)(memory_pool_ptr + (texture).offset + 16*((int)(wrap_coord((c).y) * (texture).height) * (texture).width + (int)(wrap_coord((c).x) * (texture).width))))[0]) 
 
 """"""
 
@@ -221,10 +246,10 @@ def build_kernel_function(name, arguments, return_type, body):
 
 def build_kernel_main(name, arguments, body):
     global __code__
+    signature = ', '.join([_get_annotation_as_cltype(annotation)+"" ""+ arg_name for arg_name, annotation in arguments.items()] + ['__global char* memory_pool_arg'])
     __code__ += f""""""
     __kernel void {name}({signature}) {{
+    //memory_pool_ptr = (long)memory_pool_arg;
     int thread_id = get_global_id(0);
     {body}
     }}
@@ -290,8 +315,7 @@ class Texture2D:
     offset: np.int32
 
 
+
 
 
 def create_buffer_from(ary: np.ndarray):
@@ -523,9 +547,9 @@ def perspective(fov = 3.141593 / 4, aspect_ratio = 1.0, znear = .01, zfar = 100.
 
 
 class MemoryPool:
+    def __init__(self): # 1GB by default
+        self.max_size = __MAX_SIZE__
+        self.buffer = __MEMORY_POOL_BUFFER__
         self.malloc_ptr = 0
 
     def allocate_texture(self, width, height):"
KO;5;lleonart1984;rendertoy;99460f4bc2da456bd17676f7521455833d8da618;Work around for the dynamic memory accessing in OpenCL 1.1;
OK;5;lleonart1984;rendertoy;99460f4bc2da456bd17676f7521455833d8da618;Work around for the dynamic memory accessing in OpenCL 1.1;"+import typing
+import pyopencl.array as cla
+from ._modeling import Mesh
+from ._core import kernel_main, kernel_struct, kernel_function, build_kernel_main, build_kernel_function
+from ._core import float3
+
+
+@kernel_struct
+class BVH_AABB:
+    min: float3
+    max: float3
+    count: int
+    elements: int
+
+
+@kernel_struct
+class BVH_Triangle:
+    v0: float3
+    v1: float3
+    v2: float3
+    index: int
+    mesh: int
+
+
+class Raycaster:
+    def __init__(self, models: typing.List[Mesh]):
+        self.models  = models
+        self._build_ads()
+
+    def _build_ads(self):
+        for m in self.models:
+            m: Mesh
+            triangles = m.vertices.shape[0] // 3 if m.indices is None else m.indices.shape[0] // 3
+
+    def ray_cast(self, rays: cla.Array)->cla.Array:
+        pass
+
+"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[gp]
+components = 100
+batch_size = 100"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.random_projection import GaussianRandomProjection
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""gauss_rp.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""gp"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""gp"", ""components"")
+transformer = GaussianRandomProjection(n_components=n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
+    # fit grp
+    embedding = transformer.fit_transform(
+        batch_of_images.reshape(batch_size, -1)
+    )
+    ###########################################################################
+    np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
+
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[pca]
+components = 50
+batch_size = 50"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.decomposition import IncrementalPCA
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""ipca.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""pca"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""pca"", ""components"")
+assert n_components <= batch_size
+transformer = IncrementalPCA(n_components = n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""IPCA of batch {batch:02d}"", end=""\n"")
+    # fit pca
+    transformer.fit(batch_of_images.reshape(batch_size, -1))
+###############################################################################
+with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
+
+    pickle.dump(transformer, file)
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
-        # get magnitude scale to better distinguis objects
-        # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
-    image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
+    image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+[common]
+type = magnitude
+
+[directory]
+user = /home/edgar
+work = ${user}/leos-ai
+data = ${user}/leos-data/leos-ai/data/korea
+
+[file]
+
+[gp]
+components = 100
+batch_size = 100"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.random_projection import GaussianRandomProjection
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""gauss_rp.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""gp"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""gp"", ""components"")
+transformer = GaussianRandomProjection(n_components=n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
+    # fit grp
+    embedding = transformer.fit_transform(
+        batch_of_images.reshape(batch_size, -1)
+    )
+    ###########################################################################
+    np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
+
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
+
+[pca]
+components = 50
+batch_size = 50"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"+""""""Prepare raw images for Fourier Analysis""""""
+
+###############################################################################
+from configparser import ConfigParser, ExtendedInterpolation
+import glob
+import pickle
+import random
+import time
+
+import numpy as np
+from sklearn.decomposition import IncrementalPCA
+
+from leosAi.utils.managefiles import FileDirectory
+###############################################################################
+start_time = time.time()
+###############################################################################
+parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""ipca.ini""
+parser.read(f""{config_file_name}"")
+# Check files and directory
+check = FileDirectory()
+# Handle configuration file
+# configuration = ConfigurationFile()
+###############################################################################
+# location of data
+data_directory = parser.get(""directory"", ""data"")
+data_type = parser.get(""common"", ""type"")
+path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
+
+number_of_files = len(path_to_files)
+batch_size = parser.getint(""pca"", ""batch_size"")
+number_of_batches = number_of_files // batch_size
+
+# Complete a batch in case number of batches
+# does not fit all files
+if number_of_batches % batch_size !=0:
+
+    number_of_batches += 1
+
+    remaining_number_of_files = batch_size - number_of_batches % batch_size
+    # randomly pick already used images
+    path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
+
+
+
+image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
+
+batch_shape = (batch_size, ) + image_shape
+batch_of_images = np.empty(batch_shape).astype(np.float32)
+
+n_components = parser.getint(""pca"", ""components"")
+assert n_components <= batch_size
+transformer = IncrementalPCA(n_components = n_components)
+
+save_to = f""{data_directory}/gauss_rp""
+check.check_directory(save_to, exit_program=False)
+
+for batch in range(number_of_batches):
+
+
+    # load images to current batch of images
+    index_of_images = range(batch_size*batch, batch_size*(batch+1))
+
+    for idx_batch, idx_image in enumerate(index_of_images):
+
+        batch_of_images[idx_batch, ...] = np.load(
+            path_to_files[idx_image]
+        ).astype(np.float32)
+
+    print(f""IPCA of batch {batch:02d}"", end=""\n"")
+    # fit pca
+    transformer.fit(batch_of_images.reshape(batch_size, -1))
+###############################################################################
+with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
+
+    pickle.dump(transformer, file)
+
+###############################################################################
+with open(
+    f""{save_to}/{config_file_name}"",
+    ""w"", encoding=""utf8""
+) as config_file:
+
+    parser.write(config_file)
+###############################################################################
+finish_time = time.time()
+print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"-""""""Prepare raw images for Fourier Analysis""""""
-
-###############################################################################
-from configparser import ConfigParser, ExtendedInterpolation
-import glob
-import time
-
-from astropy.io import fits as pyfits
-import numpy as np
-
-from leosAi.utils.managefiles import FileDirectory
-###############################################################################
-start_time = time.time()
-###############################################################################
-parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""ir_pca.ini""
-parser.read(f""{config_file_name}"")
-# Check files and directory
-check = FileDirectory()
-# Handle configuration file
-# configuration = ConfigurationFile()
-###############################################################################
-# location of data
-data_directory = parser.get(""directory"", ""data"")
-data_type = parser.get(""common"", ""type"")
-path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
-
-for idx, path_to_file in enumerate(path_to_files):
-
-    file_name = path_to_file.split(""/"")[-1].split(""."")[0]
-    print(f""{idx:04d}: {file_name}"", end=""\r"")
-
-    image = np.load(path_to_file)
-###############################################################################
-# with open(
-#     f""{save_to}/{config_file_name}"",
-#     ""w"", encoding=""utf8""
-# ) as config_file:
-#
-#     parser.write(config_file)
-###############################################################################
-finish_time = time.time()
-print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
KO;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
-config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
-        # get magnitude scale to better distinguis objects
-        # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
-    image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
+config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
@@ -36,16 +36,14 @@
 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
+    image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;6;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
-高效的词典匹配工具，用于查询词典中的单词是否在字符串中
 
 # 安装与使用
 ## 安装说明
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`目录中
 
 ## 测试数据
-数据存放在`eval/data`目录中，包括PKU和AS的分词数据集、jieba词库组成，分别测试三个数据集`词典装载`和`查询`的耗时
 
 | 数据集 | 单词数 |  数据大小  |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | Jieba  | 58.4W | 4050566字  |
 
 ## 评测结果
-### 词典装载性能
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
-
-### 词典查询性能
 
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
-|    ahocorapy    | 0.02 | 0.34  | 0.08 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
-| prefix_dict |   1.4 |  6.7  | 3.5 |
\ No newline at end of file
\ No newline at end of file"
OK;6;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
+高效的词典匹配工具，用于查询词典中的单词是否在字符串中，尤其在大规模词典匹配中，效果尤为显著
 
 # 安装与使用
 ## 安装说明
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`目录中
 
 ## 测试数据
+数据存放在`eval/data`目录中，包括PKU和AS的分词数据集、jieba词库组成，分别测试三个数据集`词典装载`和`查询`的耗时，以及`内存占用`多个方面进行评测
 
 | 数据集 | 单词数 |  数据大小  |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | Jieba  | 58.4W | 4050566字  |
 
 ## 评测结果
+从评测结果可以看出，本代码在较大规模的词典匹配算法时，在内存占用上，装载性能上都有极大的优势，同时查询性能也可达到每秒百万字的查询性能，如同样纯Python实现的AC自动机ahocorapy，查询性能相差无几，而在处理大型词典时，其装载时间及其内存占用都有显著提升
+
+### 词典装载性能(装载时间 秒)
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
+### 词典查询性能(查询时间 秒)
 
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
+|    ahocorapy    | 1.0 | 5.4  | 9.27 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
\ No newline at end of file
+| prefix_dict |   1.4 |  6.7  | 3.5 |
+
+
+### 内存占用
+|       数据集       |       PKU       |       AS       |    jieba     |
+| :--------------: | :-------------: | :------------: | :----------: |
+|    单词数    | 5.5W | 14.1W  | 58.4W |
+|    ahocorapy    | 300M | 800M  | 5G |
+|    dmsearch    | 1G | 1G  | 2.5G |
+| prefix_dict |   25M |  100M | 400M |
\ No newline at end of file"
KO;6;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
-
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
-        sys.path.append(""../dictmatch"") 
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
-    
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 评测不同词典匹配工具的性能
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
-
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
-      tree.search(line.strip())
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
-#   test_eval(ahocorapy, 'pku')
-#   test_eval(ahocorapy, 'as')
-#   test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
-    
\ No newline at end of file"
OK;6;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
+        # sys.path.append(""../dictmatch"")
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
+        
+    def build(self) -> None:
+        self.tree.make()
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
+    
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 评测不同词典匹配工具的性能
+# from memory_profiler import profile
+# @profile
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
+      list(tree.search(line.strip()))
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
+  test_eval(ahocorapy, 'pku')
+  test_eval(ahocorapy, 'as')
+  test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
\ No newline at end of file
+    "
KO;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
-高效的词典匹配工具，用于查询词典中的单词是否在字符串中
 
 # 安装与使用
 ## 安装说明
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`目录中
 
 ## 测试数据
-数据存放在`eval/data`目录中，包括PKU和AS的分词数据集、jieba词库组成，分别测试三个数据集`词典装载`和`查询`的耗时
 
 | 数据集 | 单词数 |  数据大小  |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | Jieba  | 58.4W | 4050566字  |
 
 ## 评测结果
-### 词典装载性能
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
-
-### 词典查询性能
 
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
-|    ahocorapy    | 0.02 | 0.34  | 0.08 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
-| prefix_dict |   1.4 |  6.7  | 3.5 |
\ No newline at end of file
\ No newline at end of file"
OK;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
+高效的词典匹配工具，用于查询词典中的单词是否在字符串中，尤其在大规模词典匹配中，效果尤为显著
 
 # 安装与使用
 ## 安装说明
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`目录中
 
 ## 测试数据
+数据存放在`eval/data`目录中，包括PKU和AS的分词数据集、jieba词库组成，分别测试三个数据集`词典装载`和`查询`的耗时，以及`内存占用`多个方面进行评测
 
 | 数据集 | 单词数 |  数据大小  |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | Jieba  | 58.4W | 4050566字  |
 
 ## 评测结果
+从评测结果可以看出，本代码在较大规模的词典匹配算法时，在内存占用上，装载性能上都有极大的优势，同时查询性能也可达到每秒百万字的查询性能，如同样纯Python实现的AC自动机ahocorapy，查询性能相差无几，而在处理大型词典时，其装载时间及其内存占用都有显著提升
+
+### 词典装载性能(装载时间 秒)
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
+### 词典查询性能(查询时间 秒)
 
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
+|    ahocorapy    | 1.0 | 5.4  | 9.27 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
\ No newline at end of file
+| prefix_dict |   1.4 |  6.7  | 3.5 |
+
+
+### 内存占用
+|       数据集       |       PKU       |       AS       |    jieba     |
+| :--------------: | :-------------: | :------------: | :----------: |
+|    单词数    | 5.5W | 14.1W  | 58.4W |
+|    ahocorapy    | 300M | 800M  | 5G |
+|    dmsearch    | 1G | 1G  | 2.5G |
+| prefix_dict |   25M |  100M | 400M |
\ No newline at end of file"
KO;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
-
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
-        sys.path.append(""../dictmatch"") 
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
-    
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 评测不同词典匹配工具的性能
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
-
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
-      tree.search(line.strip())
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
-#   test_eval(ahocorapy, 'pku')
-#   test_eval(ahocorapy, 'as')
-#   test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
-    
\ No newline at end of file"
OK;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
+        # sys.path.append(""../dictmatch"")
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
+        
+    def build(self) -> None:
+        self.tree.make()
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
+    
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 评测不同词典匹配工具的性能
+# from memory_profiler import profile
+# @profile
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
+      list(tree.search(line.strip()))
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
+  test_eval(ahocorapy, 'pku')
+  test_eval(ahocorapy, 'as')
+  test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
\ No newline at end of file
+    "
KO;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
   </component>
 </module>
\ No newline at end of file"
OK;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
+    <orderEntry type=""module"" module-name=""state_machine"" />
   </component>
 </module>
\ No newline at end of file"
KO;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
OK;7;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
+      <module fileurl=""file://$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+data/*
+src/data/converter_chunks/*
+notebooks/*
+remote/*
+yaml/*
+reduction_params.sh
+src/visualization/*.png
+.git/*
+.git*
+__pycache__/*/*"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";+*.ipynb linguist-vendored
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+env/
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+*.egg-info/
+.installed.cfg
+*.egg
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# DotEnv configuration
+.env
+
+# Database
+*.db
+*.rdb
+
+# Pycharm
+.idea
+
+# VS Code
+.vscode/
+
+# Spyder
+.spyproject/
+
+# Jupyter NB Checkpoints
+.ipynb_checkpoints/
+
+# Mac OS-specific storage files
+.DS_Store
+
+# vim
+*.swp
+*.swo
+
+# Mypy cache
+.mypy_cache/
+
+# Vim settings
+.vim/
+
+# Ignore top level data (actual .csv, .xml etc) but track the folder structure and src/data/ scripts 
+data/raw/*
+data/interim/*
+data/processed/*
+data/external/*
+data/meta/*
+
+data/raw/.gitkeep
+data/interim/.gitkeep
+data/processed/.gitkeep
+data/external/.gitkeep
+
+# Data that I don't need to upload, large uncompressed files
+organoid_T.tsv
+primary_T.tsv
+remote/*
+*/lightning_logs/*
+notebooks/poster*
+notebooks/*/*.png
+notebooks/*.png
+credentials
+src/visualization/*.png
+wandbcreds
+wandb/*
+*.ckpt
+*/wandb/*
+data/mouse/*
+src/data/converter_chunks/*"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";\ No newline at end of file
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+FROM pytorch/pytorch
+
+WORKDIR /src
+
+RUN apt-get update
+RUN apt-get install -y wget && rm -rf /var/lib/apt/lists/*
+
+RUN apt-get --allow-releaseinfo-change update && \
+    apt-get install -y --no-install-recommends \
+        curl \
+        sudo \
+        vim 
+
+RUN curl -L https://bit.ly/glances | /bin/bash
+
+RUN conda install --yes boto3 tenacity pandas numpy pip plotly scipy && \
+    conda install -c conda-forge python-kaleido dask-xgboost hdbscan dask-xgboost && \
+    pip install matplotlib && \
+    pip install umap-learn && \
+    pip install dask && \
+    pip install dask-ml && \
+    pip install pynndescent && \ 
+    pip install seaborn && \
+    pip install imbalanced-learn && \ 
+    pip install xgboost && \ 
+    pip install pytorch-lightning && \ 
+    pip install comet_ml && \ 
+    pip install wandb && \ 
+    pip install transposecsv==0.0.5 \ 
+    pip install pytorch-tabnet \
+    pip install bigcsv==0.0.6 \ 
+    pip install scanpy \ 
+    pip install anndata 
+
+COPY . .
\ No newline at end of file"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+
+The MIT License (MIT)
+Copyright (c) 2021, Julian Lehrer
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+CONTAINER = jmlehrer/cell-exploration
+
+exec:
+	docker exec -it $(CONTAINER) /bin/bash
+
+build:
+	docker build -t $(CONTAINER) .
+
+push:
+	docker push $(CONTAINER)
+
+run:
+	docker run -it $(CONTAINER) /bin/bash
+
+go:
+	make build && make push
+
+train:
+	python src/models/run_model_search.py"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";\ No newline at end of file
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+# **SIMS**: Scalable, Interpretable Modeling for Single-Cell RNA-Seq Data Classification
+
+SIMS is a pipeline for building interpretable and accurate classifiers for intentifying any target on single-cell rna-seq data. The SIMS model is based on [TabNet](https://arxiv.org/abs/1908.07442), a self-attention based model specifically built for large-scale tabular datasets.
+
+SIMS takes in a list of arbitrarily many expression matrices along with their corresponding target variables. The expression matrices may be AnnData objects with format `h5ad`, or `.csv`. 
+They must be in the matrix form `cell x gene`, and NOT `gene x cell`, since our training samples are the transcriptomes of individual cells.
+
+The data is formated like so:
+- All matrices are cell x expression
+- All label files contain a common column, known as the `class_label`, on which to train the model 
+- `datafiles` and `labelfiles` are the absolute paths to the expression matrices and labels, respectively
+
+A call to generate and train the SIMS model looks like the following:
+
+```python 
+
+from src.models.lib.lightning_train import generate_trainer 
+
+trainer, model, data = generate_trainer(
+    datafiles=['cortical_cells.csv', 'cortical_cells_2.csv', 'external/cortical_cells_3.h5ad'], # Notice we can mix and match file types
+    labelfiles=['l1.csv', 'l2.csv', 'l3.csv'],
+    class_label='cell_state', # Train to predict cell state!
+    batch_size=4,
+)
+
+trainer.fit(model, datamodule=data)
+```
+
+This will train a derivation of the TabNet model on the given expression matrices with target variable given by the `class_label` column in each label file.
\ No newline at end of file"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";+*.csv
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""70a18946"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import scanpy as sc\n"",
+    ""import anndata as an\n"",
+    ""import os\n"",
+    ""import pandas as pd\n"",
+    ""import numpy as np\n"",
+    ""from sklearn.preprocessing import LabelEncoder \n"",
+    ""import torch\n"",
+    ""from sys import getsizeof\n"",
+    ""\n"",
+    ""import os, sys\n"",
+    ""sys.path.append('../src')\n"",
+    ""from models.lib.data import *\n"",
+    ""from models.lib.lightning_train import DataModule""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""c00475cd"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""# labelfile = '../data/mouse/Adult Inhibitory Neurons in Mouse_labels.tsv'\n"",
+    ""# label_df = pd.read_csv(labelfile, sep='\\t')\n"",
+    ""\n"",
+    ""mouse_atlas = sc.read_h5ad('../data/mouse/MouseAdultInhibitoryNeurons.h5ad')\n"",
+    ""mo_data = an.read_h5ad('../data/mouse/Mo_PV_paper_TDTomato_mouseonly.h5ad')\n"",
+    ""\n"",
+    ""# current_labels = label_df.loc[:, 'numeric_class']""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""b6ef5c64"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""['0610005C13Rik',\n"",
+       "" '0610006L08Rik',\n"",
+       "" '0610009B22Rik',\n"",
+       "" '0610009E02Rik',\n"",
+       "" '0610009L18Rik',\n"",
+       "" '0610010F05Rik',\n"",
+       "" '0610010K14Rik',\n"",
+       "" '0610012D04Rik',\n"",
+       "" '0610012G03Rik',\n"",
+       "" '0610025J13Rik',\n"",
+       "" '0610030E20Rik',\n"",
+       "" '0610031O16Rik',\n"",
+       "" '0610033M10Rik',\n"",
+       "" '0610038B21Rik',\n"",
+       "" '0610039K10Rik',\n"",
+       "" '0610040B10Rik',\n"",
+       "" '0610040F04Rik',\n"",
+       "" '0610040J01Rik',\n"",
+       "" '0610043K17Rik',\n"",
+       "" '0710001A04Rik',\n"",
+       "" '1010001B22Rik',\n"",
+       "" '1110002E22Rik',\n"",
+       "" '1110002J07Rik',\n"",
+       "" '1110002L01Rik',\n"",
+       "" '1110002O04Rik',\n"",
+       "" '1110004F10Rik',\n"",
+       "" '1110006O24Rik',\n"",
+       "" '1110008P14Rik',\n"",
+       "" '1110012L19Rik',\n"",
+       "" '1110013H19Rik',\n"",
+       "" '1110015O18Rik',\n"",
+       "" '1110017D15Rik',\n"",
+       "" '1110018N20Rik',\n"",
+       "" '1110019D14Rik',\n"",
+       "" '1110020A21Rik',\n"",
+       "" '1110028F11Rik',\n"",
+       "" '1110032A03Rik',\n"",
+       "" '1110032F04Rik',\n"",
+       "" '1110035H17Rik',\n"",
+       "" '1110038B12Rik',\n"",
+       "" '1110038F14Rik',\n"",
+       "" '1110046J04Rik',\n"",
+       "" '1110051M20Rik',\n"",
+       "" '1110059E24Rik',\n"",
+       "" '1110059G10Rik',\n"",
+       "" '1110065P20Rik',\n"",
+       "" '1190001M18Rik',\n"",
+       "" '1190005I06Rik',\n"",
+       "" '1190007I07Rik',\n"",
+       "" '1200007C13Rik',\n"",
+       "" '1300002E11Rik',\n"",
+       "" '1300014J16Rik',\n"",
+       "" '1300017J02Rik',\n"",
+       "" '1500002C15Rik',\n"",
+       "" '1500002F19Rik',\n"",
+       "" '1500004A13Rik',\n"",
+       "" '1500009C09Rik',\n"",
+       "" '1500009L16Rik',\n"",
+       "" '1500011B03Rik',\n"",
+       "" '1500012K07Rik',\n"",
+       "" '1500015A07Rik',\n"",
+       "" '1500015L24Rik',\n"",
+       "" '1500026H17Rik',\n"",
+       "" '1500032F14Rik',\n"",
+       "" '1500035N22Rik',\n"",
+       "" '1520401A03Rik',\n"",
+       "" '1600002D24Rik',\n"",
+       "" '1600010M07Rik',\n"",
+       "" '1600012H06Rik',\n"",
+       "" '1600014C10Rik',\n"",
+       "" '1600014C23Rik',\n"",
+       "" '1600019K03Rik',\n"",
+       "" '1600020E01Rik',\n"",
+       "" '1600022D10Rik',\n"",
+       "" '1600027J07Rik',\n"",
+       "" '1700001C19Rik',\n"",
+       "" '1700001D01Rik',\n"",
+       "" '1700001G11Rik',\n"",
+       "" '1700001J03Rik',\n"",
+       "" '1700001K19Rik',\n"",
+       "" '1700001K23Rik',\n"",
+       "" '1700001L05Rik',\n"",
+       "" '1700001L19Rik',\n"",
+       "" '1700001O22Rik',\n"",
+       "" '1700001P01Rik',\n"",
+       "" '1700003D09Rik',\n"",
+       "" '1700003E16Rik',\n"",
+       "" '1700003E24Rik',\n"",
+       "" '1700003F12Rik',\n"",
+       "" '1700003G18Rik',\n"",
+       "" '1700003I16Rik',\n"",
+       "" '1700003I22Rik',\n"",
+       "" '1700003L19Rik',\n"",
+       "" '1700003M07Rik',\n"",
+       "" '1700006F04Rik',\n"",
+       "" '1700006J14Rik',\n"",
+       "" '1700007F19Rik',\n"",
+       "" '1700007J10Rik',\n"",
+       "" '1700007K13Rik',\n"",
+       "" '1700007L15Rik',\n"",
+       "" '1700007P06Rik',\n"",
+       "" '1700008B11Rik',\n"",
+       "" '1700008J07Rik',\n"",
+       "" '1700008K24Rik',\n"",
+       "" '1700008O03Rik',\n"",
+       "" '1700009C05Rik',\n"",
+       "" '1700009J07Rik',\n"",
+       "" '1700009N14Rik',\n"",
+       "" '1700010B08Rik',\n"",
+       "" '1700010B13Rik',\n"",
+       "" '1700010G06Rik',\n"",
+       "" '1700010H22Rik',\n"",
+       "" '1700010I14Rik',\n"",
+       "" '1700010K23Rik',\n"",
+       "" '1700010K24Rik',\n"",
+       "" '1700010L04Rik',\n"",
+       "" '1700011B04Rik',\n"",
+       "" '1700011C11Rik',\n"",
+       "" '1700011D18Rik',\n"",
+       "" '1700011L22Rik',\n"",
+       "" '1700012B07Rik',\n"",
+       "" '1700012B09Rik',\n"",
+       "" '1700012C14Rik',\n"",
+       "" '1700012D14Rik',\n"",
+       "" '1700012D16Rik',\n"",
+       "" '1700012I11Rik',\n"",
+       "" '1700012P22Rik',\n"",
+       "" '1700013D24Rik',\n"",
+       "" '1700013F07Rik',\n"",
+       "" '1700013G24Rik',\n"",
+       "" '1700013H16Rik',\n"",
+       "" '1700013M08Rik',\n"",
+       "" '1700014D04Rik',\n"",
+       "" '1700015C15Rik',\n"",
+       "" '1700015F17Rik',\n"",
+       "" '1700015G11Rik',\n"",
+       "" '1700015H07Rik',\n"",
+       "" '1700015I17Rik',\n"",
+       "" '1700015O11Rik',\n"",
+       "" '1700016C15Rik',\n"",
+       "" '1700016D06Rik',\n"",
+       "" '1700016F12Rik',\n"",
+       "" '1700016G22Rik',\n"",
+       "" '1700016H03Rik',\n"",
+       "" '1700016H13Rik',\n"",
+       "" '1700016J18Rik',\n"",
+       "" '1700016K05Rik',\n"",
+       "" '1700016K19Rik',\n"",
+       "" '1700016L21Rik',\n"",
+       "" '1700016P03Rik',\n"",
+       "" '1700017B05Rik',\n"",
+       "" '1700017G19Rik',\n"",
+       "" '1700017J07Rik',\n"",
+       "" '1700017M07Rik',\n"",
+       "" '1700017N19Rik',\n"",
+       "" '1700018A04Rik',\n"",
+       "" '1700018A23Rik',\n"",
+       "" '1700018B08Rik',\n"",
+       "" '1700018B24Rik',\n"",
+       "" '1700018F24Rik',\n"",
+       "" '1700018L02Rik',\n"",
+       "" '1700019A02Rik',\n"",
+       "" '1700019C18Rik',\n"",
+       "" '1700019D03Rik',\n"",
+       "" '1700019G24Rik',\n"",
+       "" '1700019J19Rik',\n"",
+       "" '1700019L13Rik',\n"",
+       "" '1700019N19Rik',\n"",
+       "" '1700020A23Rik',\n"",
+       "" '1700020D05Rik',\n"",
+       "" '1700020L24Rik',\n"",
+       "" '1700020M21Rik',\n"",
+       "" '1700020N01Rik',\n"",
+       "" '1700020N15Rik',\n"",
+       "" '1700021A07Rik',\n"",
+       "" '1700021F02Rik',\n"",
+       "" '1700021F07Rik',\n"",
+       "" '1700021L23Rik',\n"",
+       "" '1700021N21Rik',\n"",
+       "" '1700022H01Rik',\n"",
+       "" '1700022I11Rik',\n"",
+       "" '1700022N22Rik',\n"",
+       "" '1700023F02Rik',\n"",
+       "" '1700023G09Rik',\n"",
+       "" '1700023H06Rik',\n"",
+       "" '1700024B18Rik',\n"",
+       "" '1700024G13Rik',\n"",
+       "" '1700025D23Rik',\n"",
+       "" '1700025F24Rik',\n"",
+       "" '1700025G04Rik',\n"",
+       "" '1700025K04Rik',\n"",
+       "" '1700025K24Rik',\n"",
+       "" '1700025M24Rik',\n"",
+       "" '1700025N21Rik',\n"",
+       "" '1700025N23Rik',\n"",
+       "" '1700026D11Rik',\n"",
+       "" '1700026F02Rik',\n"",
+       "" '1700026J14Rik',\n"",
+       "" '1700026N04Rik',\n"",
+       "" '1700027A07Rik',\n"",
+       "" '1700027F09Rik',\n"",
+       "" '1700027H10Rik',\n"",
+       "" '1700027J07Rik',\n"",
+       "" '1700028D13Rik',\n"",
+       "" '1700028E10Rik',\n"",
+       "" '1700028I16Rik',\n"",
+       "" '1700028J19Rik',\n"",
+       "" '1700028K03Rik',\n"",
+       "" '1700028M03Rik',\n"",
+       "" '1700028N14Rik',\n"",
+       "" '1700028P14Rik',\n"",
+       "" '1700029B22Rik',\n"",
+       "" '1700029F12Rik',\n"",
+       "" '1700029H14Rik',\n"",
+       "" '1700029I15Rik',\n"",
+       "" '1700029J07Rik',\n"",
+       "" '1700029M20Rik',\n"",
+       "" '1700029P11Rik',\n"",
+       "" '1700030C12Rik',\n"",
+       "" '1700030C14Rik',\n"",
+       "" '1700030F04Rik',\n"",
+       "" '1700030J22Rik',\n"",
+       "" '1700030K09Rik',\n"",
+       "" '1700030L22Rik',\n"",
+       "" '1700030M09Rik',\n"",
+       "" '1700030N03Rik',\n"",
+       "" '1700031L13Rik',\n"",
+       "" '1700031P21Rik',\n"",
+       "" '1700034E13Rik',\n"",
+       "" '1700034G24Rik',\n"",
+       "" '1700034H15Rik',\n"",
+       "" '1700034I23Rik',\n"",
+       "" '1700034J05Rik',\n"",
+       "" '1700034K08Rik',\n"",
+       "" '1700034P13Rik',\n"",
+       "" '1700036A12Rik',\n"",
+       "" '1700036G14Rik',\n"",
+       "" '1700037C06Rik',\n"",
+       "" '1700037C18Rik',\n"",
+       "" '1700037H04Rik',\n"",
+       "" '1700039M10Rik',\n"",
+       "" '1700039M15Rik',\n"",
+       "" '1700040D17Rik',\n"",
+       "" '1700040F17Rik',\n"",
+       "" '1700040L08Rik',\n"",
+       "" '1700041G16Rik',\n"",
+       "" '1700041I07Rik',\n"",
+       "" '1700042O10Rik',\n"",
+       "" '1700044C05Rik',\n"",
+       "" '1700045H11Rik',\n"",
+       "" '1700047A11Rik',\n"",
+       "" '1700047E10Rik',\n"",
+       "" '1700047F07Rik',\n"",
+       "" '1700047G03Rik',\n"",
+       "" '1700047G07Rik',\n"",
+       "" '1700047K16Rik',\n"",
+       "" '1700047M11Rik',\n"",
+       "" '1700048F04Rik',\n"",
+       "" '1700048O20Rik',\n"",
+       "" '1700049E15Rik',\n"",
+       "" '1700051A21Rik',\n"",
+       "" '1700052H01Rik',\n"",
+       "" '1700052K11Rik',\n"",
+       "" '1700054A03Rik',\n"",
+       "" '1700054M17Rik',\n"",
+       "" '1700054O05Rik',\n"",
+       "" '1700055D18Rik',\n"",
+       "" '1700056E22Rik',\n"",
+       "" '1700056N10Rik',\n"",
+       "" '1700057G04Rik',\n"",
+       "" '1700057H15Rik',\n"",
+       "" '1700057H21Rik',\n"",
+       "" '1700058P15Rik',\n"",
+       "" '1700060C20Rik',\n"",
+       "" '1700060J05Rik',\n"",
+       "" '1700061E17Rik',\n"",
+       "" '1700061E18Rik',\n"",
+       "" '1700061I17Rik',\n"",
+       "" '1700061N14Rik',\n"",
+       "" '1700062C10Rik',\n"",
+       "" '1700063D05Rik',\n"",
+       "" '1700063H04Rik',\n"",
+       "" '1700063J08Rik',\n"",
+       "" '1700064H15Rik',\n"",
+       "" '1700064M15Rik',\n"",
+       "" '1700065D16Rik',\n"",
+       "" '1700065J11Rik',\n"",
+       "" '1700065J18Rik',\n"",
+       "" '1700066B17Rik',\n"",
+       "" '1700066B19Rik',\n"",
+       "" '1700066J03Rik',\n"",
+       "" '1700066M21Rik',\n"",
+       "" '1700066O22Rik',\n"",
+       "" '1700067K01Rik',\n"",
+       "" '1700067P10Rik',\n"",
+       "" '1700069B07Rik',\n"",
+       "" '1700069L16Rik',\n"",
+       "" '1700071M16Rik',\n"",
+       "" '1700072G22Rik',\n"",
+       "" '1700074H08Rik',\n"",
+       "" '1700080N15Rik',\n"",
+       "" '1700081N11Rik',\n"",
+       "" '1700082M22Rik',\n"",
+       "" '1700084C06Rik',\n"",
+       "" '1700084D21Rik',\n"",
+       "" '1700084E18Rik',\n"",
+       "" '1700084F23Rik',\n"",
+       "" '1700085C21Rik',\n"",
+       "" '1700086D15Rik',\n"",
+       "" '1700086O06Rik',\n"",
+       "" '1700086P04Rik',\n"",
+       "" '1700087I21Rik',\n"",
+       "" '1700088E04Rik',\n"",
+       "" '1700092C10Rik',\n"",
+       "" '1700092C17Rik',\n"",
+       "" '1700092K14Rik',\n"",
+       "" '1700092M07Rik',\n"",
+       "" '1700093J21Rik',\n"",
+       "" '1700093K21Rik',\n"",
+       "" '1700094D03Rik',\n"",
+       "" '1700094M23Rik',\n"",
+       "" '1700095A13Rik',\n"",
+       "" '1700095J03Rik',\n"",
+       "" '1700095J07Rik',\n"",
+       "" '1700095J12Rik',\n"",
+       "" '1700096K18Rik',\n"",
+       "" '1700097N02Rik',\n"",
+       "" '1700099I09Rik',\n"",
+       "" '1700100I10Rik',\n"",
+       "" '1700100L14Rik',\n"",
+       "" '1700101I11Rik',\n"",
+       "" '1700101I19Rik',\n"",
+       "" '1700101O22Rik',\n"",
+       "" '1700102H20Rik',\n"",
+       "" '1700102P08Rik',\n"",
+       "" '1700104B16Rik',\n"",
+       "" '1700104L18Rik',\n"",
+       "" '1700105G05Rik',\n"",
+       "" '1700105P06Rik',\n"",
+       "" '1700108F19Rik',\n"",
+       "" '1700108J01Rik',\n"",
+       "" '1700109G14Rik',\n"",
+       "" '1700109G15Rik',\n"",
+       "" '1700109H08Rik',\n"",
+       "" '1700109K24Rik',\n"",
+       "" '1700110C19Rik',\n"",
+       "" '1700110K17Rik',\n"",
+       "" '1700111E14Rik',\n"",
+       "" '1700112D23Rik',\n"",
+       "" '1700112J16Rik',\n"",
+       "" '1700113A16Rik',\n"",
+       "" '1700113B09Rik',\n"",
+       "" '1700113H08Rik',\n"",
+       "" '1700116B05Rik',\n"",
+       "" '1700116H05Rik',\n"",
+       "" '1700119H24Rik',\n"",
+       "" '1700120B22Rik',\n"",
+       "" '1700120C14Rik',\n"",
+       "" '1700120C18Rik',\n"",
+       "" '1700120G07Rik',\n"",
+       "" '1700120O09Rik',\n"",
+       "" '1700121L03Rik',\n"",
+       "" '1700121L16Rik',\n"",
+       "" '1700121N20Rik',\n"",
+       "" '1700122C19Rik',\n"",
+       "" '1700122D07Rik',\n"",
+       "" '1700122E12Rik',\n"",
+       "" '1700122O11Rik',\n"",
+       "" '1700123J17Rik',\n"",
+       "" '1700123K08Rik',\n"",
+       "" '1700123L14Rik',\n"",
+       "" '1700123M08Rik',\n"",
+       "" '1700123O20Rik',\n"",
+       "" '1700123O21Rik',\n"",
+       "" '1700124L16Rik',\n"",
+       "" '1700125G22Rik',\n"",
+       "" '1700125H03Rik',\n"",
+       "" '1700125H20Rik',\n"",
+       "" '1700126G02Rik',\n"",
+       "" '1700128E19Rik',\n"",
+       "" '1700129L04Rik',\n"",
+       "" '1810006J02Rik',\n"",
+       "" '1810007C17Rik',\n"",
+       "" '1810007D17Rik',\n"",
+       "" '1810008B01Rik',\n"",
+       "" '1810008I18Rik',\n"",
+       "" '1810009A15Rik',\n"",
+       "" '1810010H24Rik',\n"",
+       "" '1810010K12Rik',\n"",
+       "" '1810012K08Rik',\n"",
+       "" '1810012K16Rik',\n"",
+       "" '1810013D15Rik',\n"",
+       "" '1810013L24Rik',\n"",
+       "" '1810014P07Rik',\n"",
+       "" '1810019D21Rik',\n"",
+       "" '1810019N24Rik',\n"",
+       "" '1810020O05Rik',\n"",
+       "" '1810021B22Rik',\n"",
+       "" '1810024B03Rik',\n"",
+       "" '1810026B05Rik',\n"",
+       "" '1810028F09Rik',\n"",
+       "" '1810030O07Rik',\n"",
+       "" '1810034E14Rik',\n"",
+       "" '1810037I17Rik',\n"",
+       "" '1810041H14Rik',\n"",
+       "" '1810044D09Rik',\n"",
+       "" '1810046K07Rik',\n"",
+       "" '1810053B23Rik',\n"",
+       "" '1810055G02Rik',\n"",
+       "" '1810058I24Rik',\n"",
+       "" '1810058N15Rik',\n"",
+       "" '1810059C17Rik',\n"",
+       "" '1810059H22Rik',\n"",
+       "" '1810062G17Rik',\n"",
+       "" '1810062O18Rik',\n"",
+       "" '1810063I02Rik',\n"",
+       "" '1810064F22Rik',\n"",
+       "" '1810073O08Rik',\n"",
+       "" '2010001A14Rik',\n"",
+       "" '2010001K21Rik',\n"",
+       "" '2010003K11Rik',\n"",
+       "" '2010007H06Rik',\n"",
+       "" '2010009K17Rik',\n"",
+       "" '2010013B24Rik',\n"",
+       "" '2010016I18Rik',\n"",
+       "" '2010106C02Rik',\n"",
+       "" '2010106E10Rik',\n"",
+       "" '2010106G04Rik',\n"",
+       "" '2010109A12Rik',\n"",
+       "" '2010110E17Rik',\n"",
+       "" '2010110G14Rik',\n"",
+       "" '2010110K18Rik',\n"",
+       "" '2010204K13Rik',\n"",
+       "" '2010300C02Rik',\n"",
+       "" '2010308F09Rik',\n"",
+       "" '2010310C07Rik',\n"",
+       "" '2010315B03Rik',\n"",
+       "" '2010320M18Rik',\n"",
+       "" '2010320O07Rik',\n"",
+       "" '2200002D01Rik',\n"",
+       "" '2200002J24Rik',\n"",
+       "" '2210011K15Rik',\n"",
+       "" '2210016F16Rik',\n"",
+       "" '2210016L21Rik',\n"",
+       "" '2210039B01Rik',\n"",
+       "" '2210406O10Rik',\n"",
+       "" '2210408F21Rik',\n"",
+       "" '2210408I21Rik',\n"",
+       "" '2210409D07Rik',\n"",
+       "" '2210417A02Rik',\n"",
+       "" '2210420H20Rik',\n"",
+       "" '2300002M23Rik',\n"",
+       "" '2300009A05Rik',\n"",
+       "" '2310001H17Rik',\n"",
+       "" '2310001K24Rik',\n"",
+       "" '2310002D06Rik',\n"",
+       "" '2310002F09Rik',\n"",
+       "" '2310002L09Rik',\n"",
+       "" '2310003L06Rik',\n"",
+       "" '2310003N18Rik',\n"",
+       "" '2310005A03Rik',\n"",
+       "" '2310005E17Rik',\n"",
+       "" '2310008N11Rik',\n"",
+       "" '2310009B15Rik',\n"",
+       "" '2310010J17Rik',\n"",
+       "" '2310011J03Rik',\n"",
+       "" '2310014F06Rik',\n"",
+       "" '2310015A10Rik',\n"",
+       "" '2310015D24Rik',\n"",
+       "" '2310015K22Rik',\n"",
+       "" '2310016G11Rik',\n"",
+       "" '2310022A10Rik',\n"",
+       "" '2310022B05Rik',\n"",
+       "" '2310026I22Rik',\n"",
+       "" '2310026L22Rik',\n"",
+       "" '2310030G06Rik',\n"",
+       "" '2310033P09Rik',\n"",
+       "" '2310034G01Rik',\n"",
+       "" '2310034O05Rik',\n"",
+       "" '2310039H08Rik',\n"",
+       "" '2310039L15Rik',\n"",
+       "" '2310040G24Rik',\n"",
+       "" '2310043L19Rik',\n"",
+       "" '2310043M15Rik',\n"",
+       "" '2310047D07Rik',\n"",
+       "" '2310057M21Rik',\n"",
+       "" '2310058D17Rik',\n"",
+       "" '2310061I04Rik',\n"",
+       "" '2310065F04Rik',\n"",
+       "" '2310067P03Rik',\n"",
+       "" '2310068J16Rik',\n"",
+       "" '2310074N15Rik',\n"",
+       "" '2310075C17Rik',\n"",
+       "" '2310081O03Rik',\n"",
+       "" '2400006E01Rik',\n"",
+       "" '2410002F23Rik',\n"",
+       "" '2410003L11Rik',\n"",
+       "" '2410004B18Rik',\n"",
+       "" '2410004I01Rik',\n"",
+       "" '2410004P03Rik',\n"",
+       "" '2410006H16Rik',\n"",
+       "" '2410012E07Rik',\n"",
+       "" '2410017I17Rik',\n"",
+       "" '2410018L13Rik',\n"",
+       "" '2410021H03Rik',\n"",
+       "" '2410022M11Rik',\n"",
+       "" '2410080I02Rik',\n"",
+       "" '2410124H12Rik',\n"",
+       "" '2410131K14Rik',\n"",
+       "" '2410137M14Rik',\n"",
+       "" '2410141K09Rik',\n"",
+       "" '2410152P15Rik',\n"",
+       "" '2500002B13Rik',\n"",
+       "" '2500004C02Rik',\n"",
+       "" '2510002D24Rik',\n"",
+       "" '2510009E07Rik',\n"",
+       "" '2510017J16Rik',\n"",
+       "" '2510039O18Rik',\n"",
+       "" '2510046G10Rik',\n"",
+       "" '2600002D14Rik',\n"",
+       "" '2600006K01Rik',\n"",
+       "" '2600014E21Rik',\n"",
+       "" '2610001J05Rik',\n"",
+       "" '2610002M06Rik',\n"",
+       "" '2610008E11Rik',\n"",
+       "" '2610016A17Rik',\n"",
+       "" '2610020C07Rik',\n"",
+       "" '2610021A01Rik',\n"",
+       "" '2610027F03Rik',\n"",
+       "" '2610027K06Rik',\n"",
+       "" '2610028E06Rik',\n"",
+       "" '2610028H24Rik',\n"",
+       "" '2610035D17Rik',\n"",
+       "" '2610035F20Rik',\n"",
+       "" '2610037D02Rik',\n"",
+       "" '2610042L04Rik',\n"",
+       "" '2610044O15Rik8',\n"",
+       "" '2610203C22Rik',\n"",
+       "" '2610204G07Rik',\n"",
+       "" '2610206C17Rik',\n"",
+       "" '2610300M13Rik',\n"",
+       "" '2610301B20Rik',\n"",
+       "" '2610303G11Rik',\n"",
+       "" '2610306M01Rik',\n"",
+       "" '2610307P16Rik',\n"",
+       "" '2610316D01Rik',\n"",
+       "" '2610318N02Rik',\n"",
+       "" '2610507B11Rik',\n"",
+       "" '2610507I01Rik',\n"",
+       "" '2610528A11Rik',\n"",
+       "" '2610528J11Rik',\n"",
+       "" '2700033N17Rik',\n"",
+       "" '2700038G22Rik',\n"",
+       "" '2700046A07Rik',\n"",
+       "" '2700046G09Rik',\n"",
+       "" '2700049A03Rik',\n"",
+       "" '2700054A10Rik',\n"",
+       "" '2700062C07Rik',\n"",
+       "" '2700068H02Rik',\n"",
+       "" '2700069I18Rik',\n"",
+       "" '2700081O15Rik',\n"",
+       "" '2700097O09Rik',\n"",
+       "" '2810001G20Rik',\n"",
+       "" '2810002D19Rik',\n"",
+       "" '2810004N23Rik',\n"",
+       "" '2810006K23Rik',\n"",
+       "" '2810013P06Rik',\n"",
+       "" '2810021J22Rik',\n"",
+       "" '2810029C07Rik',\n"",
+       "" '2810030D12Rik',\n"",
+       "" '2810032G03Rik',\n"",
+       "" '2810049E08Rik',\n"",
+       "" '2810402E24Rik',\n"",
+       "" '2810403D21Rik',\n"",
+       "" '2810403G07Rik',\n"",
+       "" '2810404F17Rik',\n"",
+       "" '2810404M03Rik',\n"",
+       "" '2810405F17Rik',\n"",
+       "" '2810407A14Rik',\n"",
+       "" '2810408A11Rik',\n"",
+       "" '2810408I11Rik',\n"",
+       "" '2810410L24Rik',\n"",
+       "" '2810414N06Rik',\n"",
+       "" '2810429I04Rik',\n"",
+       "" '2810430I11Rik',\n"",
+       "" '2810432F15Rik',\n"",
+       "" '2810433D01Rik',\n"",
+       "" '2810442N19Rik',\n"",
+       "" '2810454H06Rik',\n"",
+       "" '2810455O05Rik',\n"",
+       "" '2810457G06Rik',\n"",
+       "" '2810459M11Rik',\n"",
+       "" '2810471M01Rik',\n"",
+       "" '2900005J15Rik',\n"",
+       "" '2900009J06Rik',\n"",
+       "" '2900026A02Rik',\n"",
+       "" '2900040C04Rik',\n"",
+       "" '2900041M22Rik',\n"",
+       "" '2900042K21Rik',\n"",
+       "" '2900045O20Rik',\n"",
+       "" '2900052L18Rik',\n"",
+       "" '2900052N01Rik',\n"",
+       "" '2900057B20Rik',\n"",
+       "" '2900060B14Rik',\n"",
+       "" '2900060N12Rik',\n"",
+       "" '2900072N19Rik',\n"",
+       "" '2900076A07Rik',\n"",
+       "" '2900079G21Rik',\n"",
+       "" '2900089D17Rik',\n"",
+       "" '2900093K20Rik',\n"",
+       "" '2900097C17Rik',\n"",
+       "" '3010001F23Rik',\n"",
+       "" '3010003L21Rik',\n"",
+       "" '3100003L05Rik',\n"",
+       "" '3110001I22Rik',\n"",
+       "" '3110004A20Rik',\n"",
+       "" '3110006O06Rik',\n"",
+       "" '3110009E18Rik',\n"",
+       "" '3110009F21Rik',\n"",
+       "" '3110015C05Rik',\n"",
+       "" '3110021N24Rik',\n"",
+       "" '3110039M20Rik',\n"",
+       "" '3110040N11Rik',\n"",
+       "" '3110045C21Rik',\n"",
+       "" '3110056K07Rik',\n"",
+       "" '3110070M22Rik',\n"",
+       "" '3110082I17Rik',\n"",
+       "" '3110082J24Rik',\n"",
+       "" '3110083C13Rik',\n"",
+       "" '3110099E03Rik',\n"",
+       "" '3200001D21Rik',\n"",
+       "" '3222401L13Rik',\n"",
+       "" '3300002A11Rik',\n"",
+       "" '3300002I08Rik',\n"",
+       "" '3300002P13Rik',\n"",
+       "" '3300005D01Rik',\n"",
+       "" '3425401B19Rik',\n"",
+       "" '3632454L22Rik',\n"",
+       "" '3830403N18Rik',\n"",
+       "" '3830406C13Rik',\n"",
+       "" '3830408C21Rik',\n"",
+       "" '3830417A13Rik',\n"",
+       "" '3830432H09Rik',\n"",
+       "" '3930402G23Rik',\n"",
+       "" '4430402I18Rik',\n"",
+       "" '4631405J19Rik',\n"",
+       "" '4632404H12Rik',\n"",
+       "" '4632404M16Rik',\n"",
+       "" '4632411P08Rik',\n"",
+       "" '4632427E13Rik',\n"",
+       "" '4632428C04Rik',\n"",
+       "" '4731419I09Rik',\n"",
+       "" '4732414G09Rik',\n"",
+       "" '4732419C18Rik',\n"",
+       "" '4732440D04Rik',\n"",
+       "" '4732463B04Rik',\n"",
+       "" '4732465J04Rik',\n"",
+       "" '4732471J01Rik',\n"",
+       "" '4732487G21Rik',\n"",
+       "" '4732490B19Rik',\n"",
+       "" '4732491K20Rik',\n"",
+       "" '4732496C06Rik',\n"",
+       "" '4831440D22Rik',\n"",
+       "" '4833403J16Rik',\n"",
+       "" '4833405L11Rik',\n"",
+       "" '4833408A19Rik',\n"",
+       "" '4833411C07Rik',\n"",
+       "" '4833412C05Rik',\n"",
+       "" '4833413E03Rik',\n"",
+       "" '4833413G10Rik',\n"",
+       "" '4833415N18Rik',\n"",
+       "" '4833417C18Rik',\n"",
+       "" '4833418N02Rik',\n"",
+       "" '4833419F23Rik',\n"",
+       "" '4833419O12Rik',\n"",
+       "" '4833420G17Rik',\n"",
+       "" '4833422C13Rik',\n"",
+       "" '4833422M21Rik',\n"",
+       "" '4833427F10Rik',\n"",
+       "" '4833427G06Rik',\n"",
+       "" '4833428L15Rik',\n"",
+       "" '4833438C02Rik',\n"",
+       "" '4833439L19Rik',\n"",
+       "" '4833445I07Rik',\n"",
+       "" '4921501I09Rik',\n"",
+       "" '4921504A21Rik',\n"",
+       "" '4921504E06Rik',\n"",
+       "" '4921507G05Rik',\n"",
+       "" '4921507P07Rik',\n"",
+       "" '4921508D12Rik',\n"",
+       "" '4921509A06Rik',\n"",
+       "" '4921509O07Rik',\n"",
+       "" '4921511C10Rik',\n"",
+       "" '4921511I17Rik',\n"",
+       "" '4921513I03Rik',\n"",
+       "" '4921513I08Rik',\n"",
+       "" '4921514A10Rik',\n"",
+       "" '4921515E04Rik',\n"",
+       "" '4921515L22Rik',\n"",
+       "" '4921517D16Rik',\n"",
+       "" '4921517D22Rik',\n"",
+       "" '4921518K17Rik',\n"",
+       "" '4921521D15Rik',\n"",
+       "" '4921522P10Rik',\n"",
+       "" '4921523L03Rik',\n"",
+       "" '4921524J17Rik',\n"",
+       "" '4921525O09Rik',\n"",
+       "" '4921528I07Rik',\n"",
+       "" '4921529L05Rik',\n"",
+       "" '4921531C22Rik',\n"",
+       "" '4921534H16Rik',\n"",
+       "" '4921536K21Rik',\n"",
+       "" '4921539E11Rik',\n"",
+       "" '4921539H07Rik',\n"",
+       "" '4922502N22Rik',\n"",
+       "" '4930401A07Rik',\n"",
+       "" '4930401G09Rik',\n"",
+       "" '4930402F11Rik',\n"",
+       "" '4930402H24Rik',\n"",
+       "" '4930403P22Rik',\n"",
+       "" '4930404H11Rik',\n"",
+       "" '4930404H24Rik',\n"",
+       "" '4930404I05Rik',\n"",
+       "" '4930404K13Rik',\n"",
+       "" '4930404N11Rik',\n"",
+       "" '4930405A21Rik',\n"",
+       "" '4930405D11Rik',\n"",
+       "" '4930405H06Rik',\n"",
+       "" '4930405J17Rik',\n"",
+       "" '4930405O22Rik',\n"",
+       "" '4930408O17Rik',\n"",
+       "" '4930412C18Rik',\n"",
+       "" '4930412F09Rik',\n"",
+       "" '4930412L05Rik',\n"",
+       "" '4930412M03Rik',\n"",
+       "" '4930412O13Rik',\n"",
+       "" '4930414F18Rik',\n"",
+       "" '4930414N06Rik',\n"",
+       "" '4930415C11Rik',\n"",
+       "" '4930415O20Rik',\n"",
+       "" '4930417H01Rik',\n"",
+       "" '4930417O13Rik',\n"",
+       "" '4930417O22Rik',\n"",
+       "" '4930419G24Rik',\n"",
+       "" '4930420G21Rik',\n"",
+       "" '4930422C21Rik',\n"",
+       "" '4930422I22Rik',\n"",
+       "" '4930422M22Rik',\n"",
+       "" '4930423D22Rik',\n"",
+       "" '4930423M02Rik',\n"",
+       "" '4930425L21Rik',\n"",
+       "" '4930425O10Rik',\n"",
+       "" '4930425P05Rik',\n"",
+       "" '4930426D05Rik',\n"",
+       "" '4930426I24Rik',\n"",
+       "" '4930426L09Rik',\n"",
+       "" '4930428E07Rik',\n"",
+       "" '4930428N03Rik',\n"",
+       "" '4930428O21Rik',\n"",
+       "" '4930429D17Rik',\n"",
+       "" '4930429F24Rik',\n"",
+       "" '4930429H19Rik',\n"",
+       "" '4930429P21Rik',\n"",
+       "" '4930430E12Rik',\n"",
+       "" '4930430F08Rik',\n"",
+       "" '4930430F21Rik',\n"",
+       "" '4930430O22Rik',\n"",
+       "" '4930431F12Rik',\n"",
+       "" '4930431P22Rik',\n"",
+       "" '4930432B10Rik',\n"",
+       "" '4930432J09Rik',\n"",
+       "" '4930432K21Rik',\n"",
+       "" '4930432L08Rik',\n"",
+       "" '4930433J02Rik',\n"",
+       "" '4930433N12Rik',\n"",
+       "" '4930434B07Rik',\n"",
+       "" '4930434F21Rik',\n"",
+       "" '4930435F18Rik',\n"",
+       "" '4930435M08Rik',\n"",
+       "" '4930438A08Rik',\n"",
+       "" '4930438E09Rik',\n"",
+       "" '4930439A04Rik',\n"",
+       "" '4930439D14Rik',\n"",
+       "" '4930440I19Rik',\n"",
+       "" '4930442G10Rik',\n"",
+       "" '4930442G15Rik',\n"",
+       "" '4930443O20Rik',\n"",
+       "" '4930444A19Rik',\n"",
+       "" '4930444E06Rik',\n"",
+       "" '4930444F02Rik',\n"",
+       "" '4930444P10Rik',\n"",
+       "" '4930445B16Rik',\n"",
+       "" '4930445E18Rik',\n"",
+       "" '4930445N06Rik',\n"",
+       "" '4930445N18Rik',\n"",
+       "" '4930447C04Rik',\n"",
+       "" '4930447F24Rik',\n"",
+       "" '4930447J18Rik',\n"",
+       "" '4930447K03Rik',\n"",
+       "" '4930447M23Rik',\n"",
+       "" '4930447N08Rik',\n"",
+       "" '4930448A20Rik',\n"",
+       "" '4930448C13Rik',\n"",
+       "" '4930448E22Rik',\n"",
+       "" '4930448H16Rik',\n"",
+       "" '4930448I06Rik',\n"",
+       "" '4930449C09Rik',\n"",
+       "" '4930449E01Rik',\n"",
+       "" '4930449E18Rik',\n"",
+       "" '4930451I11Rik',\n"",
+       "" '4930452A19Rik',\n"",
+       "" '4930452B06Rik',\n"",
+       "" '4930452G13Rik',\n"",
+       "" '4930452L12Rik',\n"",
+       "" '4930452N14Rik',\n"",
+       "" '4930453C13Rik',\n"",
+       "" '4930453H23Rik',\n"",
+       "" '4930453L07Rik',\n"",
+       "" '4930453N24Rik',\n"",
+       "" '4930455B14Rik',\n"",
+       "" '4930455C13Rik',\n"",
+       "" '4930455G09Rik',\n"",
+       "" '4930455J16Rik',\n"",
+       "" '4930455M05Rik',\n"",
+       "" '4930458A03Rik',\n"",
+       "" '4930458D05Rik',\n"",
+       "" '4930458K08Rik',\n"",
+       "" '4930461C15Rik',\n"",
+       "" '4930461G14Rik',\n"",
+       "" '4930463O16Rik',\n"",
+       "" '4930465M20Rik',\n"",
+       "" '4930467D21Rik',\n"",
+       "" '4930467K11Rik',\n"",
+       "" '4930469K13Rik',\n"",
+       "" '4930470G03Rik',\n"",
+       "" '4930470H14Rik',\n"",
+       "" '4930471E19Rik',\n"",
+       "" '4930471G03Rik',\n"",
+       "" '4930471G24Rik',\n"",
+       "" '4930471L23Rik',\n"",
+       "" '4930473A02Rik',\n"",
+       "" '4930473D10Rik',\n"",
+       "" '4930474M22Rik',\n"",
+       "" '4930474N05Rik',\n"",
+       "" '4930474N09Rik',\n"",
+       "" '4930477G07Rik',\n"",
+       "" '4930477N07Rik',\n"",
+       "" '4930478E11Rik',\n"",
+       "" '4930478K11Rik',\n"",
+       "" '4930478L05Rik',\n"",
+       "" '4930478M13Rik',\n"",
+       "" '4930479D17Rik',\n"",
+       "" '4930480K15Rik',\n"",
+       "" '4930481A15Rik',\n"",
+       "" '4930481B07Rik',\n"",
+       "" '4930483J18Rik',\n"",
+       "" '4930483O08Rik',\n"",
+       "" '4930483P17Rik',\n"",
+       "" '4930484H19Rik',\n"",
+       "" '4930484I04Rik',\n"",
+       "" '4930486F22Rik',\n"",
+       "" '4930486L24Rik',\n"",
+       "" '4930488L21Rik',\n"",
+       "" '4930488N15Rik',\n"",
+       "" '4930500A05Rik',\n"",
+       "" '4930500F04Rik',\n"",
+       "" '4930500F10Rik',\n"",
+       "" '4930500H12Rik',\n"",
+       "" '4930500L23Rik',\n"",
+       "" '4930500M09Rik',\n"",
+       "" '4930502E09Rik',\n"",
+       "" '4930503B20Rik',\n"",
+       "" '4930503E14Rik',\n"",
+       "" '4930503L19Rik',\n"",
+       "" '4930505A04Rik',\n"",
+       "" '4930505K13Rik',\n"",
+       "" '4930505M18Rik',\n"",
+       "" '4930505N22Rik',\n"",
+       "" '4930506C21Rik',\n"",
+       "" '4930507D05Rik',\n"",
+       "" '4930507D10Rik',\n"",
+       "" '4930509E16Rik',\n"",
+       "" '4930509G22Rik',\n"",
+       "" '4930509H03Rik',\n"",
+       "" '4930509J09Rik',\n"",
+       "" '4930509K18Rik',\n"",
+       "" '4930511A02Rik',\n"",
+       "" '4930511E03Rik',\n"",
+       "" '4930511J24Rik',\n"",
+       "" '4930511M06Rik',\n"",
+       "" '4930512B01Rik',\n"",
+       "" '4930512H18Rik',\n"",
+       "" '4930512J16Rik',\n"",
+       "" '4930512P04Rik',\n"",
+       "" '4930513D17Rik',\n"",
+       "" '4930513N10Rik',\n"",
+       "" '4930515G01Rik',\n"",
+       "" '4930516B21Rik',\n"",
+       "" '4930516K23Rik',\n"",
+       "" '4930517E11Rik',\n"",
+       "" '4930517E14Rik',\n"",
+       "" '4930517L18Rik',\n"",
+       "" '4930517O19Rik',\n"",
+       "" '4930518C09Rik',\n"",
+       "" '4930518J20Rik',\n"",
+       "" '4930518J21Rik',\n"",
+       "" '4930519A11Rik',\n"",
+       "" '4930519F24Rik',\n"",
+       "" '4930519K11Rik',\n"",
+       "" '4930519L02Rik',\n"",
+       "" '4930519P11Rik',\n"",
+       "" '4930520M14Rik',\n"",
+       "" '4930520O04Rik',\n"",
+       "" '4930522L14Rik',\n"",
+       "" '4930523C07Rik',\n"",
+       "" '4930524B15Rik',\n"",
+       "" '4930524C18Rik',\n"",
+       "" '4930524O05Rik',\n"",
+       "" '4930524O07Rik',\n"",
+       "" '4930524O08Rik',\n"",
+       "" '4930525G20Rik',\n"",
+       "" '4930526F13Rik',\n"",
+       "" '4930526L06Rik',\n"",
+       "" '4930527E20Rik',\n"",
+       "" '4930527F14Rik',\n"",
+       "" '4930527J03Rik',\n"",
+       "" '4930528D03Rik',\n"",
+       "" '4930528G23Rik',\n"",
+       "" '4930529I22Rik',\n"",
+       "" '4930532G15Rik',\n"",
+       "" '4930532I03Rik',\n"",
+       "" '4930532M18Rik',\n"",
+       "" '4930533D04Rik',\n"",
+       "" '4930533I22Rik',\n"",
+       "" '4930533K18Rik',\n"",
+       "" '4930533N22Rik',\n"",
+       "" '4930534D22Rik',\n"",
+       "" '4930534H03Rik',\n"",
+       "" '4930534H18Rik',\n"",
+       "" '4930535E02Rik',\n"",
+       "" '4930537H20Rik',\n"",
+       "" '4930538E20Rik',\n"",
+       "" '4930539J05Rik',\n"",
+       "" '4930539M17Rik',\n"",
+       "" '4930540M05Rik',\n"",
+       "" '4930542C12Rik',\n"",
+       "" '4930542C16Rik',\n"",
+       "" '4930542D17Rik',\n"",
+       "" '4930543E12Rik',\n"",
+       "" '4930544F09Rik',\n"",
+       "" '4930544G11Rik',\n"",
+       "" '4930544I03Rik',\n"",
+       "" '4930545E07Rik',\n"",
+       "" '4930545L23Rik',\n"",
+       "" '4930546C10Rik',\n"",
+       "" '4930546K05Rik',\n"",
+       "" '4930547E14Rik',\n"",
+       "" '4930547M16Rik',\n"",
+       "" '4930548H24Rik',\n"",
+       "" '4930549C15Rik',\n"",
+       "" '4930549G23Rik',\n"",
+       "" '4930549P19Rik',\n"",
+       "" '4930550C14Rik',\n"",
+       "" '4930550C17Rik',\n"",
+       "" '4930550L24Rik',\n"",
+       "" '4930551I15Rik',\n"",
+       "" '4930551L18Rik',\n"",
+       "" '4930551O13Rik',\n"",
+       "" '4930553E22Rik',\n"",
+       "" '4930553M12Rik',\n"",
+       "" '4930553P18Rik',\n"",
+       "" '4930554H23Rik',\n"",
+       "" '4930554I06Rik',\n"",
+       "" '4930555A03Rik',\n"",
+       "" '4930555B11Rik',\n"",
+       "" '4930555F03Rik',\n"",
+       "" '4930555G07Rik',\n"",
+       "" '4930555M17Rik',\n"",
+       "" '4930556G01Rik',\n"",
+       "" '4930556G22Rik',\n"",
+       "" '4930556I23Rik',\n"",
+       "" '4930556J02Rik',\n"",
+       "" '4930556L07Rik',\n"",
+       "" '4930556M19Rik',\n"",
+       "" '4930556N09Rik',\n"",
+       "" '4930556N13Rik',\n"",
+       "" '4930557F10Rik',\n"",
+       "" '4930557J02Rik',\n"",
+       "" '4930557K07Rik',\n"",
+       "" '4930558C23Rik',\n"",
+       "" '4930558J18Rik',\n"",
+       "" '4930558J22Rik',\n"",
+       "" '4930558K02Rik',\n"",
+       "" '4930562A09Rik',\n"",
+       "" '4930562C15Rik',\n"",
+       "" '4930562D21Rik',\n"",
+       "" '4930562F07Rik',\n"",
+       "" '4930562F17Rik',\n"",
+       "" '4930563E18Rik',\n"",
+       "" '4930563E22Rik',\n"",
+       "" ...]""
+      ]
+     },
+     ""execution_count"": 3,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""g1 = mo_data.var.index.values \n"",
+    ""g2 = mouse_atlas.var.index.values\n"",
+    ""\n"",
+    ""refgenes = sorted(list(set(g1).intersection(g2)))\n"",
+    ""\n"",
+    ""refgenes""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""2ddba099"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test_prop=0.2\n"",
+    ""# trainsplit, valsplit = train_test_split(current_labels, stratify=current_labels, test_size=test_prop)\n"",
+    ""# trainsplit, testsplit = train_test_split(trainsplit, stratify=trainsplit, test_size=test_prop)\n"",
+    ""\n"",
+    ""\n"",
+    ""# trainsplit.index""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 17,
+   ""id"": ""4f766876"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""<models.lib.lightning_train.DataModule at 0x7f8e4ba746d0>""
+      ]
+     },
+     ""execution_count"": 17,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""module = DataModule(\n"",
+    ""    datafiles=['../data/mouse/MouseAdultInhibitoryNeurons.h5ad'],\n"",
+    ""    labelfiles=['../data/mouse/Adult Inhibitory Neurons in Mouse_labels.tsv'],\n"",
+    ""    class_label='numeric_class',\n"",
+    ""    sep='\\t',\n"",
+    ""    collocate=False,\n"",
+    ""    batch_size=4,\n"",
+    ""    num_workers=0,\n"",
+    ""    refgenes=refgenes,\n"",
+    ""    currgenes=g2,\n"",
+    "")\n"",
+    ""\n"",
+    ""module""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""0e79b736"",
+   ""metadata"": {
+    ""scrolled"": false
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Creating train/val/test DataLoaders...\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""module.setup()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""6e345a8f"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""train = module.train_dataloader()\n"",
+    ""\n"",
+    ""print(next(iter(train)))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""5a126b1a"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""train = DataLoader(atlas_train, batch_size=4, num_workers=0, drop_last=True, shuffle=True)\n"",
+    ""val = DataLoader(atlas_val, batch_size=4, num_workers=0, drop_last=True, shuffle=False)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""09338062"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""# from models.lib.neural import GeneClassifier\n"",
+    ""# from pytorch_lightning import Trainer \n"",
+    ""# from pytorch_lightning.loggers import WandbLogger\n"",
+    ""\n"",
+    ""# model = GeneClassifier(\n"",
+    ""#     input_dim=34430, \n"",
+    ""#     output_dim=50,\n"",
+    ""#     optim_params={\n"",
+    ""#         'optimizer': torch.optim.Adam,\n"",
+    ""#         'lr': 3e-4,\n"",
+    ""#     }\n"",
+    ""# )\n"",
+    ""\n"",
+    ""# wandb_logger = WandbLogger(project='Mouse Classifier', name='Tabnet with Metrics')\n"",
+    ""# trainer = Trainer(logger=wandb_logger)\n"",
+    ""\n"",
+    ""# trainer.fit(model, train, val)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""50b87ee6"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""def gene_intersection(\n"",
+    ""    files\n"",
+    ""):\n"",
+    ""    import dask.dataframe as dd \n"",
+    ""    \n"",
+    ""    cols = []\n"",
+    ""    for file in files:\n"",
+    ""        temp = pd.read_csv(fpath, nrows=1, header=1).columns \n"",
+    ""        temp = [x.split('|')[0].upper().strip() for x in temp]\n"",
+    ""        cols.append(set(temp))\n"",
+    ""    \n"",
+    ""    unique = list(set.intersection(*cols))\n"",
+    ""    unique = sorted(unique)\n"",
+    ""    \n"",
+    ""    return unique ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""3f7f8e0c"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""807b699d"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5097f759"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""e737ca20"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5110b485"",
+   ""metadata"": {
+    ""scrolled"": false
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""reading in h5ad\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""generate_single_dataset(\n"",
+    ""    datafile='../data/mouse/MouseAdultInhibitoryNeurons.h5ad',\n"",
+    ""    labelfile='../data/mouse/Adult Inhibitory Neurons in Mouse_labels.tsv',\n"",
+    ""    class_label='numeric_class',\n"",
+    ""    sep='\\t'\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""0ee0b945"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""1732e41b"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""ename"": ""FileNotFoundError"",
+     ""evalue"": ""[Errno 2] No such file or directory: '../data/interim/labels/primary_bhaduri_labels.tsv'"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_41600/210508822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minterim_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/interim/labels/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterim_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'primary_bhaduri_labels.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterim_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'allen_cortex_labels.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterim_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'allen_m1_region_labels.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \""Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\""\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \""str\"", \""bool\"", \""Any\"", \""Any\"", \""Any\"", \""Any\"", \""Any\""\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\""b\""\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/interim/labels/primary_bhaduri_labels.tsv'""
+     ]
+    }
+   ],
+   ""source"": [
+    ""import dask.dataframe as da\n"",
+    ""import pandas as pd \n"",
+    ""import os \n"",
+    ""interim_data_path = '../data/interim/labels/'\n"",
+    ""\n"",
+    ""df1 = pd.read_csv(os.path.join(interim_data_path, 'primary_bhaduri_labels.tsv'), sep='\\t')\n"",
+    ""df2 = pd.read_csv(os.path.join(interim_data_path, 'allen_cortex_labels.tsv'), sep='\\t')\n"",
+    ""df3 = pd.read_csv(os.path.join(interim_data_path, 'allen_m1_region_labels.tsv'), sep='\\t')\n"",
+    ""df4 = pd.read_csv(os.path.join(interim_data_path, 'whole_brain_bhaduri_labels.tsv'), sep='\\t')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""e9013ffe"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 6,
+   ""id"": ""771838c4"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""import numpy as np\n"",
+    ""import matplotlib.pyplot as plt \n"",
+    ""import os\n"",
+    ""import dask.dataframe as dd""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""ec4a0b3a"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""primary_labels_neighbors_15_components_50_clust_size_250.csv\n"",
+      ""# label\n"",
+      "" 3.0       42958\n"",
+      ""-1.0       22291\n"",
+      "" 20.0      17267\n"",
+      "" 17.0      15723\n"",
+      "" 6.0       13646\n"",
+      "" 9.0       13583\n"",
+      "" 19.0      12511\n"",
+      "" 12.0      10323\n"",
+      "" 15.0       9060\n"",
+      "" 18.0       7749\n"",
+      "" 7.0        6224\n"",
+      "" 16.0       4528\n"",
+      "" 1.0        3261\n"",
+      "" 0.0        2539\n"",
+      "" 11.0       2070\n"",
+      "" 4.0        1675\n"",
+      "" 14.0       1242\n"",
+      "" 8.0        1140\n"",
+      "" 5.0         622\n"",
+      "" 10.0        446\n"",
+      "" 13.0        279\n"",
+      "" 2.0         272\n"",
+      ""dtype: int64 # label    22\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_500_components_50_clust_size_250.csv\n"",
+      ""# label\n"",
+      ""1.0        183632\n"",
+      ""0.0          5777\n"",
+      ""dtype: int64 # label    2\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_pca_components_100_clust_size_50.csv\n"",
+      ""# label\n"",
+      ""-1.0       180450\n"",
+      "" 0.0         8303\n"",
+      "" 1.0          656\n"",
+      ""dtype: int64 # label    3\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_pca_components_50_clust_size_50.csv\n"",
+      ""# label\n"",
+      ""-1.0       184993\n"",
+      "" 1.0         4362\n"",
+      "" 0.0           54\n"",
+      ""dtype: int64 # label    3\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_500_components_50_clust_size_50.csv\n"",
+      ""# label\n"",
+      "" 3.0       46046\n"",
+      "" 12.0      32283\n"",
+      "" 4.0       31491\n"",
+      "" 13.0      24324\n"",
+      "" 11.0      13847\n"",
+      ""-1.0       13543\n"",
+      "" 6.0        9254\n"",
+      "" 0.0        5778\n"",
+      "" 10.0       5374\n"",
+      "" 16.0       2389\n"",
+      "" 8.0        2386\n"",
+      "" 9.0         694\n"",
+      "" 15.0        317\n"",
+      "" 5.0         309\n"",
+      "" 14.0        301\n"",
+      "" 17.0        291\n"",
+      "" 18.0        285\n"",
+      "" 1.0         257\n"",
+      "" 7.0         190\n"",
+      "" 2.0          50\n"",
+      ""dtype: int64 # label    20\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_50_components_50_clust_size_250.csv\n"",
+      ""# label\n"",
+      "" 0.0       48885\n"",
+      "" 12.0      42702\n"",
+      "" 3.0       14216\n"",
+      "" 6.0       12134\n"",
+      ""-1.0       10466\n"",
+      "" 14.0       9649\n"",
+      "" 7.0        9313\n"",
+      "" 2.0        6219\n"",
+      "" 1.0        5870\n"",
+      "" 18.0       5323\n"",
+      "" 15.0       4628\n"",
+      "" 16.0       3987\n"",
+      "" 17.0       3848\n"",
+      "" 8.0        3347\n"",
+      "" 5.0        2678\n"",
+      "" 10.0       2132\n"",
+      "" 9.0        1528\n"",
+      "" 13.0       1031\n"",
+      "" 4.0         795\n"",
+      "" 11.0        658\n"",
+      ""dtype: int64 # label    20\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_50_components_100_clust_size_100.csv\n"",
+      ""# label\n"",
+      "" 1.0       182928\n"",
+      "" 0.0         5871\n"",
+      ""-1.0          365\n"",
+      "" 2.0          245\n"",
+      ""dtype: int64 # label    4\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_500_components_100_clust_size_100.csv\n"",
+      ""# label\n"",
+      "" 1.0       182718\n"",
+      "" 0.0         5773\n"",
+      ""-1.0          702\n"",
+      "" 2.0          216\n"",
+      ""dtype: int64 # label    4\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_pca_components_50_clust_size_500.csv\n"",
+      ""# label\n"",
+      ""-1.0       189409\n"",
+      ""dtype: int64 # label    1\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_15_components_100_clust_size_100.csv\n"",
+      ""# label\n"",
+      ""-1.0       38824\n"",
+      "" 10.0      36347\n"",
+      "" 16.0      28695\n"",
+      "" 12.0      26611\n"",
+      "" 21.0      20189\n"",
+      "" 8.0       12581\n"",
+      "" 18.0       7910\n"",
+      "" 7.0        5753\n"",
+      "" 1.0        2855\n"",
+      "" 20.0       2840\n"",
+      "" 0.0        2413\n"",
+      "" 6.0        1232\n"",
+      "" 11.0        528\n"",
+      "" 9.0         513\n"",
+      "" 13.0        363\n"",
+      "" 5.0         343\n"",
+      "" 2.0         273\n"",
+      "" 17.0        271\n"",
+      "" 14.0        264\n"",
+      "" 19.0        178\n"",
+      "" 3.0         155\n"",
+      "" 15.0        153\n"",
+      "" 4.0         118\n"",
+      ""dtype: int64 # label    23\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_50_components_50_clust_size_50.csv\n"",
+      ""# label\n"",
+      "" 1.0       182750\n"",
+      "" 4.0         2635\n"",
+      "" 2.0         1918\n"",
+      ""-1.0         1278\n"",
+      "" 3.0          580\n"",
+      "" 0.0          248\n"",
+      ""dtype: int64 # label    6\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_pca_components_100_clust_size_500.csv\n"",
+      ""# label\n"",
+      ""-1.0       189409\n"",
+      ""dtype: int64 # label    1\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_15_components_50_clust_size_50.csv\n"",
+      ""# label\n"",
+      "" 6.0       70950\n"",
+      "" 13.0      37412\n"",
+      ""-1.0       27223\n"",
+      "" 9.0       19593\n"",
+      "" 20.0       6984\n"",
+      "" 30.0       4959\n"",
+      "" 29.0       3181\n"",
+      "" 2.0        2704\n"",
+      "" 16.0       2605\n"",
+      "" 0.0        2578\n"",
+      "" 27.0       2483\n"",
+      "" 26.0       2005\n"",
+      "" 12.0       1032\n"",
+      "" 32.0       1004\n"",
+      "" 17.0        931\n"",
+      "" 8.0         482\n"",
+      "" 19.0        478\n"",
+      "" 25.0        380\n"",
+      "" 7.0         281\n"",
+      "" 31.0        271\n"",
+      "" 3.0         268\n"",
+      "" 18.0        235\n"",
+      "" 28.0        232\n"",
+      "" 5.0         205\n"",
+      "" 24.0        202\n"",
+      "" 22.0        118\n"",
+      "" 11.0        109\n"",
+      "" 4.0          98\n"",
+      "" 1.0          83\n"",
+      "" 10.0         79\n"",
+      "" 14.0         67\n"",
+      "" 21.0         67\n"",
+      "" 23.0         55\n"",
+      "" 15.0         55\n"",
+      ""dtype: int64 # label    34\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_pca_components_50_clust_size_15.csv\n"",
+      ""# label\n"",
+      ""-1.0       183189\n"",
+      "" 1.0         6055\n"",
+      "" 0.0          165\n"",
+      ""dtype: int64 # label    3\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_500_components_100_clust_size_50.csv\n"",
+      ""# label\n"",
+      "" 1.0       182902\n"",
+      "" 0.0         5774\n"",
+      ""-1.0          517\n"",
+      "" 2.0          216\n"",
+      ""dtype: int64 # label    4\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_pca_components_100_clust_size_15.csv\n"",
+      ""# label\n"",
+      ""-1.0       180729\n"",
+      "" 0.0         7774\n"",
+      "" 1.0          906\n"",
+      ""dtype: int64 # label    3\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_15_components_100_clust_size_50.csv\n"",
+      ""# label\n"",
+      ""-1.0       55307\n"",
+      "" 34.0      29299\n"",
+      "" 30.0      26245\n"",
+      "" 22.0      19955\n"",
+      "" 37.0      18534\n"",
+      "" 13.0      11752\n"",
+      "" 27.0      10379\n"",
+      "" 12.0       4621\n"",
+      "" 3.0        2729\n"",
+      "" 36.0       2433\n"",
+      "" 2.0        2416\n"",
+      "" 11.0       1069\n"",
+      "" 25.0        832\n"",
+      "" 10.0        507\n"",
+      "" 14.0        430\n"",
+      "" 15.0        302\n"",
+      "" 0.0         273\n"",
+      "" 29.0        190\n"",
+      "" 19.0        185\n"",
+      "" 18.0        156\n"",
+      "" 4.0         153\n"",
+      "" 26.0        142\n"",
+      "" 28.0        135\n"",
+      "" 33.0        126\n"",
+      "" 20.0        122\n"",
+      "" 8.0         107\n"",
+      "" 6.0         100\n"",
+      "" 7.0          95\n"",
+      "" 5.0          93\n"",
+      "" 1.0          93\n"",
+      "" 17.0         91\n"",
+      "" 31.0         88\n"",
+      "" 32.0         84\n"",
+      "" 24.0         74\n"",
+      "" 9.0          70\n"",
+      "" 16.0         63\n"",
+      "" 23.0         55\n"",
+      "" 35.0         52\n"",
+      "" 21.0         52\n"",
+      ""dtype: int64 # label    39\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_15_components_100_clust_size_250.csv\n"",
+      ""# label\n"",
+      "" 1.0       182065\n"",
+      "" 0.0         5824\n"",
+      ""-1.0         1246\n"",
+      "" 2.0          274\n"",
+      ""dtype: int64 # label    4\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_50_components_50_clust_size_100.csv\n"",
+      ""# label\n"",
+      "" 21.0      22119\n"",
+      ""-1.0       19816\n"",
+      "" 17.0      17762\n"",
+      "" 23.0      17550\n"",
+      "" 5.0       16793\n"",
+      "" 20.0      15513\n"",
+      "" 15.0      14668\n"",
+      "" 3.0       14075\n"",
+      "" 22.0       8926\n"",
+      "" 16.0       7496\n"",
+      "" 8.0        6237\n"",
+      "" 0.0        5872\n"",
+      "" 4.0        5851\n"",
+      "" 7.0        5261\n"",
+      "" 19.0       3934\n"",
+      "" 13.0       2536\n"",
+      "" 18.0       1190\n"",
+      "" 11.0       1175\n"",
+      "" 9.0         868\n"",
+      "" 10.0        737\n"",
+      "" 14.0        359\n"",
+      "" 1.0         249\n"",
+      "" 12.0        207\n"",
+      "" 2.0         115\n"",
+      "" 6.0         100\n"",
+      ""dtype: int64 # label    25\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_500_components_100_clust_size_250.csv\n"",
+      ""# label\n"",
+      "" 0.0       182983\n"",
+      "" 1.0         5772\n"",
+      ""-1.0          654\n"",
+      ""dtype: int64 # label    3\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_50_components_100_clust_size_250.csv\n"",
+      ""# label\n"",
+      "" 0.0       183534\n"",
+      "" 1.0         5870\n"",
+      ""-1.0            5\n"",
+      ""dtype: int64 # label    3\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_15_components_50_clust_size_100.csv\n"",
+      ""# label\n"",
+      "" 19.0      35519\n"",
+      ""-1.0       34776\n"",
+      "" 12.0      30259\n"",
+      "" 9.0       13088\n"",
+      "" 15.0      11542\n"",
+      "" 16.0      10817\n"",
+      "" 5.0        9520\n"",
+      "" 23.0       6720\n"",
+      "" 25.0       6159\n"",
+      "" 8.0        6008\n"",
+      "" 17.0       3980\n"",
+      "" 24.0       3552\n"",
+      "" 1.0        3215\n"",
+      "" 4.0        2969\n"",
+      "" 20.0       2791\n"",
+      "" 0.0        2585\n"",
+      "" 21.0       1510\n"",
+      "" 11.0       1345\n"",
+      "" 7.0         621\n"",
+      "" 18.0        615\n"",
+      "" 22.0        455\n"",
+      "" 10.0        437\n"",
+      "" 2.0         268\n"",
+      "" 6.0         222\n"",
+      "" 3.0         209\n"",
+      "" 13.0        127\n"",
+      "" 14.0        100\n"",
+      ""dtype: int64 # label    27\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_500_components_50_clust_size_100.csv\n"",
+      ""# label\n"",
+      "" 9.0       58191\n"",
+      "" 7.0       31763\n"",
+      "" 0.0       29586\n"",
+      "" 5.0       16155\n"",
+      "" 10.0      14165\n"",
+      "" 3.0       14050\n"",
+      ""-1.0        7385\n"",
+      "" 6.0        6326\n"",
+      "" 1.0        5776\n"",
+      "" 14.0       2869\n"",
+      "" 4.0        1015\n"",
+      "" 8.0         691\n"",
+      "" 15.0        400\n"",
+      "" 11.0        381\n"",
+      "" 13.0        256\n"",
+      "" 2.0         218\n"",
+      "" 12.0        182\n"",
+      ""dtype: int64 # label    17\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n"",
+      ""primary_labels_neighbors_50_components_100_clust_size_50.csv\n"",
+      ""# label\n"",
+      "" 3.0       179111\n"",
+      "" 0.0         5871\n"",
+      ""-1.0         4009\n"",
+      "" 1.0          244\n"",
+      "" 4.0          119\n"",
+      "" 2.0           55\n"",
+      ""dtype: int64 # label    6\n"",
+      ""dtype: int64 \n"",
+      ""\n"",
+      ""\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""path = '../data/processed/'\n"",
+    ""\n"",
+    ""for f in os.listdir(os.path.join(path, 'labels')):\n"",
+    ""    df = pd.read_csv(os.path.join(path, 'labels', f))\n"",
+    ""    print(f)\n"",
+    ""    print(df.value_counts(), df.nunique(), '\\n\\n')""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""7eb9d37d"",
+   ""metadata"": {},
+   ""source"": [
+    ""Ok, this clearly isn't working and I'm not sure why. Let's try using a DataLoader to get each row (line) of interest and then calculate our sample statistics based off of that ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 9,
+   ""id"": ""50d019f4"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import dask.dataframe as dd\n"",
+    ""\n"",
+    ""labels = pd.read_csv('../data/processed/labels/primary_labels_neighbors_50_components_50_clust_size_250.csv')\n"",
+    ""cols = dd.read_csv('../data/processed/primary.csv').columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""d450d474"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""['RP11-582J16.5',\n"",
+       "" 'RP11-148L24.1',\n"",
+       "" 'MTIF2',\n"",
+       "" 'MT2A',\n"",
+       "" 'RP11-96B5.3',\n"",
+       "" 'MTMR4',\n"",
+       "" 'RPS9',\n"",
+       "" 'RP11-434B12.1',\n"",
+       "" 'MTMR7',\n"",
+       "" 'RP11-81H14.2',\n"",
+       "" 'MTSS1L',\n"",
+       "" 'RPS15',\n"",
+       "" 'RP11-90K6.1',\n"",
+       "" 'RP4-625H18.2',\n"",
+       "" 'RP11-98G7.1',\n"",
+       "" 'RP11-798M19.6',\n"",
+       "" 'RP11-536G4.2',\n"",
+       "" 'RP11-110G21.1',\n"",
+       "" 'RP11-543C4.1',\n"",
+       "" 'RP13-39P12.3',\n"",
+       "" 'RP4-773N10.4',\n"",
+       "" 'RP11-706O15.1',\n"",
+       "" 'RP11-770J1.4',\n"",
+       "" 'RP11-290F24.6',\n"",
+       "" 'RP11-108M9.6',\n"",
+       "" 'RP11-490O6.2',\n"",
+       "" 'RP1-111C20.4',\n"",
+       "" 'MTHFD1',\n"",
+       "" 'RP11-834C11.4',\n"",
+       "" 'RP11-324I22.4',\n"",
+       "" 'RP5-884G6.2',\n"",
+       "" 'RP11-432J24.5',\n"",
+       "" 'RP11-521O16.1',\n"",
+       "" 'RP11-1094M14.11',\n"",
+       "" 'RP11-22N19.2',\n"",
+       "" 'RP11-977G19.11',\n"",
+       "" 'RP11-708J19.1',\n"",
+       "" 'RP11-60L3.1',\n"",
+       "" 'RP11-469H8.6',\n"",
+       "" 'RPS29',\n"",
+       "" 'RP11-126O1.5',\n"",
+       "" 'RP3-402G11.25',\n"",
+       "" 'RP5-890E16.5',\n"",
+       "" 'RP11-506M13.3',\n"",
+       "" 'RP11-298I3.4',\n"",
+       "" 'RP5-1177M21.1',\n"",
+       "" 'RP11-302B13.5',\n"",
+       "" 'RPL22L1',\n"",
+       "" 'MTR',\n"",
+       "" 'RP11-260M2.1',\n"",
+       "" 'RP11-61J19.5',\n"",
+       "" 'RP11-471B22.3',\n"",
+       "" 'RP11-315D16.4',\n"",
+       "" 'RP11-111M22.3',\n"",
+       "" 'RPLP0',\n"",
+       "" 'RP11-2E11.9',\n"",
+       "" 'RP11-491F9.5',\n"",
+       "" 'RP4-597N16.4',\n"",
+       "" 'RPS26',\n"",
+       "" 'RPL7A',\n"",
+       "" 'MTA1',\n"",
+       "" 'RP11-771K4.1',\n"",
+       "" 'RP11-279O9.4',\n"",
+       "" 'RP11-1085N6.5',\n"",
+       "" 'RP11-798L4.1',\n"",
+       "" 'RP11-571L19.8',\n"",
+       "" 'MTFR1L',\n"",
+       "" 'RP5-1112D6.8',\n"",
+       "" 'RP11-402J6.1',\n"",
+       "" 'RP11-617F23.1',\n"",
+       "" 'RP11-522B15.3',\n"",
+       "" 'RP1-78B3.1',\n"",
+       "" 'RP11-25K19.1',\n"",
+       "" 'RP11-379L18.3',\n"",
+       "" 'RP4-671O14.7',\n"",
+       "" 'RP11-464F9.20',\n"",
+       "" 'MTIF3',\n"",
+       "" 'RP11-548H3.1',\n"",
+       "" 'RP11-1348G14.8',\n"",
+       "" 'RP11-795F19.1',\n"",
+       "" 'RP11-393I2.4',\n"",
+       "" 'RP11-944L7.4',\n"",
+       "" 'RP11-517P14.2',\n"",
+       "" 'RP11-449H3.3',\n"",
+       "" 'RPL17',\n"",
+       "" 'RPP30',\n"",
+       "" 'RP11-1007O24.3',\n"",
+       "" 'RP11-166B2.1',\n"",
+       "" 'MT-ND4L',\n"",
+       "" 'RP1-313I6.12',\n"",
+       "" 'RP11-536C5.7',\n"",
+       "" 'RP1-223E5.4',\n"",
+       "" 'RP11-20I23.13',\n"",
+       "" 'RP11-430B1.2',\n"",
+       "" 'RP6-65G23.3',\n"",
+       "" 'RP11-38L15.3',\n"",
+       "" 'MTL5',\n"",
+       "" 'RP11-227G15.10',\n"",
+       "" 'RP11-403I13.5',\n"",
+       "" 'RP11-141B14.1',\n"",
+       "" 'RP11-568N6.1',\n"",
+       "" 'RPE65',\n"",
+       "" 'RP1-40E16.9',\n"",
+       "" 'RP11-966I7.4',\n"",
+       "" 'RP11-85A1.3',\n"",
+       "" 'RP11-354M20.3',\n"",
+       "" 'RP11-16P6.1',\n"",
+       "" 'RP11-245D16.4',\n"",
+       "" 'RPL18',\n"",
+       "" 'MT1X',\n"",
+       "" 'RP11-526I2.5',\n"",
+       "" 'RPTOR',\n"",
+       "" 'RP11-544A12.8',\n"",
+       "" 'RP11-776H12.1',\n"",
+       "" 'RP11-290D2.6',\n"",
+       "" 'RP11-20B24.4',\n"",
+       "" 'RP11-359E10.1',\n"",
+       "" 'RP11-510M2.2',\n"",
+       "" 'RP11-923I11.8',\n"",
+       "" 'RP11-452F19.3',\n"",
+       "" 'RP5-858B6.3',\n"",
+       "" 'RP11-505K9.1',\n"",
+       "" 'RP11-600F24.7',\n"",
+       "" 'RP4-728D4.2',\n"",
+       "" 'RP11-513M16.7',\n"",
+       "" 'RP11-511P7.5',\n"",
+       "" 'RP11-88H9.2',\n"",
+       "" 'RP11-97C16.1',\n"",
+       "" 'RP5-1112D6.4',\n"",
+       "" 'RP11-327J17.9',\n"",
+       "" 'RP11-819C21.1',\n"",
+       "" 'RP11-54O7.17',\n"",
+       "" 'RPS20',\n"",
+       "" 'RP11-140K17.3',\n"",
+       "" 'RP11-481J2.4',\n"",
+       "" 'RP11-533E19.5',\n"",
+       "" 'RPS5',\n"",
+       "" 'RP11-347P5.1',\n"",
+       "" 'RP11-637O19.2',\n"",
+       "" 'RP11-56D16.8',\n"",
+       "" 'RP11-101E13.5',\n"",
+       "" 'RP11-449P15.2',\n"",
+       "" 'MT1F',\n"",
+       "" 'RP11-572M11.4',\n"",
+       "" 'RP11-356C4.3',\n"",
+       "" 'RP11-436D23.1',\n"",
+       "" 'RP5-1148A21.3',\n"",
+       "" 'RP11-402D21.2',\n"",
+       "" 'RP11-514O12.4',\n"",
+       "" 'RPS27',\n"",
+       "" 'RP11-658F2.8',\n"",
+       "" 'RP11-63M22.2',\n"",
+       "" 'RP11-680F8.3',\n"",
+       "" 'RP11-10L12.4',\n"",
+       "" 'RP11-196G11.6',\n"",
+       "" 'MTPN',\n"",
+       "" 'MTMR2',\n"",
+       "" 'RP11-400F19.6',\n"",
+       "" 'RP13-539J13.1',\n"",
+       "" 'RP11-631N16.2',\n"",
+       "" 'RP4-533D7.5',\n"",
+       "" 'RP11-297N6.4',\n"",
+       "" 'RP11-27M24.1',\n"",
+       "" 'RPL13A',\n"",
+       "" 'RP11-18H7.1',\n"",
+       "" 'MTTP',\n"",
+       "" 'RP11-182L21.6',\n"",
+       "" 'RP11-441O15.3',\n"",
+       "" 'RPS24',\n"",
+       "" 'RP11-166P13.4',\n"",
+       "" 'RP11-297D21.4',\n"",
+       "" 'RP11-359B12.2',\n"",
+       "" 'RP11-1260E13.1',\n"",
+       "" 'RP11-352M15.2',\n"",
+       "" 'RP11-273G15.2',\n"",
+       "" 'RP11-378J18.8',\n"",
+       "" 'RP11-35O15.1',\n"",
+       "" 'RP11-484K9.4',\n"",
+       "" 'RP11-96O20.5',\n"",
+       "" 'RP11-770J1.5',\n"",
+       "" 'RP5-1186N24.3',\n"",
+       "" 'RP11-424N24.2',\n"",
+       "" 'RP1-261D10.2',\n"",
+       "" 'RP11-195F19.9',\n"",
+       "" 'RPS6KL1',\n"",
+       "" 'RP11-44F21.5',\n"",
+       "" 'RP11-710C12.1',\n"",
+       "" 'RP5-894A10.2',\n"",
+       "" 'RP11-98I9.4',\n"",
+       "" 'RP11-46H11.12',\n"",
+       "" 'MTERF2',\n"",
+       "" 'RP11-3P17.4',\n"",
+       "" 'RP11-299H21.1',\n"",
+       "" 'RP11-109N23.4',\n"",
+       "" 'RP11-180C16.1',\n"",
+       "" 'RP1-206D15.6',\n"",
+       "" 'RP11-350J20.5',\n"",
+       "" 'RP11-787I22.3',\n"",
+       "" 'RP11-768F21.1',\n"",
+       "" 'RP4-605O3.4',\n"",
+       "" 'RP11-16E18.3',\n"",
+       "" 'RPL14',\n"",
+       "" 'RP11-152N13.5',\n"",
+       "" 'RP11-149I23.3',\n"",
+       "" 'RP11-966I7.2',\n"",
+       "" 'RP11-290F5.2',\n"",
+       "" 'RP11-385F7.1',\n"",
+       "" 'RP11-74J13.8',\n"",
+       "" 'RP11-225B17.2',\n"",
+       "" 'RPS6KC1',\n"",
+       "" 'RP11-396F22.1',\n"",
+       "" 'RP11-46A10.5',\n"",
+       "" 'RP4-545L17.11',\n"",
+       "" 'RP11-342K6.1',\n"",
+       "" 'RP11-670E13.6',\n"",
+       "" 'MTRNR2L12',\n"",
+       "" 'RP11-33H15.1',\n"",
+       "" 'RP11-348J12.2',\n"",
+       "" 'RPS6KA5',\n"",
+       "" 'RP11-190C22.8',\n"",
+       "" 'RP11-571M6.7',\n"",
+       "" 'RP11-373N22.3',\n"",
+       "" 'RP11-499F3.2',\n"",
+       "" 'RP11-120K24.3',\n"",
+       "" 'RP11-159D12.2',\n"",
+       "" 'RP1-39G22.7',\n"",
+       "" 'RP5-882C2.2',\n"",
+       "" 'RP11-763B22.4',\n"",
+       "" 'RP11-263K19.4',\n"",
+       "" 'RP11-165A20.3',\n"",
+       "" 'RP11-181C3.1',\n"",
+       "" 'RP13-131K19.1',\n"",
+       "" 'RPS16',\n"",
+       "" 'RP1-293L6.1',\n"",
+       "" 'RP1-193H18.2',\n"",
+       "" 'RP11-602N24.3',\n"",
+       "" 'RPS6KA6',\n"",
+       "" 'RPS28',\n"",
+       "" 'RP11-437B10.1',\n"",
+       "" 'RP11-9G1.3',\n"",
+       "" 'RP5-940J5.9',\n"",
+       "" 'RP11-195F19.5',\n"",
+       "" 'RP11-458J1.1',\n"",
+       "" 'RP11-102M11.2',\n"",
+       "" 'RP11-867G23.4',\n"",
+       "" 'RP11-849I19.1',\n"",
+       "" 'RP11-290L1.2',\n"",
+       "" 'RP11-693J15.5',\n"",
+       "" 'RPS6',\n"",
+       "" 'RP11-345J4.5',\n"",
+       "" 'RP11-82L7.4',\n"",
+       "" 'RP11-499E18.1',\n"",
+       "" 'MT-CO2',\n"",
+       "" 'RP11-286N22.8',\n"",
+       "" 'RP11-1263C18.1',\n"",
+       "" 'RP11-118F19.1',\n"",
+       "" 'RP6-65G23.5',\n"",
+       "" 'RP11-452L6.7',\n"",
+       "" 'RP11-539G18.3',\n"",
+       "" 'RP11-996F15.5',\n"",
+       "" 'RP1-317E23.3',\n"",
+       "" 'RP11-350N15.5',\n"",
+       "" 'RP11-30K9.6',\n"",
+       "" 'RP1-198K11.5',\n"",
+       "" 'MT-ND6',\n"",
+       "" 'RP11-102N12.3',\n"",
+       "" 'RP11-305K5.1',\n"",
+       "" 'RP11-596D21.1',\n"",
+       "" 'RP11-214K3.24',\n"",
+       "" 'RP11-217B1.2',\n"",
+       "" 'RPUSD4',\n"",
+       "" 'RP11-554J4.1',\n"",
+       "" 'RP5-908M14.9',\n"",
+       "" 'RP11-356K23.1',\n"",
+       "" 'RP11-603J24.21',\n"",
+       "" 'MT-CYB',\n"",
+       "" 'RP11-722G7.1',\n"",
+       "" 'RP11-399K21.13',\n"",
+       "" 'RP5-875O13.1',\n"",
+       "" 'RP11-403P17.4',\n"",
+       "" 'RP11-665G4.1',\n"",
+       "" 'RP4-549L20.3',\n"",
+       "" 'RP5-1096D14.6',\n"",
+       "" 'RP11-337C18.8',\n"",
+       "" 'RP11-95D17.1',\n"",
+       "" 'RP5-1074L1.1',\n"",
+       "" 'RPL7',\n"",
+       "" 'RP11-379F4.4',\n"",
+       "" 'RP11-395A13.2',\n"",
+       "" 'RP11-456H18.2',\n"",
+       "" 'RPL31',\n"",
+       "" 'RP11-326I11.4',\n"",
+       "" 'RP4-594I10.3',\n"",
+       "" 'RP11-286B14.1',\n"",
+       "" 'MTHFR',\n"",
+       "" 'RP11-314N13.3',\n"",
+       "" 'RP11-503P10.1',\n"",
+       "" 'RP11-467P9.1',\n"",
+       "" 'RP11-127B20.3',\n"",
+       "" 'RP11-872J21.3',\n"",
+       "" 'RP11-147L13.15',\n"",
+       "" 'RP11-343J3.2',\n"",
+       "" 'RP11-535A19.2',\n"",
+       "" 'RP5-1180E21.5',\n"",
+       "" 'RP11-477D19.2',\n"",
+       "" 'RP11-480C16.1',\n"",
+       "" 'RP11-48B3.5',\n"",
+       "" 'RP11-126K1.2',\n"",
+       "" 'RP13-942N8.1',\n"",
+       "" 'RP11-386G11.5',\n"",
+       "" 'RP11-758H9.2',\n"",
+       "" 'RP11-454P21.1',\n"",
+       "" 'RP11-1191J2.5',\n"",
+       "" 'RPIA',\n"",
+       "" 'MTURN',\n"",
+       "" 'RP11-115D19.1',\n"",
+       "" 'RP11-1017G21.5',\n"",
+       "" 'MT1G',\n"",
+       "" 'RP11-215P8.3',\n"",
+       "" 'MTG1',\n"",
+       "" 'RP11-120J1.1',\n"",
+       "" 'RP11-255M2.3',\n"",
+       "" 'RP11-326K13.4',\n"",
+       "" 'RP11-314A20.2',\n"",
+       "" 'RP11-527L4.2',\n"",
+       "" 'RP5-1187M17.10',\n"",
+       "" 'RP11-579D7.4',\n"",
+       "" 'RP11-15A1.7',\n"",
+       "" 'RP11-1275H24.1',\n"",
+       "" 'RP11-849H4.4',\n"",
+       "" 'MTMR1',\n"",
+       "" 'RP11-79P5.9',\n"",
+       "" 'MTAP',\n"",
+       "" 'RPS19BP1',\n"",
+       "" 'RP11-21L23.3',\n"",
+       "" 'RP5-965G21.3',\n"",
+       "" 'RP11-490B18.5',\n"",
+       "" 'RP5-1074L1.4',\n"",
+       "" 'RP1-78O14.1',\n"",
+       "" 'RP11-297K8.2',\n"",
+       "" 'RP11-166O4.6',\n"",
+       "" 'RP11-69E11.4',\n"",
+       "" 'RP11-134O21.1',\n"",
+       "" 'MTRF1',\n"",
+       "" 'RP11-250B2.5',\n"",
+       "" 'RP11-328C8.4',\n"",
+       "" 'RP11-74E22.3',\n"",
+       "" 'RP1-140A9.1',\n"",
+       "" 'RP11-893F2.13',\n"",
+       "" 'RP11-497E19.1',\n"",
+       "" 'RP11-46C24.7',\n"",
+       "" 'RP11-387M24.5',\n"",
+       "" 'RP11-615I2.6',\n"",
+       "" 'RPL36',\n"",
+       "" 'RPAP3',\n"",
+       "" 'RP11-498C9.3',\n"",
+       "" 'RP4-742J24.2',\n"",
+       "" 'RP11-192H23.4',\n"",
+       "" 'RP4-753F5.1',\n"",
+       "" 'RP5-903G2.2',\n"",
+       "" 'RP11-37C7.3',\n"",
+       "" 'RP11-504A18.1',\n"",
+       "" 'RP13-753N3.3',\n"",
+       "" 'RP11-385D13.3',\n"",
+       "" 'RP11-580I16.2',\n"",
+       "" 'RP11-108M9.4',\n"",
+       "" 'RP11-390P2.4',\n"",
+       "" 'RPS6KB1',\n"",
+       "" 'RP11-347I19.7',\n"",
+       "" 'RP11-60L3.6',\n"",
+       "" 'RP11-54O7.3',\n"",
+       "" 'RP11-424G14.1',\n"",
+       "" 'RP11-219G17.4',\n"",
+       "" 'RPS3A',\n"",
+       "" 'MTMR8',\n"",
+       "" 'RP11-78A19.3',\n"",
+       "" 'RP11-111F5.3',\n"",
+       "" 'RP11-532M24.1',\n"",
+       "" 'RP5-1024C24.1',\n"",
+       "" 'RP4-798A10.7',\n"",
+       "" 'RP11-16N11.2',\n"",
+       "" 'RP11-192H23.6',\n"",
+       "" 'RP3-399L15.3',\n"",
+       "" 'RP11-22P6.2',\n"",
+       "" 'RP11-861E21.2',\n"",
+       "" 'RP4-621F18.2',\n"",
+       "" 'RP11-32K4.1',\n"",
+       "" 'RP11-395G23.3',\n"",
+       "" 'RP11-383M4.6',\n"",
+       "" 'RPUSD2',\n"",
+       "" 'RP11-429J17.2',\n"",
+       "" 'RP11-7F17.8',\n"",
+       "" 'RP11-722E23.2',\n"",
+       "" 'RP5-827C21.4',\n"",
+       "" 'RP5-919F19.5',\n"",
+       "" 'RP1-122P22.4',\n"",
+       "" 'RP11-386G11.10',\n"",
+       "" 'RP11-501C14.5',\n"",
+       "" 'RP11-420N3.2',\n"",
+       "" 'RP11-672L10.6',\n"",
+       "" 'RP11-47J17.2',\n"",
+       "" 'RP11-442H21.2',\n"",
+       "" 'RPL27A',\n"",
+       "" 'MT-CO1',\n"",
+       "" 'RP11-804H8.6',\n"",
+       "" 'RP11-158M2.3',\n"",
+       "" 'RP11-727F15.12',\n"",
+       "" 'RP11-936I5.1',\n"",
+       "" 'RP11-806H10.4',\n"",
+       "" 'RP11-996F15.6',\n"",
+       "" 'RP1-234P15.4',\n"",
+       "" 'RPL17-C18orf32',\n"",
+       "" 'RP11-14N7.2',\n"",
+       "" 'MTX3',\n"",
+       "" 'RP3-403A15.5',\n"",
+       "" 'RP1-34H18.1',\n"",
+       "" 'RP11-894P9.1',\n"",
+       "" 'RP11-1008C21.2',\n"",
+       "" 'RP4-669L17.10',\n"",
+       "" 'RP11-266L9.5',\n"",
+       "" 'RP11-17A19.2',\n"",
+       "" 'RP13-131K19.6',\n"",
+       "" 'RP11-196G18.23',\n"",
+       "" 'RP11-236L14.2',\n"",
+       "" 'RP1-30M3.5',\n"",
+       "" 'RP11-637A17.2',\n"",
+       "" 'RP13-436F16.1',\n"",
+       "" 'RP11-323F24.3',\n"",
+       "" 'RP11-496I9.1',\n"",
+       "" 'RP11-346C20.3',\n"",
+       "" 'RP11-115C21.2',\n"",
+       "" 'RP11-109G23.3',\n"",
+       "" 'RP11-736K20.5',\n"",
+       "" 'RP11-356I2.4',\n"",
+       "" 'RP11-96K19.5',\n"",
+       "" 'RPRD2',\n"",
+       "" 'RPRM',\n"",
+       "" 'RP11-706J10.2',\n"",
+       "" 'RPL4',\n"",
+       "" 'RP11-161H23.5',\n"",
+       "" 'RP1-267D11.6',\n"",
+       "" 'RP11-546K22.3',\n"",
+       "" 'MT3',\n"",
+       "" 'RP11-214N9.1',\n"",
+       "" 'RP3-508I15.9',\n"",
+       "" 'RP11-72M17.1',\n"",
+       "" 'RP4-756G23.5',\n"",
+       "" 'RP11-274B21.9',\n"",
+       "" 'RP11-172H24.4',\n"",
+       "" 'RP11-31F15.2',\n"",
+       "" 'RP1-191J18.66',\n"",
+       "" 'RP11-380L11.4',\n"",
+       "" 'RP11-73K9.2',\n"",
+       "" 'RP11-72I8.1',\n"",
+       "" 'RP11-295H24.4',\n"",
+       "" 'RP11-890B15.3',\n"",
+       "" 'RP11-746M1.1',\n"",
+       "" 'RP11-260M19.1',\n"",
+       "" 'RP13-766D20.4',\n"",
+       "" 'RP11-932O9.7',\n"",
+       "" 'RP11-603J24.17',\n"",
+       "" 'RP11-254F7.4',\n"",
+       "" 'RPL41',\n"",
+       "" 'MT-ND4',\n"",
+       "" 'RP4-555D20.4',\n"",
+       "" 'RP11-820I16.4',\n"",
+       "" 'RP11-344P13.6',\n"",
+       "" 'RP4-639F20.1',\n"",
+       "" 'RP11-111M22.2',\n"",
+       "" 'RP9',\n"",
+       "" 'RP11-713P17.3',\n"",
+       "" 'RP11-1000B6.3',\n"",
+       "" 'RP11-559M23.1',\n"",
+       "" 'RP11-1191J2.2',\n"",
+       "" 'RP5-1120P11.1',\n"",
+       "" 'RP4-616B8.4',\n"",
+       "" 'RP4-612B15.3',\n"",
+       "" 'RP11-435O5.2',\n"",
+       "" 'RP11-53O19.1',\n"",
+       "" 'RP11-77H9.2',\n"",
+       "" 'RP11-379H18.1',\n"",
+       "" 'RP11-923I11.6',\n"",
+       "" 'RP11-783K16.5',\n"",
+       "" 'RPLP2',\n"",
+       "" 'RP11-455J20.3',\n"",
+       "" 'RPS6KA3',\n"",
+       "" 'RP5-832C2.5',\n"",
+       "" 'RP5-1159O4.1',\n"",
+       "" 'RP11-160E2.6',\n"",
+       "" 'RP11-649E7.5',\n"",
+       "" 'RP11-126K1.6',\n"",
+       "" 'RP11-256I23.2',\n"",
+       "" 'RP3-402G11.26',\n"",
+       "" 'RP11-525K10.3',\n"",
+       "" 'RP13-753N3.1',\n"",
+       "" 'RP11-159D12.5',\n"",
+       "" 'RP3-525N10.2',\n"",
+       "" 'RP11-53I6.3',\n"",
+       "" 'RP11-62H20.1',\n"",
+       "" 'RP11-589C21.6',\n"",
+       "" 'RP11-444D3.1',\n"",
+       "" 'RP11-216B9.6',\n"",
+       "" 'RPL37',\n"",
+       "" 'RP11-524C21.2',\n"",
+       "" 'RP11-247C2.2',\n"",
+       "" 'RP11-252A24.3',\n"",
+       "" 'RP4-673M15.1',\n"",
+       "" 'RP11-247A12.7',\n"",
+       "" 'RP11-611O2.5',\n"",
+       "" 'RP11-383I23.2',\n"",
+       "" 'RP11-1277A3.3',\n"",
+       "" 'RP13-977J11.2',\n"",
+       "" 'RP11-479O9.4',\n"",
+       "" 'RP11-3D4.3',\n"",
+       "" 'RP11-345P4.10',\n"",
+       "" 'RP11-656D10.3',\n"",
+       "" 'RP2',\n"",
+       "" 'RP11-434H6.7',\n"",
+       "" 'RP11-66N11.8',\n"",
+       "" 'RP11-209D14.2',\n"",
+       "" 'MTRNR2L1',\n"",
+       "" 'RP11-210M15.2',\n"",
+       "" 'RP11-230B22.1',\n"",
+       "" 'RPL3',\n"",
+       "" 'RP11-522I20.3',\n"",
+       "" 'RP11-407N17.5',\n"",
+       "" 'MTBP',\n"",
+       "" 'RP11-539L10.3',\n"",
+       "" 'RP11-74E22.8',\n"",
+       "" 'RP11-66N24.3',\n"",
+       "" 'RP11-345P4.9',\n"",
+       "" 'RP11-589P10.5',\n"",
+       "" 'RP11-388C12.1',\n"",
+       "" 'RP11-48B3.4',\n"",
+       "" 'RP11-158H5.8',\n"",
+       "" 'RP11-699A5.2',\n"",
+       "" 'RP11-390F4.6',\n"",
+       "" 'RP11-734K2.4',\n"",
+       "" 'RPL30',\n"",
+       "" 'RP11-967K21.1',\n"",
+       "" 'RP11-326A19.3',\n"",
+       "" 'RP11-96L14.8',\n"",
+       "" 'MTCL1',\n"",
+       "" 'RP11-573D15.9',\n"",
+       "" 'RP11-544I20.2',\n"",
+       "" 'RP11-629G13.1',\n"",
+       "" 'RP1-257A7.4',\n"",
+       "" 'RP11-521O16.2',\n"",
+       "" 'RP11-728G15.1',\n"",
+       "" 'RP11-307C12.12',\n"",
+       "" 'RP11-715J22.3',\n"",
+       "" 'RP11-799D4.4',\n"",
+       "" 'RP1-179N16.6',\n"",
+       "" 'RP11-254F7.2',\n"",
+       "" 'RP11-365P13.5',\n"",
+       "" 'MTRF1L',\n"",
+       "" 'RPAP2',\n"",
+       "" 'RP1-266L20.2',\n"",
+       "" 'RP11-147L13.12',\n"",
+       "" 'MT-CO3',\n"",
+       "" 'RP11-112J3.16',\n"",
+       "" 'RPL5',\n"",
+       "" 'RP11-45A17.2',\n"",
+       "" 'RP13-554M15.8',\n"",
+       "" 'RP11-406H21.2',\n"",
+       "" 'RP11-90D4.3',\n"",
+       "" 'RP11-384P7.7',\n"",
+       "" 'RP11-430H10.1',\n"",
+       "" 'RP11-315I20.1',\n"",
+       "" 'RPL28',\n"",
+       "" 'RP11-354P11.3',\n"",
+       "" 'RP5-1102E8.3',\n"",
+       "" 'RP11-567M16.6',\n"",
+       "" 'RP11-55K13.1',\n"",
+       "" 'RP11-96H19.1',\n"",
+       "" 'RP11-382A20.3',\n"",
+       "" 'RP11-390E23.6',\n"",
+       "" 'RP11-849H4.2',\n"",
+       "" 'RP11-121C2.2',\n"",
+       "" 'RP11-796E2.4',\n"",
+       "" 'RP11-981G7.6',\n"",
+       "" 'RP11-154J22.1',\n"",
+       "" 'RP11-277B15.3',\n"",
+       "" 'RP4-548D19.3',\n"",
+       "" 'RP4-633O19__A.1',\n"",
+       "" 'RP11-314B1.2',\n"",
+       "" 'RP11-159K7.2',\n"",
+       "" 'RP11-38M8.1',\n"",
+       "" 'RP11-334G22.1',\n"",
+       "" 'RP11-142M10.2',\n"",
+       "" 'RPS2',\n"",
+       "" 'RP11-333E1.1',\n"",
+       "" 'RP4-622L5.7',\n"",
+       "" 'RP11-339B21.13',\n"",
+       "" 'RPP25L',\n"",
+       "" 'RP11-867G23.8',\n"",
+       "" 'RP1-315G1.3',\n"",
+       "" 'RP11-456O19.2',\n"",
+       "" 'RP3-486I3.7',\n"",
+       "" 'RP11-57G10.8',\n"",
+       "" 'RP11-375N15.2',\n"",
+       "" 'RP11-545E17.3',\n"",
+       "" 'MTSS1',\n"",
+       "" 'RP4-583P15.10',\n"",
+       "" 'RP11-17E3.1',\n"",
+       "" 'RP11-380N8.7',\n"",
+       "" 'RP11-63A1.1',\n"",
+       "" 'RP11-123K19.1',\n"",
+       "" 'RP11-1148L6.9',\n"",
+       "" 'RP11-677I18.3',\n"",
+       "" 'RPUSD3',\n"",
+       "" 'RP5-1024G6.5',\n"",
+       "" 'RP11-473I1.5',\n"",
+       "" 'RP11-178L8.7',\n"",
+       "" 'RP5-1198O20.4',\n"",
+       "" 'RP4-738P15.1',\n"",
+       "" 'RP11-1100L3.7',\n"",
+       "" 'RP11-342M1.3',\n"",
+       "" 'RP11-732A21.3',\n"",
+       "" 'RP11-436K8.1',\n"",
+       "" 'RP11-178C3.2',\n"",
+       "" 'RP11-258C19.7',\n"",
+       "" 'RP11-482M8.3',\n"",
+       "" 'RP11-1379J22.5',\n"",
+       "" 'RP3-323A16.1',\n"",
+       "" 'RP5-1085F17.3',\n"",
+       "" 'RP11-881M11.4',\n"",
+       "" 'RP11-498C9.15',\n"",
+       "" 'RP11-131L23.1',\n"",
+       "" 'MTUS1',\n"",
+       "" 'RP11-412D9.4',\n"",
+       "" 'RPL18A',\n"",
+       "" 'RP11-435O5.5',\n"",
+       "" 'RP1-187B23.1',\n"",
+       "" 'RP11-306I1.2',\n"",
+       "" 'RP11-1391J7.1',\n"",
+       "" 'RP11-38C17.1',\n"",
+       "" 'RP11-421M1.8',\n"",
+       "" 'RP1-100J12.1',\n"",
+       "" 'RP11-104N10.1',\n"",
+       "" 'RP11-475I24.3',\n"",
+       "" 'RP11-20E24.1',\n"",
+       "" 'RP11-47A8.5',\n"",
+       "" 'RP11-156E8.1',\n"",
+       "" 'RP11-17M16.2',\n"",
+       "" 'RP11-211N8.2',\n"",
+       "" 'RP11-1000B6.8',\n"",
+       "" 'RP11-197K6.1',\n"",
+       "" 'RP11-318E3.9',\n"",
+       "" 'RP11-621L6.3',\n"",
+       "" 'RP11-159D12.10',\n"",
+       "" 'RP11-416N2.4',\n"",
+       "" 'RP11-89K11.1',\n"",
+       "" 'RP11-389G6.3',\n"",
+       "" 'RP11-262H14.4',\n"",
+       "" 'MTDH',\n"",
+       "" 'RPAP1',\n"",
+       "" 'RPUSD1',\n"",
+       "" 'RP11-408A13.4',\n"",
+       "" 'RP11-303E16.5',\n"",
+       "" 'RP11-458D21.1',\n"",
+       "" 'RP11-414C23.1',\n"",
+       "" 'RP11-588H23.3',\n"",
+       "" 'RP11-983P16.4',\n"",
+       "" 'RP11-121M22.1',\n"",
+       "" 'RP11-77P6.2',\n"",
+       "" 'RP11-700J17.2',\n"",
+       "" 'RP11-11M20.4',\n"",
+       "" 'RP11-334J6.7',\n"",
+       "" 'RP11-692D12.1',\n"",
+       "" 'RP11-422P24.12',\n"",
+       "" 'RP11-351I21.11',\n"",
+       "" 'RP11-128A17.1',\n"",
+       "" 'RP11-848P1.5',\n"",
+       "" 'RP11-80A15.1',\n"",
+       "" 'RP11-161M6.2',\n"",
+       "" 'MTFR1',\n"",
+       "" 'RP5-1112D6.7',\n"",
+       "" 'RP11-65I12.1',\n"",
+       "" 'RPS4Y1',\n"",
+       "" 'RP11-372K14.2',\n"",
+       "" 'RPL15',\n"",
+       "" 'RP11-319G6.1',\n"",
+       "" 'RPLP1',\n"",
+       "" 'RP11-480A16.1',\n"",
+       "" 'MTRNR2L10',\n"",
+       "" 'RP11-1246C19.1',\n"",
+       "" 'RP11-727F15.9',\n"",
+       "" 'RP11-442N1.1',\n"",
+       "" 'RP11-646I6.5',\n"",
+       "" 'RP11-563N4.1',\n"",
+       "" 'RP11-398C13.6',\n"",
+       "" 'RPL12',\n"",
+       "" 'RPL27',\n"",
+       "" 'RPS7',\n"",
+       "" 'RP11-498C9.2',\n"",
+       "" 'MT-ND2',\n"",
+       "" 'RP11-92G12.3',\n"",
+       "" 'RP11-422P24.11',\n"",
+       "" 'RPL36A',\n"",
+       "" 'RP1-202O8.3',\n"",
+       "" 'RP11-81A22.5',\n"",
+       "" 'RP11-458F8.4',\n"",
+       "" 'RP11-33O4.1',\n"",
+       "" 'RPS25',\n"",
+       "" 'RP11-483I13.5',\n"",
+       "" 'RP13-131K19.2',\n"",
+       "" 'RP11-110I1.13',\n"",
+       "" 'RP11-214O1.3',\n"",
+       "" 'RP11-384O8.1',\n"",
+       "" 'RP11-365O16.6',\n"",
+       "" 'RP5-1068E13.7',\n"",
+       "" 'RP11-728F11.4',\n"",
+       "" 'RP11-303E16.7',\n"",
+       "" 'RP1-138B7.7',\n"",
+       "" 'RPL13',\n"",
+       "" 'RP11-499A10.3',\n"",
+       "" 'RP11-96L14.7',\n"",
+       "" 'RPL10A',\n"",
+       "" 'RP11-706O15.5',\n"",
+       "" 'RP11-850A17.1',\n"",
+       "" 'RP11-56G10.2',\n"",
+       "" 'RP11-342K6.4',\n"",
+       "" 'RP11-533E19.7',\n"",
+       "" 'RPL8',\n"",
+       "" 'RP11-573D15.2',\n"",
+       "" 'RPE',\n"",
+       "" 'RPL35A',\n"",
+       "" 'RPS10',\n"",
+       "" 'RP11-418H16.1',\n"",
+       "" 'RP11-496H1.2',\n"",
+       "" 'RP11-701H24.9',\n"",
+       "" 'RP11-493L12.4',\n"",
+       "" 'RP3-512B11.3',\n"",
+       "" 'MTHFSD',\n"",
+       "" 'RP11-932O9.8',\n"",
+       "" 'RP11-15H20.7',\n"",
+       "" 'RP11-488L18.8',\n"",
+       "" 'RP11-320M2.1',\n"",
+       "" 'RPP14',\n"",
+       "" 'RP11-486G15.2',\n"",
+       "" 'RP11-433J8.1',\n"",
+       "" 'RP11-111M22.4',\n"",
+       "" 'RP11-402L1.4',\n"",
+       "" 'RP11-73E17.2',\n"",
+       "" 'RP11-304L19.13',\n"",
+       "" 'RP5-1092A3.4',\n"",
+       "" 'RP3-395M20.12',\n"",
+       "" 'RP11-680A11.5',\n"",
+       "" 'RP11-71N10.1',\n"",
+       "" 'RPPH1',\n"",
+       "" 'RP11-363J20.1',\n"",
+       "" 'RP4-564F22.5',\n"",
+       "" 'MTMR14',\n"",
+       "" 'RP11-267M23.4',\n"",
+       "" 'RPL35',\n"",
+       "" 'RP11-849F2.9',\n"",
+       "" 'RP11-793H13.11',\n"",
+       "" 'RP5-1042K10.10',\n"",
+       "" 'RP11-438L19.1',\n"",
+       "" 'MTCH1',\n"",
+       "" 'RP11-206L10.9',\n"",
+       "" 'RP11-295M3.4',\n"",
+       "" 'RP1-239B22.5',\n"",
+       "" 'RP11-54O7.2',\n"",
+       "" 'RPS6KA1',\n"",
+       "" 'RP3-465N24.6',\n"",
+       "" 'RP11-12G12.7',\n"",
+       "" 'RPS4X',\n"",
+       "" 'MT-ND1',\n"",
+       "" 'RP4-742C19.13',\n"",
+       "" 'RP11-334C17.5',\n"",
+       "" 'RP11-429B14.4',\n"",
+       "" 'RPL29',\n"",
+       "" 'RP4-575N6.4',\n"",
+       "" 'RP5-862P8.2',\n"",
+       "" 'RP11-138I1.4',\n"",
+       "" 'RPS19',\n"",
+       "" 'RP11-128M1.1',\n"",
+       "" 'RP11-347C12.10',\n"",
+       "" 'RP11-523L20.1',\n"",
+       "" 'RP11-46H11.3',\n"",
+       "" 'RP11-531A24.3',\n"",
+       "" 'RP4-591C20.9',\n"",
+       "" 'RP11-465L10.10',\n"",
+       "" 'MTMR12',\n"",
+       "" 'RP11-179B2.2',\n"",
+       "" 'RP11-266K4.14',\n"",
+       "" 'RP3-380B8.4',\n"",
+       "" 'RP11-11N7.4',\n"",
+       "" 'MTFR2',\n"",
+       "" 'RP13-1032I1.7',\n"",
+       "" 'RP11-462G2.2',\n"",
+       "" 'RP11-795F19.5',\n"",
+       "" 'RP11-817O13.9',\n"",
+       "" 'RP11-576D8.4',\n"",
+       "" 'RPN2',\n"",
+       "" 'RP5-994D16.3',\n"",
+       "" 'RP11-712L6.5',\n"",
+       "" 'RP11-425D10.10',\n"",
+       "" 'RP11-1275H24.3',\n"",
+       "" 'RP11-108L7.15',\n"",
+       "" 'RP11-499P20.2',\n"",
+       "" 'RP11-196G18.24',\n"",
+       "" 'RP11-98D18.9',\n"",
+       "" 'RP11-212P7.2',\n"",
+       "" 'RP11-705O1.8',\n"",
+       "" 'RP11-332H14.2',\n"",
+       "" 'RP11-680F8.1',\n"",
+       "" 'RP11-181G12.2',\n"",
+       "" 'RP11-314C16.1',\n"",
+       "" 'RP11-315O6.1',\n"",
+       "" 'RP11-1C8.7',\n"",
+       "" 'MTCH2',\n"",
+       "" 'RP4-665J23.1',\n"",
+       "" 'RP11-247A12.2',\n"",
+       "" 'RP11-156K23.3',\n"",
+       "" 'RP11-410L14.2',\n"",
+       "" 'RPP40',\n"",
+       "" 'RP13-582O9.7',\n"",
+       "" 'RP13-638C3.4',\n"",
+       "" 'RP11-95O2.5',\n"",
+       "" 'RP11-125O18.1',\n"",
+       "" 'RP11-89K21.1',\n"",
+       "" 'RP11-486I11.2',\n"",
+       "" 'RPA3',\n"",
+       "" 'RP11-124N14.3',\n"",
+       "" 'MT1E',\n"",
+       "" 'RP11-498P14.5',\n"",
+       "" 'RP11-57H14.2',\n"",
+       "" 'RP11-351D16.3',\n"",
+       "" 'RP11-61K9.3',\n"",
+       "" 'RP11-774O3.3',\n"",
+       "" 'RP11-803B1.8',\n"",
+       "" 'RP11-158I9.8',\n"",
+       "" 'RPS27A',\n"",
+       "" 'RPS6KB2',\n"",
+       "" 'RP11-834C11.5',\n"",
+       "" 'RP11-159G9.5',\n"",
+       "" 'RP11-227D13.1',\n"",
+       "" 'RP11-502I4.3',\n"",
+       "" 'RPP21',\n"",
+       "" 'RP11-348P10.2',\n"",
+       "" 'RP11-127B20.2',\n"",
+       "" 'RP13-131K19.7',\n"",
+       "" 'RP11-686O6.2',\n"",
+       "" 'RPA2',\n"",
+       "" 'RP1-90J20.8',\n"",
+       "" 'RP11-26J3.1',\n"",
+       "" 'RP11-1275H24.2',\n"",
+       "" 'RP11-115H18.1',\n"",
+       "" 'RP5-1098D14.1',\n"",
+       "" 'RPL22',\n"",
+       "" 'RP5-1050D4.3',\n"",
+       "" 'RPRD1A',\n"",
+       "" 'RP11-712B9.2',\n"",
+       "" 'RP11-93G5.1',\n"",
+       "" 'RP11-49I11.1',\n"",
+       "" 'RP11-386I14.4',\n"",
+       "" 'RP4-800J21.3',\n"",
+       "" 'RP11-168K9.2',\n"",
+       "" 'MTMR11',\n"",
+       "" 'RP5-967N21.11',\n"",
+       "" 'RP11-307C12.13',\n"",
+       "" 'RPL21',\n"",
+       "" 'RP6-201G10.2',\n"",
+       "" 'MT-ND5',\n"",
+       "" 'RP11-266J6.2',\n"",
+       "" 'RP11-108E14.1',\n"",
+       "" 'RP11-23F23.2',\n"",
+       "" 'MTERF3',\n"",
+       "" 'RP11-348J24.2',\n"",
+       "" 'RP5-1024G6.7',\n"",
+       "" 'RP11-108M12.3',\n"",
+       "" 'RP11-87C12.5',\n"",
+       "" 'RP11-452L6.1',\n"",
+       "" 'RP11-1143G9.4',\n"",
+       "" 'RP11-490M8.1',\n"",
+       "" 'RPAIN',\n"",
+       "" 'RP11-262A16.1',\n"",
+       "" 'RP11-244M2.1',\n"",
+       "" 'RP3-394A18.1',\n"",
+       "" 'RP11-651L5.3',\n"",
+       "" 'RP11-54O7.1',\n"",
+       "" 'RP11-575L7.8',\n"",
+       "" 'RP11-295G20.2',\n"",
+       "" 'RP11-294J22.6',\n"",
+       "" 'RP11-53O19.3',\n"",
+       "" 'RP11-1136G4.2',\n"",
+       "" 'RPS6KA2',\n"",
+       "" 'RP11-392O17.2',\n"",
+       "" 'RPL11',\n"",
+       "" 'RP1-34B20.21',\n"",
+       "" 'RP11-357K6.1',\n"",
+       "" 'MT-ATP6',\n"",
+       "" 'RP11-176H8.1',\n"",
+       "" 'RP11-46J23.1',\n"",
+       "" 'RP11-111F5.4',\n"",
+       "" 'RP11-696N14.1',\n"",
+       "" 'RP11-481J2.3',\n"",
+       "" 'RP11-368I7.4',\n"",
+       "" 'RPL7L1',\n"",
+       "" 'RP4-736L20.3',\n"",
+       "" 'RP11-390P24.1',\n"",
+       "" 'RP11-140I16.3',\n"",
+       "" 'RP11-83A24.2',\n"",
+       "" 'RPRML',\n"",
+       "" 'RP11-15J22.8',\n"",
+       "" 'RP11-449J21.5',\n"",
+       "" 'RP11-296O14.3',\n"",
+       "" 'RPL23A',\n"",
+       "" 'RP4-569M23.2',\n"",
+       "" 'RP5-943J3.2',\n"",
+       "" 'RP1-167A14.2',\n"",
+       "" 'RP11-93H24.3',\n"",
+       "" 'RPL23',\n"",
+       "" 'RP11-313P13.5',\n"",
+       "" 'MTA3',\n"",
+       "" 'RP11-725P16.2',\n"",
+       "" 'RPS11',\n"",
+       "" 'RP11-467D6.1',\n"",
+       "" 'RP11-327P2.5',\n"",
+       "" 'RP1-43E13.2',\n"",
+       "" 'RP11-110I1.12',\n"",
+       "" 'MTFP1',\n"",
+       "" 'RP11-306G20.1',\n"",
+       "" 'RP11-728K20.3',\n"",
+       "" 'RP11-244H3.4',\n"",
+       "" 'RP11-798G7.6',\n"",
+       "" 'RP11-20I23.6',\n"",
+       "" 'RP11-136L23.2',\n"",
+       "" 'RP11-582E3.6',\n"",
+       "" 'RP1-8B1.4',\n"",
+       "" 'RP11-2B6.2',\n"",
+       "" 'MTMR10',\n"",
+       "" 'RP11-268J15.5',\n"",
+       "" 'RP11-446N19.1',\n"",
+       "" 'RP11-445F12.1',\n"",
+       "" 'RP13-516M14.1',\n"",
+       "" 'RPL6',\n"",
+       "" 'RP11-147L13.13',\n"",
+       "" 'RP11-132A1.4',\n"",
+       "" 'RP3-406A7.7',\n"",
+       "" 'RP3-461F17.3',\n"",
+       "" 'RP11-318A15.2',\n"",
+       "" 'RP11-262H14.3',\n"",
+       "" 'RP11-119B16.2',\n"",
+       "" 'RP11-51J9.5',\n"",
+       "" 'RP11-814H16.2',\n"",
+       "" 'RP11-307N16.6',\n"",
+       "" 'RP11-57A19.2',\n"",
+       "" 'RP11-382D12.2',\n"",
+       "" 'RP11-793H13.3',\n"",
+       "" 'RP11-677M14.7',\n"",
+       "" 'MTO1',\n"",
+       "" 'RPL19',\n"",
+       "" 'RP11-274B21.10',\n"",
+       "" 'RP11-500C11.3',\n"",
+       "" 'RP11-586K2.1',\n"",
+       "" 'RP11-15A1.2',\n"",
+       "" 'RP5-933K21.3',\n"",
+       "" 'RP11-367N14.3',\n"",
+       "" 'RP11-277A4.4',\n"",
+       "" 'RP11-290L1.3',\n"",
+       "" 'RP13-516M14.4',\n"",
+       "" 'RPS23',\n"",
+       "" 'RP11-474G23.2',\n"",
+       "" 'RP5-821D11.7',\n"",
+       "" 'RP11-483F11.7',\n"",
+       "" 'RP11-326I11.3',\n"",
+       "" 'RP11-317P15.4',\n"",
+       "" 'RPL36AL',\n"",
+       "" 'RP11-138P22.1',\n"",
+       "" 'RP1-28H20.3',\n"",
+       "" 'RP11-449J10.1',\n"",
+       "" 'RP11-413H22.2',\n"",
+       "" 'RP5-1065J22.8',\n"",
+       "" 'RP11-634H22.1',\n"",
+       "" 'RP11-264B17.4',\n"",
+       "" 'RP11-521B24.3',\n"",
+       "" 'RP11-77K12.9',\n"",
+       "" 'RP11-1114A5.4',\n"",
+       "" 'RP4-798A10.2',\n"",
+       "" 'MTF1',\n"",
+       "" 'RP11-495P10.1',\n"",
+       "" 'RP11-644F5.11',\n"",
+       "" 'RP11-397G17.1',\n"",
+       "" 'RP6-24A23.6',\n"",
+       "" 'RP11-89F3.2',\n"",
+       "" 'RPP25',\n"",
+       "" 'RP11-298J20.3',\n"",
+       "" 'RP11-395B7.4',\n"",
+       "" 'RP11-329B9.4',\n"",
+       "" 'RP11-362K14.6',\n"",
+       "" 'RP11-415D17.3',\n"",
+       "" 'RP13-1032I1.11',\n"",
+       "" 'RP11-37B2.1',\n"",
+       "" 'RP11-87C12.2',\n"",
+       "" 'RP11-448A19.1',\n"",
+       "" 'RP11-428J1.5',\n"",
+       "" ...]""
+      ]
+     },
+     ""execution_count"": 4,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""cluster_dict = {}\n"",
+    ""to_remove = []\n"",
+    ""\n"",
+    ""for c in cols:\n"",
+    ""    if c.startswith('RP') or c.startswith('MT'):\n"",
+    ""        to_remove.append(c)\n"",
+    ""        \n"",
+    ""to_remove""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""6d1fa3d7"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Index(['MALAT1', 'TMSB4X', 'TUBA1A', 'STMN1', 'TMSB10', 'PTMA', 'EEF1A1',\n"",
+      ""       'FTL', 'ACTB', 'TUBB2B'],\n"",
+      ""      dtype='object')\n""
+     ]
+    },
+    {
+     ""ename"": ""KeyboardInterrupt"",
+     ""evalue"": """",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_83050/2074903135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclust\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'# label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclust\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/processed/primary.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcluster_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclust\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\""nrows\""\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m     \""\""\""\n\u001b[1;32m   1422\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mKeyboardInterrupt\u001b[0m: ""
+     ]
+    }
+   ],
+   ""source"": [
+    ""for clust in labels.value_counts().index:\n"",
+    ""    rows = labels[labels['# label'] == clust].index\n"",
+    ""    df = pd.read_csv('../data/processed/primary.csv', skiprows = lambda x: x not in rows, names=cols).drop(to_remove, axis=1)\n"",
+    ""\n"",
+    ""    cluster_dict[clust] = df.sum().nlargest(1000).index \n"",
+    ""    print(cluster_dict[clust][0: 10])""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""13a7b227"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""for c in cluster_dict:\n"",
+    ""    print(cluster_dict[c][0:100])""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""8b53f525"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""cols = [c.lower() for c in df.columns]\n"",
+    ""# print(cols)\n"",
+    ""to_remove = []\n"",
+    ""\n"",
+    ""for c in cols:\n"",
+    ""    if c.startswith('mt') or c.startswith('rp'):\n"",
+    ""        to_remove.append(c)\n"",
+    ""    \n"",
+    ""to_remove""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""285dd4ae"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""top_genes = dict()\n"",
+    ""\n"",
+    ""for clust in labels['# label'].unique():\n"",
+    ""    print(f'Computing for cluster {clust}')\n"",
+    ""    to_keep = labels[labels['# label'] == clust].index\n"",
+    ""    clust_df = pd.read_csv('../data/processed/primary.csv', skiprows = lambda x: x not in to_keep, header=0)\n"",
+    ""    display(clust_df)\n"",
+    ""    top_genes[clust] = clust_df.sum().nlargest(n=1000).index\n"",
+    ""    print(top_genes[clust])""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""daf8236e"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now, let's visualize our clustering results ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""026e92a0"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""viz = pd.read_csv('../data/processed/umap/primary_reduction_neighbors_100_components_2.csv')\n""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""a80a79b9"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""bfe5f157"",
+   ""metadata"": {},
+   ""source"": [
+    ""# Data Validation and Sanity Checks\n"",
+    ""\n"",
+    ""In this notebook, we'll compare the outputs of our GeneExpressionData class with the original expression matrices and labels""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""be54c4a5"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""import os\n"",
+    ""import sys\n"",
+    ""import pandas as pd\n"",
+    ""import numpy as np\n"",
+    ""from torch.utils.data import *\n"",
+    ""from tqdm import tqdm\n"",
+    ""import linecache \n"",
+    ""from pytorch_tabnet.tab_model import TabNetClassifier\n"",
+    ""sys.path.append('../src/')\n"",
+    ""sys.path.append('..')\n"",
+    ""\n"",
+    ""from src.models.lib.neural import GeneClassifier""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""16540141"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from src.models.lib.data import *\n"",
+    ""from src.helper import *""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""3b051193"",
+   ""metadata"": {},
+   ""source"": [
+    ""## Intersection between reference columns with current dataset columns \n"",
+    ""\n"",
+    ""Here, we'll write test code for the methods used in mapping an arbitrary dataset sample to a list of given reference columns, since this method being correct is extremely important ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""25c97585"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""def clean_sample(sample, refgenes, currgenes):\n"",
+    ""    intersection = np.intersect1d(currgenes, refgenes, return_indices=True)\n"",
+    ""    indices = intersection[1] # List of indices in currgenes that equal refgenes \n"",
+    ""    \n"",
+    ""    axis = (1 if sample.ndim == 2 else 0)\n"",
+    ""    sample = np.sort(sample, axis=axis)\n"",
+    ""    sample = np.take(sample, indices, axis=axis)\n"",
+    ""\n"",
+    ""    return torch.from_numpy(sample)""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""ff6999a2"",
+   ""metadata"": {},
+   ""source"": [
+    ""### Unit Test Example 01""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""b7e7870f"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""def test1():\n"",
+    ""    ref = ['a', 'b', 'c']\n"",
+    ""    curr = ['b', 'a', 'c', 'd'] \n"",
+    ""    sample = np.array([1,2,3,4]) # Want --> [2,1,3]\n"",
+    ""\n"",
+    ""    result = clean_sample(sample, ref, curr)\n"",
+    ""    desired = torch.from_numpy(np.array([2,1,3]))\n"",
+    ""    \n"",
+    ""    assert torch.equal(result, desired)\n"",
+    ""    \n"",
+    ""def test2():\n"",
+    ""    ref = ['a', 'b', 'c']\n"",
+    ""    curr = ['c', 'd', 'b', 'a']\n"",
+    ""\n"",
+    ""    sample = np.array(\n"",
+    ""        [[1,2,3,4],\n"",
+    ""         [5,6,7,8]]\n"",
+    ""    ) \n"",
+    ""    # --> want [[4, 3, 1],\n"",
+    ""    #           [8, 7, 5]]\n"",
+    ""\n"",
+    ""    res = clean_sample(sample, ref, curr)\n"",
+    ""    desired = torch.from_numpy(np.array([\n"",
+    ""        [4,3,1],\n"",
+    ""        [8,7,5]\n"",
+    ""    ]))\n"",
+    ""    \n"",
+    ""    assert torch.equal(res, desired)\n"",
+    ""    \n"",
+    ""test1()\n"",
+    ""test2()""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""3eae7338"",
+   ""metadata"": {},
+   ""source"": [
+    ""From initial tests, `clean_sample` seems to be working correctly.""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""c5e8212a"",
+   ""metadata"": {},
+   ""source"": [
+    ""## Validation of GeneExpressionData class with the original expression matrices and label files \n"",
+    ""\n"",
+    ""In this section, we'll confirm that the GeneExpressionData method returns the correct (sample, label) pairs relative to the original expression matrices and raw label files.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""acc0ebd5"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""datafiles, labelfiles = list(INTERIM_DATA_AND_LABEL_FILES_LIST.keys()), list(INTERIM_DATA_AND_LABEL_FILES_LIST.values())\n"",
+    ""\n"",
+    ""datafiles = [os.path.join('..', 'data', 'interim', f) for f in datafiles]\n"",
+    ""labelfiles = [os.path.join('..', 'data', 'processed/labels', f) for f in labelfiles]\n""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""17ab43b0"",
+   ""metadata"": {},
+   ""source"": [
+    ""Next, we define a function that takes the first `N` samples from the GeneExpressionData object and from the raw expression matrix and compares the samples to make sure they are equal.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 9,
+   ""id"": ""3704fbd0"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""N = 5\n"",
+    ""\n"",
+    ""def test_first_n(n, datafile, labelfile):\n"",
+    ""    data = GeneExpressionData(datafile, labelfile, 'Type', skip=3)\n"",
+    ""    cols = data.columns\n"",
+    ""    \n"",
+    ""    # Generate dict with half precision values to read this into my 16gb memory\n"",
+    ""    dtype_cols = dict(zip(cols, [np.float32]*len(cols)))\n"",
+    ""    \n"",
+    ""    data_df = pd.read_csv(datafile, nrows=2*n, header=1, dtype=dtype_cols) # Might need some extras since numerical index drops some values\n"",
+    ""    label_df = pd.read_csv(labelfile, nrows=n)\n"",
+    ""\n"",
+    ""    similar = []\n"",
+    ""    for i in range(n):\n"",
+    ""        idx = label_df.loc[i, 'cell']\n"",
+    ""        \n"",
+    ""        datasample = data[i][0]\n"",
+    ""        dfsample = torch.from_numpy(data_df.loc[idx, :].values).float()\n"",
+    ""        \n"",
+    ""        isclose = all(torch.isclose(datasample, dfsample))\n"",
+    ""        \n"",
+    ""        similar.append(isclose)\n"",
+    ""    \n"",
+    ""    print(f\""First {n=} columns of expression matrix is equal to GeneExpressionData: {all(p for p in similar)}\"")\n"",
+    ""\n"",
+    ""for datafile, labelfile in zip(datafiles, labelfiles):\n"",
+    ""    print(f'{datafile=}')\n"",
+    ""    test_first_n(N, datafile, labelfile)\n"",
+    ""    ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""f5421795"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""1 False\n"",
+      ""2 False\n"",
+      ""3 False\n"",
+      ""4 False\n"",
+      ""5 False\n"",
+      ""First n=5 columns of expression matrix is equal to GeneExpressionData: False\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""test_first_n(5, datafiles[1], labelfiles[1])""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 22,
+   ""id"": ""541c6dc4"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""1""
+      ]
+     },
+     ""execution_count"": 22,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""data = GeneExpressionData(datafiles[1], labelfiles[1], 'Type', skip=3)\n"",
+    ""\n"",
+    ""data_df = pd.read_csv(datafiles[1], header=1, nrows=10)\n"",
+    ""label_df = pd.read_csv(labelfiles[1], nrows=10)\n"",
+    ""\n"",
+    ""test_idx = label_df.loc[0, 'cell']\n"",
+    ""test_idx""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 29,
+   ""id"": ""8e00668b"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""True""
+      ]
+     },
+     ""execution_count"": 29,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""s1 = data_df.loc[test_idx, :].values\n"",
+    ""s2 = data[0][0]\n"",
+    ""\n"",
+    ""all(np.isclose(s1, s2))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 30,
+   ""id"": ""09ea2c27"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>cell</th>\n"",
+       ""      <th>Type</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>1</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>2</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>3</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>4</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>5</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>5</th>\n"",
+       ""      <td>6</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>6</th>\n"",
+       ""      <td>7</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>7</th>\n"",
+       ""      <td>8</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>8</th>\n"",
+       ""      <td>9</td>\n"",
+       ""      <td>7</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>9</th>\n"",
+       ""      <td>11</td>\n"",
+       ""      <td>15</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""   cell  Type\n"",
+       ""0     1     7\n"",
+       ""1     2     7\n"",
+       ""2     3     7\n"",
+       ""3     4     7\n"",
+       ""4     5     7\n"",
+       ""5     6     7\n"",
+       ""6     7     7\n"",
+       ""7     8     7\n"",
+       ""8     9     7\n"",
+       ""9    11    15""
+      ]
+     },
+     ""execution_count"": 30,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""label_df""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""4e798d8c"",
+   ""metadata"": {},
+   ""source"": [
+    ""## TabNet Classifier validation\n"",
+    ""\n"",
+    ""Since the TabNet package is designed to be used with the `sklearn` API, we'll write a custom `pl.LightningModule` with the TabNet classifier as the base class, and make sure that the correct `forward` method is returned. Essentially, validating that our wrapper doesn't change any internals.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""09ca1986"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Model initialized. input_dim = 19765, output_dim = 17. Metrics are dict_keys(['accuracy', 'precision', 'recall']) and weighted_metrics = False\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""from pytorch_tabnet.tab_model import TabNetClassifier\n"",
+    ""from models.lib.neural import TabNetGeneClassifier\n"",
+    ""\n"",
+    ""train, val, test = generate_dataloaders(datafiles=datafiles, labelfiles=labelfiles, class_label='Type', skip=3, batch_size=4, num_workers=0)\n"",
+    ""sample = next(iter(train))\n"",
+    ""\n"",
+    ""model = TabNetGeneClassifier(input_dim=19765, output_dim=17)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""3d246864"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""AttributeError"",
+     ""evalue"": ""'tuple' object has no attribute 'dim'"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_70310/2453837396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/Documents/Projects/organoid-classification/notebooks/../src/models/lib/neural.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/Documents/Projects/organoid-classification/notebooks/../src/models/lib/neural.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# exponential_average_factor is set to self.momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             raise ValueError(\n\u001b[1;32m    300\u001b[0m                 \u001b[0;34m\""expected 2D or 3D input (got {}D input)\""\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'""
+     ]
+    }
+   ],
+   ""source"": [
+    ""model(sample)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""31804b82"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""metadata"": {},
+   ""source"": [
+    ""# Data Modeling 01\n"",
+    ""\n"",
+    ""In this notebook, we'll begin building the classifier to show that Layer 4 neurons do not exist in the organoid data. We will do this in the following manner.\n"",
+    ""\n"",
+    ""1. Identify cells in the primary data by which layer of the cortex they are in.\n"",
+    ""2. Train a classifier on the primary data.\n"",
+    ""3. Under the assumption that the space of gene expression is the same in organoids, classify the organoid cells to their respective cortex layer and show that none get classified as layer 4.\n"",
+    ""4. Conclude that layer 4 cells do not exist in the organoid data.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""import matplotlib.pyplot as plt \n"",
+    ""import numpy as np\n"",
+    ""import umap\n"",
+    ""import hdbscan\n"",
+    ""from collections import Counter\n"",
+    ""import seaborn as sns\n"",
+    ""import plotly.express as px \n"",
+    ""import torch\n"",
+    ""import torch.nn as nn\n"",
+    ""import torch.nn.functional as F\n"",
+    ""from sklearn.svm import SVC\n"",
+    ""from sklearn.model_selection import GridSearchCV\n"",
+    ""import dask.dataframe as dd\n"",
+    ""from dask_ml.model_selection import train_test_split""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""labels = pd.read_csv('../data/processed/labels/primary_labels_neighbors_500_components_50_clust_size_100.csv')\n"",
+    ""labels['# label'] = labels['# label'].astype(int) + 1\n"",
+    ""\n"",
+    ""df = pd.read_csv('../data/processed/umap/primary_reduction_neighbors_50_components_3.csv', index_col='Unnamed: 0').iloc[0:100, :]""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""metadata"": {
+    ""scrolled"": false
+   },
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""# label\n"",
+       ""5          64350\n"",
+       ""4          48078\n"",
+       ""2          29637\n"",
+       ""9          13424\n"",
+       ""8           9401\n"",
+       ""0           8640\n"",
+       ""1           5829\n"",
+       ""11          5448\n"",
+       ""7           3677\n"",
+       ""6            475\n"",
+       ""3            237\n"",
+       ""10           213\n"",
+       ""dtype: int64""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""labels.value_counts()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 30,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df = dd.from_pandas(df, npartitions=2)\n"",
+    ""labels = dd.from_pandas(labels, npartitions=2)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 32,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.1, shuffle=True)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 33,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""class GeneClassifier:\n"",
+    ""    def __init__(self, est, params):\n"",
+    ""        self.est = est\n"",
+    ""        self.params = params\n"",
+    ""        \n"",
+    ""    def generate_model(self, X, y, n_iter=10):\n"",
+    ""        grid = RandomizedSearchCV(\n"",
+    ""            n_iter=n_iter,\n"",
+    ""            estimator=self.est,\n"",
+    ""            param_distributions=self.params,\n"",
+    ""            scoring='balanced_accuracy'\n"",
+    ""        )\n"",
+    ""\n"",
+    ""        self.grid = grid.fit(X, y)\n"",
+    ""    \n"",
+    ""    def best_score(self):\n"",
+    ""        return self.grid.best_score_\n"",
+    ""    \n"",
+    ""    def best_model(self):\n"",
+    ""        return self.grid.best_estimator_\n"",
+    ""    \n"",
+    ""    def best_params(self):\n"",
+    ""        return self.grid.best_params_""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now we begin the classification process""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 40,
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""from dask_ml.xgboost import XGBClassifier\n"",
+    ""from dask_ml.model_selection import RandomizedSearchCV\n"",
+    ""\n"",
+    ""params = {\n"",
+    ""    'eta' : np.linspace(0, 1, 20),\n"",
+    ""    'gamma': np.linspace(0, 1000, 20),\n"",
+    ""    'max_depth': np.linspace(0, 1000, 20, dtype=int),\n"",
+    ""}\n"",
+    ""\n"",
+    ""xgb_est = GeneClassifier(XGBClassifier(), params)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 42,
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""ename"": ""OSError"",
+     ""evalue"": ""Timed out trying to connect to tcp://scheduler-address:8786 after 30 s"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/comm/core.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             comm = await asyncio.wait_for(\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0mconnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconnection_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/asyncio/tasks.py\u001b[0m in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout, loop)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/comm/tcp.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, address, deserialize, **connection_args)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             stream = await self.client.connect(\n\u001b[0m\u001b[1;32m    399\u001b[0m                 \u001b[0mip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_buffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_BUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/tornado/tcpclient.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, host, port, af, ssl_options, max_buffer_size, source_ip, source_port, timeout)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0maddrinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         connector = _Connector(\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/tornado/netutil.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, host, port, family)\u001b[0m\n\u001b[1;32m    443\u001b[0m     ) -> List[Tuple[int, Any]]:\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_resolve_addr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/tornado/netutil.py\u001b[0m in \u001b[0;36m_resolve_addr\u001b[0;34m(host, port, family)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# so the addresses we return should still be usable with SOCK_DGRAM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0maddrinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known"",
+      ""\nThe above exception was the direct cause of the following exception:\n"",
+      ""\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_56451/2485957782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scheduler-address:8786'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgb_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__await__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_ensure_connected\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             comm = await connect(\n\u001b[0m\u001b[1;32m   1096\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             )\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/distributed/comm/core.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         raise OSError(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;34mf\""Timed out trying to connect to {addr} after {timeout} s\""\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         ) from active_exception\n"",
+      ""\u001b[0;31mOSError\u001b[0m: Timed out trying to connect to tcp://scheduler-address:8786 after 30 s""
+     ]
+    }
+   ],
+   ""source"": [
+    ""from dask.distributed import Client\n"",
+    ""client = Client('scheduler-address:8786')\n"",
+    ""\n"",
+    ""xgb_est = xgb_est.generate_model(X_train.values, y_train.values, n_iter=2)\n"",
+    ""print(xgb_est.best_score(), xgb_est.best_params())""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 39,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""array([[0.4481088 , 3.796412  , 2.5979095 ],\n"",
+       ""       [0.4983933 , 3.9593854 , 2.3622296 ],\n"",
+       ""       [0.33222336, 3.5287693 , 2.4303825 ],\n"",
+       ""       [0.05900061, 3.8804545 , 2.4287891 ],\n"",
+       ""       [0.40310133, 3.6861796 , 2.0468378 ],\n"",
+       ""       [0.22065888, 3.6910384 , 2.8645082 ],\n"",
+       ""       [0.23353353, 3.8845916 , 2.318612  ],\n"",
+       ""       [0.19925004, 3.740593  , 2.7131994 ],\n"",
+       ""       [0.1758697 , 3.5272355 , 2.4797056 ],\n"",
+       ""       [0.30395132, 3.7074225 , 2.3528247 ],\n"",
+       ""       [0.25804892, 3.6220398 , 2.6360617 ],\n"",
+       ""       [0.34219316, 3.8034708 , 2.8354907 ],\n"",
+       ""       [0.22701918, 3.3649328 , 2.6206775 ],\n"",
+       ""       [0.3596108 , 3.506909  , 2.6567767 ],\n"",
+       ""       [0.05308454, 3.7815006 , 2.6551347 ],\n"",
+       ""       [0.2358424 , 3.4101896 , 2.5498316 ],\n"",
+       ""       [0.57771707, 3.6255016 , 3.0535412 ],\n"",
+       ""       [0.13456306, 3.684509  , 2.5766418 ],\n"",
+       ""       [0.21065377, 3.3899477 , 2.5953205 ],\n"",
+       ""       [0.29221654, 3.6722684 , 2.644179  ],\n"",
+       ""       [0.17226812, 3.6118958 , 2.5589767 ],\n"",
+       ""       [0.50930506, 3.6159663 , 2.5196338 ],\n"",
+       ""       [0.20865391, 3.5561638 , 2.3338003 ],\n"",
+       ""       [0.24331927, 3.9654984 , 2.7275486 ],\n"",
+       ""       [0.18996856, 3.443169  , 2.6745489 ],\n"",
+       ""       [0.9809639 , 4.5610595 , 4.8850036 ],\n"",
+       ""       [0.3539471 , 3.8982067 , 3.1835632 ],\n"",
+       ""       [0.12256498, 4.033651  , 2.6991746 ],\n"",
+       ""       [0.39442173, 3.5206668 , 2.4660237 ],\n"",
+       ""       [0.16473374, 3.5512629 , 2.5415728 ],\n"",
+       ""       [0.19869334, 3.5228963 , 2.6999185 ],\n"",
+       ""       [0.05418336, 4.058473  , 2.6120167 ],\n"",
+       ""       [0.42182663, 3.6081555 , 2.6032882 ],\n"",
+       ""       [0.251568  , 3.6431036 , 2.581071  ],\n"",
+       ""       [0.3552161 , 3.4223325 , 2.6451776 ],\n"",
+       ""       [0.2973176 , 3.5074792 , 2.702009  ],\n"",
+       ""       [0.27996376, 3.9319658 , 2.127165  ],\n"",
+       ""       [1.197916  , 3.7481098 , 3.3704264 ],\n"",
+       ""       [3.1857336 , 3.677913  , 5.7838964 ],\n"",
+       ""       [0.33284637, 3.5247178 , 2.537216  ],\n"",
+       ""       [0.32521796, 3.392536  , 2.638325  ],\n"",
+       ""       [0.4050715 , 3.6005664 , 2.4110246 ],\n"",
+       ""       [0.40144688, 3.4972458 , 2.5446844 ],\n"",
+       ""       [0.5739357 , 3.4890826 , 2.5359488 ],\n"",
+       ""       [0.30287665, 3.6101816 , 2.7801728 ],\n"",
+       ""       [0.21061403, 3.4255736 , 2.5801234 ],\n"",
+       ""       [0.15725614, 3.3538983 , 2.5717013 ],\n"",
+       ""       [0.54569614, 3.6140537 , 2.3728237 ],\n"",
+       ""       [0.7850952 , 3.453045  , 2.9734392 ],\n"",
+       ""       [0.65864986, 3.4495254 , 2.7431884 ],\n"",
+       ""       [0.6441666 , 3.404655  , 2.48275   ],\n"",
+       ""       [0.54749024, 3.4850497 , 2.6113458 ],\n"",
+       ""       [0.38051587, 3.6667418 , 2.2990272 ],\n"",
+       ""       [0.37935913, 3.4295883 , 2.4796576 ],\n"",
+       ""       [0.66233677, 3.4560988 , 2.5225992 ],\n"",
+       ""       [0.4506053 , 3.6268547 , 2.3965425 ],\n"",
+       ""       [0.43064335, 3.2571895 , 2.4499288 ],\n"",
+       ""       [0.668575  , 3.6104581 , 2.8128831 ],\n"",
+       ""       [0.66550756, 3.4915583 , 2.4541624 ],\n"",
+       ""       [0.5674386 , 3.5510945 , 2.4603705 ],\n"",
+       ""       [0.5324833 , 3.420628  , 2.567588  ],\n"",
+       ""       [0.5299413 , 3.2613873 , 2.4545846 ],\n"",
+       ""       [0.6979105 , 3.4146173 , 2.450323  ],\n"",
+       ""       [0.601369  , 3.8542714 , 2.5414999 ],\n"",
+       ""       [0.8493252 , 3.3665326 , 2.429282  ],\n"",
+       ""       [0.5371846 , 3.4809055 , 2.7238812 ],\n"",
+       ""       [0.2523929 , 3.5702357 , 2.3809755 ],\n"",
+       ""       [0.8964828 , 3.3332126 , 2.6961462 ],\n"",
+       ""       [0.783938  , 3.406548  , 2.5730872 ],\n"",
+       ""       [0.57872725, 3.4555273 , 2.5570467 ],\n"",
+       ""       [0.43222764, 3.5464518 , 2.4812715 ],\n"",
+       ""       [1.0989001 , 3.324563  , 2.584545  ],\n"",
+       ""       [0.26576024, 3.5040703 , 2.3978436 ],\n"",
+       ""       [0.49914366, 3.4519582 , 2.5181503 ],\n"",
+       ""       [0.81935614, 3.3390367 , 2.586278  ],\n"",
+       ""       [0.6929606 , 3.5511785 , 2.912226  ],\n"",
+       ""       [0.72900665, 3.5045    , 2.5970907 ],\n"",
+       ""       [0.43913606, 3.2712154 , 2.6003761 ],\n"",
+       ""       [0.46185032, 3.4888303 , 2.8174164 ],\n"",
+       ""       [0.8525936 , 3.336101  , 2.5256176 ],\n"",
+       ""       [0.70760554, 3.3909647 , 2.6583889 ],\n"",
+       ""       [0.44155103, 3.3180437 , 2.5741546 ],\n"",
+       ""       [0.47287798, 3.4559166 , 2.8012018 ],\n"",
+       ""       [0.29868737, 3.5219383 , 2.3505657 ],\n"",
+       ""       [0.8144549 , 3.53797   , 2.4086754 ],\n"",
+       ""       [0.65121317, 3.3385315 , 2.6607788 ]])""
+      ]
+     },
+     ""execution_count"": 39,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""X_train.values.compute()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 4
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""9765561c"",
+   ""metadata"": {},
+   ""source"": [
+    ""# Data Modeling 02\n"",
+    ""\n"",
+    ""In this notebook, we'll use Dask to tune a classifier with HyperbandSearch, so that we can train many models in parallel on the PRP. ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""8ebb675a"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import dask\n"",
+    ""import numpy as np\n"",
+    ""import matplotlib.pyplot as plt \n"",
+    ""\n"",
+    ""import dask.dataframe as dd\n"",
+    ""from dask_ml.model_selection import train_test_split, HyperbandSearchCV, RandomizedSearchCV, GridSearchCV\n"",
+    ""from dask_ml.linear_model import LogisticRegression""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""6d8ebcf4"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now, let's read in our cleaned (and for this local example, reduced) data and train a model on it ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""a9ed41d8"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""X = dd.read_csv('../data/processed/primary_reduction_neighbors_15_components_3.csv')\n"",
+    ""y = dd.read_csv('../data/processed/primary_labels_neighbors_15_components_50.csv', header=None)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""5379d332"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""# y = y + 1""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""7d265e69"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""est = LogisticRegression(class_weight='balanced')\n"",
+    ""\n"",
+    ""grid = RandomizedSearchCV(\n"",
+    ""    n_iter=15,\n"",
+    ""    estimator=est,\n"",
+    ""    param_distributions={\n"",
+    ""        'penalty' : ['l1', 'l2'],\n"",
+    ""        'C' : np.linspace(0.1, 100, 50)\n"",
+    ""    },\n"",
+    ""    scoring='balanced_accuracy',\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""166bae38"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 6,
+   ""id"": ""799c726c"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>0</th>\n"",
+       ""      <th>1</th>\n"",
+       ""      <th>2</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>67575</th>\n"",
+       ""      <td>1.960368</td>\n"",
+       ""      <td>1.299117</td>\n"",
+       ""      <td>-0.212742</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>89917</th>\n"",
+       ""      <td>4.305542</td>\n"",
+       ""      <td>0.210048</td>\n"",
+       ""      <td>7.261106</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>70364</th>\n"",
+       ""      <td>3.158221</td>\n"",
+       ""      <td>0.971542</td>\n"",
+       ""      <td>0.213203</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>150005</th>\n"",
+       ""      <td>1.889506</td>\n"",
+       ""      <td>6.531239</td>\n"",
+       ""      <td>4.297892</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>29714</th>\n"",
+       ""      <td>3.637177</td>\n"",
+       ""      <td>3.377710</td>\n"",
+       ""      <td>3.172499</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""               0         1         2\n"",
+       ""67575   1.960368  1.299117 -0.212742\n"",
+       ""89917   4.305542  0.210048  7.261106\n"",
+       ""70364   3.158221  0.971542  0.213203\n"",
+       ""150005  1.889506  6.531239  4.297892\n"",
+       ""29714   3.637177  3.377710  3.172499""
+      ]
+     },
+     ""execution_count"": 6,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""X_train.head()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 7,
+   ""id"": ""5a432019"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>0</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>67575</th>\n"",
+       ""      <td>10.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>89917</th>\n"",
+       ""      <td>7.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>70364</th>\n"",
+       ""      <td>10.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>150005</th>\n"",
+       ""      <td>-1.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>29714</th>\n"",
+       ""      <td>9.0</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""           0\n"",
+       ""67575   10.0\n"",
+       ""89917    7.0\n"",
+       ""70364   10.0\n"",
+       ""150005  -1.0\n"",
+       ""29714    9.0""
+      ]
+     },
+     ""execution_count"": 7,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""y_train.head()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 8,
+   ""id"": ""d81f4c1a"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""# best_est = grid.fit(X_train.values, y_train.values)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 9,
+   ""id"": ""9381b628"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""# best_est.cv_results_""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""4eedba6a"",
+   ""metadata"": {},
+   ""source"": [
+    ""Great, now let's see what the best estimator was!""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""462e1a02"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""# best_est.best_score_""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""cd664613"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now let's define a generalized class to do this hyperparameter tuning ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""106af143"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""class GeneClassifier:\n"",
+    ""    def __init__(self, est, params):\n"",
+    ""        self.est = est\n"",
+    ""        self.params = params\n"",
+    ""        \n"",
+    ""    def generate_model(self, X, y, n_iter=10):\n"",
+    ""        grid = RandomizedSearchCV(\n"",
+    ""            n_iter=n_iter,\n"",
+    ""            estimator=self.est,\n"",
+    ""            param_distributions=self.params,\n"",
+    ""            scoring='balanced_accuracy',\n"",
+    ""        )\n"",
+    ""\n"",
+    ""        self.grid = grid.fit(X, y)\n"",
+    ""    \n"",
+    ""    def best_score(self):\n"",
+    ""        return self.grid.best_score_\n"",
+    ""    \n"",
+    ""    def best_model(self):\n"",
+    ""        return self.grid.best_estimator_\n"",
+    ""    \n"",
+    ""    def best_params(self):\n"",
+    ""        return self.grid.best_params_""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""025b77d4"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""param_distributions = {\n"",
+    ""        'penalty' : ['l1', 'l2'],\n"",
+    ""        'C' : np.linspace(0.1, 100, 50)\n"",
+    ""    },\n"",
+    ""\n"",
+    ""logistic_est = GeneClassifier(LogisticRegression(class_weight='balanced'), param_distributions)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""f82c57f1"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""# logistic_est.generate_model(X_train.values, y_train.values, n_iter=2)""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""cdd013f6"",
+   ""metadata"": {},
+   ""source"": [
+    ""Finally, let's quickly test the balanced accuracy on the test set""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""e2495c60"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from sklearn.metrics import balanced_accuracy_score\n"",
+    ""\n"",
+    ""# est = logistic_est.best_model()""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""e8b4151b"",
+   ""metadata"": {},
+   ""source"": [
+    ""## LogisticRegression doesn't support multi-class\n"",
+    ""\n"",
+    ""So, ignore the above results""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""07be3577"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now let's try this with a simple XGBClassifier (gradient boosted tree classifier)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 17,
+   ""id"": ""777e4514"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""from xgboost import XGBClassifier\n"",
+    ""\n"",
+    ""params = {\n"",
+    ""    'eta' : np.linspace(0, 1, 20),\n"",
+    ""    'gamma': np.linspace(0, 1000, 20),\n"",
+    ""    'max_depth': np.linspace(0, 1000, 20, dtype=int),\n"",
+    ""}\n"",
+    ""\n"",
+    ""xgb_est = GeneClassifier(XGBClassifier(eval_metric='mlogloss'), params)""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""0281d1a1"",
+   ""metadata"": {},
+   ""source"": [
+    ""Using the XGBClassifier from `dask_ml` requires a distributed Client, so we'll just use the default classifier instead. ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 18,
+   ""id"": ""352158a7"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n""
+     ]
+    },
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n""
+     ]
+    },
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n""
+     ]
+    },
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"",
+      ""/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"",
+      ""  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n""
+     ]
+    },
+    {
+     ""ename"": ""KeyboardInterrupt"",
+     ""evalue"": """",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15004/1002924110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15004/2496743085.py\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(self, X, y, n_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/dask_ml/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiprocessingPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\""waiting\""\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\""ready\""\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\""running\""\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mfire_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                         \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\""'timeout' must be a non-negative number\""\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mKeyboardInterrupt\u001b[0m: ""
+     ]
+    }
+   ],
+   ""source"": [
+    ""xgb_est.generate_model(X_train.values, y_train.values.compute().ravel(), n_iter=50)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 19,
+   ""id"": ""86280591"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""AttributeError"",
+     ""evalue"": ""'GeneClassifier' object has no attribute 'grid'"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15004/2902908111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15004/2496743085.py\u001b[0m in \u001b[0;36mbest_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;31mAttributeError\u001b[0m: 'GeneClassifier' object has no attribute 'grid'""
+     ]
+    }
+   ],
+   ""source"": [
+    ""xgb_est.best_params()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 20,
+   ""id"": ""1b19ac99"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""AttributeError"",
+     ""evalue"": ""'GeneClassifier' object has no attribute 'grid'"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15004/3680487038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15004/2496743085.py\u001b[0m in \u001b[0;36mbest_score\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mAttributeError\u001b[0m: 'GeneClassifier' object has no attribute 'grid'""
+     ]
+    }
+   ],
+   ""source"": [
+    ""xgb_est.best_score()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""54bff0eb"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""metadata"": {},
+   ""source"": [
+    ""# Dataset creation""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""import matplotlib.pyplot as plt \n"",
+    ""import os, sys\n"",
+    ""from typing import *\n"",
+    ""from sklearn.model_selection import train_test_split \n"",
+    ""import torch\n"",
+    ""sys.path.append('../src/')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from models.lib.data import *\n"",
+    ""from models.lib.neural import *""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 79,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""def _generate_stratified_dataset(\n"",
+    ""    dataset_files: List[str], \n"",
+    ""    label_files: List[str],\n"",
+    ""    class_label: str,\n"",
+    ""    test_prop: float=0.2,\n"",
+    "") -> Tuple[Dataset, Dataset]:\n"",
+    ""    \n"",
+    ""    train_datasets = []\n"",
+    ""    val_datasets = []\n"",
+    ""    test_datasets = []\n"",
+    ""\n"",
+    ""    for datafile, labelfile in zip(dataset_files, label_files):\n"",
+    ""        # Read in current labelfile\n"",
+    ""        current_labels = pd.read_csv(labelfile).loc[:, class_label]\n"",
+    ""        \n"",
+    ""        # Make stratified split on labels\n"",
+    ""        # Since we are using labels this returns indices, which we then pass to the GeneExpressionDataset class\n"",
+    ""        trainsplit, testsplit = train_test_split(current_labels, stratify=current_labels, test_size=test_prop)\n"",
+    ""        trainsplit, valsplit = train_test_split(trainsplit, stratify=trainsplit, test_size=1.25*test_prop)\n"",
+    ""        \n"",
+    ""        # Generate train/test with stratified indices\n"",
+    ""        trainset = GeneExpressionData(\n"",
+    ""            filename=datafile, \n"",
+    ""            labelname=labelfile,\n"",
+    ""            class_label=class_label,\n"",
+    ""            indices=trainsplit.index \n"",
+    ""        )\n"",
+    ""        \n"",
+    ""        valset = GeneExpressionData(\n"",
+    ""            filename=datafile, \n"",
+    ""            labelname=labelfile,\n"",
+    ""            class_label=class_label,\n"",
+    ""            indices=valsplit.index \n"",
+    ""        )\n"",
+    ""        \n"",
+    ""        testset = GeneExpressionData(\n"",
+    ""            filename=datafile,\n"",
+    ""            labelname=labelfile,\n"",
+    ""            class_label=class_label,\n"",
+    ""            indices=testsplit.index,\n"",
+    ""        )\n"",
+    ""        \n"",
+    ""        train_datasets.append(trainset)\n"",
+    ""        val_datasets.append(valset)\n"",
+    ""        test_datasets.append(testset)\n"",
+    ""            \n"",
+    ""    train = torch.utils.data.ConcatDataset(train_datasets)\n"",
+    ""    val = torch.utils.data.ConcatDataset(val_datasets)\n"",
+    ""    test = torch.utils.data.ConcatDataset(test_datasets)\n"",
+    ""    \n"",
+    ""    return train, test, val\n""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 80,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""train, test, val = _generate_stratified_dataset(\n"",
+    ""    dataset_files=['../data/processed/primary.csv'],\n"",
+    ""    label_files=['../data/processed/meta_primary_labels.csv'],\n"",
+    ""    class_label='Subtype'\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 81,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(113645, 37882, 37882)""
+      ]
+     },
+     ""execution_count"": 81,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(train), len(test), len(val)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 83,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""trainlabels = [y for X, y in train]\n"",
+    ""testlabels = [y for X, y in test]\n"",
+    ""vallabels = [y for X, y in val]""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 89,
+   ""metadata"": {
+    ""scrolled"": false
+   },
+   ""outputs"": [
+    {
+     ""data"": {
+      ""image/png"": ""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\n"",
+      ""text/plain"": [
+       ""<Figure size 432x288 with 1 Axes>""
+      ]
+     },
+     ""metadata"": {
+      ""needs_background"": ""light""
+     },
+     ""output_type"": ""display_data""
+    },
+    {
+     ""data"": {
+      ""image/png"": ""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\n"",
+      ""text/plain"": [
+       ""<Figure size 432x288 with 1 Axes>""
+      ]
+     },
+     ""metadata"": {
+      ""needs_background"": ""light""
+     },
+     ""output_type"": ""display_data""
+    },
+    {
+     ""data"": {
+      ""image/png"": ""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\n"",
+      ""text/plain"": [
+       ""<Figure size 432x288 with 1 Axes>""
+      ]
+     },
+     ""metadata"": {
+      ""needs_background"": ""light""
+     },
+     ""output_type"": ""display_data""
+    }
+   ],
+   ""source"": [
+    ""import matplotlib.pyplot as plt \n"",
+    ""\n"",
+    ""for data, name in zip([trainlabels, testlabels, vallabels], ['Train', 'Test', 'Val']):\n"",
+    ""    plt.hist(data)\n"",
+    ""    plt.title(f'Distribution of {name} Set Labels')\n"",
+    ""    plt.show()\n""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 91,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(<torch.utils.data.dataset.ConcatDataset at 0x7f92f0c220a0>,\n"",
+       "" <torch.utils.data.dataset.ConcatDataset at 0x7f8e7ba799a0>)""
+      ]
+     },
+     ""execution_count"": 91,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""train, test""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 95,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""[10,\n"",
+       "" 21,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 21,\n"",
+       "" 5,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 27,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 23,\n"",
+       "" 27,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 1,\n"",
+       "" 17,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 17,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 24,\n"",
+       "" 9,\n"",
+       "" 21,\n"",
+       "" 23,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 25,\n"",
+       "" 15,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 11,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 21,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 22,\n"",
+       "" 27,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 6,\n"",
+       "" 8,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 21,\n"",
+       "" 12,\n"",
+       "" 15,\n"",
+       "" 27,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 9,\n"",
+       "" 3,\n"",
+       "" 21,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 25,\n"",
+       "" 29,\n"",
+       "" 17,\n"",
+       "" 7,\n"",
+       "" 21,\n"",
+       "" 13,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 4,\n"",
+       "" 23,\n"",
+       "" 21,\n"",
+       "" 26,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 5,\n"",
+       "" 22,\n"",
+       "" 19,\n"",
+       "" 16,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 1,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 7,\n"",
+       "" 29,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 16,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 1,\n"",
+       "" 21,\n"",
+       "" 12,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 7,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 24,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 13,\n"",
+       "" 10,\n"",
+       "" 1,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 11,\n"",
+       "" 20,\n"",
+       "" 17,\n"",
+       "" 12,\n"",
+       "" 6,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 16,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 5,\n"",
+       "" 25,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 4,\n"",
+       "" 6,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 4,\n"",
+       "" 23,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 6,\n"",
+       "" 9,\n"",
+       "" 1,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 6,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 1,\n"",
+       "" 26,\n"",
+       "" 19,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 19,\n"",
+       "" 8,\n"",
+       "" 7,\n"",
+       "" 27,\n"",
+       "" 10,\n"",
+       "" 21,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 6,\n"",
+       "" 26,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 14,\n"",
+       "" 9,\n"",
+       "" 23,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 17,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 21,\n"",
+       "" 8,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 19,\n"",
+       "" 1,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 29,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 18,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 25,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 1,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 16,\n"",
+       "" 3,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 19,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 4,\n"",
+       "" 24,\n"",
+       "" 22,\n"",
+       "" 25,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 21,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 18,\n"",
+       "" 26,\n"",
+       "" 18,\n"",
+       "" 24,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 12,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 5,\n"",
+       "" 20,\n"",
+       "" 17,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 23,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 18,\n"",
+       "" 17,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 5,\n"",
+       "" 10,\n"",
+       "" 1,\n"",
+       "" 26,\n"",
+       "" 21,\n"",
+       "" 21,\n"",
+       "" 18,\n"",
+       "" 12,\n"",
+       "" 10,\n"",
+       "" 24,\n"",
+       "" 26,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 24,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 27,\n"",
+       "" 25,\n"",
+       "" 17,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 4,\n"",
+       "" 13,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 18,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 21,\n"",
+       "" 24,\n"",
+       "" 12,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 18,\n"",
+       "" 29,\n"",
+       "" 17,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 18,\n"",
+       "" 21,\n"",
+       "" 29,\n"",
+       "" 25,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 19,\n"",
+       "" 12,\n"",
+       "" 1,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 17,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 3,\n"",
+       "" 15,\n"",
+       "" 7,\n"",
+       "" 19,\n"",
+       "" 21,\n"",
+       "" 23,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 12,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 6,\n"",
+       "" 18,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 7,\n"",
+       "" 6,\n"",
+       "" 29,\n"",
+       "" 21,\n"",
+       "" 23,\n"",
+       "" 26,\n"",
+       "" 9,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 21,\n"",
+       "" 19,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 1,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 26,\n"",
+       "" 4,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 17,\n"",
+       "" 20,\n"",
+       "" 16,\n"",
+       "" 24,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 3,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 11,\n"",
+       "" 15,\n"",
+       "" 25,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 5,\n"",
+       "" 8,\n"",
+       "" 8,\n"",
+       "" 25,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 6,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 9,\n"",
+       "" 27,\n"",
+       "" 24,\n"",
+       "" 27,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 17,\n"",
+       "" 10,\n"",
+       "" 1,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 23,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 27,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 19,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 12,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 7,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 26,\n"",
+       "" 16,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 19,\n"",
+       "" 29,\n"",
+       "" 16,\n"",
+       "" 16,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 4,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 4,\n"",
+       "" 23,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 5,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 23,\n"",
+       "" 21,\n"",
+       "" 6,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 13,\n"",
+       "" 8,\n"",
+       "" 6,\n"",
+       "" 12,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 9,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 23,\n"",
+       "" 5,\n"",
+       "" 6,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 1,\n"",
+       "" 21,\n"",
+       "" 29,\n"",
+       "" 29,\n"",
+       "" 1,\n"",
+       "" 8,\n"",
+       "" 8,\n"",
+       "" 1,\n"",
+       "" 29,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 26,\n"",
+       "" 18,\n"",
+       "" 15,\n"",
+       "" 12,\n"",
+       "" 8,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 18,\n"",
+       "" 29,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 13,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 24,\n"",
+       "" 26,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 24,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 25,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 18,\n"",
+       "" 19,\n"",
+       "" 18,\n"",
+       "" 13,\n"",
+       "" 15,\n"",
+       "" 27,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 7,\n"",
+       "" 16,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 0,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 5,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 4,\n"",
+       "" 7,\n"",
+       "" 7,\n"",
+       "" 21,\n"",
+       "" 9,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 7,\n"",
+       "" 13,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 18,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 16,\n"",
+       "" 6,\n"",
+       "" 16,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 13,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 13,\n"",
+       "" 26,\n"",
+       "" 1,\n"",
+       "" 29,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 0,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 27,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 13,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 27,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" ...]""
+      ]
+     },
+     ""execution_count"": 95,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""trainlabels""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 96,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""[10,\n"",
+       "" 18,\n"",
+       "" 23,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 27,\n"",
+       "" 16,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 7,\n"",
+       "" 1,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 26,\n"",
+       "" 20,\n"",
+       "" 12,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 17,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 3,\n"",
+       "" 29,\n"",
+       "" 17,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 6,\n"",
+       "" 6,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 19,\n"",
+       "" 1,\n"",
+       "" 23,\n"",
+       "" 17,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 12,\n"",
+       "" 5,\n"",
+       "" 25,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 17,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 1,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 19,\n"",
+       "" 7,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 4,\n"",
+       "" 7,\n"",
+       "" 24,\n"",
+       "" 23,\n"",
+       "" 10,\n"",
+       "" 9,\n"",
+       "" 21,\n"",
+       "" 7,\n"",
+       "" 20,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 18,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 4,\n"",
+       "" 23,\n"",
+       "" 1,\n"",
+       "" 21,\n"",
+       "" 25,\n"",
+       "" 23,\n"",
+       "" 23,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 29,\n"",
+       "" 27,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 7,\n"",
+       "" 9,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 18,\n"",
+       "" 26,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 2,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 29,\n"",
+       "" 26,\n"",
+       "" 16,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 23,\n"",
+       "" 26,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 29,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 6,\n"",
+       "" 6,\n"",
+       "" 24,\n"",
+       "" 6,\n"",
+       "" 8,\n"",
+       "" 21,\n"",
+       "" 8,\n"",
+       "" 13,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 17,\n"",
+       "" 6,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 29,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 19,\n"",
+       "" 23,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 19,\n"",
+       "" 27,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 6,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 16,\n"",
+       "" 8,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 2,\n"",
+       "" 26,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 6,\n"",
+       "" 22,\n"",
+       "" 23,\n"",
+       "" 8,\n"",
+       "" 23,\n"",
+       "" 7,\n"",
+       "" 9,\n"",
+       "" 17,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 8,\n"",
+       "" 18,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 23,\n"",
+       "" 6,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 17,\n"",
+       "" 7,\n"",
+       "" 21,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 1,\n"",
+       "" 16,\n"",
+       "" 12,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 25,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 24,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 16,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 11,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 5,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 13,\n"",
+       "" 6,\n"",
+       "" 8,\n"",
+       "" 3,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 3,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 7,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 1,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 21,\n"",
+       "" 16,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 6,\n"",
+       "" 26,\n"",
+       "" 1,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 16,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 17,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 19,\n"",
+       "" 23,\n"",
+       "" 11,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 29,\n"",
+       "" 7,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 1,\n"",
+       "" 7,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 9,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 13,\n"",
+       "" 10,\n"",
+       "" 7,\n"",
+       "" 5,\n"",
+       "" 24,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 29,\n"",
+       "" 27,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 21,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 23,\n"",
+       "" 24,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 3,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 14,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 3,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 19,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 8,\n"",
+       "" 8,\n"",
+       "" 18,\n"",
+       "" 23,\n"",
+       "" 23,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 9,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 18,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 9,\n"",
+       "" 18,\n"",
+       "" 21,\n"",
+       "" 23,\n"",
+       "" 10,\n"",
+       "" 9,\n"",
+       "" 1,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 29,\n"",
+       "" 23,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 16,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 23,\n"",
+       "" 6,\n"",
+       "" 22,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 26,\n"",
+       "" 24,\n"",
+       "" 1,\n"",
+       "" 24,\n"",
+       "" 22,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 29,\n"",
+       "" 1,\n"",
+       "" 26,\n"",
+       "" 27,\n"",
+       "" 24,\n"",
+       "" 26,\n"",
+       "" 15,\n"",
+       "" 7,\n"",
+       "" 9,\n"",
+       "" 16,\n"",
+       "" 2,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 23,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 7,\n"",
+       "" 22,\n"",
+       "" 6,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 17,\n"",
+       "" 10,\n"",
+       "" 1,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 18,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 16,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 21,\n"",
+       "" 10,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 4,\n"",
+       "" 15,\n"",
+       "" 4,\n"",
+       "" 27,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 19,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 3,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 11,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 5,\n"",
+       "" 8,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 16,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 6,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 15,\n"",
+       "" 17,\n"",
+       "" 27,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 13,\n"",
+       "" 26,\n"",
+       "" 10,\n"",
+       "" 9,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 6,\n"",
+       "" 27,\n"",
+       "" 19,\n"",
+       "" 9,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 26,\n"",
+       "" 8,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 8,\n"",
+       "" 19,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 26,\n"",
+       "" 26,\n"",
+       "" 23,\n"",
+       "" 18,\n"",
+       "" 23,\n"",
+       "" 12,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 1,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 10,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 3,\n"",
+       "" 18,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 8,\n"",
+       "" 8,\n"",
+       "" 1,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 10,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 9,\n"",
+       "" 7,\n"",
+       "" 20,\n"",
+       "" 1,\n"",
+       "" 6,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 17,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 8,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 11,\n"",
+       "" 29,\n"",
+       "" 17,\n"",
+       "" 18,\n"",
+       "" 24,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 18,\n"",
+       "" 20,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 6,\n"",
+       "" 12,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 23,\n"",
+       "" 15,\n"",
+       "" 4,\n"",
+       "" 6,\n"",
+       "" 27,\n"",
+       "" 23,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 19,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 29,\n"",
+       "" 22,\n"",
+       "" 23,\n"",
+       "" 6,\n"",
+       "" 16,\n"",
+       "" 23,\n"",
+       "" 7,\n"",
+       "" 22,\n"",
+       "" 20,\n"",
+       "" 9,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 6,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 23,\n"",
+       "" 9,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 9,\n"",
+       "" 9,\n"",
+       "" 17,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 16,\n"",
+       "" 10,\n"",
+       "" 17,\n"",
+       "" 20,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 27,\n"",
+       "" 8,\n"",
+       "" 8,\n"",
+       "" 1,\n"",
+       "" 1,\n"",
+       "" 9,\n"",
+       "" 12,\n"",
+       "" 26,\n"",
+       "" 19,\n"",
+       "" 29,\n"",
+       "" 12,\n"",
+       "" 15,\n"",
+       "" 24,\n"",
+       "" 1,\n"",
+       "" 27,\n"",
+       "" 3,\n"",
+       "" 22,\n"",
+       "" 10,\n"",
+       "" 26,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 8,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 6,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 1,\n"",
+       "" 6,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 25,\n"",
+       "" 10,\n"",
+       "" 17,\n"",
+       "" 29,\n"",
+       "" 6,\n"",
+       "" 23,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 6,\n"",
+       "" 16,\n"",
+       "" 10,\n"",
+       "" 21,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 20,\n"",
+       "" 19,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 20,\n"",
+       "" 26,\n"",
+       "" 19,\n"",
+       "" 19,\n"",
+       "" 26,\n"",
+       "" 22,\n"",
+       "" 1,\n"",
+       "" 20,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 17,\n"",
+       "" 19,\n"",
+       "" 8,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 8,\n"",
+       "" 24,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 17,\n"",
+       "" 20,\n"",
+       "" 22,\n"",
+       "" 13,\n"",
+       "" 20,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 25,\n"",
+       "" 8,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 6,\n"",
+       "" 26,\n"",
+       "" 22,\n"",
+       "" 29,\n"",
+       "" 9,\n"",
+       "" 4,\n"",
+       "" 26,\n"",
+       "" 6,\n"",
+       "" 15,\n"",
+       "" 15,\n"",
+       "" 13,\n"",
+       "" 22,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 22,\n"",
+       "" 21,\n"",
+       "" 15,\n"",
+       "" 10,\n"",
+       "" 8,\n"",
+       "" 22,\n"",
+       "" 19,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 4,\n"",
+       "" 10,\n"",
+       "" 20,\n"",
+       "" 25,\n"",
+       "" 22,\n"",
+       "" 15,\n"",
+       "" 7,\n"",
+       "" 15,\n"",
+       "" 22,\n"",
+       "" 23,\n"",
+       "" 29,\n"",
+       "" 20,\n"",
+       "" 24,\n"",
+       "" 10,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 16,\n"",
+       "" 20,\n"",
+       "" 10,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" 15,\n"",
+       "" 29,\n"",
+       "" 15,\n"",
+       "" 20,\n"",
+       "" ...]""
+      ]
+     },
+     ""execution_count"": 96,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""vallabels""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 4
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""ee50a885"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""import dask.dataframe as da\n"",
+    ""import matplotlib.pyplot as plt \n"",
+    ""import numpy as np \n"",
+    ""import sys, os\n"",
+    ""sys.path.append('../src/')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""3ab72101"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""FileNotFoundError"",
+     ""evalue"": ""[Errno 2] No such file or directory: '../data/external/Heterogeneity of Human Neuroepithelial Cells and Early Radial Glia labels.tsv'"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_29752/215337344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/external/Heterogeneity of Human Neuroepithelial Cells and Early Radial Glia labels.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \""Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\""\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \""str\"", \""bool\"", \""Any\"", \""Any\"", \""Any\"", \""Any\"", \""Any\""\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\""b\""\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/external/Heterogeneity of Human Neuroepithelial Cells and Early Radial Glia labels.tsv'""
+     ]
+    }
+   ],
+   ""source"": [
+    ""df = pd.read_csv('../data/external/Heterogeneity of Human Neuroepithelial Cells and Early Radial Glia labels.tsv', sep='\\t')\n"",
+    ""\n"",
+    ""df.head()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""df455590"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""6f9a87e8"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df.columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""cf5c6c30"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df['Area_As_Annotated'].unique()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""a266a2c1"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""NameError"",
+     ""evalue"": ""name 'df' is not defined"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_29752/3029673475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;31mNameError\u001b[0m: name 'df' is not defined""
+     ]
+    }
+   ],
+   ""source"": [
+    ""df['Area'].unique()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""eed729e2"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_29752/3935305708.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n"",
+      ""  df2 = pd.read_csv('../data/external/whole_brain_bhaduri_labels.tsv', sep='\\t')\n""
+     ]
+    },
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>cell.name</th>\n"",
+       ""      <th>age</th>\n"",
+       ""      <th>individual</th>\n"",
+       ""      <th>structure</th>\n"",
+       ""      <th>area</th>\n"",
+       ""      <th>area.sub</th>\n"",
+       ""      <th>clusterv2</th>\n"",
+       ""      <th>clusterv2_id</th>\n"",
+       ""      <th>cell.type.v2</th>\n"",
+       ""      <th>celltype.structure</th>\n"",
+       ""      <th>...</th>\n"",
+       ""      <th>matchname</th>\n"",
+       ""      <th>clusterv1</th>\n"",
+       ""      <th>ngene</th>\n"",
+       ""      <th>numi</th>\n"",
+       ""      <th>percent.mito</th>\n"",
+       ""      <th>percent.ribo</th>\n"",
+       ""      <th>v1</th>\n"",
+       ""      <th>ncount_rna</th>\n"",
+       ""      <th>nfeature_rna</th>\n"",
+       ""      <th>cell.name.1</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>GW14_CGE_AAACCTGGTAAAGGAG</td>\n"",
+       ""      <td>14</td>\n"",
+       ""      <td>GW14</td>\n"",
+       ""      <td>GE</td>\n"",
+       ""      <td>CGE</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>RG_4RG</td>\n"",
+       ""      <td>191</td>\n"",
+       ""      <td>radialglia</td>\n"",
+       ""      <td>radialglia_GE</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW14_RG_5RG</td>\n"",
+       ""      <td>RG_4</td>\n"",
+       ""      <td>869</td>\n"",
+       ""      <td>1481</td>\n"",
+       ""      <td>1.290000e-13</td>\n"",
+       ""      <td>0.232275</td>\n"",
+       ""      <td>5.0</td>\n"",
+       ""      <td>1481</td>\n"",
+       ""      <td>869</td>\n"",
+       ""      <td>GW14_CGE_AAACCTGGTAAAGGAG</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>GW14_CGE_AAACCTGGTTCCGTCT</td>\n"",
+       ""      <td>14</td>\n"",
+       ""      <td>GW14</td>\n"",
+       ""      <td>GE</td>\n"",
+       ""      <td>CGE</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Dividing_3</td>\n"",
+       ""      <td>20</td>\n"",
+       ""      <td>dividing</td>\n"",
+       ""      <td>dividing_GE</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW14_Dividing_29Dividing</td>\n"",
+       ""      <td>GW14_Dividing_29Dividing</td>\n"",
+       ""      <td>805</td>\n"",
+       ""      <td>1845</td>\n"",
+       ""      <td>1.220000e-37</td>\n"",
+       ""      <td>0.270461</td>\n"",
+       ""      <td>29.0</td>\n"",
+       ""      <td>1845</td>\n"",
+       ""      <td>805</td>\n"",
+       ""      <td>GW14_CGE_AAACCTGGTTCCGTCT</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>GW14_CGE_AAACCTGTCGTCCAGG</td>\n"",
+       ""      <td>14</td>\n"",
+       ""      <td>GW14</td>\n"",
+       ""      <td>GE</td>\n"",
+       ""      <td>CGE</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Dividing_5</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>dividing</td>\n"",
+       ""      <td>dividing_GE</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW14_Dividing_8Dividing</td>\n"",
+       ""      <td>Dividing_37</td>\n"",
+       ""      <td>1131</td>\n"",
+       ""      <td>3101</td>\n"",
+       ""      <td>5.790000e-19</td>\n"",
+       ""      <td>0.341503</td>\n"",
+       ""      <td>8.0</td>\n"",
+       ""      <td>3101</td>\n"",
+       ""      <td>1131</td>\n"",
+       ""      <td>GW14_CGE_AAACCTGTCGTCCAGG</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>GW14_CGE_AAACGGGCAGCCAGAA</td>\n"",
+       ""      <td>14</td>\n"",
+       ""      <td>GW14</td>\n"",
+       ""      <td>GE</td>\n"",
+       ""      <td>CGE</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Neuron_4Neuron</td>\n"",
+       ""      <td>140</td>\n"",
+       ""      <td>neuron</td>\n"",
+       ""      <td>neuron_GE</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW14_Neuron_11Neuron</td>\n"",
+       ""      <td>Neuron_4</td>\n"",
+       ""      <td>776</td>\n"",
+       ""      <td>1451</td>\n"",
+       ""      <td>4.320000e-34</td>\n"",
+       ""      <td>0.216402</td>\n"",
+       ""      <td>11.0</td>\n"",
+       ""      <td>1451</td>\n"",
+       ""      <td>776</td>\n"",
+       ""      <td>GW14_CGE_AAACGGGCAGCCAGAA</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>GW14_CGE_AAACGGGGTGAACCTT</td>\n"",
+       ""      <td>14</td>\n"",
+       ""      <td>GW14</td>\n"",
+       ""      <td>GE</td>\n"",
+       ""      <td>CGE</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Dividing_5</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>dividing</td>\n"",
+       ""      <td>dividing_GE</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW14_Dividing_8Dividing</td>\n"",
+       ""      <td>Dividing_37</td>\n"",
+       ""      <td>1097</td>\n"",
+       ""      <td>2780</td>\n"",
+       ""      <td>2.550000e-46</td>\n"",
+       ""      <td>0.267986</td>\n"",
+       ""      <td>8.0</td>\n"",
+       ""      <td>2780</td>\n"",
+       ""      <td>1097</td>\n"",
+       ""      <td>GW14_CGE_AAACGGGGTGAACCTT</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>691923</th>\n"",
+       ""      <td>GW25_thalamus_TTTGTCACATGGTTGT</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>GW25</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Endo_11</td>\n"",
+       ""      <td>32</td>\n"",
+       ""      <td>endo</td>\n"",
+       ""      <td>endo_thalamus</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW25_Endo_24Endo</td>\n"",
+       ""      <td>Endo_45</td>\n"",
+       ""      <td>2293</td>\n"",
+       ""      <td>5242</td>\n"",
+       ""      <td>5.000000e-153</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>5242</td>\n"",
+       ""      <td>2293</td>\n"",
+       ""      <td>GW25_thalamus_TTTGTCACATGGTTGT</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>691924</th>\n"",
+       ""      <td>GW25_thalamus_TTTGTCAGTAGAAGGA</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>GW25</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Astrocyte_4</td>\n"",
+       ""      <td>4</td>\n"",
+       ""      <td>astrocyte</td>\n"",
+       ""      <td>astrocyte_thalamus</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW25_Astrocyte_17Astrocyte</td>\n"",
+       ""      <td>Astrocyte_39</td>\n"",
+       ""      <td>1846</td>\n"",
+       ""      <td>3490</td>\n"",
+       ""      <td>2.260000e-115</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>3490</td>\n"",
+       ""      <td>1846</td>\n"",
+       ""      <td>GW25_thalamus_TTTGTCAGTAGAAGGA</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>691925</th>\n"",
+       ""      <td>GW25_thalamus_TTTGTCAGTAGATTAG</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>GW25</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Oligo_6</td>\n"",
+       ""      <td>165</td>\n"",
+       ""      <td>oligo</td>\n"",
+       ""      <td>oligo_thalamus</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW25_Oligo_19Oligo</td>\n"",
+       ""      <td>Oligo_45</td>\n"",
+       ""      <td>3094</td>\n"",
+       ""      <td>6232</td>\n"",
+       ""      <td>1.100000e-192</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>6232</td>\n"",
+       ""      <td>3094</td>\n"",
+       ""      <td>GW25_thalamus_TTTGTCAGTAGATTAG</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>691926</th>\n"",
+       ""      <td>GW25_thalamus_TTTGTCAGTTCGGCAC</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>GW25</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Astrocyte_4</td>\n"",
+       ""      <td>4</td>\n"",
+       ""      <td>astrocyte</td>\n"",
+       ""      <td>astrocyte_thalamus</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW25_Astrocyte_17Astrocyte</td>\n"",
+       ""      <td>Astrocyte_39</td>\n"",
+       ""      <td>1690</td>\n"",
+       ""      <td>3270</td>\n"",
+       ""      <td>1.360000e-94</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>3270</td>\n"",
+       ""      <td>1690</td>\n"",
+       ""      <td>GW25_thalamus_TTTGTCAGTTCGGCAC</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>691927</th>\n"",
+       ""      <td>GW25_thalamus_TTTGTCATCCTAGGGC</td>\n"",
+       ""      <td>25</td>\n"",
+       ""      <td>GW25</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>thalamus</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>Astrocyte_4</td>\n"",
+       ""      <td>4</td>\n"",
+       ""      <td>astrocyte</td>\n"",
+       ""      <td>astrocyte_thalamus</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>GW25_Astrocyte_17Astrocyte</td>\n"",
+       ""      <td>Astrocyte_39</td>\n"",
+       ""      <td>1728</td>\n"",
+       ""      <td>3407</td>\n"",
+       ""      <td>4.690000e-134</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>NaN</td>\n"",
+       ""      <td>3407</td>\n"",
+       ""      <td>1728</td>\n"",
+       ""      <td>GW25_thalamus_TTTGTCATCCTAGGGC</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>691928 rows × 25 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""                             cell.name  age individual structure      area  \\\n"",
+       ""0            GW14_CGE_AAACCTGGTAAAGGAG   14       GW14        GE       CGE   \n"",
+       ""1            GW14_CGE_AAACCTGGTTCCGTCT   14       GW14        GE       CGE   \n"",
+       ""2            GW14_CGE_AAACCTGTCGTCCAGG   14       GW14        GE       CGE   \n"",
+       ""3            GW14_CGE_AAACGGGCAGCCAGAA   14       GW14        GE       CGE   \n"",
+       ""4            GW14_CGE_AAACGGGGTGAACCTT   14       GW14        GE       CGE   \n"",
+       ""...                                ...  ...        ...       ...       ...   \n"",
+       ""691923  GW25_thalamus_TTTGTCACATGGTTGT   25       GW25  thalamus  thalamus   \n"",
+       ""691924  GW25_thalamus_TTTGTCAGTAGAAGGA   25       GW25  thalamus  thalamus   \n"",
+       ""691925  GW25_thalamus_TTTGTCAGTAGATTAG   25       GW25  thalamus  thalamus   \n"",
+       ""691926  GW25_thalamus_TTTGTCAGTTCGGCAC   25       GW25  thalamus  thalamus   \n"",
+       ""691927  GW25_thalamus_TTTGTCATCCTAGGGC   25       GW25  thalamus  thalamus   \n"",
+       ""\n"",
+       ""       area.sub       clusterv2  clusterv2_id cell.type.v2  \\\n"",
+       ""0           NaN          RG_4RG           191   radialglia   \n"",
+       ""1           NaN      Dividing_3            20     dividing   \n"",
+       ""2           NaN      Dividing_5            25     dividing   \n"",
+       ""3           NaN  Neuron_4Neuron           140       neuron   \n"",
+       ""4           NaN      Dividing_5            25     dividing   \n"",
+       ""...         ...             ...           ...          ...   \n"",
+       ""691923      NaN         Endo_11            32         endo   \n"",
+       ""691924      NaN     Astrocyte_4             4    astrocyte   \n"",
+       ""691925      NaN         Oligo_6           165        oligo   \n"",
+       ""691926      NaN     Astrocyte_4             4    astrocyte   \n"",
+       ""691927      NaN     Astrocyte_4             4    astrocyte   \n"",
+       ""\n"",
+       ""        celltype.structure  ...                   matchname  \\\n"",
+       ""0            radialglia_GE  ...                 GW14_RG_5RG   \n"",
+       ""1              dividing_GE  ...    GW14_Dividing_29Dividing   \n"",
+       ""2              dividing_GE  ...     GW14_Dividing_8Dividing   \n"",
+       ""3                neuron_GE  ...        GW14_Neuron_11Neuron   \n"",
+       ""4              dividing_GE  ...     GW14_Dividing_8Dividing   \n"",
+       ""...                    ...  ...                         ...   \n"",
+       ""691923       endo_thalamus  ...            GW25_Endo_24Endo   \n"",
+       ""691924  astrocyte_thalamus  ...  GW25_Astrocyte_17Astrocyte   \n"",
+       ""691925      oligo_thalamus  ...          GW25_Oligo_19Oligo   \n"",
+       ""691926  astrocyte_thalamus  ...  GW25_Astrocyte_17Astrocyte   \n"",
+       ""691927  astrocyte_thalamus  ...  GW25_Astrocyte_17Astrocyte   \n"",
+       ""\n"",
+       ""                       clusterv1 ngene  numi   percent.mito percent.ribo  \\\n"",
+       ""0                           RG_4   869  1481   1.290000e-13     0.232275   \n"",
+       ""1       GW14_Dividing_29Dividing   805  1845   1.220000e-37     0.270461   \n"",
+       ""2                    Dividing_37  1131  3101   5.790000e-19     0.341503   \n"",
+       ""3                       Neuron_4   776  1451   4.320000e-34     0.216402   \n"",
+       ""4                    Dividing_37  1097  2780   2.550000e-46     0.267986   \n"",
+       ""...                          ...   ...   ...            ...          ...   \n"",
+       ""691923                   Endo_45  2293  5242  5.000000e-153          NaN   \n"",
+       ""691924              Astrocyte_39  1846  3490  2.260000e-115          NaN   \n"",
+       ""691925                  Oligo_45  3094  6232  1.100000e-192          NaN   \n"",
+       ""691926              Astrocyte_39  1690  3270   1.360000e-94          NaN   \n"",
+       ""691927              Astrocyte_39  1728  3407  4.690000e-134          NaN   \n"",
+       ""\n"",
+       ""          v1  ncount_rna  nfeature_rna                     cell.name.1  \n"",
+       ""0        5.0        1481           869       GW14_CGE_AAACCTGGTAAAGGAG  \n"",
+       ""1       29.0        1845           805       GW14_CGE_AAACCTGGTTCCGTCT  \n"",
+       ""2        8.0        3101          1131       GW14_CGE_AAACCTGTCGTCCAGG  \n"",
+       ""3       11.0        1451           776       GW14_CGE_AAACGGGCAGCCAGAA  \n"",
+       ""4        8.0        2780          1097       GW14_CGE_AAACGGGGTGAACCTT  \n"",
+       ""...      ...         ...           ...                             ...  \n"",
+       ""691923   NaN        5242          2293  GW25_thalamus_TTTGTCACATGGTTGT  \n"",
+       ""691924   NaN        3490          1846  GW25_thalamus_TTTGTCAGTAGAAGGA  \n"",
+       ""691925   NaN        6232          3094  GW25_thalamus_TTTGTCAGTAGATTAG  \n"",
+       ""691926   NaN        3270          1690  GW25_thalamus_TTTGTCAGTTCGGCAC  \n"",
+       ""691927   NaN        3407          1728  GW25_thalamus_TTTGTCATCCTAGGGC  \n"",
+       ""\n"",
+       ""[691928 rows x 25 columns]""
+      ]
+     },
+     ""execution_count"": 4,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2 = pd.read_csv('../data/external/whole_brain_bhaduri_labels.tsv', sep='\\t')\n"",
+    ""df2""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""7649058f"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(691928, 25)""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 6,
+   ""id"": ""2349f1a8"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""Index(['cell.name', 'age', 'individual', 'structure', 'area', 'area.sub',\n"",
+       ""       'clusterv2', 'clusterv2_id', 'cell.type.v2', 'celltype.structure',\n"",
+       ""       'lamina', 'cluster', 'celltype', 'cellabbreviation', 'clustercelltype',\n"",
+       ""       'matchname', 'clusterv1', 'ngene', 'numi', 'percent.mito',\n"",
+       ""       'percent.ribo', 'v1', 'ncount_rna', 'nfeature_rna', 'cell.name.1'],\n"",
+       ""      dtype='object')""
+      ]
+     },
+     ""execution_count"": 6,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2.columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 7,
+   ""id"": ""da3e8f41"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Unique values for col cell.name are ['GW14_CGE_AAACCTGGTAAAGGAG' 'GW14_CGE_AAACCTGGTTCCGTCT'\n"",
+      "" 'GW14_CGE_AAACCTGTCGTCCAGG' ... 'GW25_thalamus_TTTGTCAGTAGATTAG'\n"",
+      "" 'GW25_thalamus_TTTGTCAGTTCGGCAC' 'GW25_thalamus_TTTGTCATCCTAGGGC']\n"",
+      ""Unique values for col age are [14 17 18 19 20 22 25]\n"",
+      ""Unique values for col individual are ['GW14' 'GW17' 'GW18' 'GW18_2' 'GW19' 'GW20' 'GW20_31' 'GW20_34' 'GW22'\n"",
+      "" 'GW22T' 'GW25']\n"",
+      ""Unique values for col structure are ['GE' 'hypothalamus' 'neocortex' 'striatum' 'thalamus' 'allocortex'\n"",
+      "" 'claustrum' 'cerebellum' 'proneocortex' 'midbrain']\n"",
+      ""Unique values for col area are ['CGE' 'LGE' 'MGE' 'hypothalamus' 'motor' 'V1' 'somatosensory' 'striatum'\n"",
+      "" 'thalamus' 'parietal' 'PFC' 'hippocampus' 'temporal' 'claustrum'\n"",
+      "" 'preoptic' 'putamen' 'cerebellum' 'caudate' 'nucleusaccumbens'\n"",
+      "" 'cingulate' 'insula' 'midbrain']\n"",
+      ""Unique values for col area.sub are [nan 'ventral' 'caudal' 'dorsal' 'ventral_medial' 'left_hemisphere'\n"",
+      "" '3rd_ventricle' 'anterior' 'pulminar']\n"",
+      ""Unique values for col clusterv2 are ['RG_4RG' 'Dividing_3' 'Dividing_5' 'Neuron_4Neuron' 'Neuron_37'\n"",
+      "" 'Dividing_4' 'Dividing_2' 'RG_8' 'Neuron_5' 'RG_45RG' 'Endo_15'\n"",
+      "" 'Astrocyte_5' 'GW14_Microglia_27MicrogliaMicroglia' 'Endo_5'\n"",
+      "" 'Interneuron_3' 'Neuron_100Neuron' 'Neuron_22' 'Interneuron_1'\n"",
+      "" 'Interneuron_24Interneuron' 'RG_47RG' 'RG_3' 'Endo_13' 'Neuron_53Neuron'\n"",
+      "" 'RG_15' 'Neuron_26' 'Neuron_24' 'RG_14' 'Dividing_13' 'Neuron_25'\n"",
+      "" 'Neuron_102Neuron' 'Dividing_48Dividing' 'Dividing_18Dividing'\n"",
+      "" 'Dividing_16' 'RG_46RG' 'Neuron_23' 'GW17_Vascular_21VascularVascular'\n"",
+      "" 'Neuron_19' 'Dividing_7' 'Neuron_36' 'RG_7' 'Endo_7' 'Endo_6' 'Neuron_27'\n"",
+      "" 'Neuron_11' 'GW17_Vascular_14VascularVascular' 'RG_13' 'Dividing_12'\n"",
+      "" 'Dividing_8' 'Neuron_10' 'Neuron_9' 'Dividing_15' 'RG_16' 'Endo_16'\n"",
+      "" 'Dividing_15Dividing' 'Interneuron_7' 'Dividing_6' 'RG_22RG'\n"",
+      "" 'Microglia_2' 'Interneuron_43Interneuron' 'RG_4' 'Interneuron_5'\n"",
+      "" 'Oligo_6' 'RG_11' 'Microglia_3' 'Oligo_12' 'Neuron_13' 'Neuron_72Neuron'\n"",
+      "" 'GW18_Astrocyte_26AstrocyteAstrocyte' 'RG_2RG' 'Dividing_9' 'Neuron_15'\n"",
+      "" 'Neuron_3' 'Neuron_101Neuron' 'Neuron_18' 'IPC_18IPC'\n"",
+      "" 'GW18_Astrocyte_17AstrocyteAstrocyte' 'Neuron_16' 'Neuron_50Neuron'\n"",
+      "" 'Neuron_37Neuron' 'Neuron_33' 'Endo_10' 'Neuron_12' 'Neuron_29' 'Oligo_9'\n"",
+      "" 'Oligo_7' 'GW18_Astrocyte_51AstrocyteAstrocyte' 'Astrocyte_2'\n"",
+      "" 'Interneuron_9' 'Neuron_31' 'Dividing_1' 'RG_15RG' 'RG_52RG' 'Endo_11'\n"",
+      "" 'Interneuron_6' 'Neuron_92Neuron' 'Neuron_34' 'Neuron_93Neuron'\n"",
+      "" 'GW18_2_45OligoOligo' 'Astrocyte_1' 'Oligo_11' 'Endo_9' 'IPC_34IPC'\n"",
+      "" 'Neuron_32' 'RG_10' 'Neuron_39Neuron' 'Dividing_21Dividing'\n"",
+      "" 'GW18_2_42NeuronNeuron' 'Endo_1' 'Neuron_28' 'Neuron_30' 'Microglia_6'\n"",
+      "" 'Dividing_14' 'RG_3RG' 'Interneuron_4'\n"",
+      "" 'GW19_Astrocyte_53AstrocyteAstrocyte'\n"",
+      "" 'GW19_Astrocyte_35AstrocyteAstrocyte' 'GW19_Dividing_39DividingDividing'\n"",
+      "" 'RG_12' 'Neuron_6' 'Dividing_10' 'Microglia_5' 'Endo_12' 'RG_17'\n"",
+      "" 'Neuron_17' 'Neuron_35Neuron' 'Neuron_1' 'IPC_29IPC' 'RG_59RG'\n"",
+      "" 'Neuron_14' 'Oligo_2' 'Neuron_11Neuron' 'Interneuron_31Interneuron'\n"",
+      "" 'GW19_2_29NeuronNeuron' 'GW19_2_45OutlierrOutlierr' 'Neuron_8'\n"",
+      "" 'GW19_2_30RGRG' 'GW19_2_43MicrogliaMicroglia' 'Oligo_4' 'Neuron_66Neuron'\n"",
+      "" 'Oligo_1' 'Endo_4' 'Endo_14' 'RG_13RG' 'Microglia_4' 'Endo_8'\n"",
+      "" 'Endo_34Endo' 'Oligo_3' 'GW19_2_1OutlierrOutlierr' 'Oligo_8' 'Neuron_21'\n"",
+      "" 'Neuron_33Neuron' 'RG_11RG' 'RG_50RG'\n"",
+      "" 'GW20_Interneuron_32InterneuronInterneuron' 'Interneuron_10'\n"",
+      "" 'GW20_Interneuron_34InterneuronInterneuron' 'RG_1'\n"",
+      "" 'GW20_Astrocyte_39AstrocyteAstrocyte' 'Interneuron_2' 'Astrocyte_4'\n"",
+      "" 'Neuron_2Neuron' 'Neuron_7' 'Astrocyte_6' 'Neuron_4' 'Endo_3'\n"",
+      "" 'Microglia_15Microglia' 'Astrocyte_3' 'Neuron_5Neuron' 'Neuron_2'\n"",
+      "" 'Dividing_13Dividing' 'Neuron_35' 'RG_9' 'Neuron_36Neuron'\n"",
+      "" 'Interneuron_8' 'Neuron_98Neuron' 'GW22both_RG_23RGRG'\n"",
+      "" 'GW22both_RG_38RGRG' 'Neuron_68Neuron' 'Oligo_5' 'Microglia_1'\n"",
+      "" 'Neuron_10Neuron' 'RG_5' 'Dividing_41Dividing' 'Microglia_16Microglia'\n"",
+      "" 'Endo_2' 'Microglia_76Microglia' 'Endo_26Endo' 'Microglia_8Microglia'\n"",
+      "" 'Dividing_11' 'Neuron_64Neuron' 'Endo_4Endo' 'RG_6' 'Dividing_42Dividing'\n"",
+      "" 'Microglia_10Microglia' 'Neuron_63Neuron' 'RG_37RG' 'Neuron_20']\n"",
+      ""Unique values for col clusterv2_id are [191  20  25 140 136  21  18 198 141 188  36   5  49  44  74  99 116  70\n"",
+      ""  73 190 184  34 143 177 120 118 176  11 119 101  24  17  16 189 117  51\n"",
+      "" 112  27 134 197  46  45 121 103  50 174  10  28  98 153  14 179  37  15\n"",
+      ""  80  26 182  90  77 187  78 165 171  91 160 106 151  55 183  29 108 125\n"",
+      "" 100 111  83  54 109 142 137 129  31 105 123 168 166  56   2  82 127   7\n"",
+      "" 178 194  32  79 154 131 155  53   1 159  48  85 128 170 138  19  52  30\n"",
+      "" 122 126  94  13 186  76  63  62  64 173 145   8  93  33 180 110 133  97\n"",
+      ""  84 195 107 161 104  75  58  61 152  59  60 163 148 157  42  35 175  92\n"",
+      ""  47  41 162  57 167 115 130 172 193  66  71  67 169  65  72   4 124 150\n"",
+      ""   6 139  40  88   3 144 113  12 132 199 135  81 156  68  69 149 164  86\n"",
+      "" 102 192  22  89  38  95  39  96   9 147  43 196  23  87 146 185 114]\n"",
+      ""Unique values for col cell.type.v2 are ['radialglia' 'dividing' 'neuron' 'endo' 'astrocyte' 'microglia'\n"",
+      "" 'interneuron' 'vascular' 'oligo' 'ipc' 'outlier']\n"",
+      ""Unique values for col celltype.structure are ['radialglia_GE' 'dividing_GE' 'neuron_GE' 'endo_GE' 'astrocyte_GE'\n"",
+      "" 'microglia_GE' 'interneuron_GE' 'endo_hypothalamus' 'neuron_hypothalamus'\n"",
+      "" 'radialglia_hypothalamus' 'dividing_hypothalamus'\n"",
+      "" 'astrocyte_hypothalamus' 'microglia_hypothalamus' 'neuron_neocortex'\n"",
+      "" 'endo_neocortex' 'radialglia_neocortex' 'dividing_neocortex'\n"",
+      "" 'interneuron_neocortex' 'microglia_neocortex' 'astrocyte_neocortex'\n"",
+      "" 'dividing_striatum' 'neuron_striatum' 'radialglia_striatum'\n"",
+      "" 'astrocyte_striatum' 'endo_striatum' 'interneuron_striatum'\n"",
+      "" 'microglia_striatum' 'neuron_thalamus' 'endo_thalamus'\n"",
+      "" 'astrocyte_thalamus' 'radialglia_thalamus' 'dividing_thalamus'\n"",
+      "" 'interneuron_thalamus' 'microglia_thalamus' 'vascular_neocortex'\n"",
+      "" 'oligo_GE' 'ipc_neocortex' 'oligo_neocortex' 'radialglia_allocortex'\n"",
+      "" 'neuron_allocortex' 'interneuron_allocortex' 'microglia_allocortex'\n"",
+      "" 'astrocyte_allocortex' 'dividing_allocortex' 'oligo_allocortex'\n"",
+      "" 'endo_allocortex' 'ipc_allocortex' 'oligo_hypothalamus'\n"",
+      "" 'interneuron_hypothalamus' 'oligo_thalamus' 'interneuron_claustrum'\n"",
+      "" 'neuron_claustrum' 'astrocyte_claustrum' 'oligo_claustrum'\n"",
+      "" 'microglia_claustrum' 'radialglia_claustrum' 'endo_claustrum'\n"",
+      "" 'dividing_claustrum' 'outlier_neocortex' 'outlier_GE' 'oligo_striatum'\n"",
+      "" 'neuron_cerebellum' 'astrocyte_cerebellum' 'microglia_cerebellum'\n"",
+      "" 'endo_cerebellum' 'interneuron_cerebellum' 'radialglia_cerebellum'\n"",
+      "" 'oligo_cerebellum' 'dividing_cerebellum' 'neuron_proneocortex'\n"",
+      "" 'oligo_proneocortex' 'interneuron_proneocortex' 'astrocyte_proneocortex'\n"",
+      "" 'endo_proneocortex' 'radialglia_proneocortex' 'microglia_proneocortex'\n"",
+      "" 'dividing_proneocortex' 'endo_midbrain' 'radialglia_midbrain'\n"",
+      "" 'oligo_midbrain' 'neuron_midbrain' 'astrocyte_midbrain'\n"",
+      "" 'dividing_midbrain' 'microglia_midbrain' 'interneuron_midbrain']\n"",
+      ""Unique values for col lamina are ['all' 'VZ' 'CP' 'UL' 'VZSVZ' 'DL' 'MZ' 'OSVZ']\n"",
+      ""Unique values for col cluster are [ 5 29  8 11 12  1 24 32 14 13 26 16  3 25  7 31 19 27 30 20 10 21  4  6\n"",
+      ""  9 22 28 33 17 23 18 15  2 35 34 36 39 49 38 46 44 50 43 42 48 45 40 37\n"",
+      "" 47 41 51 54 53 52]\n"",
+      ""Unique values for col celltype are ['Radial Glia' 'Dividing' 'Excitatory Neuron' 'Outlier' 'Endothelial'\n"",
+      "" 'Astrocyte' 'Microglia' 'Interneuron' 'Neuron' 'RG' 'IPC' 'Vascular' 'CR'\n"",
+      "" 'Oligodendrocyte']\n"",
+      ""Unique values for col cellabbreviation are ['RG' 'Dividing' 'Neuron' 'Outlier' 'Endo' 'Astrocyte' 'Microglia'\n"",
+      "" 'Interneuron' 'IPC' 'Vascular' 'Oligo']\n"",
+      ""Unique values for col clustercelltype are ['GW14_RG_5' 'GW14_Dividing_29' 'GW14_Dividing_8' 'GW14_Neuron_11'\n"",
+      "" 'GW14_Outlier_12' 'GW14_Outlier_1' 'GW14_Dividing_24' 'GW14_Dividing_32'\n"",
+      "" 'GW14_RG_14' 'GW14_Dividing_13' 'GW14_Neuron_26' 'GW14_Neuron_16'\n"",
+      "" 'GW14_RG_3' 'GW14_Neuron_25' 'GW14_RG_7' 'GW14_Endo_31'\n"",
+      "" 'GW14_Astrocyte_19' 'GW14_Microglia_27' 'GW14_Endo_30' 'GW14_Neuron_20'\n"",
+      "" 'GW14_Neuron_10' 'GW14_Neuron_21' 'GW14_Interneuron_4'\n"",
+      "" 'GW14_Interneuron_6' 'GW14_RG_9' 'GW14_Interneuron_22' 'GW14_Dividing_28'\n"",
+      "" 'GW14_Dividing_33' 'GW14_Dividing_17' 'GW14_RG_23' 'GW14_Outlier_18'\n"",
+      "" 'GW14_Endo_15' 'GW14_Neuron_2' 'GW17_Neuron_18' 'GW17_Dividing_27'\n"",
+      "" 'GW17_Neuron_19' 'GW17_Neuron_20' 'GW17_RG_24' 'GW17_IPC_23'\n"",
+      "" 'GW17_Dividing_31' 'GW17_Neuron_15' 'GW17_Dividing_7' 'GW17_Neuron_29'\n"",
+      "" 'GW17_Neuron_10' 'GW17_Dividing_2' 'GW17_Dividing_3' 'GW17_Dividing_35'\n"",
+      "" 'GW17_RG_6' 'GW17_Neuron_28' 'GW17_Vascular_21' 'GW17_Neuron_4'\n"",
+      "" 'GW17_Dividing_30' 'GW17_IPC_13' 'GW17_RG_1' 'GW17_Neuron_8'\n"",
+      "" 'GW17_Vascular_11' 'GW17_Vascular_5' 'GW17_Neuron_32' 'GW17_Neuron_34'\n"",
+      "" 'GW17_Outlier_33' 'GW17_Vascular_14' 'GW17_RG_17' 'GW17_Dividing_25'\n"",
+      "" 'GW17_Dividing_16' 'GW17_IPC_22' 'GW17_Neuron_26' 'GW17_Dividing_36'\n"",
+      "" 'GW17_RG_9' 'GW17_Vascular_12' 'GW18_Dividing_32' 'GW18_Interneuron_14'\n"",
+      "" 'GW18_Outlier_9' 'GW18_Dividing_39' 'GW18_Dividing_36' 'GW18_Dividing_30'\n"",
+      "" 'GW18_Interneuron_25' 'GW18_Dividing_49' 'GW18_RG_19' 'GW18_Microglia_35'\n"",
+      "" 'GW18_Interneuron_6' 'GW18_RG_38' 'GW18_RG_46' 'GW18_Interneuron_24'\n"",
+      "" 'GW18_Interneuron_10' 'GW18_Oligo_33' 'GW18_Endo_3' 'GW18_RG_31'\n"",
+      "" 'GW18_Microglia_20' 'GW18_Oligo_15' 'GW18_Neuron_44' 'GW18_Neuron_13'\n"",
+      "" 'GW18_Astrocyte_26' 'GW18_RG_2' 'GW18_IPC_50' 'GW18_Neuron_11'\n"",
+      "" 'GW18_Neuron_43' 'GW18_Neuron_23' 'GW18_Neuron_12' 'GW18_IPC_5'\n"",
+      "" 'GW18_IPC_42' 'GW18_Neuron_27' 'GW18_Astrocyte_17' 'GW18_Neuron_18'\n"",
+      "" 'GW18_IPC_4' 'GW18_Neuron_21' 'GW18_Oligo_48' 'GW18_Neuron_16'\n"",
+      "" 'GW18_Neuron_22' 'GW18_Neuron_7' 'GW18_Outlier_8' 'GW18_Neuron_29'\n"",
+      "" 'GW18_Endo_45' 'GW18_Outlier_1' 'GW18_Neuron_34' 'GW18_Neuron_40'\n"",
+      "" 'GW18_Neuron_28' 'GW18_Neuron_37' 'GW18_RG_47' 'GW18_Neuron_41'\n"",
+      "" 'GW18_Astrocyte_51' 'GW18_2_4' 'GW18_2_39' 'GW18_2_11' 'GW18_2_21'\n"",
+      "" 'GW18_2_35' 'GW18_2_43' 'GW18_2_10' 'GW18_2_29' 'GW18_2_22' 'GW18_2_16'\n"",
+      "" 'GW18_2_24' 'GW18_2_18' 'GW18_2_25' 'GW18_2_1' 'GW18_2_41' 'GW18_2_7'\n"",
+      "" 'GW18_2_20' 'GW18_2_5' 'GW18_2_28' 'GW18_2_37' 'GW18_2_38' 'GW18_2_17'\n"",
+      "" 'GW18_2_45' 'GW18_2_40' 'GW18_2_3' 'GW18_2_32' 'GW18_2_30' 'GW18_2_26'\n"",
+      "" 'GW18_2_2' 'GW18_2_44' 'GW18_2_31' 'GW18_2_33' 'GW18_2_19' 'GW18_2_15'\n"",
+      "" 'GW18_2_34' 'GW18_2_42' 'GW18_2_36' 'GW18_2_13' 'GW18_2_8' 'GW18_2_27'\n"",
+      "" 'GW18_2_6' 'GW18_2_9' 'GW18_2_14' 'GW18_2_12' 'GW18_2_23' 'GW19_RG_44'\n"",
+      "" 'GW19_Outlier_8' 'GW19_Dividing_54' 'GW19_Interneuron_14'\n"",
+      "" 'GW19_Dividing_5' 'GW19_Dividing_51' 'GW19_RG_16' 'GW19_Dividing_43'\n"",
+      "" 'GW19_Microglia_31' 'GW19_Interneuron_49' 'GW19_Astrocyte_53' 'GW19_RG_9'\n"",
+      "" 'GW19_Microglia_22' 'GW19_Endo_19' 'GW19_Astrocyte_35' 'GW19_RG_18'\n"",
+      "" 'GW19_Dividing_41' 'GW19_Dividing_39' 'GW19_Dividing_48'\n"",
+      "" 'GW19_Dividing_32' 'GW19_Oligo_45' 'GW19_Interneuron_21'\n"",
+      "" 'GW19_Interneuron_13' 'GW19_Dividing_47' 'GW19_Neuron_10' 'GW19_Neuron_7'\n"",
+      "" 'GW19_Oligo_30' 'GW19_Endo_12' 'GW19_Microglia_50' 'GW19_Endo_34'\n"",
+      "" 'GW19_Oligo_37' 'GW19_Neuron_6' 'GW19_Neuron_52' 'GW19_Neuron_40'\n"",
+      "" 'GW19_Neuron_27' 'GW19_Neuron_29' 'GW19_Neuron_20' 'GW19_Neuron_42'\n"",
+      "" 'GW19_IPC_15' 'GW19_Dividing_46' 'GW19_Outlier_23' 'GW19_Neuron_33'\n"",
+      "" 'GW19_RG_25' 'GW19_RG_28' 'GW19_Outlier_4' 'GW19_Neuron_38'\n"",
+      "" 'GW19_Neuron_36' 'GW19_Dividing_3' 'GW19_Outlier_11' 'GW19_Oligo_26'\n"",
+      "" 'GW19_Outlier_17' 'GW19_Outlier_2' 'GW19_Outlier_1' 'GW19_Neuron_24'\n"",
+      "" 'GW19_2_39' 'GW19_2_17' 'GW19_2_31' 'GW19_2_29' 'GW19_2_44' 'GW19_2_13'\n"",
+      "" 'GW19_2_40' 'GW19_2_23' 'GW19_2_24' 'GW19_2_45' 'GW19_2_12' 'GW19_2_33'\n"",
+      "" 'GW19_2_9' 'GW19_2_42' 'GW19_2_3' 'GW19_2_10' 'GW19_2_36' 'GW19_2_27'\n"",
+      "" 'GW19_2_30' 'GW19_2_37' 'GW19_2_20' 'GW19_2_32' 'GW19_2_38' 'GW19_2_43'\n"",
+      "" 'GW19_2_28' 'GW19_2_11' 'GW19_2_14' 'GW19_2_25' 'GW19_2_19' 'GW19_2_26'\n"",
+      "" 'GW19_2_18' 'GW19_2_41' 'GW19_2_6' 'GW19_2_8' 'GW19_2_4' 'GW19_2_15'\n"",
+      "" 'GW19_2_21' 'GW19_2_7' 'GW19_2_1' 'GW19_2_16' 'GW19_2_5' 'GW20_Neuron_1'\n"",
+      "" 'GW20_Neuron_6' 'GW20_RG_21' 'GW20_Oligo_14' 'GW20_RG_33'\n"",
+      "" 'GW20_Dividing_42' 'GW20_Dividing_16' 'GW20_Neuron_18' 'GW20_Endo_20'\n"",
+      "" 'GW20_Dividing_36' 'GW20_Microglia_37' 'GW20_RG_29' 'GW20_Interneuron_32'\n"",
+      "" 'GW20_Interneuron_5' 'GW20_Microglia_10' 'GW20_Microglia_22'\n"",
+      "" 'GW20_Dividing_24' 'GW20_Outlier_2' 'GW20_Dividing_35'\n"",
+      "" 'GW20_Interneuron_34' 'GW20_Endo_17' 'GW20_Microglia_30' 'GW20_Endo_13'\n"",
+      "" 'GW20_RG_15' 'GW20_Dividing_40' 'GW20_Neuron_25' 'GW20_Astrocyte_39'\n"",
+      "" 'GW20_Dividing_23' 'GW20_Interneuron_12' 'GW20_Astrocyte_38'\n"",
+      "" 'GW20_Interneuron_8' 'GW20_Neuron_26' 'GW20_Neuron_27' 'GW20_Endo_11'\n"",
+      "" 'GW20_Astrocyte_3' 'GW20_Neuron_31' 'GW20_Neuron_9' 'GW20_Microglia_4'\n"",
+      "" 'GW20_RG_19' 'GW20_Neuron_41' 'GW20_Neuron_7' 'GW20_Dividing_28'\n"",
+      "" 'GW20_31and34_16' 'GW20_31and34_34' 'GW20_31and34_38' 'GW20_31and34_30'\n"",
+      "" 'GW20_31and34_13' 'GW20_31and34_8' 'GW20_31and34_31' 'GW20_31and34_11'\n"",
+      "" 'GW20_31and34_25' 'GW20_31and34_17' 'GW20_31and34_35' 'GW20_31and34_2'\n"",
+      "" 'GW20_31and34_15' 'GW20_31and34_27' 'GW20_31and34_4' 'GW20_31and34_33'\n"",
+      "" 'GW20_31and34_12' 'GW20_31and34_21' 'GW20_31and34_37' 'GW20_31and34_10'\n"",
+      "" 'GW20_31and34_36' 'GW20_31and34_26' 'GW20_31and34_9' 'GW20_31and34_28'\n"",
+      "" 'GW20_31and34_22' 'GW20_31and34_29' 'GW20_31and34_6' 'GW20_31and34_19'\n"",
+      "" 'GW20_31and34_32' 'GW20_31and34_14' 'GW20_31and34_20' 'GW20_31and34_7'\n"",
+      "" 'GW20_31and34_1' 'GW20_31and34_23' 'GW20_31and34_24' 'GW20_31and34_5'\n"",
+      "" 'GW20_31and34_18' 'GW20_31and34_3' 'GW22both_Dividing_15'\n"",
+      "" 'GW22both_Outlier_30' 'GW22both_Interneuron_13' 'GW22both_RG_23'\n"",
+      "" 'GW22both_Dividing_20' 'GW22both_Interneuron_6' 'GW22both_Neuron_11'\n"",
+      "" 'GW22both_RG_38' 'GW22both_Neuron_44' 'GW22both_Neuron_18'\n"",
+      "" 'GW22both_Microglia_3' 'GW22both_Dividing_2' 'GW22both_RG_17'\n"",
+      "" 'GW22both_Oligo_9' 'GW22both_Endo_37' 'GW22both_Outlier_31'\n"",
+      "" 'GW22both_Oligo_8' 'GW22both_Dividing_36' 'GW22both_Neuron_28'\n"",
+      "" 'GW22both_Dividing_29' 'GW22both_Oligo_34' 'GW22both_Dividing_25'\n"",
+      "" 'GW22both_Microglia_24' 'GW22both_Dividing_14' 'GW22both_Microglia_22'\n"",
+      "" 'GW22both_Neuron_27' 'GW22both_Dividing_33' 'GW22both_Endo_4'\n"",
+      "" 'GW22both_Microglia_43' 'GW22both_Dividing_40' 'GW22both_Oligo_16'\n"",
+      "" 'GW22both_Endo_32' 'GW22both_RG_39' 'GW22both_Neuron_12' 'GW22both_RG_42'\n"",
+      "" 'GW22both_Dividing_35' 'GW22both_Microglia_7' 'GW22both_Interneuron_21'\n"",
+      "" 'GW22both_Interneuron_26' 'GW22both_Dividing_10' 'GW22both_Endo_41'\n"",
+      "" 'GW22both_Microglia_19' 'GW22both_Endo_5' 'GW22both_Outlier_1'\n"",
+      "" 'GW25_Oligo_33' 'GW25_Microglia_1' 'GW25_Astrocyte_17' 'GW25_Oligo_15'\n"",
+      "" 'GW25_Astrocyte_32' 'GW25_Dividing_34' 'GW25_Neuron_7' 'GW25_Dividing_2'\n"",
+      "" 'GW25_Neuron_21' 'GW25_Neuron_29' 'GW25_Outlier_36' 'GW25_Oligo_20'\n"",
+      "" 'GW25_Neuron_18' 'GW25_Dividing_25' 'GW25_Endo_24' 'GW25_Oligo_28'\n"",
+      "" 'GW25_Astrocyte_4' 'GW25_Endo_13' 'GW25_Dividing_22' 'GW25_Interneuron_5'\n"",
+      "" 'GW25_RG_27' 'GW25_Microglia_9' 'GW25_Dividing_10' 'GW25_Oligo_19'\n"",
+      "" 'GW25_Endo_26' 'GW25_Interneuron_14' 'GW25_Microglia_11' 'GW25_Endo_35'\n"",
+      "" 'GW25_Endo_30' 'GW25_RG_12' 'GW25_Outlier_6' 'GW25_Neuron_3'\n"",
+      "" 'GW25_Neuron_16' 'GW25_RG_23' 'GW25_Neuron_31' 'GW25_Neuron_8']\n"",
+      ""Unique values for col matchname are ['GW14_RG_5RG' 'GW14_Dividing_29Dividing' 'GW14_Dividing_8Dividing'\n"",
+      "" 'GW14_Neuron_11Neuron' 'GW14_Outlier_12Outlier' 'GW14_Outlier_1Outlier'\n"",
+      "" 'GW14_Dividing_24Dividing' 'GW14_Dividing_32Dividing' 'GW14_RG_14RG'\n"",
+      "" 'GW14_Dividing_13Dividing' 'GW14_Neuron_26Neuron' 'GW14_Neuron_16Neuron'\n"",
+      "" 'GW14_RG_3RG' 'GW14_Neuron_25Neuron' 'GW14_RG_7RG' 'GW14_Endo_31Endo'\n"",
+      "" 'GW14_Astrocyte_19Astrocyte' 'GW14_Microglia_27Microglia'\n"",
+      "" 'GW14_Endo_30Endo' 'GW14_Neuron_20Neuron' 'GW14_Neuron_10Neuron'\n"",
+      "" 'GW14_Neuron_21Neuron' 'GW14_Interneuron_4Interneuron'\n"",
+      "" 'GW14_Interneuron_6Interneuron' 'GW14_RG_9RG'\n"",
+      "" 'GW14_Interneuron_22Interneuron' 'GW14_Dividing_28Dividing'\n"",
+      "" 'GW14_Dividing_33Dividing' 'GW14_Dividing_17Dividing' 'GW14_RG_23RG'\n"",
+      "" 'GW14_Outlier_18Outlier' 'GW14_Endo_15Endo' 'GW14_Neuron_2Neuron'\n"",
+      "" 'GW17_Neuron_18Neuron' 'GW17_Dividing_27Dividing' 'GW17_Neuron_19Neuron'\n"",
+      "" 'GW17_Neuron_20Neuron' 'GW17_RG_24RG' 'GW17_IPC_23IPC'\n"",
+      "" 'GW17_Dividing_31Dividing' 'GW17_Neuron_15Neuron'\n"",
+      "" 'GW17_Dividing_7Dividing' 'GW17_Neuron_29Neuron' 'GW17_Neuron_10Neuron'\n"",
+      "" 'GW17_Dividing_2Dividing' 'GW17_Dividing_3Dividing'\n"",
+      "" 'GW17_Dividing_35Dividing' 'GW17_RG_6RG' 'GW17_Neuron_28Neuron'\n"",
+      "" 'GW17_Vascular_21Vascular' 'GW17_Neuron_4Neuron'\n"",
+      "" 'GW17_Dividing_30Dividing' 'GW17_IPC_13IPC' 'GW17_RG_1RG'\n"",
+      "" 'GW17_Neuron_8Neuron' 'GW17_Vascular_11Vascular'\n"",
+      "" 'GW17_Vascular_5Vascular' 'GW17_Neuron_32Neuron' 'GW17_Neuron_34Neuron'\n"",
+      "" 'GW17_Outlier_33Outlier' 'GW17_Vascular_14Vascular' 'GW17_RG_17RG'\n"",
+      "" 'GW17_Dividing_25Dividing' 'GW17_Dividing_16Dividing' 'GW17_IPC_22IPC'\n"",
+      "" 'GW17_Neuron_26Neuron' 'GW17_Dividing_36Dividing' 'GW17_RG_9RG'\n"",
+      "" 'GW17_Vascular_12Vascular' 'GW18_Dividing_32Dividing'\n"",
+      "" 'GW18_Interneuron_14Interneuron' 'GW18_Outlier_9Outlier'\n"",
+      "" 'GW18_Dividing_39Dividing' 'GW18_Dividing_36Dividing'\n"",
+      "" 'GW18_Dividing_30Dividing' 'GW18_Interneuron_25Interneuron'\n"",
+      "" 'GW18_Dividing_49Dividing' 'GW18_RG_19RG' 'GW18_Microglia_35Microglia'\n"",
+      "" 'GW18_Interneuron_6Interneuron' 'GW18_RG_38RG' 'GW18_RG_46RG'\n"",
+      "" 'GW18_Interneuron_24Interneuron' 'GW18_Interneuron_10Interneuron'\n"",
+      "" 'GW18_Oligo_33Oligo' 'GW18_Endo_3Endo' 'GW18_RG_31RG'\n"",
+      "" 'GW18_Microglia_20Microglia' 'GW18_Oligo_15Oligo' 'GW18_Neuron_44Neuron'\n"",
+      "" 'GW18_Neuron_13Neuron' 'GW18_Astrocyte_26Astrocyte' 'GW18_RG_2RG'\n"",
+      "" 'GW18_IPC_50IPC' 'GW18_Neuron_11Neuron' 'GW18_Neuron_43Neuron'\n"",
+      "" 'GW18_Neuron_23Neuron' 'GW18_Neuron_12Neuron' 'GW18_IPC_5IPC'\n"",
+      "" 'GW18_IPC_42IPC' 'GW18_Neuron_27Neuron' 'GW18_Astrocyte_17Astrocyte'\n"",
+      "" 'GW18_Neuron_18Neuron' 'GW18_IPC_4IPC' 'GW18_Neuron_21Neuron'\n"",
+      "" 'GW18_Oligo_48Oligo' 'GW18_Neuron_16Neuron' 'GW18_Neuron_22Neuron'\n"",
+      "" 'GW18_Neuron_7Neuron' 'GW18_Outlier_8Outlier' 'GW18_Neuron_29Neuron'\n"",
+      "" 'GW18_Endo_45Endo' 'GW18_Outlier_1Outlier' 'GW18_Neuron_34Neuron'\n"",
+      "" 'GW18_Neuron_40Neuron' 'GW18_Neuron_28Neuron' 'GW18_Neuron_37Neuron'\n"",
+      "" 'GW18_RG_47RG' 'GW18_Neuron_41Neuron' 'GW18_Astrocyte_51Astrocyte'\n"",
+      "" 'GW18_2_4RG' 'GW18_2_39Interneuron' 'GW18_2_11Interneuron'\n"",
+      "" 'GW18_2_21Interneuron' 'GW18_2_35Dividing' 'GW18_2_43Dividing'\n"",
+      "" 'GW18_2_10RG' 'GW18_2_29RG' 'GW18_2_22Interneuron' 'GW18_2_16Dividing'\n"",
+      "" 'GW18_2_24Endo' 'GW18_2_18Interneuron' 'GW18_2_25Dividing'\n"",
+      "" 'GW18_2_1Neuron' 'GW18_2_41Microglia' 'GW18_2_7Dividing'\n"",
+      "" 'GW18_2_20Astrocyte' 'GW18_2_5Oligo' 'GW18_2_28Microglia'\n"",
+      "" 'GW18_2_37Dividing' 'GW18_2_38RG' 'GW18_2_17Neuron' 'GW18_2_45Oligo'\n"",
+      "" 'GW18_2_40Astrocyte' 'GW18_2_3Oligo' 'GW18_2_32Dividing' 'GW18_2_30Endo'\n"",
+      "" 'GW18_2_26Neuron' 'GW18_2_2IPC' 'GW18_2_44Interneuron' 'GW18_2_31RG'\n"",
+      "" 'GW18_2_33Astrocyte' 'GW18_2_19Neuron' 'GW18_2_15Dividing'\n"",
+      "" 'GW18_2_34Neuron' 'GW18_2_42Neuron' 'GW18_2_36Endo' 'GW18_2_13Neuron'\n"",
+      "" 'GW18_2_8Neuron' 'GW18_2_27Interneuron' 'GW18_2_6Neuron' 'GW18_2_9Neuron'\n"",
+      "" 'GW18_2_14Neuron' 'GW18_2_12Microglia' 'GW18_2_23Astrocyte'\n"",
+      "" 'GW19_RG_44RG' 'GW19_Outlier_8Outlier' 'GW19_Dividing_54Dividing'\n"",
+      "" 'GW19_Interneuron_14Interneuron' 'GW19_Dividing_5Dividing'\n"",
+      "" 'GW19_Dividing_51Dividing' 'GW19_RG_16RG' 'GW19_Dividing_43Dividing'\n"",
+      "" 'GW19_Microglia_31Microglia' 'GW19_Interneuron_49Interneuron'\n"",
+      "" 'GW19_Astrocyte_53Astrocyte' 'GW19_RG_9RG' 'GW19_Microglia_22Microglia'\n"",
+      "" 'GW19_Endo_19Endo' 'GW19_Astrocyte_35Astrocyte' 'GW19_RG_18RG'\n"",
+      "" 'GW19_Dividing_41Dividing' 'GW19_Dividing_39Dividing'\n"",
+      "" 'GW19_Dividing_48Dividing' 'GW19_Dividing_32Dividing'\n"",
+      "" 'GW19_Oligo_45Oligo' 'GW19_Interneuron_21Interneuron'\n"",
+      "" 'GW19_Interneuron_13Interneuron' 'GW19_Dividing_47Dividing'\n"",
+      "" 'GW19_Neuron_10Neuron' 'GW19_Neuron_7Neuron' 'GW19_Oligo_30Oligo'\n"",
+      "" 'GW19_Endo_12Endo' 'GW19_Microglia_50Microglia' 'GW19_Endo_34Endo'\n"",
+      "" 'GW19_Oligo_37Oligo' 'GW19_Neuron_6Neuron' 'GW19_Neuron_52Neuron'\n"",
+      "" 'GW19_Neuron_40Neuron' 'GW19_Neuron_27Neuron' 'GW19_Neuron_29Neuron'\n"",
+      "" 'GW19_Neuron_20Neuron' 'GW19_Neuron_42Neuron' 'GW19_IPC_15IPC'\n"",
+      "" 'GW19_Dividing_46Dividing' 'GW19_Outlier_23Outlier'\n"",
+      "" 'GW19_Neuron_33Neuron' 'GW19_RG_25RG' 'GW19_RG_28RG'\n"",
+      "" 'GW19_Outlier_4Outlier' 'GW19_Neuron_38Neuron' 'GW19_Neuron_36Neuron'\n"",
+      "" 'GW19_Dividing_3Dividing' 'GW19_Outlier_11Outlier' 'GW19_Oligo_26Oligo'\n"",
+      "" 'GW19_Outlier_17Outlier' 'GW19_Outlier_2Outlier' 'GW19_Outlier_1Outlier'\n"",
+      "" 'GW19_Neuron_24Neuron' 'GW19_2_39IPC' 'GW19_2_17Interneuron'\n"",
+      "" 'GW19_2_31Neuron' 'GW19_2_29Neuron' 'GW19_2_44Neuron'\n"",
+      "" 'GW19_2_13Microglia' 'GW19_2_40Interneuron' 'GW19_2_23Interneuron'\n"",
+      "" 'GW19_2_24Interneuron' 'GW19_2_45Outlierr' 'GW19_2_12Interneuron'\n"",
+      "" 'GW19_2_33Neuron' 'GW19_2_9Neuron' 'GW19_2_42Dividing'\n"",
+      "" 'GW19_2_3Interneuron' 'GW19_2_10Dividing' 'GW19_2_36Dividing'\n"",
+      "" 'GW19_2_27Oligo' 'GW19_2_30RG' 'GW19_2_37Neuron' 'GW19_2_20Dividing'\n"",
+      "" 'GW19_2_32Microglia' 'GW19_2_38Dividing' 'GW19_2_43Microglia'\n"",
+      "" 'GW19_2_28Oligo' 'GW19_2_11Dividing' 'GW19_2_14Neuron' 'GW19_2_25Oligo'\n"",
+      "" 'GW19_2_19Oligo' 'GW19_2_26Endo' 'GW19_2_18Endo' 'GW19_2_41Microglia'\n"",
+      "" 'GW19_2_6RG' 'GW19_2_8Microglia' 'GW19_2_4Endo' 'GW19_2_15Endo'\n"",
+      "" 'GW19_2_21Oligo' 'GW19_2_7Oligo' 'GW19_2_1Outlierr' 'GW19_2_16RG'\n"",
+      "" 'GW19_2_5Neuron' 'GW20_Neuron_1Neuron' 'GW20_Neuron_6Neuron'\n"",
+      "" 'GW20_RG_21RG' 'GW20_Oligo_14Oligo' 'GW20_RG_33RG'\n"",
+      "" 'GW20_Dividing_42Dividing' 'GW20_Dividing_16Dividing'\n"",
+      "" 'GW20_Neuron_18Neuron' 'GW20_Endo_20Endo' 'GW20_Dividing_36Dividing'\n"",
+      "" 'GW20_Microglia_37Microglia' 'GW20_RG_29RG'\n"",
+      "" 'GW20_Interneuron_32Interneuron' 'GW20_Interneuron_5Interneuron'\n"",
+      "" 'GW20_Microglia_10Microglia' 'GW20_Microglia_22Microglia'\n"",
+      "" 'GW20_Dividing_24Dividing' 'GW20_Outlier_2Outlier'\n"",
+      "" 'GW20_Dividing_35Dividing' 'GW20_Interneuron_34Interneuron'\n"",
+      "" 'GW20_Endo_17Endo' 'GW20_Microglia_30Microglia' 'GW20_Endo_13Endo'\n"",
+      "" 'GW20_RG_15RG' 'GW20_Dividing_40Dividing' 'GW20_Neuron_25Neuron'\n"",
+      "" 'GW20_Astrocyte_39Astrocyte' 'GW20_Dividing_23Dividing'\n"",
+      "" 'GW20_Interneuron_12Interneuron' 'GW20_Astrocyte_38Astrocyte'\n"",
+      "" 'GW20_Interneuron_8Interneuron' 'GW20_Neuron_26Neuron'\n"",
+      "" 'GW20_Neuron_27Neuron' 'GW20_Endo_11Endo' 'GW20_Astrocyte_3Astrocyte'\n"",
+      "" 'GW20_Neuron_31Neuron' 'GW20_Neuron_9Neuron' 'GW20_Microglia_4Microglia'\n"",
+      "" 'GW20_RG_19RG' 'GW20_Neuron_41Neuron' 'GW20_Neuron_7Neuron'\n"",
+      "" 'GW20_Dividing_28Dividing' 'GW20_31and34_16Neuron'\n"",
+      "" 'GW20_31and34_34Astrocyte' 'GW20_31and34_38Microglia'\n"",
+      "" 'GW20_31and34_30Neuron' 'GW20_31and34_13Endo' 'GW20_31and34_8Neuron'\n"",
+      "" 'GW20_31and34_31Interneuron' 'GW20_31and34_11RG' 'GW20_31and34_25Endo'\n"",
+      "" 'GW20_31and34_17Oligo' 'GW20_31and34_35IPC' 'GW20_31and34_2Astrocyte'\n"",
+      "" 'GW20_31and34_15Outlier' 'GW20_31and34_27Microglia'\n"",
+      "" 'GW20_31and34_4Microglia' 'GW20_31and34_33Astrocyte'\n"",
+      "" 'GW20_31and34_12Neuron' 'GW20_31and34_21Endo' 'GW20_31and34_37Oligo'\n"",
+      "" 'GW20_31and34_10Dividing' 'GW20_31and34_36Neuron' 'GW20_31and34_26Neuron'\n"",
+      "" 'GW20_31and34_9RG' 'GW20_31and34_28Neuron' 'GW20_31and34_22Outlier'\n"",
+      "" 'GW20_31and34_29Dividing' 'GW20_31and34_6Neuron' 'GW20_31and34_19RG'\n"",
+      "" 'GW20_31and34_32RG' 'GW20_31and34_14Neuron' 'GW20_31and34_20Interneuron'\n"",
+      "" 'GW20_31and34_7Dividing' 'GW20_31and34_1Interneuron' 'GW20_31and34_23RG'\n"",
+      "" 'GW20_31and34_24Neuron' 'GW20_31and34_5Neuron' 'GW20_31and34_18RG'\n"",
+      "" 'GW20_31and34_3Neuron' 'GW22both_Dividing_15Dividing'\n"",
+      "" 'GW22both_Outlier_30Outlier' 'GW22both_Interneuron_13Interneuron'\n"",
+      "" 'GW22both_RG_23RG' 'GW22both_Dividing_20Dividing'\n"",
+      "" 'GW22both_Interneuron_6Interneuron' 'GW22both_Neuron_11Neuron'\n"",
+      "" 'GW22both_RG_38RG' 'GW22both_Neuron_44Neuron' 'GW22both_Neuron_18Neuron'\n"",
+      "" 'GW22both_Microglia_3Microglia' 'GW22both_Dividing_2Dividing'\n"",
+      "" 'GW22both_RG_17RG' 'GW22both_Oligo_9Oligo' 'GW22both_Endo_37Endo'\n"",
+      "" 'GW22both_Outlier_31Outlier' 'GW22both_Oligo_8Oligo'\n"",
+      "" 'GW22both_Dividing_36Dividing' 'GW22both_Neuron_28Neuron'\n"",
+      "" 'GW22both_Dividing_29Dividing' 'GW22both_Oligo_34Oligo'\n"",
+      "" 'GW22both_Dividing_25Dividing' 'GW22both_Microglia_24Microglia'\n"",
+      "" 'GW22both_Dividing_14Dividing' 'GW22both_Microglia_22Microglia'\n"",
+      "" 'GW22both_Neuron_27Neuron' 'GW22both_Dividing_33Dividing'\n"",
+      "" 'GW22both_Endo_4Endo' 'GW22both_Microglia_43Microglia'\n"",
+      "" 'GW22both_Dividing_40Dividing' 'GW22both_Oligo_16Oligo'\n"",
+      "" 'GW22both_Endo_32Endo' 'GW22both_RG_39RG' 'GW22both_Neuron_12Neuron'\n"",
+      "" 'GW22both_RG_42RG' 'GW22both_Dividing_35Dividing'\n"",
+      "" 'GW22both_Microglia_7Microglia' 'GW22both_Interneuron_21Interneuron'\n"",
+      "" 'GW22both_Interneuron_26Interneuron' 'GW22both_Dividing_10Dividing'\n"",
+      "" 'GW22both_Endo_41Endo' 'GW22both_Microglia_19Microglia'\n"",
+      "" 'GW22both_Endo_5Endo' 'GW22both_Outlier_1Outlier' 'GW25_Oligo_33Oligo'\n"",
+      "" 'GW25_Microglia_1Microglia' 'GW25_Astrocyte_17Astrocyte'\n"",
+      "" 'GW25_Oligo_15Oligo' 'GW25_Astrocyte_32Astrocyte'\n"",
+      "" 'GW25_Dividing_34Dividing' 'GW25_Neuron_7Neuron'\n"",
+      "" 'GW25_Dividing_2Dividing' 'GW25_Neuron_21Neuron' 'GW25_Neuron_29Neuron'\n"",
+      "" 'GW25_Outlier_36Outlier' 'GW25_Oligo_20Oligo' 'GW25_Neuron_18Neuron'\n"",
+      "" 'GW25_Dividing_25Dividing' 'GW25_Endo_24Endo' 'GW25_Oligo_28Oligo'\n"",
+      "" 'GW25_Astrocyte_4Astrocyte' 'GW25_Endo_13Endo' 'GW25_Dividing_22Dividing'\n"",
+      "" 'GW25_Interneuron_5Interneuron' 'GW25_RG_27RG'\n"",
+      "" 'GW25_Microglia_9Microglia' 'GW25_Dividing_10Dividing'\n"",
+      "" 'GW25_Oligo_19Oligo' 'GW25_Endo_26Endo' 'GW25_Interneuron_14Interneuron'\n"",
+      "" 'GW25_Microglia_11Microglia' 'GW25_Endo_35Endo' 'GW25_Endo_30Endo'\n"",
+      "" 'GW25_RG_12RG' 'GW25_Outlier_6Outlier' 'GW25_Neuron_3Neuron'\n"",
+      "" 'GW25_Neuron_16Neuron' 'GW25_RG_23RG' 'GW25_Neuron_31Neuron'\n"",
+      "" 'GW25_Neuron_8Neuron']\n"",
+      ""Unique values for col clusterv1 are ['RG_4' 'GW14_Dividing_29Dividing' 'Dividing_37' 'Neuron_4' 'Outlier'\n"",
+      "" 'GW14_Dividing_24Dividing' 'GW14_Dividing_32Dividing' 'RG_20'\n"",
+      "" 'Dividing_6' 'GW14_Neuron_26Neuron' 'Neuron_25' 'RG_45'\n"",
+      "" 'GW14_Neuron_25Neuron' 'GW14_Endo_31Endo' 'GW14_Astrocyte_19Astrocyte'\n"",
+      "" 'GW14_Microglia_27Microglia' 'GW14_Endo_30Endo' 'Neuron_40' 'Neuron_100'\n"",
+      "" 'Neuron_77' 'Interneuron_21' 'Interneuron_24' 'RG_47'\n"",
+      "" 'GW14_Interneuron_22Interneuron' 'GW14_Dividing_28Dividing'\n"",
+      "" 'GW14_Dividing_33Dividing' 'Dividing_31' 'GW14_RG_23RG'\n"",
+      "" 'GW14_Endo_15Endo' 'Neuron_53' 'GW17_Dividing_27Dividing'\n"",
+      "" 'GW17_Neuron_19Neuron' 'GW17_Neuron_20Neuron' 'GW17_RG_24RG'\n"",
+      "" 'GW17_IPC_23IPC' 'GW17_Dividing_31Dividing' 'Neuron_105' 'Dividing_27'\n"",
+      "" 'GW17_Neuron_29Neuron' 'Neuron_102' 'Dividing_48' 'Dividing_18'\n"",
+      "" 'GW17_Dividing_35Dividing' 'RG_46' 'GW17_Neuron_28Neuron'\n"",
+      "" 'GW17_Vascular_21Vascular' 'Neuron_83' 'GW17_Dividing_30Dividing'\n"",
+      "" 'GW17_IPC_13IPC' 'RG_31' 'GW17_Vascular_11Vascular'\n"",
+      "" 'GW17_Vascular_5Vascular' 'GW17_Neuron_32Neuron' 'GW17_Neuron_34Neuron'\n"",
+      "" 'GW17_Vascular_14Vascular' 'GW17_RG_17RG' 'GW17_Dividing_25Dividing'\n"",
+      "" 'GW17_Dividing_16Dividing' 'GW17_IPC_22IPC' 'GW17_Neuron_26Neuron'\n"",
+      "" 'GW17_Dividing_36Dividing' 'RG_41' 'GW17_Vascular_12Vascular'\n"",
+      "" 'Dividing_15' 'Interneuron_35' 'GW18_Dividing_39Dividing' 'Dividing_5'\n"",
+      "" 'GW18_Dividing_49Dividing' 'RG_22' 'GW18_Microglia_35Microglia'\n"",
+      "" 'Interneuron_43' 'GW18_RG_38RG' 'GW18_RG_46RG' 'Interneuron_23'\n"",
+      "" 'Interneuron_38' 'GW18_Oligo_33Oligo' 'Endo_6' 'GW18_RG_31RG'\n"",
+      "" 'GW18_Microglia_20Microglia' 'Oligo_58' 'GW18_Neuron_44Neuron'\n"",
+      "" 'Neuron_72' 'GW18_Astrocyte_26Astrocyte' 'RG_2' 'GW18_IPC_50IPC'\n"",
+      "" 'Neuron_29' 'GW18_Neuron_43Neuron' 'Neuron_101' 'Neuron_89' 'IPC_18'\n"",
+      "" 'GW18_IPC_42IPC' 'GW18_Astrocyte_17Astrocyte' 'Neuron_75'\n"",
+      "" 'GW18_Oligo_48Oligo' 'Neuron_50' 'Neuron_37' 'Neuron_104'\n"",
+      "" 'GW18_Endo_45Endo' 'Neuron_87' 'GW18_Neuron_40Neuron' 'Neuron_103'\n"",
+      "" 'GW18_RG_47RG' 'GW18_Neuron_41Neuron' 'GW18_Astrocyte_51Astrocyte'\n"",
+      "" 'RG_40' 'GW18_2_39Interneuron' 'Interneuron_19' 'Interneuron_20'\n"",
+      "" 'GW18_2_35Dividing' 'GW18_2_43Dividing' 'RG_15' 'RG_52' 'Interneuron_40'\n"",
+      "" 'GW18_2_24Endo' 'Interneuron_9' 'Neuron_92' 'GW18_2_41Microglia'\n"",
+      "" 'Dividing_12' 'Astrocyte_52' 'Oligo_38' 'Microglia_73'\n"",
+      "" 'GW18_2_37Dividing' 'GW18_2_38RG' 'Neuron_93' 'GW18_2_45Oligo'\n"",
+      "" 'GW18_2_40Astrocyte' 'Oligo_67' 'GW18_2_30Endo' 'IPC_34'\n"",
+      "" 'GW18_2_44Interneuron' 'Astrocyte_14' 'Neuron_39' 'Dividing_21'\n"",
+      "" 'GW18_2_34Neuron' 'GW18_2_42Neuron' 'GW18_2_36Endo' 'Neuron_55'\n"",
+      "" 'Neuron_74' 'Neuron_13' 'Microglia_61' 'GW19_RG_44RG'\n"",
+      "" 'GW19_Dividing_54Dividing' 'GW19_Dividing_51Dividing' 'RG_3'\n"",
+      "" 'GW19_Dividing_43Dividing' 'GW19_Microglia_31Microglia'\n"",
+      "" 'GW19_Interneuron_49Interneuron' 'GW19_Astrocyte_53Astrocyte'\n"",
+      "" 'GW19_Microglia_22Microglia' 'GW19_Endo_19Endo'\n"",
+      "" 'GW19_Astrocyte_35Astrocyte' 'RG_42' 'GW19_Dividing_41Dividing'\n"",
+      "" 'GW19_Dividing_39Dividing' 'GW19_Dividing_48Dividing' 'Dividing_22'\n"",
+      "" 'GW19_Oligo_45Oligo' 'GW19_Interneuron_21Interneuron' 'Interneuron_22'\n"",
+      "" 'GW19_Dividing_47Dividing' 'Neuron_80' 'GW19_Oligo_30Oligo'\n"",
+      "" 'GW19_Endo_12Endo' 'GW19_Microglia_50Microglia' 'GW19_Endo_34Endo'\n"",
+      "" 'GW19_Oligo_37Oligo' 'GW19_Neuron_52Neuron' 'GW19_Neuron_40Neuron'\n"",
+      "" 'Neuron_35' 'GW19_Neuron_42Neuron' 'IPC_29' 'GW19_Dividing_46Dividing'\n"",
+      "" 'RG_59' 'GW19_Neuron_38Neuron' 'GW19_Neuron_36Neuron' 'Dividing_36'\n"",
+      "" 'GW19_Oligo_26Oligo' 'Neuron_11' 'GW19_2_39IPC' 'Interneuron_31'\n"",
+      "" 'GW19_2_31Neuron' 'GW19_2_29Neuron' 'GW19_2_44Neuron' 'Interneuron_7'\n"",
+      "" 'GW19_2_45Outlierr' 'GW19_2_33Neuron' 'Neuron_58' 'GW19_2_42Dividing'\n"",
+      "" 'Interneuron_6' 'GW19_2_36Dividing' 'Oligo_5' 'GW19_2_30RG'\n"",
+      "" 'GW19_2_37Neuron' 'Microglia_63' 'GW19_2_38Dividing' 'GW19_2_43Microglia'\n"",
+      "" 'Oligo_24' 'Neuron_66' 'Oligo_56' 'Oligo_25' 'GW19_2_26Endo'\n"",
+      "" 'GW19_2_18Endo' 'GW19_2_41Microglia' 'RG_13' 'Microglia_21' 'Endo_29'\n"",
+      "" 'Endo_34' 'Oligo_42' 'Oligo_23' 'GW19_2_1Outlierr' 'RG_39' 'Neuron_28'\n"",
+      "" 'Neuron_33' 'RG_11' 'Oligo_20' 'Neuron_88' 'Endo_22'\n"",
+      "" 'GW20_Microglia_37Microglia' 'RG_50' 'GW20_Interneuron_32Interneuron'\n"",
+      "" 'GW20_Interneuron_5Interneuron' 'Microglia_54'\n"",
+      "" 'GW20_Microglia_22Microglia' 'GW20_Interneuron_34Interneuron'\n"",
+      "" 'GW20_Microglia_30Microglia' 'Endo_33' 'RG_25'\n"",
+      "" 'GW20_Astrocyte_39Astrocyte' 'GW20_Interneuron_12Interneuron'\n"",
+      "" 'GW20_Astrocyte_38Astrocyte' 'GW20_Interneuron_8Interneuron' 'Neuron_2'\n"",
+      "" 'Endo_38' 'Astrocyte_4' 'Neuron_62' 'Neuron_12'\n"",
+      "" 'GW20_31and34_34Astrocyte' 'GW20_31and34_38Microglia' 'Neuron_27'\n"",
+      "" 'Endo_21' 'Neuron_54' 'GW20_31and34_31Interneuron' 'GW20_31and34_25Endo'\n"",
+      "" 'GW20_31and34_17Oligo' 'GW20_31and34_35IPC' 'GW20_31and34_27Microglia'\n"",
+      "" 'Microglia_15' 'GW20_31and34_33Astrocyte' 'GW20_31and34_37Oligo'\n"",
+      "" 'GW20_31and34_36Neuron' 'Neuron_5' 'Neuron_47' 'Dividing_13' 'Neuron_59'\n"",
+      "" 'RG_32' 'Neuron_36' 'GW20_31and34_20Interneuron'\n"",
+      "" 'GW20_31and34_1Interneuron' 'RG_17' 'Neuron_98' 'Interneuron_18'\n"",
+      "" 'GW22both_RG_23RG' 'Interneuron_33' 'GW22both_RG_38RG' 'Neuron_68'\n"",
+      "" 'Neuron_48' 'Microglia_45' 'Oligo_4' 'GW22both_Endo_37Endo' 'Oligo_46'\n"",
+      "" 'Dividing_29' 'GW22both_Oligo_34Oligo' 'Dividing_28' 'Microglia_50'\n"",
+      "" 'Dividing_25' 'Microglia_31' 'Endo_32' 'GW22both_Microglia_43Microglia'\n"",
+      "" 'GW22both_Dividing_40Dividing' 'Oligo_50' 'GW22both_Endo_32Endo'\n"",
+      "" 'GW22both_RG_39RG' 'Neuron_10' 'GW22both_RG_42RG' 'Dividing_41'\n"",
+      "" 'Microglia_16' 'GW22both_Interneuron_21Interneuron'\n"",
+      "" 'GW22both_Interneuron_26Interneuron' 'GW22both_Endo_41Endo'\n"",
+      "" 'Microglia_76' 'Endo_26' 'Oligo_33' 'Microglia_8' 'Astrocyte_39'\n"",
+      "" 'Astrocyte_53' 'Dividing_40' 'Neuron_64' 'Oligo_15' 'Neuron_14'\n"",
+      "" 'Dividing_32' 'Endo_45' 'Oligo_18' 'Endo_4' 'RG_44' 'Microglia_37'\n"",
+      "" 'Dividing_42' 'Oligo_45' 'Microglia_10' 'GW25_Endo_35Endo'\n"",
+      "" 'GW25_Endo_30Endo' 'Neuron_63' 'Neuron_91' 'RG_37' 'GW25_Neuron_31Neuron']\n"",
+      ""Unique values for col ngene are [ 869  805 1131 ... 7348 5150 7668]\n"",
+      ""Unique values for col numi are [ 1481  1845  3101 ... 20777 24502 19481]\n"",
+      ""Unique values for col percent.mito are [1.29e-013 1.22e-037 5.79e-019 ... 1.25e-172 5.80e-203 9.13e-083]\n""
+     ]
+    },
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Unique values for col percent.ribo are [0.23227549 0.27046071 0.34150274 ... 0.10180891 0.12100554 0.17977073]\n"",
+      ""Unique values for col v1 are [ 5. 29.  8. 11. 12.  1. 24. 32. 14. 13. 26. 16.  3. 25.  7. 31. 19. 27.\n"",
+      "" 30. 20. 10. 21.  4.  6.  9. 22. 28. 33. 17. 23. 18. 15.  2. 35. 34. 36.\n"",
+      "" 39. 49. 38. 46. 44. 50. 43. 42. 48. 45. 40. 37. 47. 41. 51. 69. 63. 81.\n"",
+      "" 72. 53. 70. 60. 82. 65. 75. 62. 57. 74. 61. 64. 66. 56. 58. 78. 71. 80.\n"",
+      "" 52. 68. 77. 55. 73. 79. 59. 76. 54. 67. nan]\n"",
+      ""Unique values for col ncount_rna are [ 1481  1845  3101 ... 20777 24502 19481]\n"",
+      ""Unique values for col nfeature_rna are [ 869  805 1131 ... 5546 5953 5242]\n"",
+      ""Unique values for col cell.name.1 are ['GW14_CGE_AAACCTGGTAAAGGAG' 'GW14_CGE_AAACCTGGTTCCGTCT'\n"",
+      "" 'GW14_CGE_AAACCTGTCGTCCAGG' ... 'GW25_thalamus_TTTGTCAGTAGATTAG'\n"",
+      "" 'GW25_thalamus_TTTGTCAGTTCGGCAC' 'GW25_thalamus_TTTGTCATCCTAGGGC']\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""for col in df2.columns:\n"",
+    ""    print(f'Unique values for col {col} are {df2[col].unique()}')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 8,
+   ""id"": ""b08fe710"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""array([14, 17, 18, 19, 20, 22, 25])""
+      ]
+     },
+     ""execution_count"": 8,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2['age'].unique()\n""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""d0a7eb09"",
+   ""metadata"": {},
+   ""source"": [
+    ""## labels:\n"",
+    ""\n"",
+    ""parietal cortex  \n"",
+    ""prefrontal cortex (PFC)  \n"",
+    ""motor = m1 = motor cortex  \n"",
+    ""v1 = visual cortex  \n"",
+    ""samtosensory cortex  \n"",
+    ""LGE (Lateral ganglionic emminence)  \n"",
+    ""CGE (caudal ganglionic emminence)  \n"",
+    ""temporal cortex  \n"",
+    ""MGE (medial ganglionic emminence) (progenitors that give rise to interneurons)  \n"",
+    ""insula  \n"",
+    ""\n"",
+    ""## take out:\n"",
+    ""thalamus \n"",
+    ""hypothalamus  \n"",
+    ""hippocampus \n"",
+    ""claustrum  \n"",
+    ""cingulate           \n"",
+    ""caudate              \n"",
+    ""putamen              \n"",
+    ""preoptic             \n"",
+    ""cerebellum            \n"",
+    ""nucleusaccumbens  \n"",
+    ""striatum             \n"",
+    ""midbrain     ""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""e89078e3"",
+   ""metadata"": {},
+   ""source"": [
+    ""## Fixing the data\n"",
+    ""\n"",
+    ""We will not use the Aldinger dataset , since it is from the Cerebellum, which will have different forms of neurons than the cerebral cortex. Also, we can't use the Eze et. al since clusters are not labeled. Instead, we'll use the following\n"",
+    ""\n"",
+    ""1. `df1`: Bhaduri et. al, 2020, https://www.nature.com/articles/s41586-020-1962-0 (our primary dataset)\n"",
+    ""2. `df2`: Bhaduri et. al, 2021, https://www.nature.com/articles/s41586-021-03910-8 \n"",
+    ""3. `df3`: Allen brain atlas, m1 region: https://cells.ucsc.edu/?bp=brain&ds=allen-celltypes+human-cortex\n"",
+    ""4. `df4`: Allen brain atlas, cortex region: https://cells.ucsc.edu/?bp=brain&ds=allen-celltypes+human-cortex\n"",
+    ""5. `df5`: Delgado: https://cells.ucsc.edu/?ds=human-cortical-lineage\n"",
+    ""6. Chinese paper""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""8c554cab"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_29752/3559954611.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n"",
+      ""  df2 = pd.read_csv('../data/external/whole_brain_bhaduri_labels.tsv', sep='\\t')\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""df1 = pd.read_csv('../data/external/primary_bhaduri_labels.tsv', sep='\\t')\n"",
+    ""df2 = pd.read_csv('../data/external/whole_brain_bhaduri_labels.tsv', sep='\\t')\n"",
+    ""df3 = pd.read_csv('../data/external/allen_m1_region_labels.tsv', sep='\\t')\n"",
+    ""df4 = pd.read_csv('../data/external/allen_cortex_labels.tsv', sep='\\t')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""8b8aa8f5"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""Index(['cell.name', 'age', 'individual', 'structure', 'area', 'area.sub',\n"",
+       ""       'clusterv2', 'clusterv2_id', 'cell.type.v2', 'celltype.structure',\n"",
+       ""       'lamina', 'cluster', 'celltype', 'cellabbreviation', 'clustercelltype',\n"",
+       ""       'matchname', 'clusterv1', 'ngene', 'numi', 'percent.mito',\n"",
+       ""       'percent.ribo', 'v1', 'ncount_rna', 'nfeature_rna', 'cell.name.1'],\n"",
+       ""      dtype='object')""
+      ]
+     },
+     ""execution_count"": 11,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2.columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""58e01399"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""0                        GW14_RG_5RG\n"",
+       ""1           GW14_Dividing_29Dividing\n"",
+       ""2            GW14_Dividing_8Dividing\n"",
+       ""3               GW14_Neuron_11Neuron\n"",
+       ""4            GW14_Dividing_8Dividing\n"",
+       ""                     ...            \n"",
+       ""691923              GW25_Endo_24Endo\n"",
+       ""691924    GW25_Astrocyte_17Astrocyte\n"",
+       ""691925            GW25_Oligo_19Oligo\n"",
+       ""691926    GW25_Astrocyte_17Astrocyte\n"",
+       ""691927    GW25_Astrocyte_17Astrocyte\n"",
+       ""Name: matchname, Length: 691928, dtype: object""
+      ]
+     },
+     ""execution_count"": 12,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2['matchname']""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""37b579a6"",
+   ""metadata"": {},
+   ""source"": [
+    ""We'll rename some values of `df2` to make them consistent with `df1`.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""b8c5598e"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""Excitatory Neuron    257489\n"",
+       ""Interneuron          156146\n"",
+       ""Dividing              69797\n"",
+       ""Radial Glia           64608\n"",
+       ""Outlier               32377\n"",
+       ""Neuron                28586\n"",
+       ""Astrocyte             24908\n"",
+       ""Oligodendrocyte       19827\n"",
+       ""IPC                   15230\n"",
+       ""Microglia             13697\n"",
+       ""Endothelial            8899\n"",
+       ""Vascular                240\n"",
+       ""Cajal Retzius           124\n"",
+       ""Name: celltype, dtype: int64""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df2['celltype'] = df2['celltype'].replace({'RG' : 'Radial Glia', 'CR': 'Cajal Retzius'})\n"",
+    ""df2['celltype'].value_counts()""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""5ee49bcb"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now that `df1` and `df2` are properly aligned, let's map the values from `df3` and `df4` into the values from `df1` and `df2`.""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""74c2a9af"",
+   ""metadata"": {},
+   ""source"": [
+    ""Gluta = excititory\n"",
+    ""gaba = inhib""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""6eba0b3d"",
+   ""metadata"": {},
+   ""source"": [
+    ""L2/3 IT: excit, between hemispheres  \n"",
+    ""Pvalb: inhib   \n"",
+    ""sst: inhib  \n"",
+    ""vip: inhin  \n"",
+    ""lamp5: inhib  \n"",
+    ""L6 ct: excit cortical-thalamic  \n"",
+    ""oligo: oligodendricites  \n"",
+    ""L6b: excit  \n"",
+    ""L6 it: excit  \n"",
+    ""L5/6 NP: excit   \n"",
+    ""Sncg: inhib   \n"",
+    ""L5 et: excit   \n"",
+    ""astro: astrocyte  \n"",
+    ""L6 IT car3: excit   \n"",
+    ""OPC: OPC  \n"",
+    ""MicroPVM: microglia (double check)  \n"",
+    ""sst chodl: inhib  \n"",
+    ""endo: Endothelial  \n"",
+    ""VLMC: Look up  ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""55aac15e"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df3_mapping = {\n"",
+    ""    'L2/3 IT': 'Excitatory Neuron',\n"",
+    ""    'L5 IT': 'Interneuron', \n"",
+    ""    'Pvalb': 'Interneuron',\n"",
+    ""    'Sst': 'Interneuron',\n"",
+    ""    'Vip': 'Interneuron',\n"",
+    ""    'Lamp5': 'Interneuron',\n"",
+    ""    'L6 CT': 'Interneuron', \n"",
+    ""    'Oligo': 'Oligodendrocyte', \n"",
+    ""    'L6b': 'Excitatory Neuron',\n"",
+    ""    'L6 IT': 'Excitatory Neuron',\n"",
+    ""    'L5/6 NP': 'Excitatory Neuron',\n"",
+    ""    'Sncg': 'Interneuron', \n"",
+    ""    'L5 ET': 'Excitatory Neuron', \n"",
+    ""    'Astro': 'Astrocyte',\n"",
+    ""    'L6 IT Car3': 'Excitatory Neuron', \n"",
+    ""    'OPC': 'OPC',\n"",
+    ""    'Micro-PVM': 'Microglia',\n"",
+    ""    'Sst Chodl': 'Interneuron',\n"",
+    ""    'Endo': 'Endothelial',\n"",
+    ""    'VLMC': 'Vascular',\n"",
+    ""}""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 15,
+   ""id"": ""f3920847"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df3['subclass_label'] = df3['subclass_label'].replace(df3_mapping)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 16,
+   ""id"": ""ab07b384"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df4_mapping = {\n"",
+    ""    'IT': 'Interneuron',\n"",
+    ""    'L4 IT': 'Interneuron',\n"",
+    ""    'VIP': 'Interneuron',\n"",
+    ""    'PVALB': 'Interneuron',\n"",
+    ""    'L6 CT': 'Interneuron', \n"",
+    ""    'LAMP5': 'Interneuron',\n"",
+    ""    'SST': 'Interneuron',\n"",
+    ""    'Exclude': 'Exclude',\n"",
+    ""    'Oligodendrocyte': 'Oligodendrocyte',\n"",
+    ""    'Astrocyte': 'Astrocyte',\n"",
+    ""    'L6b': 'Excitatory Neuron',\n"",
+    ""    'L5/6 IT Car3': 'Excitatory Neuron',\n"",
+    ""    'L5/6 NP': 'Excitatory Neuron',\n"",
+    ""    'OPC': 'OPC',\n"",
+    ""    'Microglia': 'Microglia',\n"",
+    ""    'PAX6' : 'Progenitors', # DROP THESE, progenitors should not be in postnatal \n"",
+    ""    'L5 ET': 'Excitatory Neuron',\n"",
+    ""    'Endothelial' : 'Endothelial',\n"",
+    ""    'Pericyte': 'Pericyte',\n"",
+    ""    'VLMC': 'Vascular'\n"",
+    ""}""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""eaa7b0a7"",
+   ""metadata"": {},
+   ""source"": [
+    ""IT: excit (inter hemispheric)  \n"",
+    ""L4 IT: layer 4 excit  \n"",
+    ""VIP: interneuron (inhib)\n"",
+    ""pvalb:  \n"",
+    ""L6 CT:  corticalthalamic (excit)  \n"",
+    ""LAMP5:  \n"",
+    ""SST  :\n"",
+    ""L5/6 NP:  excit \n"",
+    ""PAX6:  drop ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 17,
+   ""id"": ""ceb58393"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df4['subclass_label'] = df4['subclass_label'].replace(df4_mapping)""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""181acc76"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now, let's grab the columns we're interested in into new DataFrames, and write these out to our `data/processed`.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 18,
+   ""id"": ""1c9e823a"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df1_reduced = df1[['Type']]\n"",
+    ""df1_reduced = df1_reduced[df1_reduced['Type'] != 'Outlier']\n"",
+    ""\n"",
+    ""df2_reduced = df2[['celltype']].rename(columns={'celltype': 'Type'})\n"",
+    ""df2_reduced = df2_reduced[df2_reduced['Type'] != 'Outlier']\n"",
+    ""\n"",
+    ""df3_reduced = df3[['subclass_label']].rename(columns={'subclass_label': 'Type'})\n"",
+    ""df3_reduced = df3_reduced[df3_reduced['Type'] != 'Outlier']\n"",
+    ""\n"",
+    ""df4_reduced = df4[['subclass_label']].rename(columns={'subclass_label': 'Type'})\n"",
+    ""df4_reduced = df4_reduced[df4_reduced['Type'] != 'Exclude']""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 111,
+   ""id"": ""119f19cb"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Excitatory Neuron     122958\n"",
+      ""Radial Glia            29563\n"",
+      ""Inhibitory Neuron      20609\n"",
+      ""Microglia               4510\n"",
+      ""IPC                     3863\n"",
+      ""Red blood cells         2451\n"",
+      ""OPC                     1888\n"",
+      ""Mural                    363\n"",
+      ""Endothelial              271\n"",
+      ""Name: Type, dtype: int64\n"",
+      ""\n"",
+      ""\n"",
+      ""Excitatory Neuron    257489\n"",
+      ""Interneuron          156146\n"",
+      ""Dividing              69797\n"",
+      ""Radial Glia           64608\n"",
+      ""Neuron                28586\n"",
+      ""Astrocyte             24908\n"",
+      ""Oligodendrocyte       19827\n"",
+      ""IPC                   15230\n"",
+      ""Microglia             13697\n"",
+      ""Endothelial            8899\n"",
+      ""Vascular                240\n"",
+      ""Cajal Retzius           124\n"",
+      ""Name: Type, dtype: int64\n"",
+      ""\n"",
+      ""\n"",
+      ""Interneuron          41560\n"",
+      ""Excitatory Neuron    30968\n"",
+      ""Oligodendrocyte       2942\n"",
+      ""Astrocyte              568\n"",
+      ""OPC                    283\n"",
+      ""Microglia              108\n"",
+      ""Endothelial             64\n"",
+      ""Vascular                40\n"",
+      ""Name: Type, dtype: int64\n"",
+      ""\n"",
+      ""\n"",
+      ""Interneuron          39313\n"",
+      ""Excitatory Neuron     3114\n"",
+      ""Oligodendrocyte       1932\n"",
+      ""Astrocyte             1188\n"",
+      ""OPC                    774\n"",
+      ""Microglia              750\n"",
+      ""Progenitors            325\n"",
+      ""Endothelial             70\n"",
+      ""Pericyte                32\n"",
+      ""Vascular                11\n"",
+      ""Name: Type, dtype: int64\n"",
+      ""\n"",
+      ""\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""from sklearn.preprocessing import LabelEncoder\n"",
+    ""\n"",
+    ""def encode(data):\n"",
+    ""    le = LabelEncoder()\n"",
+    ""    data.loc[:, 'Type'] = le.fit_transform(data.loc[:, 'Type'])\n"",
+    ""    \n"",
+    ""    return data\n"",
+    ""    \n"",
+    ""for df in df1_reduced, df2_reduced, df3_reduced, df4_reduced:\n"",
+    ""    print(df['Type'].value_counts())\n"",
+    ""    print('\\n')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 112,
+   ""id"": ""ec59e51b"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>Type</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>Radial Glia</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>Radial Glia</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>Excitatory Neuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>Excitatory Neuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>Excitatory Neuron</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""                 Type\n"",
+       ""0         Radial Glia\n"",
+       ""1         Radial Glia\n"",
+       ""2  Excitatory Neuron \n"",
+       ""3  Excitatory Neuron \n"",
+       ""4  Excitatory Neuron ""
+      ]
+     },
+     ""metadata"": {},
+     ""output_type"": ""display_data""
+    },
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>Type</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>Radial Glia</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>Dividing</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>Dividing</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>Excitatory Neuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>Dividing</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""                Type\n"",
+       ""0        Radial Glia\n"",
+       ""1           Dividing\n"",
+       ""2           Dividing\n"",
+       ""3  Excitatory Neuron\n"",
+       ""4           Dividing""
+      ]
+     },
+     ""metadata"": {},
+     ""output_type"": ""display_data""
+    }
+   ],
+   ""source"": [
+    ""display(df1_reduced.head())\n"",
+    ""display(df2_reduced.head())""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""a2308b56"",
+   ""metadata"": {},
+   ""source"": [
+    ""Let's write these iterim results out before continuing to encode our targets""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 114,
+   ""id"": ""7d019cd5"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df1_reduced.to_csv('../data/interim/bhaduri_2020_labels.csv')\n"",
+    ""df2_reduced.to_csv('../data/interim/bhaduri_2021_labels.csv')\n"",
+    ""df3_reduced.to_csv('../data/interim/allen_m1_labels.csv')\n"",
+    ""df4_reduced.to_csv('../data/interim/allen_cortex_labels.csv')""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""351e9c9a"",
+   ""metadata"": {},
+   ""source"": [
+    ""## Mapping label sets to similar categorical variables\n"",
+    ""\n"",
+    ""In order to train our model, we need to encode the categorical target variables consistently across all label files. To do this, we'll first take the union of all unique target values in df1$,...,$df4, then map these to integers and apply to our categorical columns.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 115,
+   ""id"": ""84c1b10f"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(['IPC',\n"",
+       ""  'Microglia',\n"",
+       ""  'Cajal Retzius',\n"",
+       ""  'Red blood cells',\n"",
+       ""  'Excitatory Neuron',\n"",
+       ""  'Neuron',\n"",
+       ""  'Mural',\n"",
+       ""  'Endothelial',\n"",
+       ""  'Astrocyte',\n"",
+       ""  'Oligodendrocyte',\n"",
+       ""  'Interneuron',\n"",
+       ""  'Progenitors',\n"",
+       ""  'Excitatory Neuron ',\n"",
+       ""  'OPC',\n"",
+       ""  'Radial Glia',\n"",
+       ""  'Pericyte',\n"",
+       ""  'Dividing',\n"",
+       ""  'Vascular',\n"",
+       ""  'Inhibitory Neuron'],\n"",
+       "" 19)""
+      ]
+     },
+     ""execution_count"": 115,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""datasets = [df1_reduced, df2_reduced, df3_reduced, df4_reduced]\n"",
+    ""unique_targets = list(set(np.concatenate([df['Type'].unique() for df in datasets])))\n"",
+    ""unique_targets, len(unique_targets)""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""38f16e66"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now we define an encoder on our unique targets""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 116,
+   ""id"": ""8f6c3304"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""le = LabelEncoder()\n"",
+    ""le = le.fit(unique_targets)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 117,
+   ""id"": ""9ed6ddd4"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""array([ 6,  9,  1, 17,  4, 11, 10,  3,  0, 13,  8, 15,  5, 12, 16, 14,  2,\n"",
+       ""       18,  7])""
+      ]
+     },
+     ""execution_count"": 117,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""le.transform(unique_targets)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 120,
+   ""id"": ""1264716b"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df1_reduced['Type'] = le.transform(df1_reduced['Type'])\n"",
+    ""df2_reduced['Type'] = le.transform(df2_reduced['Type'])\n"",
+    ""df3_reduced['Type'] = le.transform(df3_reduced['Type'])\n"",
+    ""df4_reduced['Type'] = le.transform(df4_reduced['Type'])""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 128,
+   ""id"": ""5e3d010e"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>Type</th>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>cell</th>\n"",
+       ""      <th></th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>16</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>16</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189404</th>\n"",
+       ""      <td>10</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189405</th>\n"",
+       ""      <td>10</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189406</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189407</th>\n"",
+       ""      <td>9</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189408</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>186476 rows × 1 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""        Type\n"",
+       ""cell        \n"",
+       ""0         16\n"",
+       ""1         16\n"",
+       ""2          5\n"",
+       ""3          5\n"",
+       ""4          5\n"",
+       ""...      ...\n"",
+       ""189404    10\n"",
+       ""189405    10\n"",
+       ""189406     5\n"",
+       ""189407     9\n"",
+       ""189408     5\n"",
+       ""\n"",
+       ""[186476 rows x 1 columns]""
+      ]
+     },
+     ""execution_count"": 128,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df1_reduced.index.name = 'cell'\n"",
+    ""df1_reduced""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 130,
+   ""id"": ""976de525"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>Type</th>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>cell</th>\n"",
+       ""      <th></th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>16</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>16</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189404</th>\n"",
+       ""      <td>10</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189405</th>\n"",
+       ""      <td>10</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189406</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189407</th>\n"",
+       ""      <td>9</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>189408</th>\n"",
+       ""      <td>5</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>186476 rows × 1 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""        Type\n"",
+       ""cell        \n"",
+       ""0         16\n"",
+       ""1         16\n"",
+       ""2          5\n"",
+       ""3          5\n"",
+       ""4          5\n"",
+       ""...      ...\n"",
+       ""189404    10\n"",
+       ""189405    10\n"",
+       ""189406     5\n"",
+       ""189407     9\n"",
+       ""189408     5\n"",
+       ""\n"",
+       ""[186476 rows x 1 columns]""
+      ]
+     },
+     ""execution_count"": 130,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df1_reduced.to_csv('test.csv')\n"",
+    ""\n"",
+    ""df = pd.read_csv('test.csv', index_col='cell')\n"",
+    ""df""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""79b00726"",
+   ""metadata"": {},
+   ""source"": [
+    ""## Cleaning the gene expression matrices\n"",
+    ""\n"",
+    ""Now that we've fixed the label files, let's make sure the gene expression matrices are formatted as we expect.\n"",
+    ""\n"",
+    ""We should have a cell $\\times$ gene expression for each label DataFrame, and the columns kept should be the intersection of all columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 26,
+   ""id"": ""7e1204bb"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""import dask.dataframe as dd\n"",
+    ""import sys \n"",
+    ""sys.path.append('../src')\n"",
+    ""from models.lib.data import *""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 27,
+   ""id"": ""2d5ca404"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""allen_cortex = GeneExpressionData(\n"",
+    ""    '../data/interim/allen_cortex_T.csv',\n"",
+    ""    '../data/interim/allen_cortex_labels.csv',\n"",
+    ""    'Type',\n"",
+    ""    skip=3,\n"",
+    "")\n"",
+    ""\n"",
+    ""bhaduri_primary = GeneExpressionData(\n"",
+    ""    '../data/processed/primary.csv',\n"",
+    ""    '../data/processed/meta_primary_labels.csv',\n"",
+    ""    'Type'\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 28,
+   ""id"": ""bdd63f8c"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(tensor([0., 0., 0.,  ..., 0., 0., 0.]), 8)""
+      ]
+     },
+     ""execution_count"": 28,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""bhaduri_primary[0]""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 29,
+   ""id"": ""bbd233fc"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(tensor([0., 0., 0.,  ..., 0., 0., 0.]), 'Interneuron')""
+      ]
+     },
+     ""execution_count"": 29,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""allen_cortex[1]""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 30,
+   ""id"": ""df7a99a2"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(tensor([0., 0., 0.,  ..., 0., 0., 5.]), 'Interneuron')""
+      ]
+     },
+     ""execution_count"": 30,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""allen_cortex[0]""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 31,
+   ""id"": ""3f78591d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""50281""
+      ]
+     },
+     ""execution_count"": 31,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(allen_cortex[1][0])""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 32,
+   ""id"": ""977cd319"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""[]""
+      ]
+     },
+     ""execution_count"": 32,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""allen_cortex.get_features()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 33,
+   ""id"": ""9fa5b604"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""NameError"",
+     ""evalue"": ""name 'm1T' is not defined"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_45646/731065727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;31mNameError\u001b[0m: name 'm1T' is not defined""
+     ]
+    }
+   ],
+   ""source"": [
+    ""m1T.columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""dcae476d"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df.columns""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""8b1f0db8"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""m1cols = [x.split('|')[0] for x in m1T.columns]\n"",
+    ""cortcols = list(df.columns)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""b12a4a3f"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""len(set(m1cols).intersection(set(cortcols)))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""e8ce5232"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""len(cortcols)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""372d650d"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""prim = pd.read_csv('../data/processed/primary.csv', nrows=1)\n"",
+    ""prim""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""3ccfe2b9"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""primcols = list(prim.columns)\n"",
+    ""\n"",
+    ""len(set(cortcols).intersection(primcols))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""f305d734"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""set(cortcols) - set(primcols)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""e497754b"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test1 = pd.DataFrame(np.ones([10, 10]))\n"",
+    ""test2 = pd.DataFrame(np.zeros([10,10]), columns=range(10, 0, -1))\n"",
+    ""\n"",
+    ""pd.concat([test1, test2])""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""5b603208"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""list(range(10, 0, -1))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""b981aebc"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""e70f9555"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""ec5ff669"",
+   ""metadata"": {},
+   ""source"": [
+    ""# Modeling Sanity Check: Making sure everything is ok\n"",
+    ""\n"",
+    ""In this notebook, we'll test our entire network pipeline because SURELY there are bugs.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""a997b07b"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import dask.dataframe as dd\n"",
+    ""import pandas as pd \n"",
+    ""import torch\n"",
+    ""import linecache \n"",
+    ""import csv\n"",
+    ""import numpy as np\n"",
+    ""import torch.nn as nn\n"",
+    ""from torch.utils.data import Dataset, DataLoader\n"",
+    ""import sys\n"",
+    ""import torch\n"",
+    ""import pytorch_lightning as pl\n"",
+    ""sys.path.append('../src/')\n"",
+    ""sys.path.append('..')""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""6123cab6"",
+   ""metadata"": {},
+   ""source"": [
+    ""Let's define our custom data class and make sure everything is being streamed in correctly""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 25,
+   ""id"": ""84c92b7f"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from models.lib.data import GeneExpressionData\n"",
+    ""from models.lib.neural import GeneClassifier""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 26,
+   ""id"": ""e77a771e"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(186476, 19765)""
+      ]
+     },
+     ""execution_count"": 26,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""data = GeneExpressionData(\n"",
+    ""    filename='../data/interim/primary_bhaduri_T.csv',\n"",
+    ""    labelname='../data/processed/labels/primary_bhaduri_labels.csv',\n"",
+    ""    class_label='Type',\n"",
+    ""    skip=3,\n"",
+    "")\n"",
+    ""\n"",
+    ""data.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 27,
+   ""id"": ""3fb5aa9d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Model initialized. N_features = 19765, N_labels = 17. Metrics are {'accuracy': <function accuracy at 0x7fc803c50430>, 'precision': <function precision at 0x7fc803c63a60>, 'recall': <function recall at 0x7fc803c63b80>} and weighted_metrics = False\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""model = GeneClassifier(\n"",
+    ""    N_features = len(data.columns),\n"",
+    ""    N_labels = max(data.labels), # Since indexed from zero\n"",
+    ""    weights=data.class_weights,\n"",
+    ""    params={\n"",
+    ""        'width' : 1024,\n"",
+    ""        'layers': 2,\n"",
+    ""        'epochs': 10,\n"",
+    ""        'lr': 3e-5,\n"",
+    ""        'momentum': 1e-4,\n"",
+    ""        'weight_decay': 1e-4\n"",
+    ""    }\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 28,
+   ""id"": ""84aeecf6"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""array([16,  4,  9, 11,  6,  8,  7,  3, 17])""
+      ]
+     },
+     ""execution_count"": 28,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""data.labels""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""5ccbc953"",
+   ""metadata"": {},
+   ""source"": [
+    ""Now that we have our dataset, at least make sure a forward pass is computing correctly, and that our model can at least overfit on a small subset of the training set. Therefore, we'll subset our dataset and create the train and val loaders this way.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 29,
+   ""id"": ""7994ea02"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from torch.utils.data import Subset\n"",
+    ""\n"",
+    ""tr_10k = Subset(data, range(10))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 31,
+   ""id"": ""2b442539"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""def train_test(data):\n"",
+    ""    train_size = int(0.80 * len(data))\n"",
+    ""    test_size = len(data) - train_size\n"",
+    ""\n"",
+    ""    train, test = torch.utils.data.random_split(data, [train_size, test_size])\n"",
+    ""\n"",
+    ""    traindata = DataLoader(train, batch_size=4)\n"",
+    ""    valdata = DataLoader(test, batch_size=4)\n"",
+    ""    \n"",
+    ""    return traindata, valdata\n"",
+    ""\n"",
+    ""train, test = train_test(tr_10k)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 32,
+   ""id"": ""5f8f9343"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(2, 1)""
+      ]
+     },
+     ""execution_count"": 32,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""len(train), len(test)""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""6c0281be"",
+   ""metadata"": {},
+   ""source"": [
+    ""Even though we'll ultimately be using PyTorch Lightning for GPU training, let's try writing the training loop here so we can debug each step. To do this, we'll need to redefine the optimizer and loss""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 33,
+   ""id"": ""5a095017"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import torch.optim as optim\n"",
+    ""\n"",
+    ""criterion = nn.CrossEntropyLoss()\n"",
+    ""optimizer = optim.SGD(model.parameters(), lr=0.001)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 34,
+   ""id"": ""ad7ea9e0"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""2it [00:00, 10.89it/s]\n"",
+      ""2it [00:00, 21.30it/s]\n"",
+      ""2it [00:00, 23.01it/s]\n"",
+      ""2it [00:00, 22.39it/s]\n"",
+      ""2it [00:00, 22.45it/s]\n"",
+      ""2it [00:00, 23.05it/s]\n"",
+      ""2it [00:00, 22.09it/s]\n"",
+      ""2it [00:00, 23.70it/s]\n"",
+      ""2it [00:00, 23.56it/s]\n"",
+      ""2it [00:00, 22.95it/s]\n"",
+      ""2it [00:00, 23.40it/s]\n"",
+      ""2it [00:00, 23.46it/s]\n"",
+      ""2it [00:00, 23.33it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 22.13it/s]\n"",
+      ""2it [00:00, 23.10it/s]\n"",
+      ""2it [00:00, 23.39it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 23.67it/s]\n"",
+      ""2it [00:00, 23.38it/s]\n"",
+      ""2it [00:00, 23.21it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.73it/s]\n"",
+      ""2it [00:00, 23.35it/s]\n"",
+      ""2it [00:00, 23.32it/s]\n"",
+      ""2it [00:00, 23.60it/s]\n"",
+      ""2it [00:00, 23.45it/s]\n"",
+      ""2it [00:00, 23.45it/s]\n"",
+      ""2it [00:00, 23.59it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.51it/s]\n"",
+      ""2it [00:00, 23.36it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.60it/s]\n"",
+      ""2it [00:00, 22.94it/s]\n"",
+      ""2it [00:00, 22.76it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.44it/s]\n"",
+      ""2it [00:00, 23.54it/s]\n"",
+      ""2it [00:00, 23.39it/s]\n"",
+      ""2it [00:00, 22.65it/s]\n"",
+      ""2it [00:00, 22.75it/s]\n"",
+      ""2it [00:00, 23.31it/s]\n"",
+      ""2it [00:00, 23.45it/s]\n"",
+      ""2it [00:00, 23.34it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 23.66it/s]\n"",
+      ""2it [00:00, 22.96it/s]\n"",
+      ""2it [00:00, 22.73it/s]\n"",
+      ""2it [00:00, 22.98it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 23.15it/s]\n"",
+      ""2it [00:00, 23.39it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 23.27it/s]\n"",
+      ""2it [00:00, 23.55it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.73it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.76it/s]\n"",
+      ""2it [00:00, 23.76it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 23.76it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 23.74it/s]\n"",
+      ""2it [00:00, 23.88it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.74it/s]\n"",
+      ""2it [00:00, 23.88it/s]\n"",
+      ""2it [00:00, 23.82it/s]\n"",
+      ""2it [00:00, 23.23it/s]\n"",
+      ""2it [00:00, 22.82it/s]\n"",
+      ""2it [00:00, 23.01it/s]\n"",
+      ""2it [00:00, 23.57it/s]\n"",
+      ""2it [00:00, 23.46it/s]\n"",
+      ""2it [00:00, 23.43it/s]\n"",
+      ""2it [00:00, 23.50it/s]\n"",
+      ""2it [00:00, 23.53it/s]\n"",
+      ""2it [00:00, 23.50it/s]\n"",
+      ""2it [00:00, 23.13it/s]\n"",
+      ""2it [00:00, 23.32it/s]\n"",
+      ""2it [00:00, 23.61it/s]\n"",
+      ""2it [00:00, 23.46it/s]\n"",
+      ""2it [00:00, 23.66it/s]\n"",
+      ""2it [00:00, 23.64it/s]\n"",
+      ""2it [00:00, 23.33it/s]\n"",
+      ""2it [00:00, 23.89it/s]\n"",
+      ""2it [00:00, 23.19it/s]\n"",
+      ""2it [00:00, 22.96it/s]\n"",
+      ""2it [00:00, 23.50it/s]\n"",
+      ""2it [00:00, 23.27it/s]\n"",
+      ""2it [00:00, 23.53it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 23.68it/s]\n"",
+      ""2it [00:00, 23.59it/s]\n"",
+      ""2it [00:00, 22.74it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 23.54it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.43it/s]\n"",
+      ""2it [00:00, 23.44it/s]\n"",
+      ""2it [00:00, 23.61it/s]\n"",
+      ""2it [00:00, 23.19it/s]\n"",
+      ""2it [00:00, 22.84it/s]\n"",
+      ""2it [00:00, 23.11it/s]\n"",
+      ""2it [00:00, 23.13it/s]\n"",
+      ""2it [00:00, 23.12it/s]\n"",
+      ""2it [00:00, 23.27it/s]\n"",
+      ""2it [00:00, 23.17it/s]\n"",
+      ""2it [00:00, 23.18it/s]\n"",
+      ""2it [00:00, 23.20it/s]\n"",
+      ""2it [00:00, 23.04it/s]\n"",
+      ""2it [00:00, 23.16it/s]\n"",
+      ""2it [00:00, 23.32it/s]\n"",
+      ""2it [00:00, 23.43it/s]\n"",
+      ""2it [00:00, 22.95it/s]\n"",
+      ""2it [00:00, 23.56it/s]\n"",
+      ""2it [00:00, 23.58it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.64it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 23.08it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 23.57it/s]\n"",
+      ""2it [00:00, 23.42it/s]\n"",
+      ""2it [00:00, 23.36it/s]\n"",
+      ""2it [00:00, 23.28it/s]\n"",
+      ""2it [00:00, 23.47it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 23.38it/s]\n"",
+      ""2it [00:00, 23.56it/s]\n"",
+      ""2it [00:00, 23.27it/s]\n"",
+      ""2it [00:00, 23.08it/s]\n"",
+      ""2it [00:00, 23.11it/s]\n"",
+      ""2it [00:00, 22.35it/s]\n"",
+      ""2it [00:00, 22.95it/s]\n"",
+      ""2it [00:00, 23.38it/s]\n"",
+      ""2it [00:00, 23.32it/s]\n"",
+      ""2it [00:00, 23.20it/s]\n"",
+      ""2it [00:00, 23.23it/s]\n"",
+      ""2it [00:00, 23.56it/s]\n"",
+      ""2it [00:00, 23.68it/s]\n"",
+      ""2it [00:00, 23.87it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 23.95it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.04it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.09it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 18.32it/s]\n"",
+      ""2it [00:00, 22.07it/s]\n"",
+      ""2it [00:00, 18.39it/s]\n"",
+      ""2it [00:00, 18.88it/s]\n"",
+      ""2it [00:00, 20.05it/s]\n"",
+      ""2it [00:00, 21.95it/s]\n"",
+      ""2it [00:00, 22.46it/s]\n"",
+      ""2it [00:00, 22.74it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 23.15it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.71it/s]\n"",
+      ""2it [00:00, 23.51it/s]\n"",
+      ""2it [00:00, 23.78it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.50it/s]\n"",
+      ""2it [00:00, 23.62it/s]\n"",
+      ""2it [00:00, 23.78it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.87it/s]\n"",
+      ""2it [00:00, 23.77it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.59it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.92it/s]\n"",
+      ""2it [00:00, 23.95it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.96it/s]\n"",
+      ""2it [00:00, 23.82it/s]\n"",
+      ""2it [00:00, 23.02it/s]\n"",
+      ""2it [00:00, 23.50it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.86it/s]\n"",
+      ""2it [00:00, 23.77it/s]\n"",
+      ""2it [00:00, 23.56it/s]\n"",
+      ""2it [00:00, 23.57it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.03it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.41it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 23.92it/s]\n"",
+      ""2it [00:00, 24.37it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.02it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 23.92it/s]\n"",
+      ""2it [00:00, 23.64it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 23.74it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.82it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.36it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.44it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.39it/s]\n"",
+      ""2it [00:00, 24.36it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.39it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n""
+     ]
+    },
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.40it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.40it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.07it/s]\n"",
+      ""2it [00:00, 23.92it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 23.95it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.10it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.41it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 23.23it/s]\n"",
+      ""2it [00:00, 23.24it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.37it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.36it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.42it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.37it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.37it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.37it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 24.36it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.41it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.36it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.38it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 23.28it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.42it/s]\n"",
+      ""2it [00:00, 24.29it/s]\n"",
+      ""2it [00:00, 23.91it/s]\n"",
+      ""2it [00:00, 22.92it/s]\n"",
+      ""2it [00:00, 23.08it/s]\n"",
+      ""2it [00:00, 20.89it/s]\n"",
+      ""2it [00:00, 19.38it/s]\n"",
+      ""2it [00:00, 21.10it/s]\n"",
+      ""2it [00:00, 22.49it/s]\n"",
+      ""2it [00:00, 22.91it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 20.73it/s]\n"",
+      ""2it [00:00, 22.02it/s]\n"",
+      ""2it [00:00, 23.35it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 23.12it/s]\n"",
+      ""2it [00:00, 22.78it/s]\n"",
+      ""2it [00:00, 22.65it/s]\n"",
+      ""2it [00:00, 23.29it/s]\n"",
+      ""2it [00:00, 22.98it/s]\n"",
+      ""2it [00:00, 23.10it/s]\n"",
+      ""2it [00:00, 23.39it/s]\n"",
+      ""2it [00:00, 23.49it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.20it/s]\n"",
+      ""2it [00:00, 23.19it/s]\n"",
+      ""2it [00:00, 23.97it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 23.80it/s]\n"",
+      ""2it [00:00, 22.69it/s]\n"",
+      ""2it [00:00, 23.17it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 23.39it/s]\n"",
+      ""2it [00:00, 23.58it/s]\n"",
+      ""2it [00:00, 23.17it/s]\n"",
+      ""2it [00:00, 23.52it/s]\n"",
+      ""2it [00:00, 23.48it/s]\n"",
+      ""2it [00:00, 22.83it/s]\n"",
+      ""2it [00:00, 23.09it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 24.00it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 23.60it/s]\n"",
+      ""2it [00:00, 22.88it/s]\n"",
+      ""2it [00:00, 23.02it/s]\n"",
+      ""2it [00:00, 22.99it/s]\n"",
+      ""2it [00:00, 23.64it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 22.85it/s]\n"",
+      ""2it [00:00, 23.24it/s]\n"",
+      ""2it [00:00, 23.46it/s]\n"",
+      ""2it [00:00, 23.91it/s]\n"",
+      ""2it [00:00, 23.89it/s]\n"",
+      ""2it [00:00, 23.97it/s]\n"",
+      ""2it [00:00, 23.82it/s]\n"",
+      ""2it [00:00, 24.06it/s]\n"",
+      ""2it [00:00, 23.78it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.73it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 23.43it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.74it/s]\n"",
+      ""2it [00:00, 23.61it/s]\n"",
+      ""2it [00:00, 23.78it/s]\n"",
+      ""2it [00:00, 23.80it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.00it/s]\n"",
+      ""2it [00:00, 23.81it/s]\n"",
+      ""2it [00:00, 23.29it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 23.88it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 23.86it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.00it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.33it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 24.03it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.07it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.35it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.07it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.04it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.09it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.73it/s]\n"",
+      ""2it [00:00, 22.95it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.82it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 23.87it/s]\n"",
+      ""2it [00:00, 23.91it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.97it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.04it/s]\n"",
+      ""2it [00:00, 24.16it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.04it/s]\n"",
+      ""2it [00:00, 24.07it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.09it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 23.98it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.41it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 23.88it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 20.76it/s]\n"",
+      ""2it [00:00, 23.53it/s]\n"",
+      ""2it [00:00, 24.06it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.10it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 24.37it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.18it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.03it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.34it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n""
+     ]
+    },
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.01it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 24.25it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.24it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 24.20it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 24.10it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 23.26it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 23.70it/s]\n"",
+      ""2it [00:00, 23.49it/s]\n"",
+      ""2it [00:00, 24.05it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 24.22it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.23it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.26it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.01it/s]\n"",
+      ""2it [00:00, 23.98it/s]\n"",
+      ""2it [00:00, 24.21it/s]\n"",
+      ""2it [00:00, 24.08it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 24.30it/s]\n"",
+      ""2it [00:00, 24.27it/s]\n"",
+      ""2it [00:00, 24.11it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 24.15it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.35it/s]\n"",
+      ""2it [00:00, 23.70it/s]\n"",
+      ""2it [00:00, 24.19it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.26it/s]\n"",
+      ""2it [00:00, 23.60it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 23.80it/s]\n"",
+      ""2it [00:00, 23.07it/s]\n"",
+      ""2it [00:00, 23.45it/s]\n"",
+      ""2it [00:00, 23.16it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 23.19it/s]\n"",
+      ""2it [00:00, 23.71it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 24.17it/s]\n"",
+      ""2it [00:00, 23.51it/s]\n"",
+      ""2it [00:00, 23.48it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 23.97it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 23.02it/s]\n"",
+      ""2it [00:00, 23.28it/s]\n"",
+      ""2it [00:00, 23.70it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 24.01it/s]\n"",
+      ""2it [00:00, 24.13it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 23.21it/s]\n"",
+      ""2it [00:00, 23.17it/s]\n"",
+      ""2it [00:00, 22.79it/s]\n"",
+      ""2it [00:00, 23.43it/s]\n"",
+      ""2it [00:00, 23.71it/s]\n"",
+      ""2it [00:00, 23.52it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.98it/s]\n"",
+      ""2it [00:00, 23.64it/s]\n"",
+      ""2it [00:00, 23.12it/s]\n"",
+      ""2it [00:00, 23.06it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.82it/s]\n"",
+      ""2it [00:00, 23.00it/s]\n"",
+      ""2it [00:00, 22.80it/s]\n"",
+      ""2it [00:00, 22.89it/s]\n"",
+      ""2it [00:00, 23.28it/s]\n"",
+      ""2it [00:00, 23.64it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.75it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.87it/s]\n"",
+      ""2it [00:00, 23.86it/s]\n"",
+      ""2it [00:00, 24.14it/s]\n"",
+      ""2it [00:00, 23.71it/s]\n"",
+      ""2it [00:00, 23.63it/s]\n"",
+      ""2it [00:00, 23.56it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 24.01it/s]\n"",
+      ""2it [00:00, 24.00it/s]\n"",
+      ""2it [00:00, 23.96it/s]\n"",
+      ""2it [00:00, 24.01it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 23.85it/s]\n"",
+      ""2it [00:00, 23.07it/s]\n"",
+      ""2it [00:00, 22.76it/s]\n"",
+      ""2it [00:00, 22.65it/s]\n"",
+      ""2it [00:00, 23.02it/s]\n"",
+      ""2it [00:00, 23.35it/s]\n"",
+      ""2it [00:00, 22.95it/s]\n"",
+      ""2it [00:00, 22.83it/s]\n"",
+      ""2it [00:00, 23.23it/s]\n"",
+      ""2it [00:00, 22.16it/s]\n"",
+      ""2it [00:00, 22.73it/s]\n"",
+      ""2it [00:00, 23.33it/s]\n"",
+      ""2it [00:00, 23.26it/s]\n"",
+      ""2it [00:00, 23.71it/s]\n"",
+      ""2it [00:00, 23.00it/s]\n"",
+      ""2it [00:00, 22.73it/s]\n"",
+      ""2it [00:00, 22.64it/s]\n"",
+      ""2it [00:00, 22.77it/s]\n"",
+      ""2it [00:00, 22.69it/s]\n"",
+      ""2it [00:00, 23.02it/s]\n"",
+      ""2it [00:00, 22.67it/s]\n"",
+      ""2it [00:00, 22.95it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 23.10it/s]\n"",
+      ""2it [00:00, 23.40it/s]\n"",
+      ""2it [00:00, 23.66it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.87it/s]\n"",
+      ""2it [00:00, 23.98it/s]\n"",
+      ""2it [00:00, 24.32it/s]\n"",
+      ""2it [00:00, 24.28it/s]\n"",
+      ""2it [00:00, 24.31it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 23.89it/s]\n"",
+      ""2it [00:00, 23.18it/s]\n"",
+      ""2it [00:00, 22.30it/s]\n"",
+      ""2it [00:00, 22.35it/s]\n"",
+      ""2it [00:00, 22.41it/s]\n"",
+      ""2it [00:00, 22.71it/s]\n"",
+      ""2it [00:00, 22.45it/s]\n"",
+      ""2it [00:00, 22.06it/s]\n"",
+      ""2it [00:00, 21.08it/s]\n"",
+      ""2it [00:00, 22.68it/s]\n"",
+      ""2it [00:00, 22.80it/s]\n"",
+      ""2it [00:00, 23.54it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 23.51it/s]\n"",
+      ""2it [00:00, 23.45it/s]\n"",
+      ""2it [00:00, 23.90it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 23.99it/s]\n"",
+      ""2it [00:00, 24.10it/s]\n"",
+      ""2it [00:00, 23.97it/s]\n"",
+      ""2it [00:00, 23.84it/s]\n"",
+      ""2it [00:00, 23.39it/s]\n"",
+      ""2it [00:00, 23.10it/s]\n"",
+      ""2it [00:00, 23.05it/s]\n"",
+      ""2it [00:00, 23.17it/s]\n"",
+      ""2it [00:00, 23.07it/s]\n"",
+      ""2it [00:00, 22.98it/s]\n"",
+      ""2it [00:00, 22.91it/s]\n"",
+      ""2it [00:00, 22.81it/s]\n"",
+      ""2it [00:00, 23.37it/s]\n"",
+      ""2it [00:00, 23.49it/s]\n"",
+      ""2it [00:00, 23.59it/s]\n"",
+      ""2it [00:00, 23.17it/s]\n"",
+      ""2it [00:00, 23.09it/s]\n"",
+      ""2it [00:00, 23.26it/s]\n"",
+      ""2it [00:00, 23.14it/s]\n"",
+      ""2it [00:00, 23.37it/s]\n"",
+      ""2it [00:00, 23.04it/s]\n"",
+      ""2it [00:00, 22.84it/s]\n"",
+      ""2it [00:00, 23.24it/s]\n"",
+      ""2it [00:00, 23.54it/s]\n"",
+      ""2it [00:00, 23.45it/s]\n"",
+      ""2it [00:00, 23.66it/s]\n"",
+      ""2it [00:00, 23.55it/s]\n"",
+      ""2it [00:00, 23.57it/s]\n"",
+      ""2it [00:00, 23.72it/s]\n"",
+      ""2it [00:00, 23.71it/s]\n"",
+      ""2it [00:00, 24.12it/s]\n"",
+      ""2it [00:00, 23.93it/s]\n"",
+      ""2it [00:00, 23.68it/s]\n"",
+      ""2it [00:00, 23.42it/s]\n"",
+      ""2it [00:00, 23.76it/s]\n"",
+      ""2it [00:00, 23.62it/s]\n"",
+      ""2it [00:00, 23.73it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 21.89it/s]\n"",
+      ""2it [00:00, 23.12it/s]\n"",
+      ""2it [00:00, 23.07it/s]\n"",
+      ""2it [00:00, 22.98it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 22.94it/s]\n"",
+      ""2it [00:00, 23.22it/s]\n"",
+      ""2it [00:00, 23.08it/s]\n"",
+      ""2it [00:00, 23.44it/s]\n"",
+      ""2it [00:00, 21.80it/s]\n"",
+      ""2it [00:00, 20.84it/s]\n"",
+      ""2it [00:00, 18.62it/s]\n"",
+      ""2it [00:00, 20.25it/s]\n"",
+      ""2it [00:00, 20.49it/s]\n"",
+      ""2it [00:00, 22.75it/s]\n"",
+      ""2it [00:00, 23.24it/s]\n"",
+      ""2it [00:00, 23.20it/s]\n"",
+      ""2it [00:00, 23.08it/s]\n"",
+      ""2it [00:00, 22.62it/s]\n"",
+      ""2it [00:00, 23.09it/s]\n"",
+      ""2it [00:00, 21.49it/s]\n"",
+      ""2it [00:00, 22.43it/s]\n"",
+      ""2it [00:00, 22.70it/s]\n"",
+      ""2it [00:00, 19.67it/s]\n"",
+      ""2it [00:00, 21.92it/s]\n"",
+      ""2it [00:00, 23.26it/s]\n"",
+      ""2it [00:00, 22.85it/s]\n"",
+      ""2it [00:00, 23.30it/s]\n"",
+      ""2it [00:00, 23.61it/s]\n"",
+      ""2it [00:00, 23.25it/s]\n"",
+      ""2it [00:00, 23.52it/s]\n"",
+      ""2it [00:00, 23.55it/s]\n"",
+      ""2it [00:00, 23.83it/s]\n"",
+      ""2it [00:00, 23.59it/s]\n"",
+      ""2it [00:00, 23.66it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.79it/s]\n"",
+      ""2it [00:00, 23.68it/s]\n"",
+      ""2it [00:00, 23.69it/s]\n"",
+      ""2it [00:00, 23.94it/s]\n"",
+      ""2it [00:00, 23.86it/s]\n"",
+      ""2it [00:00, 23.81it/s]\n"",
+      ""2it [00:00, 22.41it/s]\n"",
+      ""2it [00:00, 22.10it/s]\n"",
+      ""2it [00:00, 22.73it/s]\n"",
+      ""2it [00:00, 23.42it/s]\n"",
+      ""2it [00:00, 22.91it/s]\n"",
+      ""2it [00:00, 23.27it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 23.47it/s]\n"",
+      ""2it [00:00, 23.36it/s]\n"",
+      ""2it [00:00, 23.58it/s]\n"",
+      ""2it [00:00, 23.61it/s]\n"",
+      ""2it [00:00, 23.57it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 22.71it/s]\n"",
+      ""2it [00:00, 22.97it/s]\n"",
+      ""2it [00:00, 22.91it/s]\n"",
+      ""2it [00:00, 23.54it/s]\n"",
+      ""2it [00:00, 23.09it/s]\n"",
+      ""2it [00:00, 23.61it/s]\n"",
+      ""2it [00:00, 23.89it/s]\n"",
+      ""2it [00:00, 23.65it/s]\n"",
+      ""2it [00:00, 23.48it/s]\n"",
+      ""2it [00:00, 23.31it/s]\n"",
+      ""2it [00:00, 23.30it/s]\n"",
+      ""2it [00:00, 23.42it/s]\n"",
+      ""2it [00:00, 23.41it/s]\n"",
+      ""2it [00:00, 23.51it/s]\n"",
+      ""2it [00:00, 23.51it/s]\n"",
+      ""2it [00:00, 23.74it/s]""
+     ]
+    },
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Finished Training\n""
+     ]
+    },
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""from tqdm import tqdm \n"",
+    ""\n"",
+    ""for epoch in range(1000):  # loop over the dataset multiple times\n"",
+    ""    running_loss = 0.0\n"",
+    ""    for i, sample in tqdm(enumerate(train)):\n"",
+    ""        # get the inputs; data is a list of [inputs, labels]\n"",
+    ""        inputs, labels = sample\n"",
+    ""\n"",
+    ""        # zero the parameter gradients\n"",
+    ""        optimizer.zero_grad()\n"",
+    ""\n"",
+    ""        # forward + backward + optimize\n"",
+    ""        outputs = model(inputs)\n"",
+    ""        loss = criterion(outputs, labels)\n"",
+    ""        loss.backward()\n"",
+    ""        optimizer.step()\n"",
+    ""\n"",
+    ""        # print statistics\n"",
+    ""        running_loss += loss.item()\n"",
+    ""        if i % 100 == 0: # print every 2000 mini-batches\n"",
+    ""#             print(epoch, running_loss / 100)\n"",
+    ""            running_loss = 0.0\n"",
+    ""\n"",
+    ""print('Finished Training')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""bf976009"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from typing import *\n"",
+    ""from torchmetrics import Accuracy\n"",
+    ""import torch.nn.functional as F\n"",
+    ""\n"",
+    ""class TEST(pl.LightningModule):\n"",
+    ""    def __init__(self, \n"",
+    ""        N_features: int, \n"",
+    ""        N_labels: int, \n"",
+    ""        weights: List[torch.Tensor], \n"",
+    ""        params: Dict[str, float],\n"",
+    ""    ):\n"",
+    ""        super(TEST, self).__init__()\n"",
+    ""\n"",
+    ""        # Set hyperparameters\n"",
+    ""        self.width = params['width']\n"",
+    ""        self.layers = params['layers']\n"",
+    ""        self.lr = params['lr']\n"",
+    ""        self.momentum = params['momentum']\n"",
+    ""        self.weight_decay = params['weight_decay']\n"",
+    ""\n"",
+    ""        layers = self.layers*[\n"",
+    ""            nn.Linear(self.width, self.width),\n"",
+    ""            nn.ReLU(),\n"",
+    ""            nn.Dropout(0.5),\n"",
+    ""            nn.BatchNorm1d(self.width),\n"",
+    ""        ]\n"",
+    ""\n"",
+    ""        self.flatten = nn.Flatten()\n"",
+    ""        self.linear_relu_stack = nn.Sequential(\n"",
+    ""            nn.Linear(N_features, self.width),\n"",
+    ""            *layers,\n"",
+    ""            nn.Linear(self.width, N_labels),\n"",
+    ""        )\n"",
+    ""\n"",
+    ""        self.accuracy = Accuracy(average='weighted', num_classes=N_labels)\n"",
+    ""        self.weights = weights\n"",
+    ""\n"",
+    ""    def forward(self, x):\n"",
+    ""        x = self.flatten(x)\n"",
+    ""        logits = self.linear_relu_stack(x)\n"",
+    ""        return logits\n"",
+    ""\n"",
+    ""    def configure_optimizers(self):\n"",
+    ""        optimizer = torch.optim.SGD(\n"",
+    ""            self.parameters(),\n"",
+    ""            lr=self.lr, \n"",
+    ""            momentum=self.momentum, \n"",
+    ""            weight_decay=self.weight_decay,\n"",
+    ""        )\n"",
+    ""\n"",
+    ""        return optimizer\n"",
+    ""\n"",
+    ""    def training_step(self, batch, batch_idx):\n"",
+    ""        x, y = batch\n"",
+    ""        y_hat = self(x)\n"",
+    ""        loss = F.cross_entropy(y_hat, y, weight=self.weights)\n"",
+    ""        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n"",
+    ""\n"",
+    ""        self.log(\""train_loss\"", loss, logger=True, on_epoch=True)\n"",
+    ""        self.log(\""train_accuracy\"", acc, logger=True, on_epoch=True)\n"",
+    ""\n"",
+    ""        return loss\n"",
+    ""    \n"",
+    ""    def validation_step(self, batch, batch_idx):\n"",
+    ""        x, y = batch\n"",
+    ""        y_hat = self(x)\n"",
+    ""        val_loss = F.cross_entropy(y_hat, y, weight=self.weights)\n"",
+    ""        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n"",
+    ""\n"",
+    ""        self.log(\""val_loss\"", val_loss, logger=True, on_epoch=True)\n"",
+    ""        self.log(\""val_accuracy\"", acc, logger=True, on_epoch=True)\n"",
+    ""\n"",
+    ""        return val_loss\n"",
+    ""    \n"",
+    ""model = TEST(\n"",
+    ""    N_features = data.num_features(),\n"",
+    ""    N_labels = data.num_labels(),\n"",
+    ""    weights=data.compute_class_weights(),\n"",
+    ""    params={\n"",
+    ""        'width' : 2,\n"",
+    ""        'layers': 2,\n"",
+    ""        'epochs': 10,\n"",
+    ""        'lr': 0.001,\n"",
+    ""        'momentum': 0,\n"",
+    ""        'weight_decay':0,\n"",
+    ""    }\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""bb7183d5"",
+   ""metadata"": {},
+   ""source"": [
+    ""Our cost function is converging on a small subset of the data, which is good! Now let's try this same training routine with PyTorch Lightning to make sure nothing is going awry there.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""9c4e43af"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""from pytorch_lightning import Trainer\n"",
+    ""\n"",
+    ""run = Trainer()\n"",
+    ""run.fit(model, train, test)""
+   ]
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""964ffb0b"",
+   ""metadata"": {},
+   ""source"": [
+    ""# PCA Data Visualization\n"",
+    ""\n"",
+    ""Here, we'll visualize the PCA data and the clusters found when clustering over the PCA data.""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 4,
+   ""id"": ""9c1b35c5"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""import seaborn as sns\n"",
+    ""import numpy as np\n"",
+    ""import os ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 6,
+   ""id"": ""ae40089f"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""# label    3\n"",
+      ""dtype: int64\n"",
+      ""# label    3\n"",
+      ""dtype: int64\n"",
+      ""# label    1\n"",
+      ""dtype: int64\n"",
+      ""# label    1\n"",
+      ""dtype: int64\n"",
+      ""# label    3\n"",
+      ""dtype: int64\n"",
+      ""# label    3\n"",
+      ""dtype: int64\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""pca_folder = '../data/processed/pca/'\n"",
+    ""\n"",
+    ""for f in os.listdir(pca_folder):\n"",
+    ""    if f.startswith('primary'):\n"",
+    ""        pca_labels = pd.read_csv(pca_folder+f)\n"",
+    ""        print(pca_labels.nunique())""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""4a2bba2d"",
+   ""metadata"": {},
+   ""source"": [
+    ""So we see that there are only three distinct clusters when running HDBSCAN on the PCA data, which is not particularly interesting. ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""726f2e4a"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""id"": ""fe243745"",
+   ""metadata"": {},
+   ""source"": [
+    ""# PCA Reduction\n"",
+    ""\n"",
+    ""In this notebook, we test Dask's PCA""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 21,
+   ""id"": ""ada240d0"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import dask.dataframe as dd\n"",
+    ""from dask_ml.decomposition import PCA""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 22,
+   ""id"": ""a4ff13f4"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df = dd.read_csv('../data/processed/organoid_reduction_neighbors_5000_components_50.csv')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 23,
+   ""id"": ""2c9e2e25"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""est = PCA(n_components=3)\n"",
+    ""pca = est.fit_transform(df.values.compute_chunk_sizes())""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 24,
+   ""id"": ""3f17067c"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""df = pd.DataFrame(pca.compute())""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 27,
+   ""id"": ""7746845c"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df = dd.from_array(pca.compute())""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 31,
+   ""id"": ""5aac716b"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""['/Users/julian/Documents/Projects/organoid-classification/notebooks/asdf.csv']""
+      ]
+     },
+     ""execution_count"": 31,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df.to_csv('asdf.csv', single_file=True, index=False)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 34,
+   ""id"": ""55b3458c"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>0</th>\n"",
+       ""      <th>1</th>\n"",
+       ""      <th>2</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>-0.200472</td>\n"",
+       ""      <td>1.460368</td>\n"",
+       ""      <td>0.795623</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>-1.648086</td>\n"",
+       ""      <td>1.578398</td>\n"",
+       ""      <td>1.271326</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>-1.709614</td>\n"",
+       ""      <td>1.479038</td>\n"",
+       ""      <td>1.305297</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>-2.133705</td>\n"",
+       ""      <td>1.312050</td>\n"",
+       ""      <td>1.383508</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>-1.709739</td>\n"",
+       ""      <td>1.441671</td>\n"",
+       ""      <td>1.194556</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>235116</th>\n"",
+       ""      <td>-2.684774</td>\n"",
+       ""      <td>0.883385</td>\n"",
+       ""      <td>1.525278</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>235117</th>\n"",
+       ""      <td>-2.689195</td>\n"",
+       ""      <td>0.879093</td>\n"",
+       ""      <td>1.520911</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>235118</th>\n"",
+       ""      <td>-2.686528</td>\n"",
+       ""      <td>0.855352</td>\n"",
+       ""      <td>1.516532</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>235119</th>\n"",
+       ""      <td>-2.667541</td>\n"",
+       ""      <td>0.851988</td>\n"",
+       ""      <td>1.516948</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>235120</th>\n"",
+       ""      <td>-2.675737</td>\n"",
+       ""      <td>0.895247</td>\n"",
+       ""      <td>1.496369</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>235121 rows × 3 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""               0         1         2\n"",
+       ""0      -0.200472  1.460368  0.795623\n"",
+       ""1      -1.648086  1.578398  1.271326\n"",
+       ""2      -1.709614  1.479038  1.305297\n"",
+       ""3      -2.133705  1.312050  1.383508\n"",
+       ""4      -1.709739  1.441671  1.194556\n"",
+       ""...          ...       ...       ...\n"",
+       ""235116 -2.684774  0.883385  1.525278\n"",
+       ""235117 -2.689195  0.879093  1.520911\n"",
+       ""235118 -2.686528  0.855352  1.516532\n"",
+       ""235119 -2.667541  0.851988  1.516948\n"",
+       ""235120 -2.675737  0.895247  1.496369\n"",
+       ""\n"",
+       ""[235121 rows x 3 columns]""
+      ]
+     },
+     ""execution_count"": 34,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""df = pd.read_csv('asdf.csv')\n"",
+    ""df""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""9ce23e24"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""1c253f15"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import sklearn as sk\n"",
+    ""import umap\n"",
+    ""import pandas as pd \n"",
+    ""from sklearn.datasets import make_blobs\n""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 32,
+   ""id"": ""3d3eb3a1"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df = pd.DataFrame((make_blobs(n_features=3, centers=2)[0])).rename({0: 'weight', 1: 'height', 2: 'width'}, axis=1)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 33,
+   ""id"": ""24ebff84"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>weight</th>\n"",
+       ""      <th>height</th>\n"",
+       ""      <th>width</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>-11.980354</td>\n"",
+       ""      <td>2.621863</td>\n"",
+       ""      <td>-8.980304</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>-6.290433</td>\n"",
+       ""      <td>5.933075</td>\n"",
+       ""      <td>4.352212</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>-6.736897</td>\n"",
+       ""      <td>5.549988</td>\n"",
+       ""      <td>2.326054</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>-8.156811</td>\n"",
+       ""      <td>5.031962</td>\n"",
+       ""      <td>-9.142629</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>-7.405074</td>\n"",
+       ""      <td>6.383769</td>\n"",
+       ""      <td>2.842553</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>95</th>\n"",
+       ""      <td>-9.907771</td>\n"",
+       ""      <td>4.283176</td>\n"",
+       ""      <td>-8.469899</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>96</th>\n"",
+       ""      <td>-9.442281</td>\n"",
+       ""      <td>3.865622</td>\n"",
+       ""      <td>-5.982615</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>97</th>\n"",
+       ""      <td>-5.435963</td>\n"",
+       ""      <td>6.563417</td>\n"",
+       ""      <td>2.632139</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>98</th>\n"",
+       ""      <td>-6.403596</td>\n"",
+       ""      <td>4.358372</td>\n"",
+       ""      <td>2.461025</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>99</th>\n"",
+       ""      <td>-7.346309</td>\n"",
+       ""      <td>4.956193</td>\n"",
+       ""      <td>4.225545</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>100 rows × 3 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""       weight    height     width\n"",
+       ""0  -11.980354  2.621863 -8.980304\n"",
+       ""1   -6.290433  5.933075  4.352212\n"",
+       ""2   -6.736897  5.549988  2.326054\n"",
+       ""3   -8.156811  5.031962 -9.142629\n"",
+       ""4   -7.405074  6.383769  2.842553\n"",
+       ""..        ...       ...       ...\n"",
+       ""95  -9.907771  4.283176 -8.469899\n"",
+       ""96  -9.442281  3.865622 -5.982615\n"",
+       ""97  -5.435963  6.563417  2.632139\n"",
+       ""98  -6.403596  4.358372  2.461025\n"",
+       ""99  -7.346309  4.956193  4.225545\n"",
+       ""\n"",
+       ""[100 rows x 3 columns]""
+      ]
+     },
+     ""execution_count"": 33,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 34,
+   ""id"": ""98eb1803"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""application/vnd.plotly.v1+json"": {
+       ""config"": {
+        ""plotlyServerURL"": ""https://plot.ly""
+       },
+       ""data"": [
+        {
+         ""hovertemplate"": ""weight=%{x}<br>height=%{y}<br>width=%{z}<extra></extra>"",
+         ""legendgroup"": """",
+         ""marker"": {
+          ""color"": ""#636efa"",
+          ""symbol"": ""circle""
+         },
+         ""mode"": ""markers"",
+         ""name"": """",
+         ""scene"": ""scene"",
+         ""showlegend"": false,
+         ""type"": ""scatter3d"",
+         ""x"": [
+          -11.9803541031763,
+          -6.290432662924863,
+          -6.736897011007884,
+          -8.156811473487632,
+          -7.405073642076633,
+          -6.416653968467081,
+          -4.199043502762057,
+          -8.460163155929353,
+          -7.397911579715805,
+          -9.560239918557867,
+          -8.453370190829258,
+          -6.298130255725915,
+          -7.485201310580931,
+          -6.3101375895547385,
+          -8.496894508580661,
+          -6.874799943946652,
+          -6.187250395932087,
+          -8.714201646924252,
+          -7.120670557104987,
+          -7.495445273855449,
+          -8.921581994608681,
+          -8.734210554101844,
+          -10.27680600806525,
+          -5.9595782992971404,
+          -8.655896799724701,
+          -9.740676375133184,
+          -8.55612128866845,
+          -7.566782112826392,
+          -6.223530878233048,
+          -6.351577646939033,
+          -5.570981622085944,
+          -7.249180629203571,
+          -7.7392568599091005,
+          -9.813417235800285,
+          -9.24722775957893,
+          -9.387695116649855,
+          -7.641125213998168,
+          -8.148456482470808,
+          -7.606749794441968,
+          -5.602791904157758,
+          -10.044837244315248,
+          -6.6897716224806025,
+          -5.829061180774874,
+          -4.369373813465966,
+          -6.751313191006155,
+          -9.627757169521544,
+          -7.247582785887058,
+          -5.641135551327822,
+          -8.059769568192442,
+          -7.556781910951701,
+          -7.961956052638345,
+          -6.78607872393919,
+          -4.490649368867333,
+          -9.581429100417528,
+          -8.13756415487571,
+          -5.436586168130162,
+          -9.377155876791416,
+          -8.204760197355379,
+          -8.273400862043221,
+          -7.35809861610496,
+          -5.28911018738381,
+          -9.552126243844748,
+          -8.019895734557249,
+          -10.933507310675115,
+          -8.048142898292918,
+          -7.75595917284385,
+          -9.297213104347684,
+          -7.489361364455802,
+          -7.629443081780139,
+          -6.397586224266937,
+          -6.008785826717765,
+          -8.485243660453799,
+          -9.347816213799701,
+          -5.539371831651236,
+          -9.597806229301346,
+          -10.037967088289454,
+          -8.833514429283788,
+          -9.603975958066027,
+          -7.447962655044643,
+          -6.90013700365057,
+          -6.314725308361885,
+          -8.781288132732136,
+          -8.177933195846942,
+          -8.436176500006825,
+          -8.759380332628956,
+          -7.0656131266963715,
+          -8.376229274438645,
+          -8.167349837337998,
+          -6.873739962903461,
+          -6.377090089298508,
+          -9.793901280542459,
+          -8.197201891104232,
+          -5.237405968994661,
+          -6.319245461649304,
+          -6.12329840933832,
+          -9.907771036201598,
+          -9.4422812539316,
+          -5.435963063899374,
+          -6.40359567621175,
+          -7.346308545428884
+         ],
+         ""y"": [
+          2.621863248350037,
+          5.933074579207716,
+          5.54998824684697,
+          5.031962097109114,
+          6.3837691161203844,
+          7.565465274666399,
+          5.226470520630366,
+          3.735054758556701,
+          6.514621906918775,
+          4.146131655115221,
+          3.638158087942962,
+          4.599563825475971,
+          8.532036223566486,
+          6.659866291772025,
+          5.594064518884458,
+          5.937615519064998,
+          6.471017566389171,
+          4.351653598242385,
+          3.8335876570769023,
+          5.6701448299022035,
+          1.7709747938023435,
+          2.8660119259081855,
+          3.3944962529930263,
+          7.683219737494448,
+          4.190099031406584,
+          4.277572418292747,
+          1.9145031924932943,
+          6.151046021010221,
+          6.869859568906565,
+          6.478292642948072,
+          7.340421201948603,
+          7.632333719823853,
+          2.8048546058564705,
+          3.2556297881293594,
+          3.577846089279434,
+          6.288041785161829,
+          4.381017768199817,
+          1.8497254425499112,
+          8.188926043447623,
+          4.7998406157613696,
+          2.867222729171471,
+          8.496805642998673,
+          6.58460480198905,
+          4.586029237142924,
+          6.142619282450821,
+          6.685936369857078,
+          5.6343999297928145,
+          6.445389771714437,
+          2.537948934674584,
+          4.216087427926357,
+          3.4055450184815914,
+          2.4784538392676794,
+          6.134274131233912,
+          3.493918933447605,
+          2.598664883766191,
+          6.1708855942259735,
+          4.282373261023311,
+          3.499764788212491,
+          4.111248022976987,
+          7.409556319425051,
+          6.306164820580588,
+          2.414856508248195,
+          6.516757737056285,
+          2.5728198630398276,
+          4.877349180026583,
+          6.510434646597215,
+          4.480998547984656,
+          3.5608039396149205,
+          3.7093465134876746,
+          6.019216475336875,
+          6.220356440282124,
+          3.644172501892266,
+          2.4771042652681667,
+          6.826460857045079,
+          2.157773979270363,
+          4.606777778339397,
+          4.127295028491313,
+          1.5899549973883143,
+          5.355475116799475,
+          2.8303911841724023,
+          7.483261911325251,
+          1.8907453687901015,
+          6.752554262753341,
+          3.2162035047026567,
+          4.848001017307087,
+          2.7558616033618364,
+          4.533437174381517,
+          3.856814525466609,
+          3.590122618977335,
+          6.049719618855952,
+          2.040304844847453,
+          3.6208575024497343,
+          6.893397685958687,
+          7.231419095471904,
+          7.098750084844298,
+          4.283175899832392,
+          3.865622195665978,
+          6.563416988410524,
+          4.3583720623868665,
+          4.956193036361509
+         ],
+         ""z"": [
+          -8.980304472324729,
+          4.352211546348372,
+          2.326053796748647,
+          -9.142628524691828,
+          2.84255290491211,
+          4.194107716964702,
+          3.7310384271156027,
+          -9.283253035872296,
+          2.4317719997975193,
+          -7.6004416918539395,
+          -7.875002542478913,
+          2.3024425696293065,
+          3.661923119539432,
+          3.8007057517452827,
+          1.2809783149662328,
+          3.3369784850117754,
+          3.5264636482696785,
+          -7.759025383836713,
+          4.3193065349107185,
+          4.30152318244655,
+          -8.152944488642069,
+          -6.047771068835265,
+          -8.58381277758478,
+          4.620589056308427,
+          -8.485361165607946,
+          -8.500263615098733,
+          -8.794836967881636,
+          3.0420211613917587,
+          1.9377382614370657,
+          3.7518914900745792,
+          3.863132196473438,
+          2.842052435954093,
+          -8.191877841041215,
+          -7.741263617269853,
+          -8.789947793215035,
+          -8.372527180207,
+          -8.291608973246557,
+          -8.472443010981127,
+          5.3350381121896,
+          3.6728799764823865,
+          -9.8781365701743,
+          2.01419495062026,
+          4.607977870711361,
+          4.7293110770773605,
+          3.276772403676553,
+          3.859198719122157,
+          3.058859665091609,
+          3.2438269658152263,
+          -8.272470412395178,
+          -10.294831342267837,
+          -8.292932071303184,
+          -9.528898690390196,
+          3.9844189993182995,
+          -9.617804181978878,
+          -8.411075501978459,
+          2.9028474303345484,
+          -7.625550335965655,
+          -8.67576730179624,
+          -10.196804308938402,
+          3.294106523541445,
+          3.38963918016624,
+          -7.22961697344955,
+          2.135401828027986,
+          -7.715014142112663,
+          2.0747779853608486,
+          3.6053975917028236,
+          -10.55113161560795,
+          -7.493962463928635,
+          -8.960854349821204,
+          3.3994356544784865,
+          2.5810824758198856,
+          -9.641207186328932,
+          -9.903195222252743,
+          3.9976375682044782,
+          -7.721036209182189,
+          -8.961198389921798,
+          -9.958636142961257,
+          -9.346668103341361,
+          4.01124738943291,
+          -7.792754830497518,
+          2.4296165763135544,
+          -6.133494525352463,
+          3.3157516261437094,
+          -8.39634835040812,
+          -8.317951020165026,
+          -9.418644795470371,
+          -7.8688977161660505,
+          -7.335070874009395,
+          2.8206037963300075,
+          4.033561565986594,
+          -8.57653911512009,
+          -6.1639392851757275,
+          4.2563716557463875,
+          3.543552052697827,
+          2.5587825723261903,
+          -8.469899355989549,
+          -5.982614995951284,
+          2.6321390360439274,
+          2.4610248181199634,
+          4.225544678212692
+         ]
+        }
+       ],
+       ""layout"": {
+        ""legend"": {
+         ""tracegroupgap"": 0
+        },
+        ""margin"": {
+         ""t"": 60
+        },
+        ""scene"": {
+         ""domain"": {
+          ""x"": [
+           0,
+           1
+          ],
+          ""y"": [
+           0,
+           1
+          ]
+         },
+         ""xaxis"": {
+          ""title"": {
+           ""text"": ""weight""
+          }
+         },
+         ""yaxis"": {
+          ""title"": {
+           ""text"": ""height""
+          }
+         },
+         ""zaxis"": {
+          ""title"": {
+           ""text"": ""width""
+          }
+         }
+        },
+        ""template"": {
+         ""data"": {
+          ""bar"": [
+           {
+            ""error_x"": {
+             ""color"": ""#2a3f5f""
+            },
+            ""error_y"": {
+             ""color"": ""#2a3f5f""
+            },
+            ""marker"": {
+             ""line"": {
+              ""color"": ""#E5ECF6"",
+              ""width"": 0.5
+             },
+             ""pattern"": {
+              ""fillmode"": ""overlay"",
+              ""size"": 10,
+              ""solidity"": 0.2
+             }
+            },
+            ""type"": ""bar""
+           }
+          ],
+          ""barpolar"": [
+           {
+            ""marker"": {
+             ""line"": {
+              ""color"": ""#E5ECF6"",
+              ""width"": 0.5
+             },
+             ""pattern"": {
+              ""fillmode"": ""overlay"",
+              ""size"": 10,
+              ""solidity"": 0.2
+             }
+            },
+            ""type"": ""barpolar""
+           }
+          ],
+          ""carpet"": [
+           {
+            ""aaxis"": {
+             ""endlinecolor"": ""#2a3f5f"",
+             ""gridcolor"": ""white"",
+             ""linecolor"": ""white"",
+             ""minorgridcolor"": ""white"",
+             ""startlinecolor"": ""#2a3f5f""
+            },
+            ""baxis"": {
+             ""endlinecolor"": ""#2a3f5f"",
+             ""gridcolor"": ""white"",
+             ""linecolor"": ""white"",
+             ""minorgridcolor"": ""white"",
+             ""startlinecolor"": ""#2a3f5f""
+            },
+            ""type"": ""carpet""
+           }
+          ],
+          ""choropleth"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""type"": ""choropleth""
+           }
+          ],
+          ""contour"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""contour""
+           }
+          ],
+          ""contourcarpet"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""type"": ""contourcarpet""
+           }
+          ],
+          ""heatmap"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""heatmap""
+           }
+          ],
+          ""heatmapgl"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""heatmapgl""
+           }
+          ],
+          ""histogram"": [
+           {
+            ""marker"": {
+             ""pattern"": {
+              ""fillmode"": ""overlay"",
+              ""size"": 10,
+              ""solidity"": 0.2
+             }
+            },
+            ""type"": ""histogram""
+           }
+          ],
+          ""histogram2d"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""histogram2d""
+           }
+          ],
+          ""histogram2dcontour"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""histogram2dcontour""
+           }
+          ],
+          ""mesh3d"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""type"": ""mesh3d""
+           }
+          ],
+          ""parcoords"": [
+           {
+            ""line"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""parcoords""
+           }
+          ],
+          ""pie"": [
+           {
+            ""automargin"": true,
+            ""type"": ""pie""
+           }
+          ],
+          ""scatter"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatter""
+           }
+          ],
+          ""scatter3d"": [
+           {
+            ""line"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatter3d""
+           }
+          ],
+          ""scattercarpet"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattercarpet""
+           }
+          ],
+          ""scattergeo"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattergeo""
+           }
+          ],
+          ""scattergl"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattergl""
+           }
+          ],
+          ""scattermapbox"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattermapbox""
+           }
+          ],
+          ""scatterpolar"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatterpolar""
+           }
+          ],
+          ""scatterpolargl"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatterpolargl""
+           }
+          ],
+          ""scatterternary"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatterternary""
+           }
+          ],
+          ""surface"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""surface""
+           }
+          ],
+          ""table"": [
+           {
+            ""cells"": {
+             ""fill"": {
+              ""color"": ""#EBF0F8""
+             },
+             ""line"": {
+              ""color"": ""white""
+             }
+            },
+            ""header"": {
+             ""fill"": {
+              ""color"": ""#C8D4E3""
+             },
+             ""line"": {
+              ""color"": ""white""
+             }
+            },
+            ""type"": ""table""
+           }
+          ]
+         },
+         ""layout"": {
+          ""annotationdefaults"": {
+           ""arrowcolor"": ""#2a3f5f"",
+           ""arrowhead"": 0,
+           ""arrowwidth"": 1
+          },
+          ""autotypenumbers"": ""strict"",
+          ""coloraxis"": {
+           ""colorbar"": {
+            ""outlinewidth"": 0,
+            ""ticks"": """"
+           }
+          },
+          ""colorscale"": {
+           ""diverging"": [
+            [
+             0,
+             ""#8e0152""
+            ],
+            [
+             0.1,
+             ""#c51b7d""
+            ],
+            [
+             0.2,
+             ""#de77ae""
+            ],
+            [
+             0.3,
+             ""#f1b6da""
+            ],
+            [
+             0.4,
+             ""#fde0ef""
+            ],
+            [
+             0.5,
+             ""#f7f7f7""
+            ],
+            [
+             0.6,
+             ""#e6f5d0""
+            ],
+            [
+             0.7,
+             ""#b8e186""
+            ],
+            [
+             0.8,
+             ""#7fbc41""
+            ],
+            [
+             0.9,
+             ""#4d9221""
+            ],
+            [
+             1,
+             ""#276419""
+            ]
+           ],
+           ""sequential"": [
+            [
+             0,
+             ""#0d0887""
+            ],
+            [
+             0.1111111111111111,
+             ""#46039f""
+            ],
+            [
+             0.2222222222222222,
+             ""#7201a8""
+            ],
+            [
+             0.3333333333333333,
+             ""#9c179e""
+            ],
+            [
+             0.4444444444444444,
+             ""#bd3786""
+            ],
+            [
+             0.5555555555555556,
+             ""#d8576b""
+            ],
+            [
+             0.6666666666666666,
+             ""#ed7953""
+            ],
+            [
+             0.7777777777777778,
+             ""#fb9f3a""
+            ],
+            [
+             0.8888888888888888,
+             ""#fdca26""
+            ],
+            [
+             1,
+             ""#f0f921""
+            ]
+           ],
+           ""sequentialminus"": [
+            [
+             0,
+             ""#0d0887""
+            ],
+            [
+             0.1111111111111111,
+             ""#46039f""
+            ],
+            [
+             0.2222222222222222,
+             ""#7201a8""
+            ],
+            [
+             0.3333333333333333,
+             ""#9c179e""
+            ],
+            [
+             0.4444444444444444,
+             ""#bd3786""
+            ],
+            [
+             0.5555555555555556,
+             ""#d8576b""
+            ],
+            [
+             0.6666666666666666,
+             ""#ed7953""
+            ],
+            [
+             0.7777777777777778,
+             ""#fb9f3a""
+            ],
+            [
+             0.8888888888888888,
+             ""#fdca26""
+            ],
+            [
+             1,
+             ""#f0f921""
+            ]
+           ]
+          },
+          ""colorway"": [
+           ""#636efa"",
+           ""#EF553B"",
+           ""#00cc96"",
+           ""#ab63fa"",
+           ""#FFA15A"",
+           ""#19d3f3"",
+           ""#FF6692"",
+           ""#B6E880"",
+           ""#FF97FF"",
+           ""#FECB52""
+          ],
+          ""font"": {
+           ""color"": ""#2a3f5f""
+          },
+          ""geo"": {
+           ""bgcolor"": ""white"",
+           ""lakecolor"": ""white"",
+           ""landcolor"": ""#E5ECF6"",
+           ""showlakes"": true,
+           ""showland"": true,
+           ""subunitcolor"": ""white""
+          },
+          ""hoverlabel"": {
+           ""align"": ""left""
+          },
+          ""hovermode"": ""closest"",
+          ""mapbox"": {
+           ""style"": ""light""
+          },
+          ""paper_bgcolor"": ""white"",
+          ""plot_bgcolor"": ""#E5ECF6"",
+          ""polar"": {
+           ""angularaxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           },
+           ""bgcolor"": ""#E5ECF6"",
+           ""radialaxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           }
+          },
+          ""scene"": {
+           ""xaxis"": {
+            ""backgroundcolor"": ""#E5ECF6"",
+            ""gridcolor"": ""white"",
+            ""gridwidth"": 2,
+            ""linecolor"": ""white"",
+            ""showbackground"": true,
+            ""ticks"": """",
+            ""zerolinecolor"": ""white""
+           },
+           ""yaxis"": {
+            ""backgroundcolor"": ""#E5ECF6"",
+            ""gridcolor"": ""white"",
+            ""gridwidth"": 2,
+            ""linecolor"": ""white"",
+            ""showbackground"": true,
+            ""ticks"": """",
+            ""zerolinecolor"": ""white""
+           },
+           ""zaxis"": {
+            ""backgroundcolor"": ""#E5ECF6"",
+            ""gridcolor"": ""white"",
+            ""gridwidth"": 2,
+            ""linecolor"": ""white"",
+            ""showbackground"": true,
+            ""ticks"": """",
+            ""zerolinecolor"": ""white""
+           }
+          },
+          ""shapedefaults"": {
+           ""line"": {
+            ""color"": ""#2a3f5f""
+           }
+          },
+          ""ternary"": {
+           ""aaxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           },
+           ""baxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           },
+           ""bgcolor"": ""#E5ECF6"",
+           ""caxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           }
+          },
+          ""title"": {
+           ""x"": 0.05
+          },
+          ""xaxis"": {
+           ""automargin"": true,
+           ""gridcolor"": ""white"",
+           ""linecolor"": ""white"",
+           ""ticks"": """",
+           ""title"": {
+            ""standoff"": 15
+           },
+           ""zerolinecolor"": ""white"",
+           ""zerolinewidth"": 2
+          },
+          ""yaxis"": {
+           ""automargin"": true,
+           ""gridcolor"": ""white"",
+           ""linecolor"": ""white"",
+           ""ticks"": """",
+           ""title"": {
+            ""standoff"": 15
+           },
+           ""zerolinecolor"": ""white"",
+           ""zerolinewidth"": 2
+          }
+         }
+        }
+       }
+      },
+      ""text/html"": [
+       ""<div>                            <div id=\""4de3bb37-717e-4350-92f8-baf733bbc021\"" class=\""plotly-graph-div\"" style=\""height:525px; width:100%;\""></div>            <script type=\""text/javascript\"">                require([\""plotly\""], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\""4de3bb37-717e-4350-92f8-baf733bbc021\"")) {                    Plotly.newPlot(                        \""4de3bb37-717e-4350-92f8-baf733bbc021\"",                        [{\""hovertemplate\"":\""weight=%{x}<br>height=%{y}<br>width=%{z}<extra></extra>\"",\""legendgroup\"":\""\"",\""marker\"":{\""color\"":\""#636efa\"",\""symbol\"":\""circle\""},\""mode\"":\""markers\"",\""name\"":\""\"",\""scene\"":\""scene\"",\""showlegend\"":false,\""x\"":[-11.9803541031763,-6.290432662924863,-6.736897011007884,-8.156811473487632,-7.405073642076633,-6.416653968467081,-4.199043502762057,-8.460163155929353,-7.397911579715805,-9.560239918557867,-8.453370190829258,-6.298130255725915,-7.485201310580931,-6.3101375895547385,-8.496894508580661,-6.874799943946652,-6.187250395932087,-8.714201646924252,-7.120670557104987,-7.495445273855449,-8.921581994608681,-8.734210554101844,-10.27680600806525,-5.9595782992971404,-8.655896799724701,-9.740676375133184,-8.55612128866845,-7.566782112826392,-6.223530878233048,-6.351577646939033,-5.570981622085944,-7.249180629203571,-7.7392568599091005,-9.813417235800285,-9.24722775957893,-9.387695116649855,-7.641125213998168,-8.148456482470808,-7.606749794441968,-5.602791904157758,-10.044837244315248,-6.6897716224806025,-5.829061180774874,-4.369373813465966,-6.751313191006155,-9.627757169521544,-7.247582785887058,-5.641135551327822,-8.059769568192442,-7.556781910951701,-7.961956052638345,-6.78607872393919,-4.490649368867333,-9.581429100417528,-8.13756415487571,-5.436586168130162,-9.377155876791416,-8.204760197355379,-8.273400862043221,-7.35809861610496,-5.28911018738381,-9.552126243844748,-8.019895734557249,-10.933507310675115,-8.048142898292918,-7.75595917284385,-9.297213104347684,-7.489361364455802,-7.629443081780139,-6.397586224266937,-6.008785826717765,-8.485243660453799,-9.347816213799701,-5.539371831651236,-9.597806229301346,-10.037967088289454,-8.833514429283788,-9.603975958066027,-7.447962655044643,-6.90013700365057,-6.314725308361885,-8.781288132732136,-8.177933195846942,-8.436176500006825,-8.759380332628956,-7.0656131266963715,-8.376229274438645,-8.167349837337998,-6.873739962903461,-6.377090089298508,-9.793901280542459,-8.197201891104232,-5.237405968994661,-6.319245461649304,-6.12329840933832,-9.907771036201598,-9.4422812539316,-5.435963063899374,-6.40359567621175,-7.346308545428884],\""y\"":[2.621863248350037,5.933074579207716,5.54998824684697,5.031962097109114,6.3837691161203844,7.565465274666399,5.226470520630366,3.735054758556701,6.514621906918775,4.146131655115221,3.638158087942962,4.599563825475971,8.532036223566486,6.659866291772025,5.594064518884458,5.937615519064998,6.471017566389171,4.351653598242385,3.8335876570769023,5.6701448299022035,1.7709747938023435,2.8660119259081855,3.3944962529930263,7.683219737494448,4.190099031406584,4.277572418292747,1.9145031924932943,6.151046021010221,6.869859568906565,6.478292642948072,7.340421201948603,7.632333719823853,2.8048546058564705,3.2556297881293594,3.577846089279434,6.288041785161829,4.381017768199817,1.8497254425499112,8.188926043447623,4.7998406157613696,2.867222729171471,8.496805642998673,6.58460480198905,4.586029237142924,6.142619282450821,6.685936369857078,5.6343999297928145,6.445389771714437,2.537948934674584,4.216087427926357,3.4055450184815914,2.4784538392676794,6.134274131233912,3.493918933447605,2.598664883766191,6.1708855942259735,4.282373261023311,3.499764788212491,4.111248022976987,7.409556319425051,6.306164820580588,2.414856508248195,6.516757737056285,2.5728198630398276,4.877349180026583,6.510434646597215,4.480998547984656,3.5608039396149205,3.7093465134876746,6.019216475336875,6.220356440282124,3.644172501892266,2.4771042652681667,6.826460857045079,2.157773979270363,4.606777778339397,4.127295028491313,1.5899549973883143,5.355475116799475,2.8303911841724023,7.483261911325251,1.8907453687901015,6.752554262753341,3.2162035047026567,4.848001017307087,2.7558616033618364,4.533437174381517,3.856814525466609,3.590122618977335,6.049719618855952,2.040304844847453,3.6208575024497343,6.893397685958687,7.231419095471904,7.098750084844298,4.283175899832392,3.865622195665978,6.563416988410524,4.3583720623868665,4.956193036361509],\""z\"":[-8.980304472324729,4.352211546348372,2.326053796748647,-9.142628524691828,2.84255290491211,4.194107716964702,3.7310384271156027,-9.283253035872296,2.4317719997975193,-7.6004416918539395,-7.875002542478913,2.3024425696293065,3.661923119539432,3.8007057517452827,1.2809783149662328,3.3369784850117754,3.5264636482696785,-7.759025383836713,4.3193065349107185,4.30152318244655,-8.152944488642069,-6.047771068835265,-8.58381277758478,4.620589056308427,-8.485361165607946,-8.500263615098733,-8.794836967881636,3.0420211613917587,1.9377382614370657,3.7518914900745792,3.863132196473438,2.842052435954093,-8.191877841041215,-7.741263617269853,-8.789947793215035,-8.372527180207,-8.291608973246557,-8.472443010981127,5.3350381121896,3.6728799764823865,-9.8781365701743,2.01419495062026,4.607977870711361,4.7293110770773605,3.276772403676553,3.859198719122157,3.058859665091609,3.2438269658152263,-8.272470412395178,-10.294831342267837,-8.292932071303184,-9.528898690390196,3.9844189993182995,-9.617804181978878,-8.411075501978459,2.9028474303345484,-7.625550335965655,-8.67576730179624,-10.196804308938402,3.294106523541445,3.38963918016624,-7.22961697344955,2.135401828027986,-7.715014142112663,2.0747779853608486,3.6053975917028236,-10.55113161560795,-7.493962463928635,-8.960854349821204,3.3994356544784865,2.5810824758198856,-9.641207186328932,-9.903195222252743,3.9976375682044782,-7.721036209182189,-8.961198389921798,-9.958636142961257,-9.346668103341361,4.01124738943291,-7.792754830497518,2.4296165763135544,-6.133494525352463,3.3157516261437094,-8.39634835040812,-8.317951020165026,-9.418644795470371,-7.8688977161660505,-7.335070874009395,2.8206037963300075,4.033561565986594,-8.57653911512009,-6.1639392851757275,4.2563716557463875,3.543552052697827,2.5587825723261903,-8.469899355989549,-5.982614995951284,2.6321390360439274,2.4610248181199634,4.225544678212692],\""type\"":\""scatter3d\""}],                        {\""template\"":{\""data\"":{\""bar\"":[{\""error_x\"":{\""color\"":\""#2a3f5f\""},\""error_y\"":{\""color\"":\""#2a3f5f\""},\""marker\"":{\""line\"":{\""color\"":\""#E5ECF6\"",\""width\"":0.5},\""pattern\"":{\""fillmode\"":\""overlay\"",\""size\"":10,\""solidity\"":0.2}},\""type\"":\""bar\""}],\""barpolar\"":[{\""marker\"":{\""line\"":{\""color\"":\""#E5ECF6\"",\""width\"":0.5},\""pattern\"":{\""fillmode\"":\""overlay\"",\""size\"":10,\""solidity\"":0.2}},\""type\"":\""barpolar\""}],\""carpet\"":[{\""aaxis\"":{\""endlinecolor\"":\""#2a3f5f\"",\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""minorgridcolor\"":\""white\"",\""startlinecolor\"":\""#2a3f5f\""},\""baxis\"":{\""endlinecolor\"":\""#2a3f5f\"",\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""minorgridcolor\"":\""white\"",\""startlinecolor\"":\""#2a3f5f\""},\""type\"":\""carpet\""}],\""choropleth\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""type\"":\""choropleth\""}],\""contour\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""contour\""}],\""contourcarpet\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""type\"":\""contourcarpet\""}],\""heatmap\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""heatmap\""}],\""heatmapgl\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""heatmapgl\""}],\""histogram\"":[{\""marker\"":{\""pattern\"":{\""fillmode\"":\""overlay\"",\""size\"":10,\""solidity\"":0.2}},\""type\"":\""histogram\""}],\""histogram2d\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""histogram2d\""}],\""histogram2dcontour\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""histogram2dcontour\""}],\""mesh3d\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""type\"":\""mesh3d\""}],\""parcoords\"":[{\""line\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""parcoords\""}],\""pie\"":[{\""automargin\"":true,\""type\"":\""pie\""}],\""scatter\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatter\""}],\""scatter3d\"":[{\""line\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatter3d\""}],\""scattercarpet\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattercarpet\""}],\""scattergeo\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattergeo\""}],\""scattergl\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattergl\""}],\""scattermapbox\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattermapbox\""}],\""scatterpolar\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatterpolar\""}],\""scatterpolargl\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatterpolargl\""}],\""scatterternary\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatterternary\""}],\""surface\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""surface\""}],\""table\"":[{\""cells\"":{\""fill\"":{\""color\"":\""#EBF0F8\""},\""line\"":{\""color\"":\""white\""}},\""header\"":{\""fill\"":{\""color\"":\""#C8D4E3\""},\""line\"":{\""color\"":\""white\""}},\""type\"":\""table\""}]},\""layout\"":{\""annotationdefaults\"":{\""arrowcolor\"":\""#2a3f5f\"",\""arrowhead\"":0,\""arrowwidth\"":1},\""autotypenumbers\"":\""strict\"",\""coloraxis\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""colorscale\"":{\""diverging\"":[[0,\""#8e0152\""],[0.1,\""#c51b7d\""],[0.2,\""#de77ae\""],[0.3,\""#f1b6da\""],[0.4,\""#fde0ef\""],[0.5,\""#f7f7f7\""],[0.6,\""#e6f5d0\""],[0.7,\""#b8e186\""],[0.8,\""#7fbc41\""],[0.9,\""#4d9221\""],[1,\""#276419\""]],\""sequential\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""sequentialminus\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]]},\""colorway\"":[\""#636efa\"",\""#EF553B\"",\""#00cc96\"",\""#ab63fa\"",\""#FFA15A\"",\""#19d3f3\"",\""#FF6692\"",\""#B6E880\"",\""#FF97FF\"",\""#FECB52\""],\""font\"":{\""color\"":\""#2a3f5f\""},\""geo\"":{\""bgcolor\"":\""white\"",\""lakecolor\"":\""white\"",\""landcolor\"":\""#E5ECF6\"",\""showlakes\"":true,\""showland\"":true,\""subunitcolor\"":\""white\""},\""hoverlabel\"":{\""align\"":\""left\""},\""hovermode\"":\""closest\"",\""mapbox\"":{\""style\"":\""light\""},\""paper_bgcolor\"":\""white\"",\""plot_bgcolor\"":\""#E5ECF6\"",\""polar\"":{\""angularaxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""},\""bgcolor\"":\""#E5ECF6\"",\""radialaxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""}},\""scene\"":{\""xaxis\"":{\""backgroundcolor\"":\""#E5ECF6\"",\""gridcolor\"":\""white\"",\""gridwidth\"":2,\""linecolor\"":\""white\"",\""showbackground\"":true,\""ticks\"":\""\"",\""zerolinecolor\"":\""white\""},\""yaxis\"":{\""backgroundcolor\"":\""#E5ECF6\"",\""gridcolor\"":\""white\"",\""gridwidth\"":2,\""linecolor\"":\""white\"",\""showbackground\"":true,\""ticks\"":\""\"",\""zerolinecolor\"":\""white\""},\""zaxis\"":{\""backgroundcolor\"":\""#E5ECF6\"",\""gridcolor\"":\""white\"",\""gridwidth\"":2,\""linecolor\"":\""white\"",\""showbackground\"":true,\""ticks\"":\""\"",\""zerolinecolor\"":\""white\""}},\""shapedefaults\"":{\""line\"":{\""color\"":\""#2a3f5f\""}},\""ternary\"":{\""aaxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""},\""baxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""},\""bgcolor\"":\""#E5ECF6\"",\""caxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""}},\""title\"":{\""x\"":0.05},\""xaxis\"":{\""automargin\"":true,\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\"",\""title\"":{\""standoff\"":15},\""zerolinecolor\"":\""white\"",\""zerolinewidth\"":2},\""yaxis\"":{\""automargin\"":true,\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\"",\""title\"":{\""standoff\"":15},\""zerolinecolor\"":\""white\"",\""zerolinewidth\"":2}}},\""scene\"":{\""domain\"":{\""x\"":[0.0,1.0],\""y\"":[0.0,1.0]},\""xaxis\"":{\""title\"":{\""text\"":\""weight\""}},\""yaxis\"":{\""title\"":{\""text\"":\""height\""}},\""zaxis\"":{\""title\"":{\""text\"":\""width\""}}},\""legend\"":{\""tracegroupgap\"":0},\""margin\"":{\""t\"":60}},                        {\""responsive\"": true}                    ).then(function(){\n"",
+       ""                            \n"",
+       ""var gd = document.getElementById('4de3bb37-717e-4350-92f8-baf733bbc021');\n"",
+       ""var x = new MutationObserver(function (mutations, observer) {{\n"",
+       ""        var display = window.getComputedStyle(gd).display;\n"",
+       ""        if (!display || display === 'none') {{\n"",
+       ""            console.log([gd, 'removed!']);\n"",
+       ""            Plotly.purge(gd);\n"",
+       ""            observer.disconnect();\n"",
+       ""        }}\n"",
+       ""}});\n"",
+       ""\n"",
+       ""// Listen for the removal of the full notebook cells\n"",
+       ""var notebookContainer = gd.closest('#notebook-container');\n"",
+       ""if (notebookContainer) {{\n"",
+       ""    x.observe(notebookContainer, {childList: true});\n"",
+       ""}}\n"",
+       ""\n"",
+       ""// Listen for the clearing of the current output cell\n"",
+       ""var outputEl = gd.closest('.output');\n"",
+       ""if (outputEl) {{\n"",
+       ""    x.observe(outputEl, {childList: true});\n"",
+       ""}}\n"",
+       ""\n"",
+       ""                        })                };                });            </script>        </div>""
+      ]
+     },
+     ""metadata"": {},
+     ""output_type"": ""display_data""
+    }
+   ],
+   ""source"": [
+    ""import plotly.express as px \n"",
+    ""\n"",
+    ""px.scatter_3d(df, x='weight', y='height', z='width')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 54,
+   ""id"": ""4d22bc5f"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""red = umap.UMAP(n_neighbors=50)\n"",
+    ""red = pd.DataFrame(red.fit_transform(df))""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 55,
+   ""id"": ""2517670b"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""red = red.rename({0: 'UMAP_0', 1: 'UMAP_1'}, axis=1)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 56,
+   ""id"": ""f6390839"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""application/vnd.plotly.v1+json"": {
+       ""config"": {
+        ""plotlyServerURL"": ""https://plot.ly""
+       },
+       ""data"": [
+        {
+         ""hovertemplate"": ""UMAP_0=%{x}<br>UMAP_1=%{y}<extra></extra>"",
+         ""legendgroup"": """",
+         ""marker"": {
+          ""color"": ""#636efa"",
+          ""symbol"": ""circle""
+         },
+         ""mode"": ""markers"",
+         ""name"": """",
+         ""orientation"": ""v"",
+         ""showlegend"": false,
+         ""type"": ""scatter"",
+         ""x"": [
+          9.851866722106934,
+          -1.1699390411376953,
+          -2.622649908065796,
+          11.262018203735352,
+          -2.037487030029297,
+          -0.2628401815891266,
+          -0.8326402306556702,
+          11.145909309387207,
+          -2.0103039741516113,
+          10.289457321166992,
+          10.358893394470215,
+          -1.7963621616363525,
+          -0.9666600227355957,
+          -0.569824755191803,
+          -2.5836198329925537,
+          -1.7741761207580566,
+          -0.8624570369720459,
+          10.515462875366211,
+          -2.168975591659546,
+          -1.980228066444397,
+          9.573882102966309,
+          9.104683876037598,
+          10.179268836975098,
+          -0.08714575320482254,
+          10.90218734741211,
+          10.676301956176758,
+          9.580646514892578,
+          -2.1103267669677734,
+          -1.2407029867172241,
+          -0.663328230381012,
+          -0.23362024128437042,
+          -1.353824496269226,
+          9.796468734741211,
+          9.974513053894043,
+          11.006808280944824,
+          11.187666893005371,
+          10.677352905273438,
+          9.780780792236328,
+          -0.6205756068229675,
+          -1.3722842931747437,
+          10.291952133178711,
+          -0.9992368221282959,
+          -0.1177491769194603,
+          -0.9720577597618103,
+          -1.789400339126587,
+          -1.8384041786193848,
+          -2.1194944381713867,
+          -0.7480528950691223,
+          9.680861473083496,
+          11.378955841064453,
+          10.458616256713867,
+          10.397171974182129,
+          -0.48733383417129517,
+          11.46220588684082,
+          9.805781364440918,
+          -0.9225258827209473,
+          10.570934295654297,
+          10.820212364196777,
+          11.407652854919434,
+          -1.4236366748809814,
+          -0.45211660861968994,
+          9.575149536132812,
+          -2.3706600666046143,
+          9.784771919250488,
+          -2.5338518619537354,
+          -2.0867013931274414,
+          11.53003215789795,
+          10.010552406311035,
+          10.728723526000977,
+          -1.3003811836242676,
+          -1.2796906232833862,
+          11.125068664550781,
+          10.831075668334961,
+          -0.28412190079689026,
+          9.631058692932129,
+          10.745695114135742,
+          11.408463478088379,
+          9.815439224243164,
+          -2.063011884689331,
+          10.006865501403809,
+          -1.044961929321289,
+          9.26891040802002,
+          -2.0191943645477295,
+          10.262826919555664,
+          10.864398002624512,
+          10.52357292175293,
+          10.655434608459473,
+          10.1184720993042,
+          -1.988417387008667,
+          -1.068209171295166,
+          9.750044822692871,
+          9.213772773742676,
+          -0.23060563206672668,
+          -0.5095036625862122,
+          -0.9011011719703674,
+          10.853052139282227,
+          9.30288314819336,
+          -0.8550420999526978,
+          -1.677507996559143,
+          -2.1619932651519775
+         ],
+         ""xaxis"": ""x"",
+         ""y"": [
+          15.398341178894043,
+          -0.4370143711566925,
+          -0.12123975157737732,
+          14.253175735473633,
+          0.5813454985618591,
+          0.6753065586090088,
+          -0.9017056226730347,
+          13.661595344543457,
+          0.6617279648780823,
+          15.091212272644043,
+          14.071357727050781,
+          -0.5317171812057495,
+          1.2863191366195679,
+          0.0763539969921112,
+          0.3888145387172699,
+          0.17450635135173798,
+          -0.09092652052640915,
+          14.406778335571289,
+          -0.5007661581039429,
+          -0.14100627601146698,
+          14.044113159179688,
+          14.35205364227295,
+          15.433647155761719,
+          0.4693155884742737,
+          14.423702239990234,
+          15.31278133392334,
+          13.971692085266113,
+          0.4411333501338959,
+          0.5559178590774536,
+          0.22521577775478363,
+          0.2823254466056824,
+          1.1754090785980225,
+          13.201626777648926,
+          15.0649995803833,
+          14.554424285888672,
+          14.984282493591309,
+          13.871996879577637,
+          13.641237258911133,
+          0.9841935038566589,
+          -0.7552729249000549,
+          14.717740058898926,
+          0.9167194366455078,
+          -0.06425944715738297,
+          -0.964782178401947,
+          0.13510948419570923,
+          0.8523427844047546,
+          0.05082878842949867,
+          -0.39205947518348694,
+          13.491372108459473,
+          13.741097450256348,
+          13.495356559753418,
+          13.035935401916504,
+          -0.6337272524833679,
+          14.420759201049805,
+          13.393428802490234,
+          -0.5628682374954224,
+          14.90792465209961,
+          13.401888847351074,
+          13.805981636047363,
+          1.135977029800415,
+          -0.42745035886764526,
+          14.69139289855957,
+          0.6120988726615906,
+          15.254090309143066,
+          0.14223739504814148,
+          1.0048173666000366,
+          14.058809280395508,
+          13.484514236450195,
+          13.234078407287598,
+          -0.03058146871626377,
+          0.11376651376485825,
+          13.6862154006958,
+          14.15890884399414,
+          -0.04365652799606323,
+          14.761767387390137,
+          15.160792350769043,
+          13.986006736755371,
+          14.405827522277832,
+          -0.1883876770734787,
+          13.104535102844238,
+          0.722169816493988,
+          14.160948753356934,
+          0.9227487444877625,
+          13.652749061584473,
+          14.50057315826416,
+          13.036229133605957,
+          14.231407165527344,
+          14.017730712890625,
+          -0.6183841228485107,
+          0.17771868407726288,
+          14.61141586303711,
+          14.423773765563965,
+          -0.1879328489303589,
+          0.4626389741897583,
+          0.5348778963088989,
+          15.076766967773438,
+          14.602413177490234,
+          -0.22370970249176025,
+          -0.6904502511024475,
+          -0.2123483568429947
+         ],
+         ""yaxis"": ""y""
+        }
+       ],
+       ""layout"": {
+        ""legend"": {
+         ""tracegroupgap"": 0
+        },
+        ""margin"": {
+         ""t"": 60
+        },
+        ""template"": {
+         ""data"": {
+          ""bar"": [
+           {
+            ""error_x"": {
+             ""color"": ""#2a3f5f""
+            },
+            ""error_y"": {
+             ""color"": ""#2a3f5f""
+            },
+            ""marker"": {
+             ""line"": {
+              ""color"": ""#E5ECF6"",
+              ""width"": 0.5
+             },
+             ""pattern"": {
+              ""fillmode"": ""overlay"",
+              ""size"": 10,
+              ""solidity"": 0.2
+             }
+            },
+            ""type"": ""bar""
+           }
+          ],
+          ""barpolar"": [
+           {
+            ""marker"": {
+             ""line"": {
+              ""color"": ""#E5ECF6"",
+              ""width"": 0.5
+             },
+             ""pattern"": {
+              ""fillmode"": ""overlay"",
+              ""size"": 10,
+              ""solidity"": 0.2
+             }
+            },
+            ""type"": ""barpolar""
+           }
+          ],
+          ""carpet"": [
+           {
+            ""aaxis"": {
+             ""endlinecolor"": ""#2a3f5f"",
+             ""gridcolor"": ""white"",
+             ""linecolor"": ""white"",
+             ""minorgridcolor"": ""white"",
+             ""startlinecolor"": ""#2a3f5f""
+            },
+            ""baxis"": {
+             ""endlinecolor"": ""#2a3f5f"",
+             ""gridcolor"": ""white"",
+             ""linecolor"": ""white"",
+             ""minorgridcolor"": ""white"",
+             ""startlinecolor"": ""#2a3f5f""
+            },
+            ""type"": ""carpet""
+           }
+          ],
+          ""choropleth"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""type"": ""choropleth""
+           }
+          ],
+          ""contour"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""contour""
+           }
+          ],
+          ""contourcarpet"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""type"": ""contourcarpet""
+           }
+          ],
+          ""heatmap"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""heatmap""
+           }
+          ],
+          ""heatmapgl"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""heatmapgl""
+           }
+          ],
+          ""histogram"": [
+           {
+            ""marker"": {
+             ""pattern"": {
+              ""fillmode"": ""overlay"",
+              ""size"": 10,
+              ""solidity"": 0.2
+             }
+            },
+            ""type"": ""histogram""
+           }
+          ],
+          ""histogram2d"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""histogram2d""
+           }
+          ],
+          ""histogram2dcontour"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""histogram2dcontour""
+           }
+          ],
+          ""mesh3d"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""type"": ""mesh3d""
+           }
+          ],
+          ""parcoords"": [
+           {
+            ""line"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""parcoords""
+           }
+          ],
+          ""pie"": [
+           {
+            ""automargin"": true,
+            ""type"": ""pie""
+           }
+          ],
+          ""scatter"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatter""
+           }
+          ],
+          ""scatter3d"": [
+           {
+            ""line"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatter3d""
+           }
+          ],
+          ""scattercarpet"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattercarpet""
+           }
+          ],
+          ""scattergeo"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattergeo""
+           }
+          ],
+          ""scattergl"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattergl""
+           }
+          ],
+          ""scattermapbox"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scattermapbox""
+           }
+          ],
+          ""scatterpolar"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatterpolar""
+           }
+          ],
+          ""scatterpolargl"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatterpolargl""
+           }
+          ],
+          ""scatterternary"": [
+           {
+            ""marker"": {
+             ""colorbar"": {
+              ""outlinewidth"": 0,
+              ""ticks"": """"
+             }
+            },
+            ""type"": ""scatterternary""
+           }
+          ],
+          ""surface"": [
+           {
+            ""colorbar"": {
+             ""outlinewidth"": 0,
+             ""ticks"": """"
+            },
+            ""colorscale"": [
+             [
+              0,
+              ""#0d0887""
+             ],
+             [
+              0.1111111111111111,
+              ""#46039f""
+             ],
+             [
+              0.2222222222222222,
+              ""#7201a8""
+             ],
+             [
+              0.3333333333333333,
+              ""#9c179e""
+             ],
+             [
+              0.4444444444444444,
+              ""#bd3786""
+             ],
+             [
+              0.5555555555555556,
+              ""#d8576b""
+             ],
+             [
+              0.6666666666666666,
+              ""#ed7953""
+             ],
+             [
+              0.7777777777777778,
+              ""#fb9f3a""
+             ],
+             [
+              0.8888888888888888,
+              ""#fdca26""
+             ],
+             [
+              1,
+              ""#f0f921""
+             ]
+            ],
+            ""type"": ""surface""
+           }
+          ],
+          ""table"": [
+           {
+            ""cells"": {
+             ""fill"": {
+              ""color"": ""#EBF0F8""
+             },
+             ""line"": {
+              ""color"": ""white""
+             }
+            },
+            ""header"": {
+             ""fill"": {
+              ""color"": ""#C8D4E3""
+             },
+             ""line"": {
+              ""color"": ""white""
+             }
+            },
+            ""type"": ""table""
+           }
+          ]
+         },
+         ""layout"": {
+          ""annotationdefaults"": {
+           ""arrowcolor"": ""#2a3f5f"",
+           ""arrowhead"": 0,
+           ""arrowwidth"": 1
+          },
+          ""autotypenumbers"": ""strict"",
+          ""coloraxis"": {
+           ""colorbar"": {
+            ""outlinewidth"": 0,
+            ""ticks"": """"
+           }
+          },
+          ""colorscale"": {
+           ""diverging"": [
+            [
+             0,
+             ""#8e0152""
+            ],
+            [
+             0.1,
+             ""#c51b7d""
+            ],
+            [
+             0.2,
+             ""#de77ae""
+            ],
+            [
+             0.3,
+             ""#f1b6da""
+            ],
+            [
+             0.4,
+             ""#fde0ef""
+            ],
+            [
+             0.5,
+             ""#f7f7f7""
+            ],
+            [
+             0.6,
+             ""#e6f5d0""
+            ],
+            [
+             0.7,
+             ""#b8e186""
+            ],
+            [
+             0.8,
+             ""#7fbc41""
+            ],
+            [
+             0.9,
+             ""#4d9221""
+            ],
+            [
+             1,
+             ""#276419""
+            ]
+           ],
+           ""sequential"": [
+            [
+             0,
+             ""#0d0887""
+            ],
+            [
+             0.1111111111111111,
+             ""#46039f""
+            ],
+            [
+             0.2222222222222222,
+             ""#7201a8""
+            ],
+            [
+             0.3333333333333333,
+             ""#9c179e""
+            ],
+            [
+             0.4444444444444444,
+             ""#bd3786""
+            ],
+            [
+             0.5555555555555556,
+             ""#d8576b""
+            ],
+            [
+             0.6666666666666666,
+             ""#ed7953""
+            ],
+            [
+             0.7777777777777778,
+             ""#fb9f3a""
+            ],
+            [
+             0.8888888888888888,
+             ""#fdca26""
+            ],
+            [
+             1,
+             ""#f0f921""
+            ]
+           ],
+           ""sequentialminus"": [
+            [
+             0,
+             ""#0d0887""
+            ],
+            [
+             0.1111111111111111,
+             ""#46039f""
+            ],
+            [
+             0.2222222222222222,
+             ""#7201a8""
+            ],
+            [
+             0.3333333333333333,
+             ""#9c179e""
+            ],
+            [
+             0.4444444444444444,
+             ""#bd3786""
+            ],
+            [
+             0.5555555555555556,
+             ""#d8576b""
+            ],
+            [
+             0.6666666666666666,
+             ""#ed7953""
+            ],
+            [
+             0.7777777777777778,
+             ""#fb9f3a""
+            ],
+            [
+             0.8888888888888888,
+             ""#fdca26""
+            ],
+            [
+             1,
+             ""#f0f921""
+            ]
+           ]
+          },
+          ""colorway"": [
+           ""#636efa"",
+           ""#EF553B"",
+           ""#00cc96"",
+           ""#ab63fa"",
+           ""#FFA15A"",
+           ""#19d3f3"",
+           ""#FF6692"",
+           ""#B6E880"",
+           ""#FF97FF"",
+           ""#FECB52""
+          ],
+          ""font"": {
+           ""color"": ""#2a3f5f""
+          },
+          ""geo"": {
+           ""bgcolor"": ""white"",
+           ""lakecolor"": ""white"",
+           ""landcolor"": ""#E5ECF6"",
+           ""showlakes"": true,
+           ""showland"": true,
+           ""subunitcolor"": ""white""
+          },
+          ""hoverlabel"": {
+           ""align"": ""left""
+          },
+          ""hovermode"": ""closest"",
+          ""mapbox"": {
+           ""style"": ""light""
+          },
+          ""paper_bgcolor"": ""white"",
+          ""plot_bgcolor"": ""#E5ECF6"",
+          ""polar"": {
+           ""angularaxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           },
+           ""bgcolor"": ""#E5ECF6"",
+           ""radialaxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           }
+          },
+          ""scene"": {
+           ""xaxis"": {
+            ""backgroundcolor"": ""#E5ECF6"",
+            ""gridcolor"": ""white"",
+            ""gridwidth"": 2,
+            ""linecolor"": ""white"",
+            ""showbackground"": true,
+            ""ticks"": """",
+            ""zerolinecolor"": ""white""
+           },
+           ""yaxis"": {
+            ""backgroundcolor"": ""#E5ECF6"",
+            ""gridcolor"": ""white"",
+            ""gridwidth"": 2,
+            ""linecolor"": ""white"",
+            ""showbackground"": true,
+            ""ticks"": """",
+            ""zerolinecolor"": ""white""
+           },
+           ""zaxis"": {
+            ""backgroundcolor"": ""#E5ECF6"",
+            ""gridcolor"": ""white"",
+            ""gridwidth"": 2,
+            ""linecolor"": ""white"",
+            ""showbackground"": true,
+            ""ticks"": """",
+            ""zerolinecolor"": ""white""
+           }
+          },
+          ""shapedefaults"": {
+           ""line"": {
+            ""color"": ""#2a3f5f""
+           }
+          },
+          ""ternary"": {
+           ""aaxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           },
+           ""baxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           },
+           ""bgcolor"": ""#E5ECF6"",
+           ""caxis"": {
+            ""gridcolor"": ""white"",
+            ""linecolor"": ""white"",
+            ""ticks"": """"
+           }
+          },
+          ""title"": {
+           ""x"": 0.05
+          },
+          ""xaxis"": {
+           ""automargin"": true,
+           ""gridcolor"": ""white"",
+           ""linecolor"": ""white"",
+           ""ticks"": """",
+           ""title"": {
+            ""standoff"": 15
+           },
+           ""zerolinecolor"": ""white"",
+           ""zerolinewidth"": 2
+          },
+          ""yaxis"": {
+           ""automargin"": true,
+           ""gridcolor"": ""white"",
+           ""linecolor"": ""white"",
+           ""ticks"": """",
+           ""title"": {
+            ""standoff"": 15
+           },
+           ""zerolinecolor"": ""white"",
+           ""zerolinewidth"": 2
+          }
+         }
+        },
+        ""xaxis"": {
+         ""anchor"": ""y"",
+         ""domain"": [
+          0,
+          1
+         ],
+         ""title"": {
+          ""text"": ""UMAP_0""
+         }
+        },
+        ""yaxis"": {
+         ""anchor"": ""x"",
+         ""domain"": [
+          0,
+          1
+         ],
+         ""title"": {
+          ""text"": ""UMAP_1""
+         }
+        }
+       }
+      },
+      ""text/html"": [
+       ""<div>                            <div id=\""0def8425-c829-406a-8b6b-1bd42b8c373f\"" class=\""plotly-graph-div\"" style=\""height:525px; width:100%;\""></div>            <script type=\""text/javascript\"">                require([\""plotly\""], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\""0def8425-c829-406a-8b6b-1bd42b8c373f\"")) {                    Plotly.newPlot(                        \""0def8425-c829-406a-8b6b-1bd42b8c373f\"",                        [{\""hovertemplate\"":\""UMAP_0=%{x}<br>UMAP_1=%{y}<extra></extra>\"",\""legendgroup\"":\""\"",\""marker\"":{\""color\"":\""#636efa\"",\""symbol\"":\""circle\""},\""mode\"":\""markers\"",\""name\"":\""\"",\""orientation\"":\""v\"",\""showlegend\"":false,\""x\"":[9.851866722106934,-1.1699390411376953,-2.622649908065796,11.262018203735352,-2.037487030029297,-0.2628401815891266,-0.8326402306556702,11.145909309387207,-2.0103039741516113,10.289457321166992,10.358893394470215,-1.7963621616363525,-0.9666600227355957,-0.569824755191803,-2.5836198329925537,-1.7741761207580566,-0.8624570369720459,10.515462875366211,-2.168975591659546,-1.980228066444397,9.573882102966309,9.104683876037598,10.179268836975098,-0.08714575320482254,10.90218734741211,10.676301956176758,9.580646514892578,-2.1103267669677734,-1.2407029867172241,-0.663328230381012,-0.23362024128437042,-1.353824496269226,9.796468734741211,9.974513053894043,11.006808280944824,11.187666893005371,10.677352905273438,9.780780792236328,-0.6205756068229675,-1.3722842931747437,10.291952133178711,-0.9992368221282959,-0.1177491769194603,-0.9720577597618103,-1.789400339126587,-1.8384041786193848,-2.1194944381713867,-0.7480528950691223,9.680861473083496,11.378955841064453,10.458616256713867,10.397171974182129,-0.48733383417129517,11.46220588684082,9.805781364440918,-0.9225258827209473,10.570934295654297,10.820212364196777,11.407652854919434,-1.4236366748809814,-0.45211660861968994,9.575149536132812,-2.3706600666046143,9.784771919250488,-2.5338518619537354,-2.0867013931274414,11.53003215789795,10.010552406311035,10.728723526000977,-1.3003811836242676,-1.2796906232833862,11.125068664550781,10.831075668334961,-0.28412190079689026,9.631058692932129,10.745695114135742,11.408463478088379,9.815439224243164,-2.063011884689331,10.006865501403809,-1.044961929321289,9.26891040802002,-2.0191943645477295,10.262826919555664,10.864398002624512,10.52357292175293,10.655434608459473,10.1184720993042,-1.988417387008667,-1.068209171295166,9.750044822692871,9.213772773742676,-0.23060563206672668,-0.5095036625862122,-0.9011011719703674,10.853052139282227,9.30288314819336,-0.8550420999526978,-1.677507996559143,-2.1619932651519775],\""xaxis\"":\""x\"",\""y\"":[15.398341178894043,-0.4370143711566925,-0.12123975157737732,14.253175735473633,0.5813454985618591,0.6753065586090088,-0.9017056226730347,13.661595344543457,0.6617279648780823,15.091212272644043,14.071357727050781,-0.5317171812057495,1.2863191366195679,0.0763539969921112,0.3888145387172699,0.17450635135173798,-0.09092652052640915,14.406778335571289,-0.5007661581039429,-0.14100627601146698,14.044113159179688,14.35205364227295,15.433647155761719,0.4693155884742737,14.423702239990234,15.31278133392334,13.971692085266113,0.4411333501338959,0.5559178590774536,0.22521577775478363,0.2823254466056824,1.1754090785980225,13.201626777648926,15.0649995803833,14.554424285888672,14.984282493591309,13.871996879577637,13.641237258911133,0.9841935038566589,-0.7552729249000549,14.717740058898926,0.9167194366455078,-0.06425944715738297,-0.964782178401947,0.13510948419570923,0.8523427844047546,0.05082878842949867,-0.39205947518348694,13.491372108459473,13.741097450256348,13.495356559753418,13.035935401916504,-0.6337272524833679,14.420759201049805,13.393428802490234,-0.5628682374954224,14.90792465209961,13.401888847351074,13.805981636047363,1.135977029800415,-0.42745035886764526,14.69139289855957,0.6120988726615906,15.254090309143066,0.14223739504814148,1.0048173666000366,14.058809280395508,13.484514236450195,13.234078407287598,-0.03058146871626377,0.11376651376485825,13.6862154006958,14.15890884399414,-0.04365652799606323,14.761767387390137,15.160792350769043,13.986006736755371,14.405827522277832,-0.1883876770734787,13.104535102844238,0.722169816493988,14.160948753356934,0.9227487444877625,13.652749061584473,14.50057315826416,13.036229133605957,14.231407165527344,14.017730712890625,-0.6183841228485107,0.17771868407726288,14.61141586303711,14.423773765563965,-0.1879328489303589,0.4626389741897583,0.5348778963088989,15.076766967773438,14.602413177490234,-0.22370970249176025,-0.6904502511024475,-0.2123483568429947],\""yaxis\"":\""y\"",\""type\"":\""scatter\""}],                        {\""template\"":{\""data\"":{\""bar\"":[{\""error_x\"":{\""color\"":\""#2a3f5f\""},\""error_y\"":{\""color\"":\""#2a3f5f\""},\""marker\"":{\""line\"":{\""color\"":\""#E5ECF6\"",\""width\"":0.5},\""pattern\"":{\""fillmode\"":\""overlay\"",\""size\"":10,\""solidity\"":0.2}},\""type\"":\""bar\""}],\""barpolar\"":[{\""marker\"":{\""line\"":{\""color\"":\""#E5ECF6\"",\""width\"":0.5},\""pattern\"":{\""fillmode\"":\""overlay\"",\""size\"":10,\""solidity\"":0.2}},\""type\"":\""barpolar\""}],\""carpet\"":[{\""aaxis\"":{\""endlinecolor\"":\""#2a3f5f\"",\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""minorgridcolor\"":\""white\"",\""startlinecolor\"":\""#2a3f5f\""},\""baxis\"":{\""endlinecolor\"":\""#2a3f5f\"",\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""minorgridcolor\"":\""white\"",\""startlinecolor\"":\""#2a3f5f\""},\""type\"":\""carpet\""}],\""choropleth\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""type\"":\""choropleth\""}],\""contour\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""contour\""}],\""contourcarpet\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""type\"":\""contourcarpet\""}],\""heatmap\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""heatmap\""}],\""heatmapgl\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""heatmapgl\""}],\""histogram\"":[{\""marker\"":{\""pattern\"":{\""fillmode\"":\""overlay\"",\""size\"":10,\""solidity\"":0.2}},\""type\"":\""histogram\""}],\""histogram2d\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""histogram2d\""}],\""histogram2dcontour\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""histogram2dcontour\""}],\""mesh3d\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""type\"":\""mesh3d\""}],\""parcoords\"":[{\""line\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""parcoords\""}],\""pie\"":[{\""automargin\"":true,\""type\"":\""pie\""}],\""scatter\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatter\""}],\""scatter3d\"":[{\""line\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatter3d\""}],\""scattercarpet\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattercarpet\""}],\""scattergeo\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattergeo\""}],\""scattergl\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattergl\""}],\""scattermapbox\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scattermapbox\""}],\""scatterpolar\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatterpolar\""}],\""scatterpolargl\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatterpolargl\""}],\""scatterternary\"":[{\""marker\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""type\"":\""scatterternary\""}],\""surface\"":[{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""},\""colorscale\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""type\"":\""surface\""}],\""table\"":[{\""cells\"":{\""fill\"":{\""color\"":\""#EBF0F8\""},\""line\"":{\""color\"":\""white\""}},\""header\"":{\""fill\"":{\""color\"":\""#C8D4E3\""},\""line\"":{\""color\"":\""white\""}},\""type\"":\""table\""}]},\""layout\"":{\""annotationdefaults\"":{\""arrowcolor\"":\""#2a3f5f\"",\""arrowhead\"":0,\""arrowwidth\"":1},\""autotypenumbers\"":\""strict\"",\""coloraxis\"":{\""colorbar\"":{\""outlinewidth\"":0,\""ticks\"":\""\""}},\""colorscale\"":{\""diverging\"":[[0,\""#8e0152\""],[0.1,\""#c51b7d\""],[0.2,\""#de77ae\""],[0.3,\""#f1b6da\""],[0.4,\""#fde0ef\""],[0.5,\""#f7f7f7\""],[0.6,\""#e6f5d0\""],[0.7,\""#b8e186\""],[0.8,\""#7fbc41\""],[0.9,\""#4d9221\""],[1,\""#276419\""]],\""sequential\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]],\""sequentialminus\"":[[0.0,\""#0d0887\""],[0.1111111111111111,\""#46039f\""],[0.2222222222222222,\""#7201a8\""],[0.3333333333333333,\""#9c179e\""],[0.4444444444444444,\""#bd3786\""],[0.5555555555555556,\""#d8576b\""],[0.6666666666666666,\""#ed7953\""],[0.7777777777777778,\""#fb9f3a\""],[0.8888888888888888,\""#fdca26\""],[1.0,\""#f0f921\""]]},\""colorway\"":[\""#636efa\"",\""#EF553B\"",\""#00cc96\"",\""#ab63fa\"",\""#FFA15A\"",\""#19d3f3\"",\""#FF6692\"",\""#B6E880\"",\""#FF97FF\"",\""#FECB52\""],\""font\"":{\""color\"":\""#2a3f5f\""},\""geo\"":{\""bgcolor\"":\""white\"",\""lakecolor\"":\""white\"",\""landcolor\"":\""#E5ECF6\"",\""showlakes\"":true,\""showland\"":true,\""subunitcolor\"":\""white\""},\""hoverlabel\"":{\""align\"":\""left\""},\""hovermode\"":\""closest\"",\""mapbox\"":{\""style\"":\""light\""},\""paper_bgcolor\"":\""white\"",\""plot_bgcolor\"":\""#E5ECF6\"",\""polar\"":{\""angularaxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""},\""bgcolor\"":\""#E5ECF6\"",\""radialaxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""}},\""scene\"":{\""xaxis\"":{\""backgroundcolor\"":\""#E5ECF6\"",\""gridcolor\"":\""white\"",\""gridwidth\"":2,\""linecolor\"":\""white\"",\""showbackground\"":true,\""ticks\"":\""\"",\""zerolinecolor\"":\""white\""},\""yaxis\"":{\""backgroundcolor\"":\""#E5ECF6\"",\""gridcolor\"":\""white\"",\""gridwidth\"":2,\""linecolor\"":\""white\"",\""showbackground\"":true,\""ticks\"":\""\"",\""zerolinecolor\"":\""white\""},\""zaxis\"":{\""backgroundcolor\"":\""#E5ECF6\"",\""gridcolor\"":\""white\"",\""gridwidth\"":2,\""linecolor\"":\""white\"",\""showbackground\"":true,\""ticks\"":\""\"",\""zerolinecolor\"":\""white\""}},\""shapedefaults\"":{\""line\"":{\""color\"":\""#2a3f5f\""}},\""ternary\"":{\""aaxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""},\""baxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""},\""bgcolor\"":\""#E5ECF6\"",\""caxis\"":{\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\""}},\""title\"":{\""x\"":0.05},\""xaxis\"":{\""automargin\"":true,\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\"",\""title\"":{\""standoff\"":15},\""zerolinecolor\"":\""white\"",\""zerolinewidth\"":2},\""yaxis\"":{\""automargin\"":true,\""gridcolor\"":\""white\"",\""linecolor\"":\""white\"",\""ticks\"":\""\"",\""title\"":{\""standoff\"":15},\""zerolinecolor\"":\""white\"",\""zerolinewidth\"":2}}},\""xaxis\"":{\""anchor\"":\""y\"",\""domain\"":[0.0,1.0],\""title\"":{\""text\"":\""UMAP_0\""}},\""yaxis\"":{\""anchor\"":\""x\"",\""domain\"":[0.0,1.0],\""title\"":{\""text\"":\""UMAP_1\""}},\""legend\"":{\""tracegroupgap\"":0},\""margin\"":{\""t\"":60}},                        {\""responsive\"": true}                    ).then(function(){\n"",
+       ""                            \n"",
+       ""var gd = document.getElementById('0def8425-c829-406a-8b6b-1bd42b8c373f');\n"",
+       ""var x = new MutationObserver(function (mutations, observer) {{\n"",
+       ""        var display = window.getComputedStyle(gd).display;\n"",
+       ""        if (!display || display === 'none') {{\n"",
+       ""            console.log([gd, 'removed!']);\n"",
+       ""            Plotly.purge(gd);\n"",
+       ""            observer.disconnect();\n"",
+       ""        }}\n"",
+       ""}});\n"",
+       ""\n"",
+       ""// Listen for the removal of the full notebook cells\n"",
+       ""var notebookContainer = gd.closest('#notebook-container');\n"",
+       ""if (notebookContainer) {{\n"",
+       ""    x.observe(notebookContainer, {childList: true});\n"",
+       ""}}\n"",
+       ""\n"",
+       ""// Listen for the clearing of the current output cell\n"",
+       ""var outputEl = gd.closest('.output');\n"",
+       ""if (outputEl) {{\n"",
+       ""    x.observe(outputEl, {childList: true});\n"",
+       ""}}\n"",
+       ""\n"",
+       ""                        })                };                });            </script>        </div>""
+      ]
+     },
+     ""metadata"": {},
+     ""output_type"": ""display_data""
+    }
+   ],
+   ""source"": [
+    ""px.scatter(red, x='UMAP_0', y='UMAP_1')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""856d2483"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""2693dc71"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import os, sys \n"",
+    ""import pandas as pd \n"",
+    ""import numpy as np\n"",
+    ""sys.path.append('..')\n"",
+    ""sys.path.append('../src/')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""c756cb2c"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""from src.data.calculate_intersection import *\n"",
+    ""from src.models.lib.data import GeneExpressionData""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""9cb8b738"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""test = GeneExpressionData(\n"",
+    ""    filename='../data/interim/allen_cortex_T.csv',\n"",
+    ""    labelname='../data/interim/labels/allen_cortex_labels.csv',\n"",
+    ""    class_label='Type',\n"",
+    ""    skip=3,\n"",
+    ""    cast=False,\n"",
+    "")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 14,
+   ""id"": ""f1c74cf8"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(['0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '204',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '148',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '156',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '171',\n"",
+       ""  '15',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '21',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '174',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '21',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '9',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '35',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '101',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '9',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '117',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '44',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '17',\n"",
+       ""  '82',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '56',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '0',\n"",
+       ""  '110',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '46',\n"",
+       ""  '0',\n"",
+       ""  '12',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '96',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '160',\n"",
+       ""  '208',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '43',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '4',\n"",
+       ""  '0',\n"",
+       ""  '342',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '7',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '134',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '6',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '18',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '74',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '17',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '75',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '26',\n"",
+       ""  '15',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '311',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '107',\n"",
+       ""  '0',\n"",
+       ""  '152',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '27',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '75',\n"",
+       ""  '113',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '8',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '43',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '88',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '65',\n"",
+       ""  '0',\n"",
+       ""  '81',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '157',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '151',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1699',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '3',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '5',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '5',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '57',\n"",
+       ""  '72',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '127',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '9',\n"",
+       ""  '0',\n"",
+       ""  '26',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '28',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '568',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '14',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '31',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '89',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '2',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '15',\n"",
+       ""  '102',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '3',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '52',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '429',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '439',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '5',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '80',\n"",
+       ""  '13',\n"",
+       ""  '55',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '72',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '6',\n"",
+       ""  '0',\n"",
+       ""  '257',\n"",
+       ""  '9',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '1',\n"",
+       ""  '42',\n"",
+       ""  '0',\n"",
+       ""  '6',\n"",
+       ""  '43',\n"",
+       ""  '0',\n"",
+       ""  '6',\n"",
+       ""  '6',\n"",
+       ""  '1',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '78',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '132',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '461',\n"",
+       ""  '81',\n"",
+       ""  '0',\n"",
+       ""  '45',\n"",
+       ""  '393',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '60',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '5',\n"",
+       ""  '0',\n"",
+       ""  '29',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '154',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  '0',\n"",
+       ""  ...],\n"",
+       "" 'Exclude')""
+      ]
+     },
+     ""execution_count"": 14,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""test[0]""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 17,
+   ""id"": ""3737e6ef"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""testdf = pd.read_csv('../data/interim/allen_cortex_T.csv', nrows=2, header=1)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 18,
+   ""id"": ""128b9e1f"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>3.8-1.2</th>\n"",
+       ""      <th>3.8-1.3</th>\n"",
+       ""      <th>3.8-1.4</th>\n"",
+       ""      <th>3.8-1.5</th>\n"",
+       ""      <th>5-HT3C2</th>\n"",
+       ""      <th>A1BG</th>\n"",
+       ""      <th>A1BG-AS1</th>\n"",
+       ""      <th>A1CF</th>\n"",
+       ""      <th>A2M</th>\n"",
+       ""      <th>A2M-AS1</th>\n"",
+       ""      <th>...</th>\n"",
+       ""      <th>HIST1H2AG</th>\n"",
+       ""      <th>HIST1H2AH</th>\n"",
+       ""      <th>HIST1H2AI</th>\n"",
+       ""      <th>HIST1H2AJ</th>\n"",
+       ""      <th>HIST1H2AK</th>\n"",
+       ""      <th>HIST1H2AL</th>\n"",
+       ""      <th>HIST1H2AM</th>\n"",
+       ""      <th>HIST1H2APS1</th>\n"",
+       ""      <th>HIST1H2APS2</th>\n"",
+       ""      <th>HIST1H2APS3</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>118</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>2 rows × 50281 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""   3.8-1.2  3.8-1.3  3.8-1.4  3.8-1.5  5-HT3C2  A1BG  A1BG-AS1  A1CF  A2M  \\\n"",
+       ""0        0        0        0        0        0     0         0     0    0   \n"",
+       ""1        0        0        0        0        0   118         0     0    0   \n"",
+       ""\n"",
+       ""   A2M-AS1  ...  HIST1H2AG  HIST1H2AH  HIST1H2AI  HIST1H2AJ  HIST1H2AK  \\\n"",
+       ""0        0  ...          0          0          0          0          0   \n"",
+       ""1        0  ...          0          0          0          0          0   \n"",
+       ""\n"",
+       ""   HIST1H2AL  HIST1H2AM  HIST1H2APS1  HIST1H2APS2  HIST1H2APS3  \n"",
+       ""0          0          0            0            0            0  \n"",
+       ""1          0          0            0            0            0  \n"",
+       ""\n"",
+       ""[2 rows x 50281 columns]""
+      ]
+     },
+     ""execution_count"": 18,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""testdf""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 19,
+   ""id"": ""dc4543cb"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""testlabels = pd.read_csv('../data/interim/labels/allen_cortex_labels.csv')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 20,
+   ""id"": ""76cc2928"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>Unnamed: 0</th>\n"",
+       ""      <th>Type</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>Exclude</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>1</td>\n"",
+       ""      <td>Interneuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>2</td>\n"",
+       ""      <td>Interneuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>3</td>\n"",
+       ""      <td>Interneuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>4</td>\n"",
+       ""      <td>Interneuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>49489</th>\n"",
+       ""      <td>49489</td>\n"",
+       ""      <td>Astrocyte</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>49490</th>\n"",
+       ""      <td>49490</td>\n"",
+       ""      <td>Interneuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>49491</th>\n"",
+       ""      <td>49491</td>\n"",
+       ""      <td>Interneuron</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>49492</th>\n"",
+       ""      <td>49492</td>\n"",
+       ""      <td>Oligodendrocyte</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>49493</th>\n"",
+       ""      <td>49493</td>\n"",
+       ""      <td>Oligodendrocyte</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>49494 rows × 2 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""       Unnamed: 0             Type\n"",
+       ""0               0          Exclude\n"",
+       ""1               1      Interneuron\n"",
+       ""2               2      Interneuron\n"",
+       ""3               3      Interneuron\n"",
+       ""4               4      Interneuron\n"",
+       ""...           ...              ...\n"",
+       ""49489       49489        Astrocyte\n"",
+       ""49490       49490      Interneuron\n"",
+       ""49491       49491      Interneuron\n"",
+       ""49492       49492  Oligodendrocyte\n"",
+       ""49493       49493  Oligodendrocyte\n"",
+       ""\n"",
+       ""[49494 rows x 2 columns]""
+      ]
+     },
+     ""execution_count"": 20,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""testlabels""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""0887998f"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 27,
+   ""id"": ""d8ed7769"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th># label</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>6.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>2</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>3</th>\n"",
+       ""      <td>6.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>4</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""   # label\n"",
+       ""0      6.0\n"",
+       ""1      0.0\n"",
+       ""2      0.0\n"",
+       ""3      6.0\n"",
+       ""4      0.0""
+      ]
+     },
+     ""execution_count"": 27,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""import pandas as pd \n"",
+    ""\n"",
+    ""df = pd.read_csv('../data/processed/labels/primary_labels_neighbors_15_components_100_clust_size_100.csv') + 1\n"",
+    ""df.head()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 36,
+   ""id"": ""f8712d11"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""# label\n"",
+       ""0.0        43689\n"",
+       ""1.0         5876\n"",
+       ""2.0          299\n"",
+       ""3.0          143\n"",
+       ""4.0          319\n"",
+       ""5.0          216\n"",
+       ""6.0         5714\n"",
+       ""7.0          439\n"",
+       ""8.0          517\n"",
+       ""9.0         8037\n"",
+       ""10.0        1126\n"",
+       ""11.0         104\n"",
+       ""12.0       12061\n"",
+       ""13.0       26527\n"",
+       ""14.0       29756\n"",
+       ""15.0       32772\n"",
+       ""16.0        2656\n"",
+       ""17.0       19158\n"",
+       ""dtype: int64""
+      ]
+     },
+     ""execution_count"": 36,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df.value_counts().sort_index()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 23,
+   ""id"": ""5956b257"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""array([2.28890567e-05, 1.70183799e-04, 3.34448161e-03, 6.99300699e-03,\n"",
+       ""       3.13479624e-03, 4.62962963e-03, 1.75008750e-04, 2.27790433e-03,\n"",
+       ""       1.93423598e-03, 1.24424537e-04, 8.88099467e-04, 9.61538462e-03,\n"",
+       ""       8.29118647e-05, 3.76974403e-05, 3.36066676e-05, 3.05138533e-05,\n"",
+       ""       3.76506024e-04, 5.21975154e-05])""
+      ]
+     },
+     ""execution_count"": 23,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""def class_weights(label_df):\n"",
+    ""    label_df = pd.read_csv(label_df)\n"",
+    ""    vals = label_df.value_counts().sort_index().values\n"",
+    ""    weights = 1 / vals\n"",
+    ""\n"",
+    ""    return weights\n"",
+    ""\n"",
+    ""class_weights('../data/processed/labels/primary_labels_neighbors_15_components_100_clust_size_100.csv')""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 37,
+   ""id"": ""7936870e"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""ename"": ""TypeError"",
+     ""evalue"": ""unhashable type: 'numpy.ndarray'"",
+     ""output_type"": ""error"",
+     ""traceback"": [
+      ""\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"",
+      ""\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)"",
+      ""\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_80344/91095760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m"",
+      ""\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\""classes should include all valid labels that can be in y\""\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"",
+      ""\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'""
+     ]
+    }
+   ],
+   ""source"": [
+    ""from sklearn.utils.class_weight import compute_class_weight\n"",
+    ""import numpy as np\n"",
+    ""\n"",
+    ""compute_class_weight(class_weight='balanced', classes=np.unique(df), y=df.values)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""93ea4f50"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""id"": ""186d8682"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""import scanpy as sc \n"",
+    ""import pandas as pd ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 2,
+   ""id"": ""ea597b79"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stderr"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""adata = sc.read_h5ad(\""/Users/julian/Downloads/Mo_PV_paper_TDTomato_mouseonly.h5ad\"")\n"",
+    ""adata.var_names_make_unique()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""id"": ""b53f0bb5"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(1624, 32286)""
+      ]
+     },
+     ""execution_count"": 3,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""adata.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""id"": ""7dea8490"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""<1624x32286 sparse matrix of type '<class 'numpy.float32'>'\n"",
+       ""\twith 4503058 stored elements in Compressed Sparse Row format>""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""adata.X""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""ce16fdf8"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""Index(['Xkr4', 'Gm1992', 'Gm19938', 'Gm37381', 'Rp1', 'Sox17', 'Gm37587',\n"",
+       ""       'Gm37323', 'Mrpl15', 'Lypla1',\n"",
+       ""       ...\n"",
+       ""       'AC163611.1', 'AC163611.2', 'AC140365.1', 'AC124606.2', 'AC124606.1',\n"",
+       ""       'AC133095.2', 'AC133095.1', 'AC234645.1', 'AC149090.1', 'TDTomato'],\n"",
+       ""      dtype='object', length=32286)""
+      ]
+     },
+     ""execution_count"": 10,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""adata.var.index""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 12,
+   ""id"": ""b88052bd"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""AnnData object with n_obs × n_vars = 1624 × 32286\n"",
+       ""    var: 'gene_ids', 'feature_types', 'genome'""
+      ]
+     },
+     ""execution_count"": 12,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""adata""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 6,
+   ""id"": ""5a85aa5d"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""image/png"": ""iVBORw0KGgoAAAANSUhEUgAAAWsAAAD1CAYAAACWXdT/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABQOElEQVR4nO29yW8zb7vn9a3R5fIYD7EzOc7wDL+XQ9OiUS+QkGiBBL1g0RLqDUJiAajZsT38AyzY9BIJHQnEBqkXh0WD2NBCOhvUvK/UcM57ftPzZHAGO/Fsl12umUXe+/5VlctDEjuJk/uzepzkKZfL9ve+6rqv63txnueBwWAwGG8b/rVPgMFgMBiLYWLNYDAYGwATawaDwdgAmFgzGAzGBsDEmsFgMDYAcR0HLRQKXrVaXcehGQwG493yhz/8oeV5XjHqd2sR62q1it///vfrODSDwWC8WziOu5z1O5YGYTAYjA2AiTWDwWBsAEysGQwGYwNgYs1gMBgbABNrBoPB2ACYWDMYDMYGsJbSPQaD8TI0Gg30+33EYjHs7e1BkqTXPiXGmmCRNYOxofT7fbRaLViWBU3TUK/XX/uUGGuEiTWDsaFYljX3MeN9wcSawdhQ0uk0BEGgj3O53CueDWPdsJw1g7GhyLKM09NTjEYjyLIMVVVf+5QYa4SJNYOxwUiShGw2+9qnwXgBWBqEwWAwNgAm1gwGg7EBMLFmMBiMDYCJNYPBYGwATKwZDAZjA2BizWAwGBsAE2sGg8HYAJhYMxgMxgbAxJrBYDA2ACbWDAaDsQEwsWYwGIwNgIk1g8FgbABMrBkMBmMDYGLNYDAYGwATawaDwdgAmFgzGAzGBsDEmsFgMDYAJtYMBoOxATCxZjAYjA2AiTWDwWBsAEysGQwGYwNgYs1gMBgbgPjaJ8BgvBSe56HZbELXdSQSCRQKhZU/h6ZpcBwHqVQKPM9iIcbqYGLN+DC0Wi3c398DAIbDIQRBwNbW1sqO32g00Gq1AACKouD4+JgJNmNlsE8S48MwmUwCj3VdX+nx2+124LnG4/FKj8/42DCxZnwYkslk4HEqlVrp8SVJmvuYwXgOLA3C+DBsbW1BEASMx2Mkk8kp8X4uBwcHuL29hW3bKBaLiMViKz0+42PDxJrxoUin00in02s5djwex8nJyVqOzWCwNAiDwWBsAEysGQwGYwNgYs1gMBgbABNrBoPB2ACYWDMYDMYGwKpB3jmWZcG2bSiKAo7jXvt0VspkMsHNzQ1s20ahUEA+n3/tU2Iw1gYT63dMv9/H9fU1PM+DqqqoVqvvqv35+vqadiXW63Woqop4PP7KZ8VgrIf3881lTHF3dwfP8wAA4/EYw+Hwlc9otdi2HXhsWdYrnQmDsX6YWL9jwlH0e4qqASCXy9F/x2IxJBKJtT7fZDLB9fU1bm5uYJrmWp+LwQjD0iDvmN3dXdRqNdi2ja2trZW3V78229vbSCQSsG0byWQSgiCs7bkcx8HFxQWN5kejET59+rQR+wD9fh/NZhOCIGBnZweKorz2KTGeABPrd4yqqvj69Ss8z9sIUXkK646mCWSjlmCaJlzXXesCsQoMw8DV1RV9fHV1hU+fPr3iGTGeChPrDwDHcRgMBuh0OhBFEeVyGaLI3vrHIMsyJEmieXFFUd68UAPTeXyWvtlc2Df2A0CiK7LZaFkWjo6OXvmsNgue53F8fIx2uw2O4zamTFBVVcRiMRiGAQDIZrOve0KMJ8PE+gNgGAYVamDahJ+xHJIkoVwuv/ZpPAqyyPT7fQiCsDbHQcb6YWL9AVBVFYIgwHEcAKs33fej6zomkwkSiQRkWV7b8zCWRxCEQOUMYzNhYv0BEEURJycn6PV6EEVxpXMH/fT7fbqZRSI6VnnAYKwGJtYfBFmWsb29vdbn6Ha79N+u62IwGDCxnoNt2+B5/t3VvzPWAxPrDcNxHDQaDRiGgXQ6jUKh8NqnRGEzCJfD8zxcX1+j3++D53lUKpV3VwPPWD1sSd8wGo0Gut0uxuMxGo0GBoPBa58SpVwuI51OQ5Zl5PN5VnkwA03T0O/3ATzcgdze3r7yGTE2ARZZbxikBGvW49dEEARUKpW1Hd91XXQ6HTiOg2w2u7EDaf2VOVGPGYwoWGS9YfhLrziOm1nZ4bruVMnepnNzc4NGo4Fms4mzs7MpI6dNIZVK0bQHx3HY2dl55TNibAIsst4wCoUCZFmGYRhIpVKRG3i6ruPi4gKO40BRFFSr1Sd3LJL86mAwQCwWQ6VSebWSPL9roOM40HV9rWWI64LjOBweHsI0TQiCwLpJGUvBIusNJJ1Oo1gszqy0uL+/pzXVk8kEnU7nyc/V7XbR7/fheR4mkwkajcaTj/Vc/K+X47iNTYMAv53/WxBqz/Nwf3+Pi4sLNJvNd3U39p54/U8KY+WETZueY+Lkum7gMVkEXoNKpYK7uzvYto18Ps+ablaA4zjodDq4v78H8LD5yZpo3iZMrN8h29vbGI/HsG0b8Xj8WV+8TCaDdrsNy7LAcdyrlgqKooi9vb1Xe/73RqPRQKvVmvo5syN4mzCxfocoioIvX77AcRwIgvCsyFqSJJyenkLXdciyvFQ02+12YRgGkskkqx9+o+i6HinUANh79kZhYv1O4ThuZflQQRCW/gK3Wi2a1261Wjg6Onoxz+m3jGmauLu7g+u6KBaLUFX1tU8pALlrSiQSTKzfKGyD8RUwTTNQHz0YDHBzc4NOp7PxmzvhOY+apj37mKZpYjweP/naaJqGTqfzql7OFxcX6Pf7GA6HuLy8fNXcPwDE4/FA09LOzg5KpRIT6jcMi6xfGH/kubW1hXQ6jVqtBuAhfeC67qvlhe/v79FqtSAIAg4ODp4U/cXjcYxGo8Dj59DpdGiHX3hCu+u6C301/NdbEAScnJy8+Mak67qBhcJxHFiW9erDC/b391EsFsHzPLMG2ACYWL8gnucFSt+63e5UhNVqtWjL9kui6zqtCHBdF9fX1/j8+fOjj1MqlcBxHK0Df65/Mjkn4GFC+2g0gqqquLi4gK7riMViqFarM8Wm1+vRfzuOA03TXrzSged5qKqK8XgMAFO5/06ng0ajAZ7nsbe396K145tc/vjRYGmQFya82Rf29rBtG5eXly95SgCmS/KWvU03TZN6lQAPr69UKqFSqazEijUcffI8j1arBV3XATy02/sFPUxYxJ+7CNq2PVXOuAzVahWlUgnb29s4Pj6mdwSmaeL29hau68K27cBEHwbDD4usXxCO47C3t4ebm5u5X0jSJv6SQ25VVQ1Ef8ViceH/MQwD379/p+K1v7+/cvOm3d1dXF1d0drqRCIxlRefdy3J9TZN81kT3j3PQ61Ww3A4BM/zODw8fNTGKc/zkdc0LPyu677rAceMp8PE+oVwXRftdhuu6+L09BSSJOHnn3+OjGBTqdTML6vneTAMA5IkrTTnyfM8qtUqxuMxBEFYKtfc7/cDYtPtdpcW6/F4jGazSSPxWbfjiURiakJ7LpdDv9+ned95OX5RFHF4eLjUOc1jMBjQRcJ1XTQaDZycnDz7uLFYDKlUih67UCgwf2tGJEys14jneRgMBnAcB/1+n2689Xo9nJ6eYm9vD9fX13BdF/l8HqIoguf5mekDx3Fwfn6OyWSytA8yqTyJx+MLS/l4nn9U5PlU/2rHcQIVEbqu48uXLwAeovVmswngobmHpC38i5csyzg9PYVpmpBl+UU26sLR+6xUiKZpdJO2XC4vvCYcx6FSqWA8HoPjuDdX0sd4OzCxXiO3t7eB6SkEy7Lo8IAffvgBwPyWcF3XYVkWTNOk3WWu66JerwN4ENmdnZ2pL/p4PMbFxQVc14UgCDg6OnrU5BbP8zAajegGWZhsNgvDMOhEmGXd4yzLCtxRkMccx+H8/Jy66Y1GI3z+/Dny2iwb/a+KdDqNeDxOc+WmaULTtMDiZpomLi8vA1Pkj4+PFx6b4zhWi85YCBPrNeKvRPDD83xkxBiFv3QtHEH6a7UvLy/x9evXwPFI2gV4iGa73e7Sgup5Hi4vL2mddKFQmJrszXEcyuXyoyd+k2oIUs5GBvoahhGwPSUi/hbMjnieRyKRoGLteR6urq7w5csXmrZgU+QZ6+T1vwXvmFgsFvjCklx0oVBYWoD8LcGO4yAWi8EwjMC0cvK78MZUWNzJY03TMB6PEY/HZ5aJTSaTQENLq9WiZXnzME2TOrcVi8XIXDQZptvpdMDzPC2lkyQpIOKKorx6LbKf8HvmOA5GoxG9hiTVRBacVZXgeZ5H9wcymcybuiaMl4OJ9QogX1pBENDr9TAcDqEoCnZ3d6ldaaFQQCaTefSxRVEMNFSUy2WoqgqO43BxcUGrNzKZzNTG1Pb2NgzDgK7rSCQSKBQKtIOOcHBwEHleUUK/SKg9z8PFxQU9X03T8Pnz58gNM1EUpwb48jyPo6MjtNttcByHfD7/pqoicrkc7u/vA/lq/2sTRRHHx8fo9Xorda67urqiJZ6dTidQ+sf4ODCxfia2bePs7GyqlVnTNPA8D9u2YZomhsNhpCj2ej2MRqOZ7nhkE9I0TeRyuUC0Vq1WMRgMwPN8ZBQniiKOjo4CPwuXvQ0Gg8jzkmUZOzs7uL+/p80ai3AcJ3AdbNuGZVmR0bWu6+B5fup3kiQ9Oq0yC8/zoGkaOI5bSRs1qZip1WpwHIeWEvpZ9RR5MiWeMJlM6IbxpuO6Lv08ptPpN7Uwv0WYWD8TTdNmek4Mh0OawyTRFskZ27aNWq1GI2OyERkW7FgsNrNEjOf5R9c1h8Vx3oZjPp9HPp9f+tiCIEBRFJr6kWU5shri6uqKDoyNx+OIxWLI5/MrFSB/XTTw0Nq/CntVVVWnSgnXCTHkIqkVjuPeRWs42RMhFVKpVGolJZbvGSbWz2Re7jlc7uXPAV9dXVGhJoxGo7W3QudyOZq2UVV1pT4kHMehWq3SPHs+n5+6XZ9MJlSogYcIW9d19Pt9fPny5dGbibZtR6ZoyN0ModvtolwuPynfOxgMcHd3R+clJhKJqecj11SSpJUuOmQEWL1eh+d52N7efhMbrs/FsqyAh8xwOIRt2+/ita0LdmWeSTKZRDqdnmobj8Jf/hZVKfASNbYcx630Nj2MKIooFAq4vr7G9+/fkUwmsbe3R8VtVq6VjA1bNl3hui7N2YuiiGq1GrhLiGpTf0qeN9wCXqvVpqpuHMfB2dkZrc7Z2dl51B3JIuLx+FIlgJsEWWDJdX3q+/ORYFfnCfR6Pfz000/4+eefcXV1FYgQwiQSCYiiCFVVAyLpzzGTLr5VfsFfk7u7O2iaBtu20ev1AjMgZVmemZN+jKlQp9Ohdya2bePu7o7+zrZtXFxc0MeSJKFSqTwpbUGqbPyPww0xw+EwUEbZbrcf/TwfDUEQUKlUEIvFoCgKDg8PmVgvgEXWj8S27YC3h/+WnoiuJEmYTCZIJBJwXZemPM7OzlAul9Hr9cDzPBXvbDa7Npe9yWRCjYK2t7ef7YK3DP5a6ajHhUIBuVwO5+fntG5ZkiS0Wi0Ui8Un3Qr7BbXZbAbuXNLp9JM3GGVZRiKRoAtyOp2eitrD58tK65YjlUpt5HT614ItZY8kHGn5IRaotm1TI/dWqxXoaLu+vsZwOES/30e/38d4PMb3799xdXW10M3tKSb8l5eXGI/HmEwmuLq6gmVZy7/YJ5LL5WgUKwhC5CYoKdMrl8vgOA6WZaHdbuPq6mqp59ja2qJpD0EQAnct4ev4HBc7kjM+ODhApVLBwcHB1N8kk0kUi0W6wcrmRDLWAYusl8TfmJBMJudOQKnX69ja2orMw/mFwzAMevvc7/cRj8dnbvh1u13c3NwAmDbhn3fOfnEmj9ddTZBKpXB6ekpLzKKez3Vd1Gq1qetIIu1FkEECpmlCFMVANEvqycmG1XM3UXmeX1gjXyqVUCqVnvU8DMY8mFgvib8MbJncWrvdhmEYVIw4jqMbkf5NFX8UGE4X+Amb8GuatjClwXEcstksbXtXFOVR3iDL0Ov1oGkaVFUNVLLEYrGZOWjP89DpdCIXvMd4ZHAcF/kcsVgMnz59gmVZkGV5o3KhZKrMql0Vl2E0GsE0TSSTyXdRHvjeYGI9A3JbTgTPXwa2jPm8f8MLeBCWYrEYyHGTn3ueNzNdQAgLzrICRCaPuK6LdDq9UuHq9/u4vr4G8CDanuct3CS1LAsXFxeBDTngIe+by+WmouDRaITb21t4nodSqbR0F6ggCG8id2yaJur1Ou1inbfAmqaJ8/NzWJZFG5peapJLu92mxmCiKOLk5IQJ9htjc0KOF8R1XZyfn6PVaqHZbOLs7GypSgJJkub6UIe9qz3PQzqdhiiKSCQSczcZ9/b26EZWPp+nG2au66LT6aDT6UQuIhzHIZPJYGtra+XiFVUnPg9i8RoWap7nsb+/j+3t7cBiQhonDMOAaZq4vr6ee/fxFiF3ZOPxGFdXV3OH9rbbbZq2sm2bWsW+BP6KHdu2pzpdGa8Pi6wjIHakhGVHXM3bvCMWpn6j+UQiQSPtwWCAZrM5M+85q3PO3wXW7XZxfHz8rM66+/t7dLtdSJKE/f39uQuIqqqBMrVFKYxGozElVul0Gru7u5EVIK7rBhYgz/M2rnHC/3o9z6Me3FGE37eXTN9IkhRYRFlU/fbYnE/9CyJJUqDF96kUi0W60UgEhhjNG4YxFb0sU6nh/0Lbth2IZnVdh2maT7511jSN5sYty8LNzQ2q1So0TYPneVMTbDKZDPW8nuVt4ics1DzPo1wuzxRfQRCQyWTogpZIJB712jzPg+d5r5qzTqfTdM9gUXdjoVCApmmYTCaIxWJLjVZbFf7xZ9lslpXUvUGYWIcgE7BlWY5MXTyGWCw2FUWR8r6oqofHuvKRvCw5R/+i8BTCi4Vt27i+vqZimUwmcXh4GBDsbDa70J+EjDTz/z9SujcvcidlkqIoIh6P4+DgYOm7Bl3XcXl5Cdu2kUqlntwU81z29vaQSCTgOM5Ce1NRFHF6egrHccDz/IueryRJqFarL/Z8jMfDxNrH7e1tIHf3HDiOi6y8GI/HU0KdTCZRKpUWekr0+33akp1IJOB5Hp1HSMZIPSeKTKVSkCSJinYmkwlUoRDTqsdG7re3tzS6JButmUxm4XEajQZt4ye16ctOTCf17uT/9nq9lUxbfywcxz36ed/Cxijj7cHEGg/R7ng8fpJQC4IAnucDUSmJiM7Pz5HL5ZDJZKhwR0W+mqYtrAX279Y3m01UKpVADlhVVdzd3WE8HtP23cfmHUkVgKZpkCQJqqqi1WrRvDHHcUsLied5cF0Xtm0HfFM8z5sq6xuNRuj1epBlGYVCgV6/cNrkMQ094UaYZRtjHMehfiObakOq6zrq9Tpc10WpVGIpjXfChxdrTdNQq9WWKseL4uDgAI7jBDrviDA4joNms4lms4n9/X1ks1koioJyuYxGoxE4zqL8eDi/fXNzE0jR+CszJpMJzs7OaMT+mNSIKIqBtEalUqFf/Hn5ZT/j8TgwENdP+I5jMpng4uIi0OW5u7sL4KFLkeTkeZ5/VKv89vY2nYeoKMpSKSbHcfD9+3e6SOzu7q7dBXHVkAoa8nmq1WpPcjNkvD0+/DtIfDMeC9kcUxQF9Xp9qsElTKvVoiJYKBTA8zydrRiLxRZGP4qiBJpIwkLodzADHkSv2+1C0zTs7+8/eSBrMpnEp0+fFv4dye+TMsIooSYLlT+qDrfP+zdMiWeKYRgLSxujzvvLly+wbXvpxpjBYBCI5tvt9krFutvtotlsQhAE7O7uriVyJxUz4cdMrDefD/sOko2+eXWv83BdF61WiwouYX9/H/1+fyoSDn9Zcrkc4vE4bNumA2PJeY1Go6mJ18T7IuzwBjwsHKVSCY1GY+p237IsnJ+fo1qtPntaSrfbRbfbpc555DXd3d0tVRPsrw93HAeTyWQqVRO2iVVVNfAzz/PQbDapH/f29nbkRpxlWeB5/lEdm081ZCIlhvME0TAMahcAPPiZf/78Ga7rQtM0CIKwkgnn5A6EpJ7IcAfG5vNhxbrdbj/byjJK6A3DQLFYnBLrqKni4ciKTMwmXzRFUZDL5Wj5H3Hs8yPLMiqVCr3ln0Wv13u0WJNIU1EUeJ5HxWY8HsN1XVQqlaU3Zf1VI5Zl4ddff4XrutSp0DAMmrOeR6fToZueo9EIg8EABwcHtLwwm82iXq9jMBiA4zjs7+8vTIHYto16vQ7LspBMJqHrOiRJWsqQaTAYUL/rTCaD/f39mYtH+LHrujg7O6MOgcVicSX+IgcHBxgMBnTALhuX9T74cGJNcqRPqaH2V0rMO364s4/juKVu4U3TnJq3d3t7C8MwqNiHv3i7u7tQFGXheZEvb7hDkNwih497c3NDR41Foes67Z5chKqq2N/fp4/9qScSKX/69GmpDdHw0AbDMPD9+3e6UHU6HXotPM9DvV5fKNY3NzeBxfX4+HjpQRBkggvwUK0zq0ZZVdXAyLNcLkfdEAntdnslYk26Vhnviw8j1q1WK9DO+xSWyW1Htelms9mloptZt93D4RA7OztwXXdmlcOiJh5iEETSArZt09ZvSZKmap7DEXwUHMfNzdXzPA9Zlqci1HBbuuu6+P79O/WjME0Tl5eXME0T6XQ6EK1GPVc4Vx8+x0WE00qGYaxkas9gMIBt29RS4OjoCMPhEIIgIJVKTS08LK/MmMeH8AbRdR2NRuNZQq0oypMbZJatsxVFEfv7+1OiTQTWMIwpQSZVF8T0RxCESKGRJImKMcnXE5GyLGsq57xI5Gzbxo8//jg3H1qpVHB8fAxN09BqtTAej3F9fR2ZrvF3Y9brdRiGQW1pf/75Z/z000+o1+tTRlhRkKoRMjNxEf4oVBCER6WLdnZ26LXyDzloNBqo1Wq4vb3F9+/f6axIf+StKAp2dnYgiiIURYn0ymYwCAuXco7j0gCKnud9D/3873ie9/+t7cxWgOu6cBxnqkzuKTy22SQWi0FVVWQymUdFaSS32+/3qUcHGYM1K/K+vr7G4eEhdnd3YZomVFXF7e0tRqMRjeJKpRJ4nofnebi4uJiKbv0C6routra25ub0SSv3LP/peDwOVVUD3iWLmBU9kwUq6nzy+fzUz5PJJAqFAl28/Liui0ajgfF4jEQigXK5jFKpRFNJ6XSapmOI/StppY9awNLpNL5+/QrHcWbemViWhfF4HFl6+NgJ8oyPy1yx5jjuHwP4pwDuOY6TAPxnnuf9P3/69f8I4N9c69k9A03TAnP4nst4PJ4qj5tHtVqNzMF2Oh3c3d2B53ns7e3RSMyyLFxdXdFxYKVSaar9d1ZqYjQaodPp0MoUf9s5aV32m/GHxVOSJOpD0e/3cXt7++S7CDJTj9yJLCvUwG8Lhr++eh6kFHAwGATumsh1IJYBZCp4LpdDq9WieXZSjVIoFKZyvFH2r7M2P6PsWGVZDtwFMWMkxnNZFFn/NwD+nud5dY7j/j6A/5njuD/3PO8vAbzpLWZ/mdSqCAs12TgM5zwFQYjMP5qmSYWENNKQSdmk+xB4yFEPh0MUCoXAcNlZtpWqqgY2A0l+muDfBIy6Q/j06RN4noemaUuN1fLnx8O5cjL9hpQvzsqlEzH3i6xhGHBdN9Di7vc+URQFyWQSw+EQsViMTk0/OjrCt2/fpiJy/zW4vb2FbdtT79Ws0s3wJnH48SIKhQJqtRp9DW+1hZxMDxJFcaOGNHxEFom14HleHQA8z/uXHMf9AwD/nOO4AwBPH2y3ZsLjrNbJ8fFxoOtNluWZ5VvhaNX/OErQyADZWV90SZKQSqXged7UZtUsojboSE512aaTdDqNXC4H27Zpx1wU4/E4UgAkSaLlgP4cNKmWiLKnLRaLKBaLtIQxfLxlNn/v7+8D+eh5VROJROJR9q9h/K+LNAvNmuo+j8FgAE3TaBnnKglvMlerVVaT/YZZJNZDjuNOSL76TxH2vwvgfwXwr6331B6P67r46aefntw6/liIV0apVApEpH7RM00ThmHQ1IB/fqPfByOfz2M0Gs1Ns5AaYMLe3h56vV4gPUIEvNvtzjxWOJ1Dos1l5x+GDaP8o8PCRL0XlmWh1+tBEARsbW3BNE2kUilkMpmZi2y73Yau6xiPx1BVFQcHB3QR4zgu4BNOiKpUmUwmODk5oceZ1UWYTqdp/fYy9q9hVuFNPRwOaXQOPFzL586T9NNqtaY2mf0lloy3xSKx/q8QSnd4njfkOO4/BPCP13ZWT8C2bfz0008v+pyk+6zVatGfkVppUkd7cXEB13UhCAKOjo5weHiI0WgEnucDG49kyGyr1UK326WVDIIgUKtS0qzB8zy2traQTCanNk8LhQLy+Tz1Rr6/v5+K2nmen5mTJh7VxKDKcZwpIa7VaqhUKlSAyBQb4sW8LMQ21B/tSpKEWCw2la4g1xp42I9oNpuBSPXg4ADdbpcekxyr3+8HUmKKoiAejy9s9SaLrKqqCy1goyiVStB1nR4jl8uh2WxiMpkglUotdcxw3n40Gq1UrBmbxSKxHgEoAfgW+vnfB/B/r+WMnoDjOPj5559f5bmj2qw9z8NgMECv16ORneM46Ha72NnZmVkaRvKwxMiIRGe1Wo3mTInI3t3dIZVKIZlMUoH0t6jLsky75Px5X/8xwsiyjJ2dHTiOg2/fvs2MzDVNQ6/Xo9Emx3Eol8vwPA9//OMf51ytYFQvCEJkO/jx8THu7++nNg79RC1A4aqK4XAIz/Owt7eHwWAAURSXajqxLAvfv3+n10nXdfqeEEajEc2tl8vlKfGXJAmfPn2ijUj39/f07/v9/lLGVOFjrqL22w+ZAk/SIC857IDxeBaJ9T8F8OcRPx/86Xf/0YrP59F4noeffvppYZUG2fBb9Qw/Ev2QjTRBEKiVafhWeN4mE2m9JvhtQqM2tzzPw/X1NXK5HBKJBARBQKFQoOI3Go2WnuFXKBSQSCSoR0m/3194PQ3DwPn5Oa20SCaTC9NPJO0xGo0gyzKKxWLkRiwZjru9vY0ff/wx8likVtkwDNRqNViWhWw2S+ue/eL42AGw4/E4sKANBoOAWDuOg8vLS/p6Ly8v8eXLl8h9CnL3EU4xzSrl85PJZOgdRTweX3mJHxl2EN5gJPX88XicbTq+IRaJdcnzvL8O/9DzvL/mOK66nlNaDtd1H1XD+1KDVv1fcjLlxHVdJBKJyFvYTqeDVqsF0zRpjpVEqvl8fmpKuh9d1wO3+P68ajiNMIvT09Op6HaR+ZEsy+j3+/SaErESRTFgIhTGcRyaMnJdF7FYDIPBAP1+n/qCXF5e0sVpljiJokjFmrTjAw/XMplMBkZpAQ/vvaZpSzUnhedvApjadLNtO7Aw2bYNx3HmdiAmEolATn3ZDcutra21Dk0IWyH4fdNVVUW1WqWVQqZpIplMPsr9kLE6Fol1ds7vXtWZ3W+A8xaYFVV6nocffviBRl2apmEwGECWZXAcR78Y/mMQTwtFUZbq2COMRiOaYlm21EzX9SlxjrrdzufzEAQB9/f3U2LmeR6tHd/e3kY6nQ5MaonCMAxqgkQYj8eB845qgkmlUtjZ2aER36wKG1mWA+e5jMAYhoGzs7NAqSCp5fYjyzLi8TiNlhOJxMJWcWKLS3LWb3UggP9ubDweYzQawbKsQA3/yckJqxp5BRaJ9e85jvsvPM/7H/w/5DjuPwfwh/Wd1ny63e6bEmpgtlgTK9B4PE43HAnLjLUKE7X5RvDnOKNqwqNSG+HUgGVZ1KKV/D0Rc3/9cxhS501GjC26kxFFcSofPc+uVhAEnJ6eTp1voVCgzSuxWIymFvb29nB7ewvLsrC1tUUjWdM0cXV1RQfDlstlupD2+/2A+JP2/zAcx6FaraLX64HjuKU3IDdhkEH4veN5PnCX4rourXNnvCyLxPq/BvCXHMf9J/hNnP8tADKAf7TG84rEdV3U6/W5bnCvzdbW1tT5EQGIMjCaRzjPWSqVEIvFAuVchHg8HsiB5vN5DIdDusFYrVbpqK16vU7bo+/u7tDv95FOp9HpdCIbbyaTydKLo23bC4U6Fovh4OAAnucFFoVUKjUlmARVVSNzztlsFvF4HJZlIR6Pg+M41Go1WptcrVYDUW+9XqfXtd1uQ1EUmmYIR8f+57NtG81mk3YyyrL8LtvE9/b2UKvV4DgO8vk8Hfrgv+Nh3Zivw1yx9jzvDsC//admmD/704//N8/z/sXazyyE67r427/925d+2inCNqnh6oadnR1omkb/xj/GKry7T0ZoLbsR2Ov1cHx8HKjVJkwmE7iuS3O4qVQKnz59ojv9pPQvmUzi+PgYZ2dn9Auo6/qzFsBFU3LCGIZBHfWq1SpNC+VyOZRKJbRaLVo+2O12wfP8XEMm/0zHVqtFc+bj8Rh3d3cB17/wQnJzcwNBEJBOp7G1tYXJZILhcAhFUQKVI6R5BHioMiFdn+8NVVXx9evXwM/K5TLtik2n08x+9ZVY5A2iAPgnAE4B/DWAv/A872V26nw4jjOzKuAlOD4+pt144XKybDZLO/mIWdLR0RHu7u5oEwOJ2MK3/bqu04iVjPYyDGNmW7lhGNS0qdlsBtISiqKg0WjQW1Zd1yHLMrLZLG5vb2k0qWlaoONyFbiuS1MBJMe5CPI3iUQisNlGmoxs28avv/5Ko+ybmxscHR0tPG5YjMNRej6fp2kTQr1eRzqdBsdxUyV65Bj+1BPZhHzMFJooyOKqaRpUVcXe3t6bbEsXRRGVSuW1T+PDsygN8j8BsAD8FYB/COAHPKRGXoxWq7US17ynsru7GxgtRZo/gIeomZRg+cdLybIcaXcZlTMmPzMMg0aV8yCGUtvb24jH42i1WrBtOzJVQaLtsIA9V6ijIvvxeIzT01P8+uuvC/8/WfTu7+/pOKxwq7NhGAGhJd2dUeVxpmni5uYGtm0jlUrRunKO46ZSFdlsFo7jBDZ2F9nBCoIQ2CvgeR7NZpOWHz41wm6323RxHQwGkCRpKUtXxsdkkVj/zvO8fx0AOI77CwD/cv2n9BvdbvdVhTqqzTidTuP09BR3d3e0oaDZbEIUxYAwGIaBu7s7eJ6HYrFIO+Hu7u5mNqUsk0ogJYv7+/tIpVIQRRHfv3+P/NtWq4VWq7V0tEY8NhY57hmGgUKhMNW5aVnWXOEjvh7tdjuQvydzIv2335IkBVJMqqrOPPbNzQ09nmEYqFQq4DgOsVgssgqEdJeS5pRZAkmaimKxGKrVKlqtFizLouWG5G/Cm5Cj0YjmzOelDMKLqG3bME0T3W4XgiAgl8s9aSEgizTJ4TPeB4vEmt7Pep5nv+QbP5lM1uKc99hziEJRlKkvUbPZRCKRoLfGZNIJ8PDl/fz5M0RRxJcvX2jFhGmadDHa2tpCJpOhm3+z8DwPw+EQl5eXqFarkX8brvxYJL7xeJzahI7HY5ydnc39+6jOSFJPPg+O45DL5SIXYJJKIhFxeEjBvEqKcNrFn5YBHhatXq8HWZahqip6vR4kSaLvSZQg+i1nyWzFnZ0ddLvdQB15uERyNBrh/Pw88LpmbURms1l0u11aW59Op3F2dkZFfDwePzr90Gw2aW1+MpnE4eEhE+x3wiKx/jc4jiOfTA5A/E+POQCe53nzW7CegOd5qNVqM/O268YvQPNqYdPpdKAG2rZt2hwStih1XTfQJeZvciBeHCQCLBaLVMxILXZUqd54PMa3b99otYM/SlvWc5ug6zp6vR7S6TSdlxjO64bx265yHIdCoYCbm5u5KRb/hJSwyMViMfz4448zBb/X6yGbzcJ1XbppSvYCVFWdut4ETdPo9ZxMJlRoJ5MJLMvC6elp5PP5F5R+v49YLEbdAv2LYbi5JZweGg6HM8U6Ho/j9PSU1rpblhV4H5/yHfDvY2iaBl3Xn9Smbts2NSpjvA0WVYO8+Dv1yy+/vJi9aRQksisUCnO9EjKZDJ2KTbAsC57nged5JBIJemsuy/LMulR/GZR/UngqlaJm+bPqqk3TxHA4RLlcXiiuiyAe2ul0GtlslrrcLQNpipkXwcXjcezu7tKqFD+KotAxXrOQJClg6cnzPA4PDyM7AVutFo3E5y0ek8kEjuNEClL4tRARjMfjqFar6Pf7kCRpSoif0g1KFmqO4wILwSKzqSjCJl1PSaM0Gg2a4trd3d2I+vCPwJuqPbq4uHhVoSZ4ngdN02Z+0F3Xxd3dHSaTSSAnSgbjep6HSqVC63E5jpvZgk2wbTsQrZJUh997JIrhcLi0UC+Kkq6vr/Ht2zcqhlHMEmRSwz0LXddxdXVFp674icfjkUJNzldVVZTLZfR6Pbpw+YcUhK+NaZr0XJLJ5NzXPatscnd3N/K16roOQRCwu7sbubmYyWSws7ODVCqFYrH4qGnlsiyjWq3SMsJZKRCSPova/yDWsRzH0XFlj8EwjMBehH96O+N1eTPjlJvN5tQt5EtAKitardZSznQAcHV1RW9RiY8H8BD9DQYDXF9fTwnX9fU1bm9vaa53d3c3sPm0KAIidcck8vc8L9LDeR6Lcteu62IymeD6+hqlUinSd4WMI+t0OhiPx5ECTUQu/CUP3yHwPI9sNhu5NyDLMj59+hQQzPA1Io+3trYCAkPEihzn5OQEmqZhNBpNte/PWmAymQxSqRRc1w2UEHIct7C1/DlzFcOljGEcx8H5+TkNajRNw8nJCf19MpnEDz/88KjnJFa6PM+zKPoN82Yi63mGRasglUpFGuIQa83w7+bl+fwiRqbSNBoNXF1doVarzRQAv13q1dVVIL+4yJDKcRwqGCRyj6oJXgW6rgd8RsLnkU6n5zbCkLmHYfyLE8dx2N/fx2AwmMpfcxyH4+PjqcjWPxlclmW6SBJrWVEU6UxFf+7X33DjvxMi4kTulGq1WqC1msyyrFardFBBpVJZKNbrhFTdEHRdf9awDbLXMh6PoWka6vV6YKHxT29nvC5vJrJeJ/F4HMViMXK+IPHCDpdYOY6D8XgM27apBSlBFEWaC+U47sl3BPf39xAEIdLpbRFkEvc8l7tlKBaLyGazqNVqgci32WxSf2z/QkLGiD32ObPZLEqlEtLpNN300nU9sjXd30jkh+SpiUe0X6S2tragKArOz89xd3eHZrOJo6MjmvedTCY4Pz+H4zi0nllVVYiiiHq9Tk2jiO+1f6GKx+M4Pj5e6nWSc1pXd6Msy4FN8Fgs9qznIvss/selUolOMXrswjQcDtHtdiFJEra3t9kG5Qp592JN3NHmlaOFB7eSn5H/oygKjo+P6ZRwv7AqigJBEOZaks4TVP8GpZ+trS0IgoBOpxMZObXbbXAct5KqmVgshuPj46kuUc/zMBqNkM1mwfM8Xbii0imJRAIcx9EGnTD+lnsioLPSMoIg0DrlWWJxfn6O0WiEWCyGw8NDyLIcuFau66LT6dBW82azSZ+P1Eo7jjM1Kg14EPZZAyL8kAWdbCD77UXL5fJaprqQiUPNZpO6HD4H0qpPPr+pVAo8zz+5vts/j9OyLNb5uELejFifnp7i27fwQJrns2xFg6Io1HM4bE06mUygaRrS6fRUJOi6Lg4ODgJObvl8njYmALNTPOF6aFVVqYAQNzjbtiPnG/onpT8Hkqdut9szUxuTyYR6Tw8Gg6k66XQ6jUqlgsvLy5m16YPBYEq8EokEyuUybR4ikMeiKOL4+HiqscXfVEOaj/wzGQnzojoyu1IURVpf7j+vRViWhbOzM9oItLe3F1h4G40GstnsoyJTx3EwGAyoV8ksFEWJ7JB9CjzP4/j4mDoIPsc7O/zeL/vdYyzHmxFrMhvvNd5g4qAmiiIVlNFoFBBmIlgkJUKitFwuRzexgN/8PsgXft4Um7AdJREM27axtbWFwWAwlcueZXX6VEzTxNnZ2dy8ZzweDxg9hZ+fiOm8VI5ftDzPg2EYEEVxqmTPH+XZto1utztVURE+V/L/i8UiDMPAaDRCIpEIlF6SmYimaQauIZnqvru7C8MwkE6nlyqZ6/f79G7M87xI7+3HvE+u6+Ls7Iy+9lwut7Y9iTCCICCfzz/7c6WqamDBX+buhLE8b0asAeDk5AR/8zd/86LPmclkIj+opDHEsiw4jkPHRCmKQoU6kUgENmP8t8FknuG8L4Bt23RwrmVZ9IvqOA5qtdqU+JEmhVVMvSH+HvPSKIqi0BI0kjaIotVq0frsqLsIci0ABMZtAdNld8uMQsvlcuj1erAsCzzPU1EWBAGHh4f074bDIU1pxONxOhPx+vo68LplWX70MIDweUmShGQySUsBC4XCo6xEx+NxIJXW7XafJNZ+k6nH5ItbrRatld/b23uSs54syzRKlySJVZasmDcl1gDwu9/97kWtUIfDIfr9PjiOw+HhIY0GFEXB6ekprq+vA2kI/63eaDTCzz//jEqlgng8HoiuTNOciuKiIN2N4Zx3WKhJeuX29namWGezWXqu4ecNdzkuKuMDHgQonU4H6pbnvY5isQhFUdBut+mmK8mxSpIUGDJLiPLHiMfjVGSjvvBkGG24kxF4ED3SnUiE8/7+HsfHx4jH47RsknRb+itMCJqm4fb2Fp7noVwuRwpXNpvFaDTCYDBALBZDuVyGLMs0jfDY0VdhYX/K6CxN03B5eQnP8yBJEo6Pj5daMPy2B2S2J3EhfCxRk3UYq+HNiTXP8/jy5cuLTSv3j9JqNptTt26LPuxk5NHJyUmgSgR4uKU/OTmhE0tmNfwsmpdIGhw4jovMCZMStFKphGKxiOFwiPv7+4BYkw5AgizLC1NOpKtRUZS5C44oijTtk0gkqEjyPI/9/X16DZcZYEDO0fM86hcSBc/zU+mKwWAQOZiBVK+Qv5ckCdVqNfK4xO6AfC6ur68jx3aR0sMwT51PGIvFsL+/j2azSReUx0J8wIGHz2W3211qAzIqrTTL4fA94Louer0e9ZB5zVLMx/Amz1KSpMiJK1E81vh+0bHCFItFOmGc5KuJ+Q6B/LtQKKDRaMBxHORyOSr8x8fHgfKwx0IMjsi/w7iuC13X4XkeYrEYzs/PA+eXTqdRKpWouVQqlcLu7i7i8ThGoxGtapjFIpElbnGxWAzdbpcey3Vd3Nzc0Gksy+xH+NM8uq6j3W5TwSFt7WRYwf7+fuCLFrURS5jVyed5Hh1WTFwH/deOXPuX+EJns9mlR4RFEf78LpsGIe6AZFP9Obavm4C/qa3T6eD09HQjXu+bFGsAtORqnmATE6NFVSSko81xnJmRmr/Jwg/P8ygUCjBNE6PRKFJwi8Uibm5u6LmKojgV0ZA86zLphyhc14WiKIENOH+qYzQaodfrIZfLTT0HqR3e29ujaYpGowGe5+nQg0UmTADmbgCTTcNwzTmZLK5p2tLlcLPodru0U9E0Tfz666/48uUL/aLNWrRJxYfruhiPxxBFkd4t+E3DyuUyNdUi18Lv3fHWKZfLME2TppA0TUOr1UIikcDu7u5cQdrf36e11c8dqvCWIa6VBHK9nmJ29dK8WbEGHgR7NBrNFJGbmxtUKpWpxo0wiwTyd7/73cwPcq/Xm+m9wfM8PM+bGotl2zZarRaKxSLtbnQcBzs7O7TLzm+hCsyvxSYVChzH4ejoiJZZtdvtwDFITXQul5taVDqdDjqdTmQOvdvt4uDggOY7Z2EYBj59+oROpwPHcTAcDuF5HlRVXarskuSibduGKIo4OjqC53kYj8eRZYixWCyQsw5/DhzHoQsU+fvwYkE2zFzXxfn5OV1sdnd3Icty4IsbZd1qmmakc914PMZwOJyaffmayLKM09NTegdCFrZer4dYLDbXmIzjuCcZR20aHMcFFmOe5zdmMX7TYg0Anz9/nlkhQmpdH1NyFJ6huKgDzG+uFIZEclHRdqfTQT6fx9XVFU0LjEYjnJ6e0kYOItjJZDLyNfA8j93dXequR8z0C4UCer3elHj1+31qvq9pWmQuPOp5SPXJouvoui6azSbN1Q4GA3Q6nZkdnPl8nl4bnueRyWRQKBRgWRZkWabXXZKkKbE+OTmhdqSETCYzNUknvOD534vt7W1sbW1Rzxb/XYH/dSwivAka9vze2dl5U8NzOY6b2h9ZRQXRe+Hw8BCNRoNuirOc9Qo5OjoKGLr7mSUwqqpO5WFJZcJoNEKn04GiKAs3csJvZCwWo3nOeViWhX6/H8j3kv9HusZ2d3dxf38/s+uP7MyHX8Pnz59n5mcnkwl++eWXR38Al83793o97O3todvtLmzK0XUde3t7NAr99u0bstks9vb2AiIsCAL29vbo8XZ2diKjvHg8HkgDAUG/kUQigePjY4xGI1p2SIjK5yYSCeRyOXrHAUx/nhRFmUrfhMsd53lWvxakTp9Y9j4nF/7eIMHSprERYp1IJPD161f8/PPPc6M/4iOdzWaRSCSo9zHHcchmsxgOh/jll18gSRKOjo6Wuv3Z2dmBZVlUTA3DQC6Xo2kIP1EphlQqRTduRFGkImRZ1sK0w6wo+Pb2duG527YNRVHoeYfvKMI8ptmG47i5dxyE8XiMfD4PwzBoZNfr9ZBKpabK4ba2tqigdDodfPv2jXp4+F/r0dER6vU6bNtGLpebEnX/vEzgt6k2iUQChUIB7XYbkiTRPZHd3V3aLToej2kbN7G7TSQSU0L/WM/q1yCZTOLk5ASTyYQucozNZiPEGngQusPDQ1xcXET+Ph6P4+DgIPDFPjk5gWmaEEUR5+fnVDAsy8L19fVCc552u43BYIDJZBIQsk6ng8PDQ6TTabRaLWiaRjcxiSCqqopMJkMXDtu2aZnQcDgMlFk9lslkguPjY9oqrihKZKRNmlEuLy+nhDpcd10qlZaad+nPD4fvBqLy7qPRaCpqnxXFcxyH0WhEG4smkwmGwyGSySQqlQp1wVu21do/miudTuPg4IBuIpPOTcuyqMnUIntSAhk8QcoayWaybdvQdX3m7McoHMfBzc0NdF1HKpVaqcudoihvciFhPI2NEWsAtKkhasNQ13WaCyWQiKjb7U6J1aIhB71eb6bJEvCQHx4Oh3Bdd2rEFxDMhfs3yXRdD5jd+P+e3N4vioLJdfDnXPv9fkD8iU838dAOUyqVIIoiLUlMJpPo9Xpzy/SITSgpnQu/F2RCt//cO50OMpkMPW4sFpu7IReVXtI0DWdnZ7SbctkyK383JbFiJUZUfk/oZrMJVVUjuxjJqK3w3kbYs5qIPxmH5W+wmkej0aALHEnNsc4/RhQbJdaCIODTp0+01jYs2rVaLVDKNZlMZvpezHJEI2mGebankiQFItmwUJPjRBEufRNFEaVSCdlsltpVyrKM0WgEXdeh6zo4jgs8R9S5+yNjnudxcnJC8+thyMYb8JvlKTGkurm5geM4yOfzU1Uu4/F4ah8glUoFoueoRWY4HNI0i23bsG2bCj0xbCLMimwnkwntTFx2YzAcoZJziJpIFLUBNxgMcHV1Bc/zEI/HcXR0NLdqyN/Q0263lxLr8PO+hUlJjLfJRok1AFovDEzXYDuOg2/fvtH2Zk3TAkItyzKSySQymQwVBVI6JggCFEXB3d1dpPiSW8pMJoNOp7PwSzXLvUxV1UB+mIxwIudHSCaT9Mse3mQkA1b9FAoFpFIpGIZBfZrJz6N8vP3Hury8hG3bSKfTODo6oiJnGAZdFGfltMfj8VK37eT/knI74iMNIDD+atFG52OMvsjMR9d1kc/noaoqTVX4EUUxMqr2uwHquo7BYDBzo+4xjn9+tra26IYlqZiZB9mkFkWReUV/MDZOrAn5fD6yYcY0TVxcXODTp09TmyqqqtLqD9JdR3bMgYfoNCzCiqJAlmWoqopcLgee5yNbmv1E+U34j0cGrhK3v0WEz4nUV4dTAqTKxA8pdyMCxXEcnahOJn+T6G4wGKDf71NBIvsEpJY8KqctiiL1LIlCEISp1nae5wMppmazGSix8xNeJGRZxv39PVRVXRi5ptNp/PDDD7QigpyPP1VDLEJnDTsIn8sstra2aNWLoihLz15Mp9M4OTmhi+y8XDdJ30wmE2patQnNHIzVsLFiTUqqotIVhmHgj3/8I3Z2drCzs4N+v0/Ndgj39/dTEXSz2cTe3l5gviIpqyN2pUS85rFIRJbdyAIevqDhPC5JCZimCVmW4XketWqN4uTkhLrUZTIZSJKEi4uLyEYi8trG4zEuLy9pVB2LxZDJZGie3v9ac7kcUqkUarXaVNTqOE5goUin08jlclPufKQD039OiqJQ10MAdHOWvD/ZbDYyJWKaJq6vr2GaJjKZTOB95zgO1WqVeqdsb2/PvG67u7v0riOTyczNtfM8/2SPaf9Ahnl0u12a+ydOkLM8Thjvj40Va+ChuL3X6+Hm5mbqd57n4fb2Fqenp5HRa1QaQxAEKmaTyQQ8zwdSEKRjL1zr60dV1aU62izLQq1Wo63BBwcHkflQTdNmplz8dwX9fh+fPn2amVMlOXHiIxIl1PF4nN6G39/fB/xISL6YpBYA0Bb+8XgMVVVxdHSE6+vrmZ2YsViMCpq/rBBA5HsY/ptwfrfX6yGRSEylnOr1Os2tt9ttWpkTdR5hLMuiZYmFQgFfv36lY8Rem6gcPOPjsNFiTSZbWJY1Zb1JmNW55W8aABCoriC1usQvmUSSpKPu5OQE3759C0S8kiTh8PAQsVhsqS/R3d0djTiHwyE6nU7kxuG8vGR4dp5hGDMjtG63SwUxHPnzPI9yuYytrS167lGvwXEcpFIpnJyc4O7uDpqm0YkrxOx/3m2867rwPI92Wi4iXI4YVSUTVb0SZbu6DKQlnbyvZHL4WxBq4OEz2+/3qb/JsqkWxvtgo8WasL29Taczh7m6usLBwcGUQJGmAeL7ENU0QAS41WqB4zhwHIerqyvk83lks9nAAiFJ0qNqWsMbabOqR5LJJG3m8ItzPp/HYDCg4kVyw2FIhYk/36xpGlKpFK3ScF0Xt7e3MAyDDgkgk1X8QpdKpSAIAuLx+JQAjkYjjEYj2r4bJZA8z6PRaCx0H4xyUiR19PV6PdBBGLU3kM/nafQviuLS3h3hwcW6rsNxnDezkUfy66SahkXWH4t3IdYAUKlUIocWOI6Dy8tL/PDDD5GdaIsEluSXz87O6K11v9+nXxYioGSg7LJt3vl8nlariKI4d/ZduVxGuVzGaDSCYRi0q86fcy+XyxAEgaYrVFVFp9OhueHwF5sInn8BaLfb1F+DlEm6rgtN0+jGpK7rqNfrM1MznU4HBwcHtFnIj+M4U5vCfmHmOA7lcjlygk08Hken06F12qZpIpFI0NcJgA72zWazUBQFpmkGKmMWQQYZkIXG71/yltgULwvGank37zoZWvDLL79MbQASM/XnELVxJggCrVHWNA0XFxc4PT1deCwyJ7BUKtEFY5nozb8xSTxFCCSlQ6oywimDZV4/x3EBDxZN0/D58+fAQkI23Pz/J3zsWV2mUQIf9o5OJpNIpVJ00SHPoWkajXpjsRi9zv6N0l6vR0sPiZ3seDyGZVlLbeDxPI9qtYpms0mbilj0yngrvBuxBh4E6ocffkC9Xg9EcOl0+tm3sslkciraIzlYAploPi8aM00T379/pyJVLBaXrgzxE34OnucD6YWnNFfwPB9Ix5DuPTLpxXXdqfTG4eEh6vU69WBZZgIN8NvIMP8gBzJjUhRFfPnyBZPJhI7u8rvcGYZBX59/o5QIM6mQuby8pNG9v5Z7HqucHM5grJK3d4/3THiex97eHr5+/Yq9vT1UKpWVfPkODg6wvb0duAUtFAqBx4qiLLxtDjfqzKqciMI/PME/iSYWi6FUKs1dkMKRZdTfhhcNIp7Ag1CHm2s4jgPP8/RvPM+L9CAhI8n8WJaFdruNra0tyLIMSZKmJr+QJiRFUQLj1Ui6ImrYLzmXyWQSSMM0m82VToVnMF6adxVZ+wnngUmUGPZIXhae57G9vY1isRjoeMxms2i32+A4bq65OyG8kbmMGxqxSiW5ctIMUa1WA7Py9vb2AlE78Juox+Nx3N3dYTwewzTNqQ1NVVVxcHCARqOBbrdLZwySvyee2uHzuri4CDxfOPJOp9MoFovodruRvh+GYeDz589zX78/PQH8NnYqahOSiHWUJSpLaTA2mXcr1n76/X4gKlRVFYeHh09KjRDbTALxpV6WRCKBvb099Ho9agG6CDKBHfjNu4TkbP0CFIvFUCgUAlUqZEKMJEk4OTnBcDiMrGkmlqCkkQh42HCcZ2YFTFe1kMG8pGuQmBKRDb8wy1p3yrIMRVGg6zrG4zFisRiy2Sxt/Sc5Zv9xd3Z2cHd3Rwf3MhibzIcQ63C33Hg8piPBXoOtra251R9hojZMZ0FK5yaTCa0eAX4bhpBMJgObgpIkIZ/PR55PuM45ajOR+K10u10IgoCDgwNalRKPx2k54e7uLrWQJSZOs9qyHccBz/OBgQD+WYn9fp96lJ+enlJHPeJjkk6nwfP8lDMeg7HJfAixjrr9HQwGaDabS6UuXptUKkUn33AcN3ejjOM4Gs1eXl4Gmk8EQUAsFqNzHEVRpH4UfmFsNpt0Wrof/wxJ0s2Zz+chiuKUD3M4Yl7Gh9rzPNzc3KDX69FomLSgh9M2uq5TD5O7uzuaful2u1AUBcfHx2+y7I7BeCofQqyJx0P4lv3+/p5OdH7L8DyPo6MjGIYBURQfVTdM8E9nUVUViqLg/Pycpkz29vawtbWFdrsdSKOQOnTi6UEIb1iSa0gGBNu2jXw+v7AhZTKZwHEcxONx6g8CPKRXiGNeFCQV5Z9C4z/maDSaaabFYGwiH0KsE4kEfvjhh6m5gf6ZiG8dUju8LOPxODB6yzTNwKI0HA6nBshubW1Nld35nQqXwT8geDwe0wHBwEMlTKvVou3tw+GQ5sRVVZ1KWcxK9xQKBboIkMaVsKizxhHGe+PD3CcSH5FwFP1ev9ThtEH4cVSdNjDdvr2Mgb6fqAHBwG8zJzVNw2AwwOXlJa3uAH7zxSbPR94vAumg9I/mIv/PL+qCIKBcLi/VBMNgbBLvU6lmQOwx6/U6PM+jLdrvkUQigXg8TiPlcG4+lUrRyd6iKNLombRskxFYj00lpNNpmsrwDwg2TTMgqqSJxg/xzzZNkzbHZLNZ2mIf5X1C3ktClA8Mg/Ee+FBiDTyI2DIt4ZsOyXMTh7aoFMru7m7kgNZ0Oj0310wG/gqCgEQiQY2udnZ2sLe3RyeyEOEnhlH+Fviw/SkZ8EAGHZDJNf5p5ctYlb63xhdN02jufnt7e+Y4Osb758OJdRjXddFqtahR/bo2pXRdpxt329vbL3KbzvP8wihz2c1V0nxjGAZqtRoVRX8H5tXVFb5+/Uo3Il3XxdnZGRVl0szC8/yUEyBJR/ld+ZrNJk5PT+G6Lp2bmEqlAt7fOzs7dE5iOp1+d1H19fU13UBtNBpIpVIbscfCWD0fXqwbjQbdiOv1ejg4OFg4B++xELEheePxeBwY7PuW0XUdtVoNlmVha2sL6XR6ZvQa9krRNC0QPXe7XXz9+hXAQ6djp9OhG58kYvSLPxlsQKbcAA9Rfa/XowtCOp2mAwL81S+bwO3tLbrdLiRJQqVSmbr78Txv4d4D4+Pw9tVizYSndV9dXeHXX38NDEt9LrZtB75kjuNszJfu9vaWCmW324XjOIFNWf+/s9lsYA8gvHkb/n8nJyc4OjrCp0+faNolHDXKsjxV6RF+TOYqLgu5m6rX65HDC16CwWCATqdDN2GjZlj6FzEA1DaA8TH58JF1IpGY+sIahkErFVYxjUOSpMBmXzwe35gqlLAwkmEGwINIHh0d0eg4bASlqipKpRJarVZgKj1B13Xc3d1RH2tVVbG/v0/9snO5HOLxOPVjISPVZk0YX5bb21u6CdrtdnF6ejp3ws06WHb4RKlUQjqdhuu6UFX1zfcEMNbHZijGGimXy/A8L1CTTFjG6nMZSBUKsW2NKiF8q2xvb1NfFeLNQYTFcRz0er25C1qxWIzsEnUcB7VajYrW5eUlvn79GtnpmEwm8fnzZ2iahlgs9uyFzu/GR2ZSvrRYp1KpwCbrvE5aFk0zACbW4DgOu7u7EAQhUPcLPL7GeB6CILy5nXzP8zAcDun07qgyxkwmQ8d4KYoSebu+DOFKDtu2A9Gl4zhwXTfyHEgbOhHZUqn0LJuAeDwemGD/mGajVSEIAo6Pj6HrOkRRZJuGjIV8eLEmlEolbG9vYzQaQdM06qVMWrzfSz2231K1Xq/TO4pWq4WTk5PI1ynLMo08yTWyLAuKoixcgIgP9nA4pDMtFUWBLMuB1FAymZx5jXVdD0TD9/f3zxLr/f193N/f01TLawklz/NPGjzhxzRN1Ot1OI6DfD6/8s1xxtuBibUP0kGXTCah6zp+/PFHuK4LjuNweHi40WVho9EItVoNjuNAVVVks9mAq55pmtB1feFrlGUZnz9/pmPNFqVzer0ejWKJb0i1WgXHcTg6OqLWr/NEJsqb+jkIgrCUNe0mcHV1RRe88XhMx5kx3h8fvhpkFldXV/Q23T/bcFO5ubkJlA5GvZ5lKyo4joMoikvl3efZu/I8T+1i55UxEitVMrmGeFOTNE6/359p+LRuTNPE9fU1arXao/Y4XNfF3d0dvet4zvPPe8x4P7DIegabUlq3LFFi5rouEokEHMdBoVBYS0RGBgQYhgGe51EsFtHpdNDr9SDLMnZ2dpaKlKM2KkmdMvBQeUKG5T4H13VpiiSbzS5skvLb0JIBw8tsgPorUvr9Pk5OTp60kZjJZGgqi1QdMd4nTKxnUCwWAx12m37bvL29PRVNK4qCo6OjtT6vIAg4OTmhg28nkwk9D1KO95QZmZ7nBYYij8djOvDgOTxGRF3XDfiFu64L0zSXEutwFK7r+pPOfWdnB6qq0qELm1ISyng87J2dQaFQgKqqmEwm7+JLQGYxDgYDDAYDiKIYcK9bB47jYDQaBSI+v7hFPXYcB9fX1xiNRnQuZFTkTVIxpBWbPH4ujxFRskFIJqxLkrT03UkikaCvneM46n/yFBKJxNJpKcbmstkKtGb8JkLvAVmWUSgUXqSE0HEcfP/+neZQd3Z2kM/nkUwmA/7TYcOoVqtFc7iapqHZbM5cVCqVCm5vb6nJ0SrazR8ropVKBe12G57nIZfLLb35ubOzA1mWYZomstnsk8oHbdvG+fk5vWupVqtsc/Edw8SasRaGw2Fgs6vdbiOfzyMWi9HBvbIsT4l1eLNt3oaZqqord1AkImpZFjKZzEIRFQQhMKh3WcKt5E+h3W4HZmw2m002GPgdw8T6CYxGI0wmEyQSiVdpqNgE5vmCxGKxyAjQdd2p1v+XvrVfhYgyGOuAle49km63i/Pzc9TrdXz//n1lLenvjWQyiWKxCEEQoCjKlC9IFFElbP1+nxpJMYKQOxXgYTFc1Cik6zpubm7QaDTeXbXTR4BF1o+ENHEADxUJg8GAlUvNoFQqPcoIa1attW3bG2F/ats2+v0+eJ5HNptd+12BKIo4PT2FZVkQRXFurbplWTg/P6d7BZPJBNVqda3nx1gtTKwfSdjw56UNgN4z/lFjhHg8/uY2zVzXhW3bAYF0HAdnZ2c0xz4ajV4kf8xx3FKfwclkEqi1D1sDM94+TKwfSalUornVVCoVGOrKeD67u7sol8vQNA2u6yKdTr+pIQ2maeL8/ByWZUGSJBwdHUGWZUwmk8BmaK/Xw/7+PgzDwO3tLfXueK3Pi6IogSqc91Tl9FYgU45EUUSlUln5HTcT60fib3dmLAfxV1k2LcDz/NwZkK9Jq9WiOXTLstBqtbC7uwtJksBxXKCd/v7+HoPBgG6a3tzcIB6Pv8qmtCRJOD4+RqfTeZMOkJuOpmlotVoAHj4XNzc3K69UYmLNWCv1eh3tdhs8z6NSqWy0GVYUZAGSZRn5fJ5+YQGg0+lEDm94rQoiRVHoFHvGall2mMRzeDv3l4x3x3g8psNvXdfFzc3NK5/R8ykWizRHTJqMCOG7AUmSAmkPMsGd8f4Ij1x7joXvLFhkzVgbi2YnbiKSJOHTp090g9Gf2lFVFTs7OzRvube3h1gsRs2yUqnUu/FFZwTheR5HR0drHSbBxJqxNhKJBJLJZGDCy3uA47iZpYT5fB75fD7ws+fk30ejEfr9PmKxGHK5HPP/eMOsYpjEPJhYM9YGGdpA7FFZmePj0HUdFxcXdNPSsqy1m28x3i5MrBlr5bVmHL4HRqNRoLqEuPsxPiZsg5HBeKOENyNZp+zHhkXWDMYbRVVVVCoV9Pt9yLK8lgoDxubAxJrBeMOk0+k32yDEeFlYGoTBYDA2ABZZrxlN02iLb6lU2vjxYAwG43VgyrFGTNPE5eUl3dE3TXPtA2oZ75tWq4X7+3vwPI/9/f13177PmA1Lg6wRwzCmSq/q9XrgZwzGshiGgUajQS1ar66uXvuUGC8IE+s1Eo/Hp9qL2+12YIABg7EsUWZBbOH/ODCxXiOiKOL4+HgqT23b9iudEWOTURQlkPYoFous/fwDwXLWayYWi6FcLuP6+hrAg4CzUizGUyDt++PxGDzPsyaZDwYT6xcgm80iFovBNE0kEglWEcJ4MhzHrdUsiPF2YarxQsTjcRYJvRC6rmMymSCRSKzEPKrT6aDT6UCSJDoVhsF4aZhYM94V/X6fVknwPI+Tk5NneQuPx2Pc3t4CeBg6e319zcovGa8C22BkvCu63S79t+u6z668IfMWCf6huAzGS8LEmvGuCKconpuyCO8xZDKZZx2PwXgqLA3CeFeUy2U4joPJZIJUKoVsNvus44miiJOTEwwGA0iSxCp5GK8GE2vGu0IQBFQqlZUeU5KkqVFdDMZLw9IgDAaDsQEwsWYwGIwNgIk1Y2OxbRu6rk95ZjAY7xGWs2ZsJJqmoVarwXVdxGIxHB8fT5lmMRjvCRZZMzaSZrNJI2rDMNDr9V73hBiMNcPEmrGRhN3mmPsc473DxJqxkZTLZdrwsop6agbjrcNy1oyNRFEUfPnyBa7rgudZzMF4/7BPOWOjYULN+CiwTzqDwWBsAEysGQwGYwNgYs1gMBgbABNrBoPB2ACYWDMYDMYGwMSawWAwNgDO87zVH5TjmgAuV35gBoPBeN8cep5XjPrFWsSawWAwGKuFpUEYDAZjA2BizWAwGBsAE2sGg8HYAJhYM94VHMc5HMf9K47j/objuH/GcZz6p5+XOY77XziO+85x3B84jvvfOY77/Kff/R8cx/U4jvvnr3v2DMZsmFgz3hu653l/1/O8PwNgAvgn3IPZ9V8C+L88zzvxPO/vAfhzAKU//Z//DsB/+jqny2AsBxNrxnvmrwCcAvgHACzP8/578gvP8/5fz/P+6k///j8BDF/nFBmM5WBizXiXcBwnAviHAP4awJ8B+MPrnhGD8TyYWDPeG3GO4/4VgN8DqAH4i9c9HQZjNbBJMYz3hu553t/1/4DjuD8C+I9f53QYjNXAImvGR+BfAIhxHPdfkh9wHPd3OI77d17xnBiMR8HEmvHu8R48Ff4RgH//T6V7fwTw3wJoAADHcX8F4J8B+Pc4jrvmOO4/eL2zZTCiYd4gDAaDsQGwyJrBYDA2ACbWDAaDsQEwsWYwGIwNgIk1g8FgbABMrBkMBmMDYGLNYDAYGwATawaDwdgA/n/IDbI4RCxhHAAAAABJRU5ErkJggg==\n"",
+      ""text/plain"": [
+       ""<Figure size 432x288 with 1 Axes>""
+      ]
+     },
+     ""metadata"": {},
+     ""output_type"": ""display_data""
+    }
+   ],
+   ""source"": [
+    ""sc.pl.pca(adata)""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 13,
+   ""id"": ""8e6f9162"",
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>Xkr4</th>\n"",
+       ""      <th>Gm1992</th>\n"",
+       ""      <th>Gm19938</th>\n"",
+       ""      <th>Gm37381</th>\n"",
+       ""      <th>Rp1</th>\n"",
+       ""      <th>Sox17</th>\n"",
+       ""      <th>Gm37587</th>\n"",
+       ""      <th>Gm37323</th>\n"",
+       ""      <th>Mrpl15</th>\n"",
+       ""      <th>Lypla1</th>\n"",
+       ""      <th>...</th>\n"",
+       ""      <th>AC163611.1</th>\n"",
+       ""      <th>AC163611.2</th>\n"",
+       ""      <th>AC140365.1</th>\n"",
+       ""      <th>AC124606.2</th>\n"",
+       ""      <th>AC124606.1</th>\n"",
+       ""      <th>AC133095.2</th>\n"",
+       ""      <th>AC133095.1</th>\n"",
+       ""      <th>AC234645.1</th>\n"",
+       ""      <th>AC149090.1</th>\n"",
+       ""      <th>TDTomato</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>AAACCCAGTTCCCAAA-1</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>AAACCCATCGATCCAA-1</th>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>6.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>AAACGAAGTCAGCGTC-1</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>3.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>9.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>AAACGAATCATTTCCA-1</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>AAACGAATCTATGTGG-1</th>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>...</th>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>...</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>TTTGATCTCCGGCTTT-1</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>TTTGGAGAGGAGAGTA-1</th>\n"",
+       ""      <td>2.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>TTTGGTTGTGCCCGTA-1</th>\n"",
+       ""      <td>3.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>3.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>TTTGGTTTCTTCTGGC-1</th>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>TTTGTTGGTACCCACG-1</th>\n"",
+       ""      <td>1.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""      <td>4.0</td>\n"",
+       ""      <td>0.0</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>1624 rows × 32286 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""                    Xkr4  Gm1992  Gm19938  Gm37381  Rp1  Sox17  Gm37587  \\\n"",
+       ""AAACCCAGTTCCCAAA-1   0.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""AAACCCATCGATCCAA-1   1.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""AAACGAAGTCAGCGTC-1   0.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""AAACGAATCATTTCCA-1   0.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""AAACGAATCTATGTGG-1   1.0     0.0      1.0      0.0  0.0    0.0      0.0   \n"",
+       ""...                  ...     ...      ...      ...  ...    ...      ...   \n"",
+       ""TTTGATCTCCGGCTTT-1   0.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""TTTGGAGAGGAGAGTA-1   2.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""TTTGGTTGTGCCCGTA-1   3.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""TTTGGTTTCTTCTGGC-1   0.0     0.0      1.0      0.0  0.0    0.0      0.0   \n"",
+       ""TTTGTTGGTACCCACG-1   1.0     0.0      0.0      0.0  0.0    0.0      0.0   \n"",
+       ""\n"",
+       ""                    Gm37323  Mrpl15  Lypla1  ...  AC163611.1  AC163611.2  \\\n"",
+       ""AAACCCAGTTCCCAAA-1      0.0     0.0     0.0  ...         0.0         0.0   \n"",
+       ""AAACCCATCGATCCAA-1      0.0     1.0     0.0  ...         0.0         0.0   \n"",
+       ""AAACGAAGTCAGCGTC-1      0.0     3.0     0.0  ...         0.0         0.0   \n"",
+       ""AAACGAATCATTTCCA-1      0.0     0.0     0.0  ...         0.0         0.0   \n"",
+       ""AAACGAATCTATGTGG-1      0.0     1.0     0.0  ...         0.0         0.0   \n"",
+       ""...                     ...     ...     ...  ...         ...         ...   \n"",
+       ""TTTGATCTCCGGCTTT-1      0.0     0.0     0.0  ...         0.0         0.0   \n"",
+       ""TTTGGAGAGGAGAGTA-1      0.0     0.0     0.0  ...         0.0         0.0   \n"",
+       ""TTTGGTTGTGCCCGTA-1      0.0     1.0     0.0  ...         0.0         0.0   \n"",
+       ""TTTGGTTTCTTCTGGC-1      0.0     0.0     0.0  ...         0.0         0.0   \n"",
+       ""TTTGTTGGTACCCACG-1      0.0     0.0     0.0  ...         0.0         0.0   \n"",
+       ""\n"",
+       ""                    AC140365.1  AC124606.2  AC124606.1  AC133095.2  \\\n"",
+       ""AAACCCAGTTCCCAAA-1         0.0         0.0         0.0         0.0   \n"",
+       ""AAACCCATCGATCCAA-1         0.0         0.0         0.0         0.0   \n"",
+       ""AAACGAAGTCAGCGTC-1         0.0         0.0         0.0         0.0   \n"",
+       ""AAACGAATCATTTCCA-1         0.0         0.0         0.0         0.0   \n"",
+       ""AAACGAATCTATGTGG-1         0.0         0.0         0.0         0.0   \n"",
+       ""...                        ...         ...         ...         ...   \n"",
+       ""TTTGATCTCCGGCTTT-1         0.0         0.0         0.0         0.0   \n"",
+       ""TTTGGAGAGGAGAGTA-1         0.0         0.0         0.0         0.0   \n"",
+       ""TTTGGTTGTGCCCGTA-1         0.0         0.0         0.0         0.0   \n"",
+       ""TTTGGTTTCTTCTGGC-1         0.0         0.0         0.0         0.0   \n"",
+       ""TTTGTTGGTACCCACG-1         0.0         0.0         0.0         0.0   \n"",
+       ""\n"",
+       ""                    AC133095.1  AC234645.1  AC149090.1  TDTomato  \n"",
+       ""AAACCCAGTTCCCAAA-1         0.0         0.0         0.0       0.0  \n"",
+       ""AAACCCATCGATCCAA-1         0.0         0.0         6.0       0.0  \n"",
+       ""AAACGAAGTCAGCGTC-1         0.0         0.0         9.0       0.0  \n"",
+       ""AAACGAATCATTTCCA-1         0.0         0.0         1.0       0.0  \n"",
+       ""AAACGAATCTATGTGG-1         0.0         0.0         1.0       0.0  \n"",
+       ""...                        ...         ...         ...       ...  \n"",
+       ""TTTGATCTCCGGCTTT-1         0.0         0.0         0.0       0.0  \n"",
+       ""TTTGGAGAGGAGAGTA-1         0.0         0.0         0.0       0.0  \n"",
+       ""TTTGGTTGTGCCCGTA-1         0.0         0.0         3.0       0.0  \n"",
+       ""TTTGGTTTCTTCTGGC-1         0.0         0.0         0.0       0.0  \n"",
+       ""TTTGTTGGTACCCACG-1         0.0         0.0         4.0       0.0  \n"",
+       ""\n"",
+       ""[1624 rows x 32286 columns]""
+      ]
+     },
+     ""execution_count"": 13,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""adata.to_df()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 19,
+   ""id"": ""f796cd2d"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""arr2 = sc.read_text(\""exprMatrix.tsv.gz\"")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 20,
+   ""id"": ""9e10d32b"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""AnnData object with n_obs × n_vars = 56864 × 4261""
+      ]
+     },
+     ""execution_count"": 20,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""arr2""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 10,
+   ""id"": ""1d0932e2"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""arr2 = arr2.transpose()""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 11,
+   ""id"": ""b00ac6d4"",
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/plain"": [
+       ""(4261, 56864)""
+      ]
+     },
+     ""execution_count"": 11,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""arr2.X.shape""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""edb9e364"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""58335830"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""id"": ""c594f6a2"",
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science]"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 5
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+{
+ ""cells"": [
+  {
+   ""cell_type"": ""markdown"",
+   ""metadata"": {},
+   ""source"": [
+    ""# Transposed Data Exploration 01\n"",
+    ""\n"",
+    ""In this notebook, we'll look at the data now that it is transposed and the column space is gene expressions as we expect.\n""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""metadata"": {
+    ""scrolled"": true
+   },
+   ""outputs"": [],
+   ""source"": [
+    ""import pandas as pd\n"",
+    ""import numpy as np\n"",
+    ""from bigcsv import BigCSV""
+   ]
+  },
+  {
+   ""cell_type"": ""markdown"",
+   ""metadata"": {},
+   ""source"": [
+    ""Since we're using Dask, let's register the progress bar ""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 3,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>gene</th>\n"",
+       ""      <th>LS-15005h_S01_E1-50</th>\n"",
+       ""      <th>LS-15005h_S02_E1-50</th>\n"",
+       ""      <th>LS-15005h_S03_E1-50</th>\n"",
+       ""      <th>LS-15005h_S04_E1-50</th>\n"",
+       ""      <th>LS-15005h_S05_E1-50</th>\n"",
+       ""      <th>LS-15005h_S06_E1-50</th>\n"",
+       ""      <th>LS-15005h_S07_E1-50</th>\n"",
+       ""      <th>LS-15005h_S10_E1-50</th>\n"",
+       ""      <th>LS-15005h_S11_E1-50</th>\n"",
+       ""      <th>...</th>\n"",
+       ""      <th>SM-GE4QU_S182_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S183_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S184_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S185_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S186_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S187_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S189_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S190_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S191_E1-50</th>\n"",
+       ""      <th>SM-GE4QU_S192_E1-50</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>3.8-1.2</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>3.8-1.3</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>2 rows × 49495 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""      gene  LS-15005h_S01_E1-50  LS-15005h_S02_E1-50  LS-15005h_S03_E1-50  \\\n"",
+       ""0  3.8-1.2                    0                    0                    0   \n"",
+       ""1  3.8-1.3                    0                    0                    0   \n"",
+       ""\n"",
+       ""   LS-15005h_S04_E1-50  LS-15005h_S05_E1-50  LS-15005h_S06_E1-50  \\\n"",
+       ""0                    0                    0                    0   \n"",
+       ""1                    0                    0                    0   \n"",
+       ""\n"",
+       ""   LS-15005h_S07_E1-50  LS-15005h_S10_E1-50  LS-15005h_S11_E1-50  ...  \\\n"",
+       ""0                    0                    0                    0  ...   \n"",
+       ""1                    0                    0                    0  ...   \n"",
+       ""\n"",
+       ""   SM-GE4QU_S182_E1-50  SM-GE4QU_S183_E1-50  SM-GE4QU_S184_E1-50  \\\n"",
+       ""0                    0                    0                    0   \n"",
+       ""1                    0                    0                    0   \n"",
+       ""\n"",
+       ""   SM-GE4QU_S185_E1-50  SM-GE4QU_S186_E1-50  SM-GE4QU_S187_E1-50  \\\n"",
+       ""0                    0                    0                    0   \n"",
+       ""1                    0                    0                    0   \n"",
+       ""\n"",
+       ""   SM-GE4QU_S189_E1-50  SM-GE4QU_S190_E1-50  SM-GE4QU_S191_E1-50  \\\n"",
+       ""0                    0                    0                    0   \n"",
+       ""1                    0                    0                    0   \n"",
+       ""\n"",
+       ""   SM-GE4QU_S192_E1-50  \n"",
+       ""0                    0  \n"",
+       ""1                    0  \n"",
+       ""\n"",
+       ""[2 rows x 49495 columns]""
+      ]
+     },
+     ""execution_count"": 3,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df = pd.read_csv('../data/raw/allen_cortex.tsv.gz', sep='\\t', nrows=2)\n"",
+    ""df""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 5,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""data"": {
+      ""text/html"": [
+       ""<div>\n"",
+       ""<style scoped>\n"",
+       ""    .dataframe tbody tr th:only-of-type {\n"",
+       ""        vertical-align: middle;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe tbody tr th {\n"",
+       ""        vertical-align: top;\n"",
+       ""    }\n"",
+       ""\n"",
+       ""    .dataframe thead th {\n"",
+       ""        text-align: right;\n"",
+       ""    }\n"",
+       ""</style>\n"",
+       ""<table border=\""1\"" class=\""dataframe\"">\n"",
+       ""  <thead>\n"",
+       ""    <tr style=\""text-align: right;\"">\n"",
+       ""      <th></th>\n"",
+       ""      <th>gene</th>\n"",
+       ""      <th>AAACCCAAGGATTTCC-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCAAGTATGGCG-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCACAAAGTGTA-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCACACTACTTT-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCACAGTGAGCA-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCAGTCACCCTT-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCAGTGTCCACG-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCATCATAAGGA-LKTX_190129_01_A01</th>\n"",
+       ""      <th>AAACCCATCTGTCCCA-LKTX_190129_01_A01</th>\n"",
+       ""      <th>...</th>\n"",
+       ""      <th>TTTGGTTGTAGTTCCA-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGGTTGTTCGGTCG-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGGTTTCCTTATGT-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGGTTTCGGTATGT-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGGTTTCGGTCGGT-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGTTGAGATGGCGT-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGTTGCACAGCCAC-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGTTGCAGAGACTG-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGTTGCATAATGAG-LKTX_190130_01_H01</th>\n"",
+       ""      <th>TTTGTTGTCTACTCAT-LKTX_190130_01_H01</th>\n"",
+       ""    </tr>\n"",
+       ""  </thead>\n"",
+       ""  <tbody>\n"",
+       ""    <tr>\n"",
+       ""      <th>0</th>\n"",
+       ""      <td>DDX11L1|DDX11L1</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""    </tr>\n"",
+       ""    <tr>\n"",
+       ""      <th>1</th>\n"",
+       ""      <td>WASH7P|WASH7P</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>...</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""      <td>0</td>\n"",
+       ""    </tr>\n"",
+       ""  </tbody>\n"",
+       ""</table>\n"",
+       ""<p>2 rows × 76534 columns</p>\n"",
+       ""</div>""
+      ],
+      ""text/plain"": [
+       ""              gene  AAACCCAAGGATTTCC-LKTX_190129_01_A01  \\\n"",
+       ""0  DDX11L1|DDX11L1                                    0   \n"",
+       ""1    WASH7P|WASH7P                                    0   \n"",
+       ""\n"",
+       ""   AAACCCAAGTATGGCG-LKTX_190129_01_A01  AAACCCACAAAGTGTA-LKTX_190129_01_A01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   AAACCCACACTACTTT-LKTX_190129_01_A01  AAACCCACAGTGAGCA-LKTX_190129_01_A01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   AAACCCAGTCACCCTT-LKTX_190129_01_A01  AAACCCAGTGTCCACG-LKTX_190129_01_A01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   AAACCCATCATAAGGA-LKTX_190129_01_A01  AAACCCATCTGTCCCA-LKTX_190129_01_A01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   ...  TTTGGTTGTAGTTCCA-LKTX_190130_01_H01  \\\n"",
+       ""0  ...                                    0   \n"",
+       ""1  ...                                    0   \n"",
+       ""\n"",
+       ""   TTTGGTTGTTCGGTCG-LKTX_190130_01_H01  TTTGGTTTCCTTATGT-LKTX_190130_01_H01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   TTTGGTTTCGGTATGT-LKTX_190130_01_H01  TTTGGTTTCGGTCGGT-LKTX_190130_01_H01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   TTTGTTGAGATGGCGT-LKTX_190130_01_H01  TTTGTTGCACAGCCAC-LKTX_190130_01_H01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   TTTGTTGCAGAGACTG-LKTX_190130_01_H01  TTTGTTGCATAATGAG-LKTX_190130_01_H01  \\\n"",
+       ""0                                    0                                    0   \n"",
+       ""1                                    0                                    0   \n"",
+       ""\n"",
+       ""   TTTGTTGTCTACTCAT-LKTX_190130_01_H01  \n"",
+       ""0                                    0  \n"",
+       ""1                                    0  \n"",
+       ""\n"",
+       ""[2 rows x 76534 columns]""
+      ]
+     },
+     ""execution_count"": 5,
+     ""metadata"": {},
+     ""output_type"": ""execute_result""
+    }
+   ],
+   ""source"": [
+    ""df = pd.read_csv('../data/raw/allen_m1_region.tsv.gz', sep='\\t', nrows=2)\n"",
+    ""df""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""df = pd.read_csv('../data/raw/primary_bhaduri.tsv')""
+   ]
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""Python [conda env:base-data-science] *"",
+   ""language"": ""python"",
+   ""name"": ""conda-env-base-data-science-py""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.9.7""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 4
+}"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+import linecache
+from typing import *
+from functools import cached_property, partial
+from itertools import chain 
+import inspect
+import warnings 
+import pathlib
+
+import pandas as pd 
+import torch
+import numpy as np
+import scanpy as sc 
+import pytorch_lightning as pl 
+
+from scipy.sparse import issparse 
+from torch.utils.data import Dataset, DataLoader, ConcatDataset
+from sklearn.model_selection import train_test_split
+from sklearn.utils.class_weight import compute_class_weight
+
+class GeneExpressionData(Dataset):
+    """"""
+    Defines a PyTorch Dataset for a CSV too large to fit in memory. 
+    """"""
+    def __init__(
+        self, 
+        filename: str,
+        labelname: str,
+        class_label: str,
+        indices: Iterable[int]=None,
+        skip=3,
+        cast=True,
+        sep=',',
+        index_col=None,
+        columns: List[any]=None,
+        **kwargs, # To handle extraneous inputs
+    ):
+        """"""
+        Initialization method for GeneExpressionData
+
+        :param filename: Path to csv data file, where rows are samples and columns are features
+        :type filename: str
+        :param labelname: Path to label file, where column '# labels' defines classification labels
+        :type labelname: str
+        :param class_label: Label to train on, must be in labelname file
+        :type class_label: str
+        :param indices: List of indices to use in dataset. If None, all indices given in labelname are used., defaults to None
+        :type indices: Iterable[int], optional
+        :param skip: number of lines to skip in datafile csv, since header lines are often unneeded, defaults to 3
+        :type skip: int, optional
+        :param cast: cast sample to float32, defaults to True
+        :type cast: bool, optional
+        :param index_col: column in labelfile that contains numerical indices of datafiles, defaults to 'cell'
+        :type index_col: str, optional
+        :param sep: separator for labelfile and datafile, defaults to ','
+        :type sep: str, optional
+        """"""
+
+        self.filename = filename
+        self.labelname = labelname # alias 
+        self.class_label = class_label
+        self.index_col = index_col 
+        self.skip = skip
+        self.cast = cast
+        self.indices = indices
+        self.sep = sep
+        self._cols = columns
+
+        if indices is None:
+            self._labeldf = pd.read_csv(labelname, sep=self.sep).reset_index(drop=True)
+        else:
+            self._labeldf = pd.read_csv(labelname, sep=self.sep).loc[indices, :].reset_index(drop=True)
+
+    def __getitem__(self, idx: int):
+        """"""Get sample at index
+
+        :param idx: Numerical index between 0, len(self) -1 
+        :type idx: int
+        :raises ValueError: Errors in the case of unbounded slicing, which is normally supported in this method 
+        :return: Returns a data, label sample
+        :rtype: Tuple[torch.Tensor, Any]
+        """"""        
+        # Handle slicing 
+        if isinstance(idx, slice):
+            if idx.start is None or idx.stop is None:
+                raise ValueError(
+                    f""Error: Unlike other iterables, {self.__class__.__name__} does not support unbounded slicing since samples are being read as needed from disk, which may result in memory errors.""
+                )
+
+            step = (1 if idx.step is None else idx.step)
+            idxs = range(idx.start, idx.stop, step)
+            return [self[i] for i in idxs]
+
+        # The actual line in the datafile to get, corresponding to the number in the self.index_col values, if we need to
+        data_index = (self._labeldf.loc[idx, self.index_col] if self.index_col is not None else idx)
+
+        # get gene expression for current cell from csv file
+        # We skip some lines because we're reading directly from 
+        line = linecache.getline(self.filename, data_index + self.skip)
+        
+        if self.cast:
+            data = torch.from_numpy(np.array(line.split(self.sep), dtype=np.float32)).float()
+        else:
+            data = np.array(line.split(self.sep))
+
+        label = self._labeldf.loc[idx, self.class_label]
+
+        return data, label
+
+    def __len__(self):
+        return len(self._labeldf) # number of total samples 
+
+    @cached_property
+    def columns(self): # Just an alias...
+        return self.features
+
+    @cached_property # Worth caching, since this is a list comprehension on up to 50k strings. Annoying. 
+    def features(self):
+        if self._cols is not None:
+            return self._cols 
+        else:
+            data = linecache.getline(self.filename, self.skip - 1)
+            data = [x.split('|')[0].upper().strip() for x in data.split(self.sep)]
+            return data
+
+    @cached_property
+    def labels(self):
+        return self._labeldf.loc[:, self.class_label].unique()
+
+    @property
+    def shape(self):
+        return (self.__len__(), len(self.features))
+    
+    @cached_property 
+    def class_weights(self):
+        labels = self._labeldf.loc[:, self.class_label].values
+
+        return compute_class_weight(
+            class_weight='balanced',
+            classes=np.unique(labels),
+            y=labels
+        )
+
+    def __repr__(self) -> str:
+        return f""{self.__class__.__name__}(filename={self.filename}, labelname={self.labelname})""
+
+    def __str__(self) -> str:
+        return (
+            f""{self.__class__.__name__}""
+            f""(filename={self.filename}, ""
+            f""labelname={self.labelname}, ""
+            f""skip={self.skip}, ""
+            f""cast={self.cast}, ""
+            f""indices={self.indices})""
+        )
+
+class NumpyStream(Dataset):
+    def __init__(self,
+        matrix: np.ndarray,
+        labels: List[any]=None,
+        labelfile: str=None, 
+        class_label: str=None,
+        index_col=None,
+        sep=',',
+        columns: List[any]=None,
+        *args,
+        **kwargs,
+    ) -> None:
+        super().__init__()
+
+        # Make sure labels and labelfile aren't both passed 
+        if labels is not None and labelfile is not None:
+            raise ValueError(""Either a list of labels may be passed or a .csv file to read from, but not both."")
+
+        # Make sure neither labels nor labelfile isn't passed 
+        if labels is None and labelfile is None:
+            raise ValueError(""One of labels or labelfile must be passed."")
+
+        # If labelfile is passed, then we need an associated column to pull the class_label from 
+        if labelfile is not None and class_label is None:
+            raise ValueError(f""If labelfile is passed, column to corresponding class must be passed in class_label. Got {class_label = }."")
+
+        # If we're using labels but the user passes a class label, just warn 
+        if labels is not None and class_label is not None:
+            warnings.warn(f""{class_label = } but labels passed, using labels and ignoring class_label. To silence this warning remove the class_labels positional or keyword argument."")
+
+        if columns is None:
+            warnings.warn(f""{self.__class__.__name__} initialized without columns. This will error if training with multiple Datasets with posssibly columns."")
+
+        self.data = matrix
+        self.labelfile = labelfile
+        self.class_label = class_label
+        self.index_col = index_col
+        self.sep = sep
+        self.columns = columns
+
+        # We have a labelfile and some specified indices 
+        if labelfile is not None:
+            self.labels = pd.read_csv(labelfile, sep=self.sep).reset_index(drop=True).loc[:, class_label].values 
+        else:
+            self.labels = labels 
+
+    def __getitem__(self, idx):
+        if isinstance(idx, slice):
+            step = (1 if idx.step is None else idx.step)
+            idxs = range(idx.start, idx.stop, step)
+            return [self[i] for i in idxs]
+
+        data = self.data[idx]
+
+        # If matrix is sparse, then densify it for training
+        if issparse(data):
+            data = data.todense()
+
+        return (
+            torch.from_numpy(data), 
+            self.labels[idx]
+        )
+    
+    def __len__(self):
+        return len(self.data)
+
+class CollateLoader(DataLoader):
+    def __init__(
+        self,
+        dataset: Type[Dataset],
+        refgenes: List[str]=None,
+        currgenes: List[str]=None,
+        transpose: bool=False, 
+        normalize: bool=False,
+        *args,
+        **kwargs,
+    ) -> None:
+        """"""
+        Initializes a CollateLoader for efficient numerical batch-wise transformations
+
+        :param dataset: GeneExpressionDataset to create DataLoader from
+        :type dataset: Type[Dataset]
+        :param refgenes: Optional, list of columns to take intersection with , defaults to None
+        :type refgenes: List[str], optional
+        :param currgenes: Optional, list of current dataset columns, defaults to None
+        :type currgenes: List[str], optional
+        :param transpose: Boolean indicating whether to tranpose the batch data , defaults to False
+        :type transpose: bool, optional
+        :param normalize: Boolean indicating whether to normalize the batch data, defaults to False
+        :type normalize: bool, optional
+        :raises ValueError: If refgenes are passed, currgenes also have to be passed otherwise we dont know what to align with
+        """"""
+
+        if refgenes is None and currgenes is not None or refgenes is not None and currgenes is None:
+            raise ValueError(""If refgenes is passed, currgenes must be passed too. If currgenes is passed, refgenes must be passed too."")
+        
+        # Create collate_fn via a partial of the possible collators, depending on if columns intersection is being calculated
+        if refgenes is not None:
+            collate_fn = partial(
+                _collate_with_refgenes, 
+                refgenes=refgenes, 
+                currgenes=currgenes, 
+                transpose=transpose, 
+                normalize=normalize
+            )
+        else:
+            collate_fn = partial(
+                _standard_collate, 
+                normalize=normalize, 
+                transpose=transpose
+            )
+
+        # This is awkward, but Dataloader init doesn't handle optional keyword arguments
+        # So we have to take the intersection between the passed **kwargs and the DataLoader named arguments
+        allowed_args = inspect.signature(super().__init__).parameters
+        new_kwargs = {}
+
+        for key in allowed_args:
+            name = allowed_args[key].name
+            if name in kwargs:
+                new_kwargs[key] = kwargs[key]
+
+        # Finally, initialize the DataLoader
+        super().__init__(
+            dataset=dataset,
+            collate_fn=collate_fn,
+            **new_kwargs,
+        )
+
+class SequentialLoader:
+    """"""
+    Class to sequentially stream samples from an arbitrary number of DataLoaders.
+
+    :param dataloaders: List of DataLoaders or DataLoader derived class, such as the CollateLoader from above 
+    :type dataloaders: List[Union[DataLoader, SequentialLoader]]
+    """"""
+    def __init__(self, dataloaders):
+        self.dataloaders = dataloaders
+
+    def __len__(self):
+        return sum([len(dl) for dl in self.dataloaders])
+
+    def __iter__(self):
+        yield from chain(*self.dataloaders)
+
+def _collate_with_refgenes(
+    sample: List[tuple], 
+    refgenes: List[str], 
+    currgenes: List[str],
+    transpose: bool,
+    normalize: bool,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """"""
+    Collate minibatch of samples where we're intersecting the columns between refgenes and currgenes,
+
+    :param sample: List of samples from GeneExpressionData object
+    :type sample: List[tuple]
+    :param refgenes: List of reference genes
+    :type refgenes: List[str]
+    :param currgenes: List of current columns from sample 
+    :type currgenes: List[str]
+    :param transpose: boolean, indicates if we should transpose the minibatch (in the case of incorrectly formatted .csv data)
+    :type transpose: bool
+    :param normalize: boolean, indicates if we should normalize the minibatch
+    :type normalize: bool
+    :return: Two torch.Tensors containing the data and labels, respectively
+    :rtype: Tuple[torch.Tensor, torch.Tensor]
+    """"""
+
+    data = clean_sample(torch.stack([x[0] for x in sample]), refgenes, currgenes)
+    labels = torch.tensor([x[1] for x in sample])
+
+    return _transform_sample(data, normalize, transpose), labels 
+
+def _standard_collate(
+    sample: List[tuple],
+    normalize: bool,
+    transpose: bool,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """"""
+    Collate minibatch of samples, optionally normalizing and transposing. 
+
+    :param sample: List of GeneExpressionData items to collate
+    :type sample: List[tuple]
+    :param normalize: boolean, indicates if we should transpose the minibatch (in the case of incorrectly formatted .csv data)
+    :type normalize: bool
+    :param transpose: boolean, indicates if we should normalize the minibatch
+    :type transpose: bool
+    :return: Collated samples and labels, respectively
+    :rtype: Tuple[torch.Tensor, torch.Tensor]
+    """"""    
+    data = torch.stack([x[0] for x in sample])
+    labels = torch.tensor([x[1] for x in sample])
+
+    return _transform_sample(data, normalize, transpose), labels 
+
+def _transform_sample(
+    data: torch.Tensor, 
+    normalize: bool, 
+    transpose: bool
+) -> torch.Tensor:
+    """"""
+    Optionally normalize and tranpose a torch.Tensor
+
+    :param data: Input sample
+    :type data: torch.Tensor
+    :param normalize: To normalize sample or not 
+    :type normalize: bool
+    :param transpose: to transpose sample or not 
+    :type transpose: bool
+    :return: Modified sample 
+    :rtype: torch.Tensor
+    """"""    
+    if transpose:
+        data = data.T
+
+    if normalize:
+        data = torch.nn.functional.normalize(data)
+
+    return data 
+
+def clean_sample(
+    sample: torch.Tensor, 
+    refgenes: List[str], 
+    currgenes: List[str],
+) -> torch.Tensor:
+    # currgenes and refgenes are already sorted
+    # Passed froem calculate_intersection
+    """"""
+    Remove uneeded gene columns for given sample.
+
+    :param sample: n samples to clean
+    :type sample: torch.Tensor
+    :param refgenes: list of reference genes from helper.generate_intersection(), contains the genes we want to keep
+    :type refgenes: List[str]
+    :param currgenes: list of reference genes for the current sample
+    :type currgenes: List[str]
+    :return: Sample reordered and intersected with the list of refgenes
+    :rtype: torch.Tensor
+    """"""
+    intersection = np.intersect1d(currgenes, refgenes, return_indices=True)
+    indices = intersection[1] # List of indices in currgenes that equal refgenes 
+    
+    axis = (1 if sample.ndim == 2 else 0)
+    sample = np.sort(sample, axis=axis)
+    sample = np.take(sample, indices, axis=axis)
+
+    return torch.from_numpy(sample)
+
+def generate_datasets(
+    datafiles: List[str],
+    labelfiles: List[str],
+    class_label: str,
+    test_prop: float=0.2,
+    combine=False,
+    *args,
+    **kwargs,
+) -> Tuple[Dataset, Dataset]:
+    """"""
+    Generates the COMBINED train/val/test datasets with stratified label splitting. 
+    This means that the proportion of each label is the same in the training, validation and test set. 
+
+    :param datafiles: List of absolute paths to csv files under data_path/ that define cell x expression matrices
+    :type datafiles: List[str]
+    :param labelfiles: ist of absolute paths to csv files under data_path/ that define cell x class matrices
+    :type labelfiles: List[str]
+    :param class_label: Column in label files to train on. Must exist in all datasets, this should throw a natural error if it does not. 
+    :type class_label: str
+    :param test_prop: Proportion of data to use as test set , defaults to 0.2
+    :type test_prop: float, optional
+    :param combine: Whether to join Datasets into ConcatDataset, defaults to False
+    :type combine: bool, optional
+    :raises ValueError: Errors if user requests to combine datasets but there is only one. This is probability misinformed and should raise an error.
+    :return: Training, validation and test datasets, respectively
+    :rtype: Tuple[Dataset, Dataset]
+    """"""    
+    if combine and len(datafiles) == 1:
+        raise ValueError('Cannot combine datasets when number of datafiles == 1.')
+
+    if len(datafiles) == 1:
+        return generate_single_dataset(
+            datafiles[0],
+            labelfiles[0],
+            class_label,
+            test_prop,
+            *args,
+            **kwargs,
+        )
+
+    train_datasets, val_datasets, test_datasets = [], [], []
+
+    for datafile, labelfile in zip(datafiles, labelfiles):
+        train, val, test = generate_single_dataset(
+            datafile,
+            labelfile,
+            class_label,
+            test_prop,
+            *args,
+            **kwargs,
+        )
+
+        train_datasets.append(train)
+        val_datasets.append(val)
+        test_datasets.append(test)
+
+    # Flexibility to generate single stratified dataset from a single file 
+    # Just in generate_single_dataset
+    if combine:
+        train_datasets = ConcatDataset(train_datasets)
+        val_datasets = ConcatDataset(val_datasets)
+        test_datasets = ConcatDataset(test_datasets)
+
+    return train_datasets, val_datasets, test_datasets
+
+def generate_single_dataset(
+    datafile: str,
+    labelfile: str,
+    class_label: str,
+    test_prop=0.2,
+    sep=',',
+    *args,
+    **kwargs,
+) -> Tuple[Dataset, Dataset, Dataset]:
+    """"""
+    Generate a train/test split for the given datafile and labelfile.
+
+    :param datafile: Path to dataset csv file
+    :type datafile: str
+    :param labelfile: Path to label csv file 
+    :type labelfile: str
+    :param class_label: Column (label) in labelfile to train on 
+    :type class_label: str
+    :param test_prop: Proportion of dataset to use in val/test, defaults to 0.2
+    :type test_prop: float, optional
+    :return: train, val, test Datasets
+    :rtype: Tuple[Dataset, Dataset, Dataset]
+    """"""    
+    current_labels = pd.read_csv(labelfile, sep=sep).loc[:, class_label]
+    suffix = pathlib.Path(datafile).suffix
+
+    # Make stratified split on labels
+    trainsplit, valsplit = train_test_split(current_labels, stratify=current_labels, test_size=test_prop)
+    trainsplit, testsplit = train_test_split(trainsplit, stratify=trainsplit, test_size=test_prop)
+
+    if suffix == '.h5ad':
+        data = sc.read_h5ad(datafile)
+
+        train, val, test = (
+            NumpyStream(
+                matrix=data.X[split.index],
+                labels=split.values,
+                class_label=class_label,
+                *args,
+                **kwargs,
+            )
+            for split in [trainsplit, valsplit, testsplit]
+        )
+        
+    else:
+        if suffix != '.csv' or suffix != '.tsv':
+            warnings.warn(f'Extension {suffix} not recognized, \
+                interpreting as .csv. To silence this warning, pass in explicit file types.')
+
+        train, val, test = (
+            GeneExpressionData(
+                filename=datafile,
+                labelname=labelfile,
+                class_label=class_label,
+                indices=indices,
+                *args,
+                **kwargs,
+            )
+            for indices in [trainsplit, valsplit, testsplit]  
+        )
+
+    return train, val, test 
+
+def generate_single_dataloader(
+    *args,
+    **kwargs,
+) -> Tuple[CollateLoader, CollateLoader, CollateLoader]:
+    """"""
+    Generates a train, val, test CollateLoader
+
+    :return: train, val, test loaders
+    :rtype: Tuple[CollateLoader, CollateLoader, CollateLoader]
+    """"""
+
+    train, val, test = generate_single_dataset(
+        *args,
+        **kwargs,
+    )
+
+    loaders = (
+        CollateLoader(
+                dataset=dataset, 
+                currgenes=(dataset.columns if 'refgenes' in kwargs.keys() else None),
+                *args,
+                **kwargs,
+            )
+        for dataset in [train, val, test]
+    )
+
+    return loaders 
+
+def generate_dataloaders(
+    datafiles: List[str], 
+    labelfiles: List[str],
+    collocate: bool=True, 
+    *args,
+    **kwargs,
+) -> Union[Tuple[List[CollateLoader], List[CollateLoader], List[CollateLoader]], Tuple[SequentialLoader, SequentialLoader, SequentialLoader]]:
+    """"""
+    Generates DataLoaders for training, either as a combined list from each datafile or a SequentialLoader to allow sequentially sampling between DataLoaders or DataLoader derived classes.
+
+    :param datafiles: List of absolute paths to datafiles
+    :type datafiles: List[str]
+    :param labelfiles: List of absolute paths to labelfiles
+    :type labelfiles: List[str]
+    :param collocate: Whether to combine DataLoaders into one SequentialLoader to allow sequential sampling from all continuously, defaults to True
+    :type collocate: bool, optional
+    :raises ValueError: Errors if num(labelfiles) != num(datafiles)
+    :raises ValueError: Errors if user requests to collocate but only one loader is passed -- probably misinformed 
+    :return: Either lists containing train, val, test or SequentialLoader's for train, val, test 
+    :rtype: Union[Tuple[List[CollateLoader], List[CollateLoader], List[CollateLoader]], Tuple[SequentialLoader, SequentialLoader, SequentialLoader]]
+    """"""
+    
+    if len(datafiles) != len(labelfiles):
+        raise ValueError(""Must have same number of datafiles and labelfiles"")
+    
+    # We dont need this error check, just handle it later.
+    if collocate and len(datafiles) == 1:
+        warnings.warn(""Cannot collocate dataloaders with only one dataset file, ignoring. Pass collocate=False to silence this warning."")
+
+    train, val, test = [], [], []
+    for datafile, labelfile in zip(datafiles, labelfiles):
+        trainloader, valloader, testloader = generate_single_dataloader(
+            datafile=datafile,
+            labelfile=labelfile,
+            *args,
+            **kwargs,
+        )
+
+        train.append(trainloader)
+        val.append(valloader)
+        test.append(testloader)
+
+    if len(datafiles) == 1:
+        train = train[0]
+        val = val[0]
+        test = test[0]
+
+    if collocate and len(datafiles) > 1: 
+        # Join these together into sequential loader if requested, shouldn't error if only one training file passed, though
+        train, val, test = SequentialLoader(train), SequentialLoader(val), SequentialLoader(test)
+
+    return train, val, test 
+
+
+
+"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";\ No newline at end of file
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+import pathlib 
+import os 
+import argparse
+import helper 
+import boto3 
+import urllib 
+
+from typing import * 
+
+here = pathlib.Path(__file__).parent.absolute()
+data_path = os.path.join(here, '..', '..', 'data')
+
+with open(os.path.join(here, '..', 'credentials')) as f:
+    key, access = [line.rstrip() for line in f.readlines()]
+
+s3 = boto3.resource(
+    's3',
+    endpoint_url=""https://s3-west.nrp-nautilus.io/"",
+    aws_access_key_id=key,
+    aws_secret_access_key=access,
+)
+
+def _download_from_key(
+    key: str, 
+    localpath: str,
+) -> None:
+    """"""Helper function that downloads all files recursively from the given key (folder) from the braingeneersdev S3 bucket
+
+    :param key: S3 folder (key) to start downloading recursively from
+    :type key: str
+    :param localpath: Optional argument, downloads to a subfolder under the data/processed/ folder # TODO add folder generation
+    :type localpath: str 
+    """"""    
+
+    print(f'Key is {key}')
+    reduced_files = helper.list_objects(key)
+
+    if not os.path.exists(localpath):
+        print(f'Download path {localpath} doesn\'t exist, creating...')
+        os.makedirs(localpath, exist_ok=True)
+
+    for f in reduced_files:
+        if not os.path.isfile(os.path.join(data_path, 'processed', f.split('/')[-1])):
+            print(f'Downloading {f} from S3')
+            helper.download(
+                f,
+                os.path.join(localpath, f.split('/')[-1]) # Just the file name in the list of objects
+            )
+
+def download_clean_from_s3(
+    file: str=None,
+    local_path: str=None,
+) -> None:
+    """"""Downloads the cleaned data from s3 to be used in model training
+
+    :param file: file name to download from braingeneersdev S3 bucket, defaults to None
+    :type file: str, optional
+    :param local_path: path to download file to, defaults to None
+    :type local_path: str, optional
+    """"""    
+    os.makedirs(os.path.join(data_path, 'processed'), exist_ok=True)
+    if not file: # No single file passed, so download recursively
+        print('Downloading all clean data...')
+        key = os.path.join('jlehrer', 'expression_data', 'processed')
+        local_path = os.path.join(data_path, 'processed')
+
+        _download_from_key(key, local_path) 
+    else:
+        print(f'Downloading {file} from clean data')
+        local_path = (os.path.join(data_path, 'processed', file) if not local_path else local_path)
+        helper.download(
+            os.path.join('jlehrer', 'expression_data', 'processed', file),
+            local_path
+        )
+
+def download_interim_from_s3(
+    file: str=None,
+    local_path: str=None,
+) -> None:
+    """"""Downloads the interim data from S3. Interim data is in the correct structural format but has not been cleaned
+
+    :param file: _description_, defaults to None
+    :type file: str, optional
+    :param local_path: _description_, defaults to None
+    :type local_path: str, optional
+    """"""
+    os.makedirs(os.path.join(data_path, 'interim'), exist_ok=True)
+
+    if not file:
+        print('Downloading all interim data')
+        key = os.path.join('jlehrer', 'expression_data', 'interim')
+        local_path = os.path.join(data_path, 'interim')
+        _download_from_key(key, local_path)
+    else:
+        print(f'Downloading {file} from interim data')
+        local_path = (os.path.join(data_path, 'interim', file) if not local_path else local_path)
+        helper.download(
+            os.path.join('jlehrer', 'expression_data', 'interim', file),
+            local_path,
+        )
+        
+def download_raw_from_s3(
+    file: str=None,
+    local_path: str=None,
+) -> None:
+    """"""Downloads the raw expression matrices from s3
+
+
+    :param file: _description_, defaults to None
+    :type file: str, optional
+    :param local_path: _description_, defaults to None
+    :type local_path: str, optional
+    """"""
+    os.makedirs(os.path.join(data_path, 'raw'), exist_ok=True)
+    if not file: 
+        print('Downloading all raw data')
+        key = os.path.join('jlehrer', 'expression_data', 'raw')
+        local_path = os.path.join(data_path, 'raw')
+        _download_from_key(key, local_path)
+    else:
+        print(f'Downloading {file} from raw data')
+        local_path = (os.path.join(data_path, 'raw', file) if not local_path else local_path)
+        helper.download(
+            os.path.join('jlehrer', 'expression_data', 'raw', file), 
+            local_path
+        )
+
+def upload(file_name, remote_name=None) -> None:
+    """"""
+    Uploads a file to the braingeneersdev S3 bucket
+    
+    Parameters:
+    file_name: Local file to upload
+    remote_name: Key for S3 bucket. Default is file_name
+    """"""
+    if remote_name == None:
+        remote_name = file_name
+
+    s3.Bucket('braingeneersdev').upload_file(
+        Filename=file_name,
+        Key=remote_name,
+)
+
+def download(remote_name, file_name=None) -> None:
+    """"""
+    Downloads a file from the braingeneersdev S3 bucket 
+
+    Parameters:
+    remote_name: S3 key to download. Must be a single file
+    file_name: File name to download to. Default is remote_name
+    """"""
+    if file_name == None:
+        file_name == remote_name
+
+    s3.Bucket('braingeneersdev').download_file(
+        Key=remote_name,
+        Filename=file_name,
+    )
+
+def download_raw_expression_matrices(
+    datasets: Dict[str, Tuple[str, str]]=None,
+    upload: bool=False,
+    unzip: bool=True,
+    datapath: str=None
+) -> None:
+    """"""Downloads all raw datasets and label sets from cells.ucsc.edu, and then unzips them locally
+
+    :param datasets: uses helper.DATA_FILES_AND_URLS_DICT if None. Dictionary of datasets such that each key maps to a tuple containing the expression matrix csv url in the first element,
+                    and the label csv url in the second url, defaults to None
+    :type datasets: Dict[str, Tuple[str, str]], optional
+    :param upload: Whether or not to also upload data to the braingeneersdev S3 bucket , defaults to False
+    :type upload: bool, optional
+    :param unzip: Whether to also unzip expression matrix, defaults to False
+    :type unzip: bool, optional
+    :param datapath: Path to folder to download data to. Otherwise, defaults to data/
+    :type datapath: str, optional
+    """"""    
+    # {local file name: [dataset url, labelset url]}
+    datasets = (datasets if datasets is not None else helper.DATA_FILES_AND_URLS_DICT)
+    data_path = (datapath if datapath is not None else os.path.join(here, '..', '..', '..', 'data'))
+
+    for file, links in datasets.items():
+        datafile_path = os.path.join(data_path, 'raw', file)
+
+        labelfile = f'{file[:-4]}_labels.tsv'
+
+        datalink, _ = links
+
+        # First, make the required folders if they do not exist 
+        for dir in 'raw':
+            os.makedirs(os.path.join(data_path, dir), exist_ok=True)
+
+        # Download and unzip data file if it doesn't exist 
+        if not os.path.isfile(datafile_path):
+            print(f'Downloading zipped data for {file}')
+            urllib.request.urlretrieve(
+                datalink,
+                f'{datafile_path}.gz',
+            )
+
+            if unzip:
+                print(f'Unzipping {file}')
+                os.system(
+                    f'zcat < {datafile_path}.gz > {datafile_path}'
+                )
+
+                print(f'Deleting compressed data')
+                os.system(
+                    f'rm -rf {datafile_path}.gz'
+                )
+
+
+        # If upload boolean is passed, also upload these files to the braingeneersdev s3 bucket
+        if upload:
+            print(f'Uploading {file} and {labelfile} to braingeneersdev S3 bucket')
+            helper.upload(
+                datafile_path,
+                os.path.join('jlehrer', 'expression_data', 'raw', file)
+            )
+
+def download_labels(
+    datasets: Dict[str, Tuple[str, str]]=None,
+    upload: bool=False,
+    datapath: str=None,
+) -> None:
+    """"""Downloads raw label files from given Dictionary
+
+    :param datasets: Dictionary containing the datafile name as the key, and a tuple of the data download url and label download url as the value, defaults to None
+    :type datasets: Dict[str, Tuple[str, str]], optional
+    :param upload: Whether to upload data to S3, defaults to False
+    :type upload: bool, optional
+    :param datapath: Path to download data, defaults to None
+    :type datapath: str, optional
+    """"""    
+    datasets = helper.DATA_FILES_AND_URLS_DICT
+    data_path = (datapath if datapath is not None else os.path.join(here, '..', '..', '..', 'data', 'raw', 'labels'))
+    
+    if not os.path.isdir(data_path):
+        os.makedirs(data_path, exist_ok=True)
+
+    for labelfile, (_, labellink) in datasets.items():
+        labelfile_path = os.path.join(data_path, f""{labelfile[:-4]}_labels.tsv"")
+
+        # Download label file if it doesn't exist 
+        if not os.path.isfile(labelfile_path):
+            print(f'Downloading label for {labelfile}')
+            urllib.request.urlretrieve(
+                labellink,
+                labelfile_path,
+            )
+        else:
+            print(f'{labelfile} exists, continuing...')
+
+        if upload:
+            helper.upload(
+                labelfile_path,
+                os.path.join('jlehrer', 'expression_data', 'raw', f'{labelfile[:-4]}_labels.tsv')
+            )
+
+if __name__ == ""__main__"":
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        '--type',
+        type=str,
+        required=False,
+        default='clean',
+        help=""Type of data to download""
+    )
+
+    parser.add_argument(
+        '--key',
+        required=False,
+        default=None,
+        type=str,
+        help='If not None, only download the specific key passed in this argument from the braingeneersdev s3 bucket'
+    )   
+
+    parser.add_argument(
+        '--local-name',
+        required=False,
+        default=None,
+        help='If not None, download the key specified from the --file flag into this local filename'
+    )
+
+    args = parser.parse_args()
+
+    type = args.type
+    key = args.key 
+    local = args.local_name 
+
+    if local is not None and not key:
+        parser.error('Error: If --local-name is passed in specified download, s3 key must be passed as well via --key')
+
+    if type == 'interim':
+        download_interim_from_s3(key, local)
+    elif type == 'raw':
+        download_raw_from_s3(key, local)
+    elif type == 'processed' or type == 'clean':
+        download_clean_from_s3(key, local)
+    else:
+        raise ValueError('Unknown type specified for data downloading.')
\ No newline at end of file"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+import sys
+import os
+import pathlib 
+from typing import *
+
+import torch
+import pandas as pd 
+import anndata as an
+from functools import cached_property
+
+import pytorch_lightning as pl
+from pytorch_lightning.loggers import WandbLogger
+from pytorch_lightning.callbacks.early_stopping import EarlyStopping
+
+from .neural import GeneClassifier
+from .train import UploadCallback
+from .data import generate_dataloaders
+
+import sys, os 
+from helper import gene_intersection, download
+
+class DataModule(pl.LightningDataModule):
+    """"""
+    Creates the DataModule for PyTorch-Lightning training.
+    """"""
+    def __init__(
+        self, 
+        datafiles,
+        labelfiles,
+        class_label,
+        *args,
+        **kwargs,
+    ):
+        super().__init__()
+
+        self.datafiles = datafiles 
+        self.labelfiles = labelfiles 
+        self.class_label = class_label
+
+        self.args = args 
+        self.kwargs = kwargs
+        
+    def setup(self, stage: Optional[str] = None):
+        print('Creating train/val/test DataLoaders...')
+        trainloader, valloader, testloader = generate_dataloaders(
+            datafiles=self.datafiles,
+            labelfiles=self.labelfiles,
+            class_label=self.class_label,
+            *self.args,
+            **self.kwargs,
+            pin_memory=True, # For gpu training
+        )
+
+        print('Done, continuing to training.')
+        self.trainloader = trainloader
+        self.valloader = valloader
+        self.testloader = testloader
+        
+    def train_dataloader(self):
+        return self.trainloader
+
+    def val_dataloader(self):
+        return self.valloader
+
+    def test_dataloader(self):
+        return self.testloader
+
+    @cached_property
+    def num_labels(self):
+        val = []
+        sep = self.kwargs['sep'] if 'sep' in self.kwargs else ','
+        for file in self.labelfiles:
+            val.append(pd.read_csv(file, sep=sep).loc[:, self.class_label].values.max())
+
+        return max(val) + 1
+
+    @cached_property
+    def num_features(self):
+        if 'refgenes' in self.kwargs:
+            return len(self.kwargs['refgenes'])
+        elif hasattr(self, 'trainloader'):
+            return next(iter(self.trainloader))[0].shape[1]
+        elif pathlib.Path(self.datafiles[0]).suffix == '.h5ad':
+            return an.read_h5ad(self.datafiles[0]).X.shape[1]
+        else:
+            return pd.read_csv(self.datafiles[0], nrows=1).shape[1]
+    
+# This has to be outside of the datamodule 
+# Since we have to download the files to calculate the gene intersection 
+def prepare_data(
+    data_path: str, 
+    datafiles: List[str], 
+    labelfiles: List[str],
+) -> None:
+    """"""
+    Prepare data for model training, by downloading the transposed and clean labels from the S3 bucket
+
+    :param data_path: Path to the top-level folder containing the data subfolders
+    :type data_path: str
+    :param datafiles: List of absolute paths to datafiles 
+    :type datafiles: List[str]
+    :param labelfiles: List of absolute paths to labelfiles
+    :type labelfiles: List[str]
+    """"""    
+    os.makedirs(os.path.join(data_path, 'interim'), exist_ok=True)
+    os.makedirs(os.path.join(data_path, 'processed', 'labels'), exist_ok=True)
+
+    for datafile, labelfile in zip(datafiles, labelfiles):
+        if not os.path.isfile(datafile):
+            print(f'Downloading {datafile}')
+            download(
+                remote_name=os.path.join('jlehrer/expression_data/interim/', datafile.split('/')[-1]),
+                file_name=datafile,
+            )
+        else:
+            print(f'{datafile} exists, continuing...')
+
+        if not os.path.isfile(labelfile):
+            print(f'Downloading {labelfile}')
+            download(
+                remote_name=os.path.join('jlehrer/expression_data/labels/', labelfile.split('/')[-1]),
+                file_name=labelfile,
+            )
+        else:
+            print(f'{labelfile} exists, continuing...\n')    
+
+def generate_trainer(
+    datafiles: List[str],
+    labelfiles: List[str],
+    class_label: str,
+    weighted_metrics: bool,
+    batch_size: int,
+    num_workers: int,
+    optim_params: Dict[str, Any],
+    wandb_name='',
+    *args,
+    **kwargs,
+):
+    """"""
+    Generates PyTorch Lightning trainer and datasets for model training.
+
+    :param datafiles: List of absolute paths to datafiles
+    :type datafiles: List[str]
+    :param labelfiles: List of absolute paths to labelfiles
+    :type labelfiles: List[str]
+    :param class_label: Class label to train on 
+    :type class_label: str
+    :param weighted_metrics: To use weighted metrics in model training 
+    :type weighted_metrics: bool
+    :param batch_size: Batch size in dataloader
+    :type batch_size: int
+    :param num_workers: Number of workers in dataloader
+    :type num_workers: int
+    :param optim_params: Dictionary defining optimizer and any needed/optional arguments for optimizer initializatiom
+    :type optim_params: Dict[str, Any]
+    :param wandb_name: Name of run in Wandb.ai, defaults to ''
+    :type wandb_name: str, optional
+    :return: Trainer, model, datamodule 
+    :rtype: Trainer, model, datamodule 
+    """"""
+
+    device = ('cuda:0' if torch.cuda.is_available() else 'cpu')
+    print(f'Device is {device}')
+    
+    here = pathlib.Path(__file__).parent.absolute()
+    data_path = os.path.join(here, '..', '..', '..', 'data')
+
+    wandb_logger = WandbLogger(
+        project=f""tabnet-classifer-sweep"",
+    )
+
+    uploadcallback = UploadCallback(
+        path=os.path.join(here, 'checkpoints'),
+        desc=f'TabNet Gene Classifier'
+    )
+
+    earlystoppingcallback = EarlyStopping(
+        monitor=""val_loss"",
+        patience=50,
+        verbose=True
+    )
+
+    prepare_data(
+        data_path=data_path,
+        datafiles=datafiles,
+        labelfiles=labelfiles,
+    )
+
+    refgenes = gene_intersection()
+    module = DataModule(
+        datafiles=datafiles, 
+        labelfiles=labelfiles, 
+        class_label=class_label, 
+        refgenes=refgenes,
+        batch_size=batch_size,
+        num_workers=num_workers,
+        *args,
+        **kwargs,
+    )
+
+    model = GeneClassifier(
+        input_dim=len(refgenes),
+        output_dim=19,
+        weighted_metrics=weighted_metrics,
+        optim_params=optim_params
+    )
+    
+    trainer = pl.Trainer(
+        gpus=(1 if torch.cuda.is_available() else 0),
+        auto_lr_find=False,
+        gradient_clip_val=0.5,
+        logger=wandb_logger,
+        callbacks=[
+            uploadcallback, 
+            earlystoppingcallback,
+        ],
+        max_epochs=kwargs['max_epochs'],
+        val_check_interval=0.25, # Calculate validation every quarter epoch instead of full since dataset is large, and would like to test this 
+        profiler=""advanced"",
+    )
+
+    return trainer, model, module
+"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";\ No newline at end of file
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+from multiprocessing.sharedctypes import Value
+from typing import *
+
+import torch
+import torch.nn.functional as F
+import pytorch_lightning as pl
+from torchmetrics.functional import accuracy, precision, recall 
+from pytorch_tabnet.tab_network import TabNet
+
+# Set all seeds for reproducibility
+class GeneClassifier(pl.LightningModule):
+    def __init__(
+        self, 
+        input_dim: int, 
+        output_dim: int,
+        base_model=None,
+        optim_params: Dict[str, float]={
+            'optimizer': torch.optim.Adam,
+            'lr': 0.001,
+            'weight_decay': 0.01,
+        },
+        metrics: Dict[str, Callable]={
+            'accuracy': accuracy,
+            'precision': precision,
+            'recall': recall,
+        },
+        weighted_metrics=False,
+        *args,
+        **kwargs,
+    ):
+        """"""
+        Initialize the gene classifier neural network
+
+        :param input_dim: Number of features in the inpute matrix 
+        :type input_dim: Number of classes
+        :param output_dim: Number of classes
+        :type output_dim: int
+        :param base_model: Model to use in training, otherwise defaults to TabNet, defaults to None
+        :type base_model: _type_, optional
+        :param optim_params: Dictionary containing information to instantiate optimizer, defaults to { 'optimizer': torch.optim.Adam, 'lr': 0.001, 'weight_decay': 0.01, }
+        :type optim_params: _type_, optional
+        :param metrics: List of pl_lightning.functional to compute, defaults to { 'accuracy': accuracy, 'precision': precision, 'recall': recall, }
+        :type metrics: _type_, optional
+        :param weighted_metrics: To use class-weighted metric calcuation, defaults to False
+        :type weighted_metrics: bool, optional
+        """"""    
+        super().__init__()
+        print(f'Model initialized. {input_dim = }, {output_dim = }. Metrics are {metrics.keys()} and {weighted_metrics = }')
+
+        self.input_dim = input_dim
+        self.output_dim = output_dim
+        self.optim_params = optim_params
+        self.metrics = metrics
+        self.weighted_metrics = weighted_metrics
+
+        if base_model is None:
+            self.base_model = TabNetGeneClassifier(
+                input_dim=input_dim,
+                output_dim=output_dim,
+                *args,
+                **kwargs
+            )
+        else:
+            self.base_model = base_model
+
+    def forward(self, x):
+        return self.base_model(x)
+
+    def _step(self,
+        batch: Tuple[torch.Tensor, torch.Tensor], 
+        batch_idx: int
+    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
+        """"""
+        Calculate the model output and loss for a given batch, to be used in training and validation steps
+
+        :param batch: Standard DataLoader batch
+        :type batch: Tuple[torch.Tensor, torch.Tensor]
+        :param batch_idx: index of batch, irrelevant
+        :type batch_idx: int
+        :return: label tensor, logits tensor, loss 
+        :rtype: Tuple[torch.Tensor, torch.Tensor, float]
+        """"""        
+        if isinstance(self.base_model, TabNetGeneClassifier):
+            # Hacky and annoying, but the extra M_loss from TabNet means we need to handle this specific case 
+            y_hat, y, loss = self.base_model._step(batch, batch_idx)
+        else:
+            x, y = batch
+            y_hat = self(x)
+            loss = F.cross_entropy(y_hat, y)
+
+        return y, y_hat, loss 
+
+    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):
+        y, y_hat, loss = self._step(batch, batch_idx)        
+
+        self.log(""train_loss"", loss, logger=True, on_epoch=True, on_step=True)
+        self._compute_metrics(y_hat, y, 'train')
+
+        return loss
+    
+    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):
+        y, y_hat, loss = self._step(batch, batch_idx)    
+
+        self.log(""val_loss"", loss, logger=True, on_epoch=True, on_step=True)
+        self._compute_metrics(y_hat, y, 'val')
+        
+        return loss
+    
+    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):
+        y, y_hat, loss = self._step(batch, batch_idx)    
+
+        self.log(""test_loss"", loss, logger=True, on_epoch=True, on_step=True)
+        self._compute_metrics(y_hat, y, 'test')
+        
+        return loss
+
+    def _compute_metrics(self, 
+        y_hat: torch.Tensor, 
+        y: torch.Tensor, 
+        tag: str, 
+        on_epoch=True, 
+        on_step=False,
+    ):
+        """"""
+        Compute metrics for the given batch
+
+        :param y_hat: logits of model
+        :type y_hat: torch.Tensor
+        :param y: tensor of labels
+        :type y: torch.Tensor
+        :param tag: log name, to specify train/val/test batch calculation
+        :type tag: str
+        :param on_epoch: log on epoch, defaults to True
+        :type on_epoch: bool, optional
+        :param on_step: log on step, defaults to True
+        :type on_step: bool, optional
+        """"""
+        for name, metric in self.metrics.items():
+            if self.weighted_metrics: # We dont consider class support in calculation
+                val = metric(y_hat, y, average='weighted', num_classes=self.output_dim)
+                self.log(
+                    f""weighted_{tag}_{name}"", 
+                    val, 
+                    on_epoch=on_epoch, 
+                    on_step=on_step,
+                    logger=True,
+                )
+            else:
+                val = metric(y_hat, y)
+                self.log(
+                    f""{tag}_{name}"", 
+                    val, 
+                    on_epoch=on_epoch, 
+                    on_step=on_step,
+                    logger=True,
+                )
+
+    def configure_optimizers(self):
+        optimizer = self.optim_params.pop('optimizer')
+        optimizer = optimizer(self.parameters(), **self.optim_params)
+
+        return optimizer
+
+class TabNetGeneClassifier(TabNet):
+    """"""
+    Just a simple wrapper to only return the regular output instead the output and M_loss as defined in the tabnet paper.
+    This allows me to use a single train/val/test loop for both models. 
+    """"""
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+
+    def forward(self, x):
+        out, M_loss = super().forward(x)
+        return out, M_loss
+
+    # leaving this in case we ever want to add lambda_sparse parameter, should be easy 
+    def _step(self, batch, batch_idx):
+        x, y = batch
+        y_hat, _ = self.forward(x)
+        
+        # Add extra sparsity as required by TabNet 
+        loss = F.cross_entropy(y_hat, y)
+
+        return y_hat, y, loss
\ No newline at end of file"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+import torch 
+import numpy as np 
+import tabnet 
+import shutil 
+import json 
+import zipfile 
+import torch.nn.functional as F 
+import io 
+
+import torch.nn.functional as F
+import pytorch_lightning as pl 
+from scipy.sparse import csc_matrix 
+from pathlib import Path 
+from pytorch_tabnet.utils import (
+    create_explain_matrix,
+    ComplexEncoder,
+)
+
+class TabNetLightning(pl.LightningModule):
+    def __init__(
+        self,
+        *args,
+        **kwargs,
+    ) -> None:
+        super().__init__()
+
+        self.network = tabnet.tab_network.TabNet(*args, **kwargs)
+        self.reducing_matrix = create_explain_matrix(
+            self.network.input_dim,
+            self.network.cat_emb_dim,
+            self.network.cat_idxs,
+            self.network.post_embed_dim,
+        )
+
+    def forward(self, x):
+        return self.base_model.forward(x)
+
+    def _compute_loss(self, y, y_hat):
+
+        # If user doesn't specify, just set to cross_entropy
+        if self.loss is None:
+            self.loss = F.cross_entropy 
+
+        return self.loss(y, y_hat, weight=self.weights)    
+
+    def _step(self, y, batch):
+        output, M_loss = self.network(batch)
+
+        loss = self.compute_loss(output, y)
+        # Add the overall sparsity loss
+        loss = loss - self.lambda_sparse * M_loss
+
+    def training_step(self, batch, batch_idx):
+        pass
+
+    def validation_step(self, batch, batch_idx):
+        pass 
+
+    def test_step(self, batch, batch_idx):
+        pass 
+
+    def configure_optimizers(self):
+        pass 
+
+    def explain(self, loader, normalize=False):
+        self.network.eval()
+        res_explain = []
+
+        for batch_nb, data in enumerate(loader):
+            data = data.to(self.device).float()
+
+            M_explain, masks = self.network.forward_masks(data)
+            for key, value in masks.items():
+                masks[key] = csc_matrix.dot(
+                    value.cpu().detach().numpy(), self.reducing_matrix
+                )
+
+            original_feat_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),
+                                                   self.reducing_matrix)
+            res_explain.append(original_feat_explain)
+
+            if batch_nb == 0:
+                res_masks = masks
+            else:
+                for key, value in masks.items():
+                    res_masks[key] = np.vstack([res_masks[key], value])
+
+        res_explain = np.vstack(res_explain)
+
+        if normalize:
+            res_explain /= np.sum(res_explain, axis=1)[:, None]
+
+        return res_explain, res_masks
+
+    def _compute_feature_importances(self, dataloader):
+        M_explain, _ = self.explain(dataloader, normalize=False)
+        sum_explain = M_explain.sum(axis=0)
+        feature_importances_ = sum_explain / np.sum(sum_explain)
+        return feature_importances_
+
+    def save_model(self, path):
+        saved_params = {}
+        init_params = {}
+        for key, val in self.get_params().items():
+            if isinstance(val, type):
+                # Don't save torch specific params
+                continue
+            else:
+                init_params[key] = val
+        saved_params[""init_params""] = init_params
+
+        class_attrs = {
+            ""preds_mapper"": self.preds_mapper
+        }
+        saved_params[""class_attrs""] = class_attrs
+
+        # Create folder
+        Path(path).mkdir(parents=True, exist_ok=True)
+
+        # Save models params
+        with open(Path(path).joinpath(""model_params.json""), ""w"", encoding=""utf8"") as f:
+            json.dump(saved_params, f, cls=ComplexEncoder)
+
+        # Save state_dict
+        torch.save(self.network.state_dict(), Path(path).joinpath(""network.pt""))
+        shutil.make_archive(path, ""zip"", path)
+        shutil.rmtree(path)
+        print(f""Successfully saved model at {path}.zip"")
+        return f""{path}.zip""
+
+
+    def load_model(self, filepath):
+        try:
+            with zipfile.ZipFile(filepath) as z:
+                with z.open(""model_params.json"") as f:
+                    loaded_params = json.load(f)
+                    loaded_params[""init_params""][""device_name""] = self.device_name
+                with z.open(""network.pt"") as f:
+                    try:
+                        saved_state_dict = torch.load(f, map_location=self.device)
+                    except io.UnsupportedOperation:
+                        # In Python <3.7, the returned file object is not seekable (which at least
+                        # some versions of PyTorch require) - so we'll try buffering it in to a
+                        # BytesIO instead:
+                        saved_state_dict = torch.load(
+                            io.BytesIO(f.read()),
+                            map_location=self.device,
+                        )
+        except KeyError:
+            raise KeyError(""Your zip file is missing at least one component"")
+
+        self.__init__(**loaded_params[""init_params""])
+
+        self._set_network()
+        self.network.load_state_dict(saved_state_dict)
+        self.network.eval()
+        self.load_class_attrs(loaded_params[""class_attrs""])"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+[bumpversion]
+current_version = 0.0.1
+commit = True
+tag = True
+
+[bumpversion:file:setup.py]
+search = version='{current_version}'
+replace = version='{new_version}'
+
+[bumpversion:file:statdepth/__init__.py]
+search = __version__ = '{current_version}'
+replace = __version__ = '{new_version}'
+
+[bdist_wheel]
+universal = 1
+
+[flake8]
+exclude = docs
+
+[aliases]
+# Define setup.py command aliases here
+"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+#!/usr/bin/env python
+
+""""""The setup script.""""""
+
+from setuptools import setup, find_packages
+import pathlib 
+
+requirements = [
+    ""pandas"",
+    ""numpy"",
+    ""tqdm"",
+    ""boto3"",
+]
+
+setup_requirements = requirements.copy()
+
+test_requirements = []
+
+# The directory containing this file
+HERE = pathlib.Path(__file__).parent
+
+# The text of the README file
+README = (HERE / ""README.md"").read_text()
+
+setup(
+    author=""Julian Lehrer"",
+    author_email='jmlehrer@ucsc.edu',
+    python_requires='>=3.6',
+    classifiers=[
+        'Development Status :: 3 - Alpha',
+        'Intended Audience :: Developers',
+        'License :: OSI Approved :: GNU General Public License v2 (GPLv2)',
+        'Natural Language :: English',
+        'Programming Language :: Python :: 3',
+        'Programming Language :: Python :: 3.6',
+        'Programming Language :: Python :: 3.7',
+        'Programming Language :: Python :: 3.8',
+    ],
+    description=""A small library for taking the transpose of arbitrarily large .csvs"",
+    install_requires=requirements.copy(),
+    license=""MIT license"",
+    long_description=README,
+    long_description_content_type=""text/markdown"",
+    include_package_data=True,
+    keywords='scsims',
+    name='scsims',
+    packages=find_packages(exclude=['tests']),
+    setup_requires=setup_requirements,
+    test_suite='tests',
+    tests_require=test_requirements,
+    url='https://github.com/jlehrer1/sims',
+    version='0.0.1',
+    zip_safe=False,
+)"
KO;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";
OK;8;jlehrer1;SIMS;5e6fba045829132c78c5c897677a46a1a7e95b12;"Initial commit

Updated

Added makefile and dockerfile

Updated script, added unzip shell script since prp decompression is not
working

Updated script

Updated job yaml

Fixed sample

More print statements, writing csv's to correct output

Added upload

Added script to generate transposed data

Added transposed data script

Fixed test script

Fixed file names, removed setting index for umap computation since it is
very slow

Added yaml subfolder

Debugging

Added transpose script using awk, doesnt require whole file to be in
memory

Added yaml script for generating transpose file

Added more transpose scripts to test, started main data exploration

Added data cleaning script

Added more umap visualizations

Added umap calculation script

Fixed dockerfile, added pandas transform script in memory

Fixed data cleaning script, umap script

Bugfixes

Bugfixes

Bugfixes, added dist options to umap calculations

Started clustering, removing genes with zero expression in data
cleaning. I suspect when we have the full dataset there will be no gene
dimensions that are all zero, but it can't hurt to add it to the pipline

Added remvoing zero columns from data cleaning

Added upload to data cleaning script

Added more scripts

Fixed gitignore

Added dockerignore

Removed all files related to transposition since those exist in a
separate repo now

Cleaned up

Added data download file

Fixed data download script

Added progress bar

Removed test, converting scripts to use Dask

Disgusting fix where i edit sys.path, but it makes the code a lot
cleaner overall

Added reduction script

Bugfixes, updated download script, reduction script, removed all raw
data from dockerfile since it can be downloaded from S3

Updated pipeline scripts

Bugfixes

Bugfixes, cleanuo

Updated gitignore

Added start scripts

Code cleanup

Code cleanup

Changed umapr reduction to be file specific since we're getting
OOMKilled errors even with 500gb of memory.

Updated dockerignore, removed visualization since it doesn't work for
dim>3 (duh)

Added documentation, list_objects method, generalized download and
upload method

Added download_reduced method

Started classification

Started classification

Updated

Updated documentation

Added clustering

Updated dockerignore

Added scripts for clustering, code cleanup

Added argument params for clustering

Clearer pathing, method to download labeled data

Bugfixes

Code cleanup

Bugfixes, started modeling, code cleanup

Updated dockerfile

Added model trainign script

Bugfixes, more classification stuff

Started model testing

Working on modeling, going to try PCA

Added pca script

Test pca script, works

Debugging, added file option to clean data download but still needs to
be debugged

Code cleanup

Changing dockerfile to build from pytorch

Cleaned up dockerfile

Added pca clustering, need to clean up scripts

Moved to PyTorch Lighning

Increased model complexity, code cleanup

Added class weighting

Added dropout, changed to sgd with momentum

Bugfix

Cleaned up model architecture and code, made it more modular

Updated network architecture, passing in width and depth for NAS

Checkpointing my messy code

Added custom uppload callaback for model saving

Bug squashin

Added weight decay

Added random hyperparameter search

Bugfix, still need to log hyperparams to cometml

Added model search!

Updated gitignore, moved to weighted accuracy

Added confusion matrix, trying logging directly with comet ml logger

Using comet logger directly, trying to log confusion matrix

Fixed logging

Anonymized S3 login info, can make repo public

Started cluster annotation script, added method for getting all primary
genes

Added annotation script. untested

Bugfix

Added error checking

Started cluster viz, added annotation download

Added gitattributes to fix the repo classification, and updated the
readme

Started viz

Added visualization code, still need to test

Tested viz script, works for 2d visualizations, updated gitignore

Added lots of documentation, fixed visualization file, runs now

Plotting all umap runs with 2 dimensions

Moved to using preannotated clusters for testing

Generalized class column name

Generalized model to train on any label column, generating numeric
labels for all labels

Added type hints, debugging

Added batch size as param

Reduced batch size, automated where weight tensor needs to be

Moved fix_files to helper, changed genes to return from json

Added train label as cli argument

Refractored model search script

Set random seeds to fixed value for reproducibility

Code cleanup

Refractored encoding file

Refractored dataset generation, more generalizable. Still need to
compute class weights across multiple datasets

Generalized dataset generation method, still need to return input # and
class # cleanly

Moved models into separate file, added gradient clipping

Generalized dataset to fit multiple files, writing manual training loop
to debug gradient explosion

Moved datasets to new file

Added nn subfolder

Bugfixes

Added batch size and num_workers as arguments to model training script

Reordered inports (refractor) and added script to train on all 3 labels

Fixed label encoding script, added stratified dataset generation method,
changed GeneExpressionData to accept particular indices to generate data
from

Added stratified option to generate_datasets, new method for
nonstratified splits (in case I ever want a truly random split)

Code cleanup on len method of custom dataset class

Bugfixes. Added test_size as a parameter to dataset generation. Added
default values for GeneClassifier in the case of tests. Should consider
removing at a future date to guarantee values are passed

Moved model search params to dict

Removed logging on step, throttling comet

Bugfixes, moved to weightsandbiases

Added more metrics, code cleanup

Bugfix, removed old code

Fixed logging

Bugfix

Added weighted_metrics as argparse param

Added weighted_metrics as param, bugfixes

Fixed typing, added test set to train/val, still need to add early
stopping(?). Started dataset composition

Combining external data

Added data transpose script using my library transposecsv

Bugfixes

Added label combining method

Added data cleaning method

Code cleanup, added upload to data cleaning

Added raw data download method

Code cleanup, added unzipping in data download, updated readme

Added upload to label encoding, updated readme

Bugfixes

Added datafiles list to helper.py for consistency across files

Bufix

Code cleanup, small refractor

Added ability to parallelize transpose script

Code cleanup, added ability to download individual files in S3 download
methods

Fixed label file, transpose file, data cleaning file

Bugfix

Added saving interim results in label cleaning

Updated transpose to use new api from transposecsv

Added documentation, using transposecsv to upload chunks and file to s3

Bugfixes

Changed clean data, hopefully faster now

Added method to generate list of genes so script can be run in parallel

Added ability to calculate gene intersection for single file so this can
be run in parallel

Moved gene intersection generation to helper, started rowwise cleaning

Added rowwise data cleaning, need to test and do this in chunks +
parallelize

Bugfixes, added sample-wise cleaning

Moved clean_sample to data file, code cleanup, added some caching

Added more properties to custom dataset

Bugfix, need to figure out why allen_cortex_T.csv is WEIRD

Doubled speed of dataset :)

Added generating train/val/test for single dataset

Added initial train/val loop

Moved things into their own folder, some code cleanup, rewriting train
loop.

Need to add more torchmetrics`

Added files for lightning training, regular training, and metric
calculation via wandb

Updated training loop

Added sample-wise normalization, code cleanup, bugfixes, generate
dataloaders for a single file

Better train loop, added wrapped for tabnet

Removed redudant methods (non-stratified splitting), added val/test
dataste generation instead of just validation

Big bugfix! Forgot to reset indices in GeneExpressionData method

Added repr/str methods to dataset

Added test loss/metrics. still need to calculate metrics over epoch /
overall weighted accuacy

Some code cleanup, added option to align sample in dataset (for speed
testing)

Added dataloader to subclass dataloader and change output

Added pytorch datamodule for training

Added collate dataloader

Added sequential loader!!!!!! Big milestone

Moved standard NN architecture to tabnet

Code cleanup, bugfixes with imports

Code refractoring, added shuffle parameter to dataloader generation

Added shuffle and data_path params, data downloading from S3 for model
training

Bugfixes

Annoying ass bugfix holy fucgkkk

Bugfixes

Added transpose option and accompying collate_fns

A whole bunch of code cleanup, removing explicit function keywords and
handling most things in kwargs

Added linesep option to dataset class, some code cleanup, documentation

More refractoring

Refractor, bugfix

Added args for positional arguments, set up model search

Code cleanup

Added M_loss to tabnet model, need to integrate into pytorch lightning

Added tabnet as base model correctly

Bugfixes

Moved download to two files since things were getting messy, should be
easier for api as well

Added an ungodly amount of documentation, switched to sphinx style for
api down the line

Added initial data handler for in-memory matrices

Refractored into NumpyStreamable, this is for HDF5 files

Bugfixes -- need to integrate gene intersection into entire pipeline

Added h5ad handlign

Bugfixes, code cleanup, added h5ad as file option

Added property methods to calculate input dim/output dim to data module,
this should be defined by end user now

Added h5ad converter

Bugfix

Cleanup, defining package

Cleanup, added setup.py/cfg

UPdated";"+import pandas as pd 
+import os
+import sys
+import pandas as pd
+import numpy as np
+from torch.utils.data import *
+from tqdm import tqdm
+import linecache 
+from pytorch_tabnet.tab_model import TabNetClassifier
+sys.path.append('../src/')
+sys.path.append('..')
+
+from src.models.lib.data import *
+from src.helper import *
+
+def map_cols_test():
+    ref = ['a', 'b', 'c']
+    curr = ['b', 'a', 'c', 'd'] 
+    sample = np.array([1,2,3,4]) # Want --> [2,1,3]
+
+    result = clean_sample(sample, ref, curr)
+    desired = torch.from_numpy(np.array([2,1,3]))
+    
+    assert torch.equal(result, desired)
+
+    ref = ['a', 'b', 'c']
+    curr = ['c', 'd', 'b', 'a']
+
+    sample = np.array(
+        [[1,2,3,4],
+         [5,6,7,8]]
+    ) 
+    # --> want [[4, 3, 1],
+    #           [8, 7, 5]]
+
+    res = clean_sample(sample, ref, curr)
+    desired = torch.from_numpy(np.array([
+        [4,3,1],
+        [8,7,5]
+    ]))
+    
+    assert torch.equal(res, desired)
+    
+def _test_first_n_samples(n, datafile, labelfile):
+    data = GeneExpressionData(datafile, labelfile, 'Type', skip=3)
+    cols = data.columns
+    
+    # Generate dict with half precision values to read this into my 16gb memory
+    dtype_cols = dict(zip(cols, [np.float32]*len(cols)))
+    
+    data_df = pd.read_csv(datafile, nrows=2*n, header=1, dtype=dtype_cols) # Might need some extras since numerical index drops some values
+    label_df = pd.read_csv(labelfile, nrows=n)
+
+    similar = []
+    for i in range(n):
+        datasample = data[i][0]
+        dfsample = torch.from_numpy(data_df.loc[label_df.loc[i, 'cell'], :].values).float()
+        
+        isclose = all(torch.isclose(datasample, dfsample))
+        similar.append(isclose)
+    
+    print(f""First {n=} columns of expression matrix is equal to GeneExpressionData: {all(p for p in similar)}"")
+
+    assert (all(p for p in similar))
+
+def test_datamodule():
+    N = 50
+    datafiles, labelfiles = list(INTERIM_DATA_AND_LABEL_FILES_LIST.keys()), list(INTERIM_DATA_AND_LABEL_FILES_LIST.values())
+    datafiles = [os.path.join('..', 'data', 'interim', f) for f in datafiles]
+    processed_labels = [os.path.join('..', 'data', 'processed/labels', f) for f in labelfiles]
+
+    for datafile, labelfile in zip(datafiles, processed_labels):
+        print(f'Testing {datafile=}')
+        _test_first_n_samples(N, datafile, labelfile)
+
+if __name__ == ""__main__"":
+    map_cols_test()
+    test_datamodule()
+"
KO;8;wltn0029;codingProblems;fb769371dff8d8de08957d6e61c899460996baa0;reduce memory usage;" def solution(phone_book):
-    front = {}
-    for i in range(1, 21):
-        front[i] = {}
     
     for p in phone_book :
-        phoneLen = len(p)
-        for i in range(1, phoneLen+1):
-            if p[0:i] in front[i]:
-                front[i][p[0:i]] += 1
-            else :
-                front[i][p[0:i]] = 1
-        
-    ans = True
     for p in phone_book :
-        phoneLen = len(p)
-        if p in front[phoneLen] and front[phoneLen][p] > 1:
-            ans = False
-            break
     return ans
\ No newline at end of file"
OK;8;wltn0029;codingProblems;fb769371dff8d8de08957d6e61c899460996baa0;reduce memory usage;" def solution(phone_book):
+    phone = {}
+    ans = True 
     
     for p in phone_book :
+        phone[p] = 1
+    
     for p in phone_book :
+        for i in range(1, len(p)):
+            if p[0:i] in phone : 
+                ans = False
+                break
     return ans
\ No newline at end of file"
KO;8;marc-dantas;somescript;3fe684915bbfb3aefc7952cabbfc92231e8d5f73;More memory operators and some corrections.;" 
 from rich import print as color_print
 from sys import exit, stdout
-from typing import List, Tuple, Dict, Union, Generator
 from enum import Enum, auto
 from os.path import abspath, isfile
 
@@ -34,26 +34,19 @@
 Position = Tuple[str, int, int]
 
 # Mode value
-filemode = 1
-
-
-def mode(value: int) -> None:
-    global filemode
-    filemode = value
 
 
 def err(position: Position, msg: str, hint: str):
     color_print(f'[red bold]ERROR[/] at [blue underline]{"":"".join(str(i) for i in position)}[/]:\n\t[yellow]{msg}\n\t{hint}[/]')
-    if filemode:
-        exit(1)
-        
 
 def arbitrary_err(msg: str) -> None:
     color_print(f'[red bold]ERROR[/]: [yellow]{msg}[/]')
     exit(1)
 
 
-def parse_str_escape(pos: Position, value: str) -> str:
     try:
         return bytes(value, 'utf-8').decode('unicode_escape')
     except UnicodeDecodeError as e:
@@ -128,7 +121,7 @@ def __generate_tokens(self) -> Generator:
                 if line[col] == '""':  # String lexer
                     col_end = self.__find_column(line, col + 1, lambda x: x == '""')
                     pos = (self.__filename, ln + 1, col + 1)
-                    yield (pos, (TokenType.STRING, parse_str_escape(pos, line[col+1:col_end])), None)
                     col = self.__find_column(line, col_end + 1, lambda x: not x.isspace())
                 else:  # Word/Number lexer
                     col_end = self.__find_column(line, col, lambda x: x.isspace())
@@ -177,7 +170,7 @@ class Stack:
     
     def __init__(self) -> None:
         self.__values = []
-        
     def pop(self) -> Union[str, int]:
         return self.__values.pop()
 
@@ -217,11 +210,13 @@ def __len__(self) -> int:
 
 
 # Some useful constants
-STACK = Stack()
-STRING_CAPACITY = 512_000
-VALUE_CAPACITY = 512_000
-MEMORY_CAPACITY = STRING_CAPACITY + VALUE_CAPACITY  # 1 MegaByte should be enough
-MEMORY = bytearray(MEMORY_CAPACITY)
 
 
 class Interpreter:
@@ -252,8 +247,14 @@ def __init__(self, program: ParsedProgram) -> None:
             'swp': self.__swp,
             'ovr': self.__ovr,
             'mem': self.__addr,
-            'load8': self.__loadbyte,
-            'store8': self.__storebyte,
             'write': self.__write,
             'writeln': self.__writeln,
             'sysexit': self.__exit
@@ -290,8 +291,7 @@ def run(self) -> None:
     
     def __get_memory_value(self, stack: Stack) -> str:
         start, end = stack.pop(), stack.pop()
-        encoding = 'utf-8'
-        return MEMORY[start:end].decode(encoding)
 
     # Control flow actions
     
@@ -336,7 +336,7 @@ def __push_num(self, stack: Stack, value: int) -> None:
         stack.push(value)
     
     def __push_str(self, stack: Stack, string_token: Token) -> None:
-        value_in_bytes = bytes(string_token['value'], 'utf-8')
         size = len(value_in_bytes)
         # Verify if the string isn't allocated yet to
         # avoid memory leaks in case of loops or something like that.
@@ -464,14 +464,14 @@ def __dmp(self, pos: Position, stack: Stack) -> None:
     
     def __write(self, pos: Position, stack: Stack) -> None:
         if Stack.support_bin_operation(stack):
-            stdout.write(self.__get_memory_value(stack))
         else:
             err(pos, 'Not enough values on the stack to write.',
                 'The values on the stack do not match the values required for the operation.')
     
     def __writeln(self, pos: Position, stack: Stack) -> None:
         if Stack.support_bin_operation(stack):
-            stdout.write(self.__get_memory_value(stack) + '\n')
         else:
             err(pos, 'Not enough values on the stack to write.',
                 'The values on the stack do not match the values required for the operation.')
@@ -526,25 +526,26 @@ def __ovr(self, pos: Position, stack: Stack) -> None:
     def __addr(self, pos: Position, stack: Stack) -> None:
         stack.push(STRING_CAPACITY)
     
-    def __loadbyte(self, pos: Position, stack: Stack) -> None:
         if Stack.support_un_operation(stack):
-            if (addr := stack.pop()) < STRING_CAPACITY:
                 err(pos, f'Address {hex(addr)} ({addr}) out of range.',
                 f'The address {hex(addr)} is invalid for arbitrary data memory.')
-            stack.push(MEMORY[addr])
         else:
             err(pos, 'Not enough values on the stack to load a byte.',
                 'The values on the stack do not match the values required for the operation.')
     
-    def __storebyte(self, pos: Position, stack: Stack) -> None:
         if Stack.support_bin_operation(stack):
-            if (val := stack.pop()) > 255:
-                err(pos, f'Value {val} is not a single byte (0..256).',
-                    f'The {val} value is not a byte because its value is greater than 255.')
-            if (addr := stack.pop()) < STRING_CAPACITY:
                 err(pos, f'Address {hex(addr)} ({addr}) out of range.',
-                    f'Try using `mem {addr} + {val} store8`')
-            MEMORY[addr] = val
         else:
             err(pos, 'Not enough values on the stack to store a byte.',
                 'The values on the stack do not match the values required for the operation.')"
OK;8;marc-dantas;somescript;3fe684915bbfb3aefc7952cabbfc92231e8d5f73;More memory operators and some corrections.;" 
 from rich import print as color_print
 from sys import exit, stdout
+from typing import Iterable, List, Tuple, Dict, Union, Generator
 from enum import Enum, auto
 from os.path import abspath, isfile
 
@@ -34,26 +34,19 @@
 Position = Tuple[str, int, int]
 
 # Mode value
 
 
 def err(position: Position, msg: str, hint: str):
     color_print(f'[red bold]ERROR[/] at [blue underline]{"":"".join(str(i) for i in position)}[/]:\n\t[yellow]{msg}\n\t{hint}[/]')
+    exit(1)
+
 
 def arbitrary_err(msg: str) -> None:
     color_print(f'[red bold]ERROR[/]: [yellow]{msg}[/]')
     exit(1)
 
 
+def decode_str_escape(pos: Position, value: str) -> str:
     try:
         return bytes(value, 'utf-8').decode('unicode_escape')
     except UnicodeDecodeError as e:
@@ -128,7 +121,7 @@ def __generate_tokens(self) -> Generator:
                 if line[col] == '""':  # String lexer
                     col_end = self.__find_column(line, col + 1, lambda x: x == '""')
                     pos = (self.__filename, ln + 1, col + 1)
+                    yield (pos, (TokenType.STRING, decode_str_escape(pos, line[col+1:col_end])), None)
                     col = self.__find_column(line, col_end + 1, lambda x: not x.isspace())
                 else:  # Word/Number lexer
                     col_end = self.__find_column(line, col, lambda x: x.isspace())
@@ -177,7 +170,7 @@ class Stack:
     
     def __init__(self) -> None:
         self.__values = []
+
     def pop(self) -> Union[str, int]:
         return self.__values.pop()
 
@@ -217,11 +210,13 @@ def __len__(self) -> int:
 
 
 # Some useful constants
+DEFAULT_MEMORY_ENCODING = 'ascii'
+DEFAULT_STRING_ENCODING = 'utf-8'
+STACK                   = Stack()
+STRING_CAPACITY         = 512_000
+VALUE_CAPACITY          = 512_000
+MEMORY_CAPACITY         = STRING_CAPACITY + VALUE_CAPACITY  # 1 MegaByte should be enough
+MEMORY                  = bytearray(MEMORY_CAPACITY)
 
 
 class Interpreter:
@@ -252,8 +247,14 @@ def __init__(self, program: ParsedProgram) -> None:
             'swp': self.__swp,
             'ovr': self.__ovr,
             'mem': self.__addr,
+            'load8': lambda x, y: self.__load(x, y, 1),
+            'store8': lambda x, y: self.__store(x, y, 1),
+            'load16': lambda x, y: self.__load(x, y, 2),
+            'store16': lambda x, y: self.__store(x, y, 2),
+            'load32': lambda x, y: self.__load(x, y, 4),
+            'store32': lambda x, y: self.__store(x, y, 4),
+            'load64': lambda x, y: self.__load(x, y, 8),
+            'store64': lambda x, y: self.__store(x, y, 8),
             'write': self.__write,
             'writeln': self.__writeln,
             'sysexit': self.__exit
@@ -290,8 +291,7 @@ def run(self) -> None:
     
     def __get_memory_value(self, stack: Stack) -> str:
         start, end = stack.pop(), stack.pop()
+        return MEMORY[start:end]
 
     # Control flow actions
     
@@ -336,7 +336,7 @@ def __push_num(self, stack: Stack, value: int) -> None:
         stack.push(value)
     
     def __push_str(self, stack: Stack, string_token: Token) -> None:
+        value_in_bytes = bytes(string_token['value'], DEFAULT_STRING_ENCODING)
         size = len(value_in_bytes)
         # Verify if the string isn't allocated yet to
         # avoid memory leaks in case of loops or something like that.
@@ -464,14 +464,14 @@ def __dmp(self, pos: Position, stack: Stack) -> None:
     
     def __write(self, pos: Position, stack: Stack) -> None:
         if Stack.support_bin_operation(stack):
+            stdout.write(self.__get_memory_value(stack).decode(DEFAULT_MEMORY_ENCODING))
         else:
             err(pos, 'Not enough values on the stack to write.',
                 'The values on the stack do not match the values required for the operation.')
     
     def __writeln(self, pos: Position, stack: Stack) -> None:
         if Stack.support_bin_operation(stack):
+            stdout.write(self.__get_memory_value(stack).decode(DEFAULT_MEMORY_ENCODING) + '\n')
         else:
             err(pos, 'Not enough values on the stack to write.',
                 'The values on the stack do not match the values required for the operation.')
@@ -526,25 +526,26 @@ def __ovr(self, pos: Position, stack: Stack) -> None:
     def __addr(self, pos: Position, stack: Stack) -> None:
         stack.push(STRING_CAPACITY)
     
+    def __load(self, pos: Position, stack: Stack, byte_number: int) -> None:
         if Stack.support_un_operation(stack):
+            if (addr := stack.pop()) < STRING_CAPACITY or addr > MEMORY_CAPACITY:
                 err(pos, f'Address {hex(addr)} ({addr}) out of range.',
                 f'The address {hex(addr)} is invalid for arbitrary data memory.')
+            stack.push(int.from_bytes(MEMORY[addr:addr + byte_number], 'little'))
         else:
             err(pos, 'Not enough values on the stack to load a byte.',
                 'The values on the stack do not match the values required for the operation.')
     
+    def __store(self, pos: Position, stack: Stack, byte_number: int) -> None:
         if Stack.support_bin_operation(stack):
+            max_val = 2 ** (byte_number * 8) - 1
+            if (val := stack.pop()) >= max_val:
+                err(pos, f'Value {val} is out of range (0..{max_val}).',
+                    f'The value {val} is invalid for {byte_number} byte{""s"" if byte_number > 1 else """"} ({byte_number * 8} bits).')
+            if (addr := stack.pop()) < STRING_CAPACITY or addr > MEMORY_CAPACITY:
                 err(pos, f'Address {hex(addr)} ({addr}) out of range.',
+                    f'Try using `mem {addr} + {val} store{byte_number * 8}`.')
+            MEMORY[addr:addr + byte_number] = val.to_bytes(byte_number, 'little')
         else:
             err(pos, 'Not enough values on the stack to store a byte.',
                 'The values on the stack do not match the values required for the operation.')"
KO;8;marc-dantas;somescript;3fe684915bbfb3aefc7952cabbfc92231e8d5f73;More memory operators and some corrections.;" # SomeScript Full Documentation (markdown)
 
-#### Word Types (Token Types)
 + Number: Any unsigned integer (e.g `123`, `13`, `0`)
 + Math Operator
     + Add: `+`
     + Subtract: `-`
@@ -31,8 +32,8 @@
     + Over: `ovr`
 + Memory Operator
     + Mem: `mem`
-    + Store Byte: `store8`
-    + Load Byte: `load8`
     + Write: `write`
     + Write Line: `writeln`
 
@@ -67,48 +68,42 @@
     + Over: (`x, y -> x, y, x`) Put `x` on top of `y`
 + Memory functions: [click to see a example](./examples/memory.sscript)
     + Mem: (`... -> first_addr`) Push the first memory address onto the stack
-    + Store Byte: (`addr, x >> x`) Store the value `x` at the memory address `addr`
-    + Load Byte: (`addr -> x`) Load the value at the memory address `addr`
     + Write: (`end, start >> mem[start:end]`) Print the bytes between `start` and `end`
     + Write Line: (`end, start >> mem[start:end] + '\n'`) Print the bytes between `start` and `end` with a newline character
 
-#### /* Subtitle
-- `...` - Any value on the stack
-- `->` - Action performed in the stack
-- `>>` - Action performed in the memory
-- `_` - Empty value
-
-#### Control flow: [click to see a example](./examples\control_flow.sscript)
 ##### If condition
 ```pas
 <condition> if
   <code>
 end
 ```
-##### Else condition
 ```pas
 <condition> if
     <code>
 else
     <code>
 end
 ```
-##### While Loop
 ```pas
 while <condition> do
    <code>
 repeat
 ```
 
-#### Strings: [click to see a example](./examples\strings.sscript)
-+ String: `""...""` (any Unicode text between double quotes)
 + Escapes:
     + `\n`: Newline
     + `\r`: Carriage return
     + `\t`: Tab
     + `\\`: Backslash
 + How it works:
-    + Every string literal works by taking the value of the string itself and allocating it to a reserved space in the memory (512 KB). Then the address that is allocated in memory is placed on the stack (value where the string starts on the memory, and the value where it ends)
 
 #### Usage
 - Run a file:
@@ -123,11 +118,12 @@ repeat
 + Interactive shell
     + `./somescript.py` will enter the interactive shell
 
 > Interactive Shell Hint: type `%clear` to clear the interactive shell
 
 #### File extensions
 + `*.somescript` or `*.sscript`
-> NOTE: Somescript support other extensions, but the convention is `*.sscript` 
 
 ---
 "
OK;8;marc-dantas;somescript;3fe684915bbfb3aefc7952cabbfc92231e8d5f73;More memory operators and some corrections.;" # SomeScript Full Documentation (markdown)
 
+#### All SomeScript words
 + Number: Any unsigned integer (e.g `123`, `13`, `0`)
+> In the future, SomeScript's numbers can be signed & hexadecimal (e.g `-6`, `0xFF`)
 + Math Operator
     + Add: `+`
     + Subtract: `-`
@@ -31,8 +32,8 @@
     + Over: `ovr`
 + Memory Operator
     + Mem: `mem`
+    + Store: `store<bits>` (`<bits>` can be 8 (1 byte), 16 (2 bytes), 32 (4 bytes) and 64 (8 bytes))
+    + Load: `load<bits>` (`<bits>` can be 8 (1 byte), 16 (2 bytes), 32 (4 bytes) and 64 (8 bytes))
     + Write: `write`
     + Write Line: `writeln`
 
@@ -67,48 +68,42 @@
     + Over: (`x, y -> x, y, x`) Put `x` on top of `y`
 + Memory functions: [click to see a example](./examples/memory.sscript)
     + Mem: (`... -> first_addr`) Push the first memory address onto the stack
+    + Store: (`addr, x >> x`) Store the value `x` at the address `addr`
+    + Load: (`addr -> x`) Load the value at the memory address `addr`
     + Write: (`end, start >> mem[start:end]`) Print the bytes between `start` and `end`
     + Write Line: (`end, start >> mem[start:end] + '\n'`) Print the bytes between `start` and `end` with a newline character
 
+#### Control flow: [click to see a example](./examples/control_flow.sscript)
 ##### If condition
 ```pas
 <condition> if
   <code>
 end
 ```
+##### If-else condition
 ```pas
 <condition> if
     <code>
 else
     <code>
 end
 ```
+##### While loop
 ```pas
 while <condition> do
    <code>
 repeat
 ```
 
+#### Strings: [click to see a example](./examples/strings.sscript)
++ String: `""...""` (any text between double quotes)
 + Escapes:
     + `\n`: Newline
     + `\r`: Carriage return
     + `\t`: Tab
     + `\\`: Backslash
 + How it works:
+    + Every string literal works by taking the value of the string itself and allocating it to a reserved space in the memory (512 KB). Then the address that is allocated in memory is placed on the stack (value where the string starts on the ""byte tape"") and the address where it ends.
 
 #### Usage
 - Run a file:
@@ -123,11 +118,12 @@ repeat
 + Interactive shell
     + `./somescript.py` will enter the interactive shell
 
+> NOTE: The required python version is 3.8.0 or higher.<br>
 > Interactive Shell Hint: type `%clear` to clear the interactive shell
 
 #### File extensions
 + `*.somescript` or `*.sscript`
+> NOTE: SomeScript support other extensions, but the convention is `*.sscript` 
 
 ---
 "
KO;8;savabush;places-remember;8d91dd3b6d3170c63cebdd317b7af0b75e8cdec0;Added README, changed test to addmemory page with nolog;\ No newline at end of file
OK;8;savabush;places-remember;8d91dd3b6d3170c63cebdd317b7af0b75e8cdec0;Added README, changed test to addmemory page with nolog;"+# places-remember
+A web application for remembering places where people have been and which they remembered. Social network VK is used for authentication.
+
+When using the project, you should change <b><i>example_secret.py</i></b> on <b><i>secret.py</i></b>, and hence add hidden keys.
+
+The project uses VK API and Mapbox GL JS. All the necessary keys can be found there.
\ No newline at end of file"
KO;8;savabush;places-remember;8d91dd3b6d3170c63cebdd317b7af0b75e8cdec0;Added README, changed test to addmemory page with nolog;"def test_index_post(self):
         object_of_model.delete()
         self.assertEquals(models.Memory.objects.all().count(), 14)
 
-    def test_addmemory_get(self):
         response = self.client.get(reverse('addmemory'))
-
-        self.assertURLEqual(response.status_code, 200)
-        self.assertTemplateUsed(response, 'core/addmemory.html')"
OK;8;savabush;places-remember;8d91dd3b6d3170c63cebdd317b7af0b75e8cdec0;Added README, changed test to addmemory page with nolog;"def test_index_post(self):
         object_of_model.delete()
         self.assertEquals(models.Memory.objects.all().count(), 14)
 
+    def test_addmemory_nolog_get(self):
         response = self.client.get(reverse('addmemory'))
+        self.assertURLEqual(response.status_code, 302)"
KO;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;" 
 
 def get_access_token(request):
     username_vk = User.objects.get(username=f'{request.user.username}')
     owner_id = UserSocialAuth.objects.get(user_id=username_vk.id).extra_data['id']
     access_token = UserSocialAuth.objects.get(user_id=username_vk.id).extra_data['access_token']"
OK;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;" 
 
 def get_access_token(request):
+    """"""
+    Getting VK_TOKEN and OWNER_ID for VK_API using username
+     and library python-social-django
+    """"""
     username_vk = User.objects.get(username=f'{request.user.username}')
     owner_id = UserSocialAuth.objects.get(user_id=username_vk.id).extra_data['id']
     access_token = UserSocialAuth.objects.get(user_id=username_vk.id).extra_data['access_token']"
KO;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;" <html>
 <head>
 <meta charset=""utf-8"">
-<title>Add a mempory</title>
 <meta name=""viewport"" content=""initial-scale=1,maximum-scale=1,user-scalable=no"">
 <link href=""https://api.mapbox.com/mapbox-gl-js/v2.8.2/mapbox-gl.css"" rel=""stylesheet"">
 <script src=""https://api.mapbox.com/mapbox-gl-js/v2.8.2/mapbox-gl.js""></script>
 <script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
 <style>
 body { margin: 0; padding: 0; }
 #map { position: absolute; top: -1; bottom: 0; width: 100%; height: 50% }
 </style>
 </head>
 <body>
@@ -47,7 +55,7 @@
         const map = new mapboxgl.Map({
             container: 'map',
             style: 'mapbox://styles/mapbox/streets-v11',
-            center: [-74.5, 40],
             zoom: 9,
         });
         const coordinatesGeocoder = function (query) {
@@ -115,11 +123,27 @@
             trackUserLocation: true,
             showUserHeading: true
         }));
         map.on('click', (e) => {
             const url = new URL(window.location.href);
             url.searchParams.set('points', JSON.stringify(e.lngLat.wrap()));
             window.history.pushState({ path: url.href }, '', url.href);
         });
     }
 </script>
 </body>"
OK;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;" <html>
 <head>
 <meta charset=""utf-8"">
+<title>Add a memory</title>
 <meta name=""viewport"" content=""initial-scale=1,maximum-scale=1,user-scalable=no"">
 <link href=""https://api.mapbox.com/mapbox-gl-js/v2.8.2/mapbox-gl.css"" rel=""stylesheet"">
 <script src=""https://api.mapbox.com/mapbox-gl-js/v2.8.2/mapbox-gl.js""></script>
 <script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
 <style>
 body { margin: 0; padding: 0; }
 #map { position: absolute; top: -1; bottom: 0; width: 100%; height: 50% }
+#marker {
+    background: rgba(0, 0, 200, 0.8);
+    background-size: cover;
+    width: 15px;
+    height: 15px;
+    border-radius: 50%;
+    cursor: pointer;
+    }
 </style>
 </head>
 <body>
@@ -47,7 +55,7 @@
         const map = new mapboxgl.Map({
             container: 'map',
             style: 'mapbox://styles/mapbox/streets-v11',
+            center: [50, 50],
             zoom: 9,
         });
         const coordinatesGeocoder = function (query) {
@@ -115,11 +123,27 @@
             trackUserLocation: true,
             showUserHeading: true
         }));
+
+
+        const popup = new mapboxgl.Popup({ offset: 25 });
+        const el = document.createElement('div');
+        el.id = 'marker';
+        pop = new mapboxgl.Marker(el)
+            .setLngLat([50, 50])
+            .setPopup(popup)
+            .addTo(map);
+
+
         map.on('click', (e) => {
             const url = new URL(window.location.href);
             url.searchParams.set('points', JSON.stringify(e.lngLat.wrap()));
             window.history.pushState({ path: url.href }, '', url.href);
+            const popup = new mapboxgl.Popup({ offset: 25 });
+            const el = document.createElement('div');
+            el.id = 'marker';
+            pop.setLngLat([Object.values(e.lngLat.wrap())[0], Object.values(e.lngLat.wrap())[1]]).addTo(map);
         });
+
     }
 </script>
 </body>"
KO;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;"                                                         container: 'map{{ memory.id }}',
                                                         style: 'mapbox://styles/mapbox/streets-v11',
                                                         center: [{{ memory.point_lng }}, {{ memory.point_lat }}],
-                                                        zoom: 13,
                                                         interactive: false
                                                     });
                                                     const popup = new mapboxgl.Popup({ offset: 25 });
@@ -81,7 +81,7 @@
                                             <h5>{{ memory.name }}</h5>
                                             <p class=""card-text"">{{ memory.comment }}</p>
                                         </div>
-                                        <input name='{{ memory.id }}' type=""submit"" value=""Delete"">
                                     </form>
                                 </div>
                             </div>"
OK;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;"                                                         container: 'map{{ memory.id }}',
                                                         style: 'mapbox://styles/mapbox/streets-v11',
                                                         center: [{{ memory.point_lng }}, {{ memory.point_lat }}],
+                                                        zoom: 15,
                                                         interactive: false
                                                     });
                                                     const popup = new mapboxgl.Popup({ offset: 25 });
@@ -81,7 +81,7 @@
                                             <h5>{{ memory.name }}</h5>
                                             <p class=""card-text"">{{ memory.comment }}</p>
                                         </div>
+                                        <input name='{{ memory.id }}' type=""submit"" value=""Delete memory"">
                                     </form>
                                 </div>
                             </div>"
KO;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;"-from django.test import TestCase
-
-# Create your tests here."
OK;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;
KO;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;
OK;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;"+from django.test import SimpleTestCase
+from .. import forms
+
+
+class TestForms(SimpleTestCase):
+
+    def test_memory_form_valid_data(self):
+        form = forms.MemoryForm(data={
+            'name': 'Town',
+            'comment': 'Some comment'
+        })
+
+        self.assertTrue(form.is_valid())
+
+    def test_memory_form_no_data(self):
+        form = forms.MemoryForm(data={})
+
+        self.assertFalse(form.is_valid())
+        self.assertEquals(len(form.errors), 2)"
KO;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;
OK;8;savabush;places-remember;0b80f691eb2b8fbdac990130a94229bdeb93dd54;Added docs to handler va_access_token, added marker to map on addmemory, created unit-tests;"+from django.test import TestCase, Client
+from django.urls import reverse
+from django.contrib.auth.models import User
+from .. import models
+
+
+class TestViews(TestCase):
+
+    def setUp(self):
+        self.client = Client()
+        User.objects.create_user('test_user')
+        author = User.objects.get(username='test_user')
+        self.project1 = models.Memory.objects.create(
+            author=author,
+            name='Town',
+            comment='Some comment',
+            point_lng='50',
+            point_lat='50'
+        )
+
+    def test_index_get(self):
+        response = self.client.get(reverse('index'))
+
+        self.assertURLEqual(response.status_code, 200)
+        self.assertTemplateUsed(response, 'core/index.html')
+        self.assertEquals(models.Memory.objects.all().count(), 1)
+
+    def test_index_post(self):
+        object_of_model = models.Memory.objects.get(name='Town')
+        object_of_model.delete()
+        self.assertEquals(models.Memory.objects.all().count(), 0)
+
+    def test_addmemory_get(self):
+        response = self.client.get(reverse('addmemory'))
+
+        self.assertURLEqual(response.status_code, 200)
+        self.assertTemplateUsed(response, 'core/addmemory.html')"
KO;8;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el número de taps que realice el usuario
 
-    if taps==64:
        up()
        goto(0, 0)
        color (""red"")"
OK;8;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el número de taps que realice el usuario
 
+    if taps==300:
        up()
        goto(0, 0)
        color (""red"")"
KO;9;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
-    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     val_dataset = MapDataset(VALIDATION)
-    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
OK;9;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
+    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     val_dataset = MapDataset(VALIDATION)
+    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
KO;10;douglaspetrin;fmproject;07c305354514a508b9a5c11629e8cedb5d35a05c;removes test because of lack of memory in free circle-ci plan;"jobs:
             apk add --no-cache py-pip=9.0.0-r1
             pip install docker-compose==1.15.0
       - run:
-          name: Run tests
           command: |
-            docker-compose build
-            docker-compose up -d
-            docker-compose exec web pytest --cov=fmapi tests/
\ No newline at end of file
\ No newline at end of file"
OK;10;douglaspetrin;fmproject;07c305354514a508b9a5c11629e8cedb5d35a05c;removes test because of lack of memory in free circle-ci plan;"jobs:
             apk add --no-cache py-pip=9.0.0-r1
             pip install docker-compose==1.15.0
       - run:
+          name: Build
           command: |
\ No newline at end of file
+            docker-compose build
\ No newline at end of file"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"if(USE_GTEST)
     find_package(GTest REQUIRED)
   endif()
   if(GTEST_FOUND)
     enable_testing()
     include(CTest)
   endif()
@@ -626,7 +645,7 @@ if(GTEST_FOUND)
   add_executable(cpptest ${TEST_SRCS})
   # include runtime files for unit testing
   target_include_directories(cpptest PUBLIC ""src/runtime"")
-  target_link_libraries(cpptest PRIVATE ${TVM_TEST_LIBRARY_NAME} GTest::GTest GTest::Main pthread dl)
   set_target_properties(cpptest PROPERTIES EXCLUDE_FROM_ALL 1)
   set_target_properties(cpptest PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD 1)
   # For some reason, compile definitions are not propagated correctly, so we manually add them here"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"if(USE_GTEST)
     find_package(GTest REQUIRED)
   endif()
   if(GTEST_FOUND)
+    if(NOT TARGET GTest::gmock)
+      # GMock is formally supported in CMake 3.20; for now, expect libgmock.a in the same directory,
+      # and require that folks compiling against GTest::gmock also link against GTest::GTest
+      # (for the includes dir).
+      add_library(GTest::gmock STATIC IMPORTED GLOBAL)
+      get_target_property(GTEST_LIB_PATH GTest::GTest IMPORTED_LOCATION)
+      if(""${GTEST_LIB_PATH}"" STREQUAL ""GTEST_LIB_PATH-NOTFOUND"")
+        # CMake >= 3.20 makes GTest::GTest into a compatibility target. The real import location is in
+        # GTest::gtest.
+        get_target_property(GTEST_LIB_PATH GTest::gtest IMPORTED_LOCATION)
+        if(""${GTEST_LIB_PATH}"" STREQUAL ""GTEST_LIB_PATH-NOTFOUND"")
+          message(FATAL_ERROR ""Neither GTest::GTest nor GTets::gtest targets defined IMPORTED_LOCATION"")
+        endif()
+      endif()
+      get_filename_component(GTEST_LIB_DIR ""${GTEST_LIB_PATH}"" DIRECTORY)
+      set_target_properties(GTest::gmock PROPERTIES
+          IMPORTED_LOCATION ""${GTEST_LIB_DIR}/libgmock.a"")
+    endif()
+
     enable_testing()
     include(CTest)
   endif()
@@ -626,7 +645,7 @@ if(GTEST_FOUND)
   add_executable(cpptest ${TEST_SRCS})
   # include runtime files for unit testing
   target_include_directories(cpptest PUBLIC ""src/runtime"")
+  target_link_libraries(cpptest PRIVATE ${TVM_TEST_LIBRARY_NAME} GTest::GTest GTest::Main GTest::gmock pthread dl)
   set_target_properties(cpptest PROPERTIES EXCLUDE_FROM_ALL 1)
   set_target_properties(cpptest PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD 1)
   # For some reason, compile definitions are not propagated correctly, so we manually add them here"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class MetadataNode : public MetadataBaseNode {
  public:
   explicit MetadataNode(const struct ::TVMMetadata* data) : data_{data} {}
   static constexpr const char* _type_key = ""metadata.MetadataNode"";
   inline int64_t version() const { return int64_t(data_->version); }
   inline int64_t num_inputs() const { return data_->num_inputs; }
   ArrayAccessor<struct TVMTensorInfo, TensorInfo> inputs();
@@ -141,6 +142,7 @@ class TensorInfoNode : public MetadataBaseNode {
  public:
   explicit TensorInfoNode(const struct ::TVMTensorInfo* data) : data_{data} {}
   static constexpr const char* _type_key = ""metadata.TensorInfoNode"";
   inline ::tvm::runtime::String name() const { return ::tvm::runtime::String(data_->name); }
   inline int64_t num_shape() const { return data_->num_shape; }
   inline ::tvm::support::Span<const int64_t, int64_t> shape() const {"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class MetadataNode : public MetadataBaseNode {
  public:
   explicit MetadataNode(const struct ::TVMMetadata* data) : data_{data} {}
   static constexpr const char* _type_key = ""metadata.MetadataNode"";
+  const char* get_c_struct_name() const override;
   inline int64_t version() const { return int64_t(data_->version); }
   inline int64_t num_inputs() const { return data_->num_inputs; }
   ArrayAccessor<struct TVMTensorInfo, TensorInfo> inputs();
@@ -141,6 +142,7 @@ class TensorInfoNode : public MetadataBaseNode {
  public:
   explicit TensorInfoNode(const struct ::TVMTensorInfo* data) : data_{data} {}
   static constexpr const char* _type_key = ""metadata.TensorInfoNode"";
+  const char* get_c_struct_name() const override;
   inline ::tvm::runtime::String name() const { return ::tvm::runtime::String(data_->name); }
   inline int64_t num_shape() const { return data_->num_shape; }
   inline ::tvm::support::Span<const int64_t, int64_t> shape() const {"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"namespace metadata {
  */
 class MetadataBaseNode : public ::tvm::runtime::Object {
  public:
   static constexpr const char* _type_key = ""metadata.MetadataBaseNode"";
   TVM_DECLARE_BASE_OBJECT_INFO(MetadataBaseNode, ::tvm::runtime::Object);
 };
@@ -157,7 +159,7 @@ class ArrayAccessor<const char*, ::tvm::runtime::String> {
  *
  * These are separate from TIR DataType because TIR does not model structs.
  */
-enum MetadataTypeIndex : uint8_t {
   kUint64 = 0,
   kInt64 = 1,
   kBool = 2,
@@ -173,20 +175,37 @@ enum MetadataTypeIndex : uint8_t {
  */
 class MetadataArrayNode : public MetadataBaseNode {
  public:
-  MetadataArrayNode(Array<ObjectRef> array, MetadataTypeIndex type_index, const char* struct_name)
-      : array(::std::move(array)), type_index{type_index}, struct_name{struct_name} {}
 
   Array<ObjectRef> array;
-  MetadataTypeIndex type_index;
-  const char* struct_name;
   static constexpr const char* _type_key = ""metadata.MetadataArrayNode"";
   TVM_DECLARE_BASE_OBJECT_INFO(MetadataArrayNode, MetadataBaseNode);
 };
 
 /*! \brief Reference class for MetadataArray. */
 class MetadataArray : public MetadataBase {
  public:
-  MetadataArray(Array<ObjectRef> array, MetadataTypeIndex type_index, const char* struct_name);
 
   TVM_DEFINE_MUTABLE_OBJECT_REF_METHODS(MetadataArray, MetadataBase, MetadataArrayNode);
 };"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"namespace metadata {
  */
 class MetadataBaseNode : public ::tvm::runtime::Object {
  public:
+  virtual const char* get_c_struct_name() const = 0;
+
   static constexpr const char* _type_key = ""metadata.MetadataBaseNode"";
   TVM_DECLARE_BASE_OBJECT_INFO(MetadataBaseNode, ::tvm::runtime::Object);
 };
@@ -157,7 +159,7 @@ class ArrayAccessor<const char*, ::tvm::runtime::String> {
  *
  * These are separate from TIR DataType because TIR does not model structs.
  */
+enum MetadataKind : uint8_t {
   kUint64 = 0,
   kInt64 = 1,
   kBool = 2,
@@ -173,20 +175,37 @@ enum MetadataTypeIndex : uint8_t {
  */
 class MetadataArrayNode : public MetadataBaseNode {
  public:
+  MetadataArrayNode(Array<ObjectRef> array, MetadataKind kind, const char* type_key)
+      : array(::std::move(array)), kind{kind}, type_key{type_key} {}
+
+  const char* get_c_struct_name() const final;
+
+  std::string get_element_c_struct_name() const {
+    CHECK(kind == MetadataKind::kMetadata)
+        << ""cannot get struct name for MetadataArray with kind="" << kind;
+    constexpr int prefix_size = sizeof(""metadata."") - 1;
+    constexpr int suffix_size = sizeof(""Node"") - 1;
+    std::string type_key_str(type_key);
+    return std::string(""TVM"") +
+           type_key_str.substr(prefix_size, type_key_str.size() - prefix_size - suffix_size);
+  }
 
   Array<ObjectRef> array;
+
+  /*! \brief Describes the storage class of the emitted struct member. */
+  MetadataKind kind;
+
+  /*! \brief When `kind` is Metadata, type_key of the MetadataBaseNode used with this array. */
+  const char* type_key;
+
   static constexpr const char* _type_key = ""metadata.MetadataArrayNode"";
   TVM_DECLARE_BASE_OBJECT_INFO(MetadataArrayNode, MetadataBaseNode);
 };
 
 /*! \brief Reference class for MetadataArray. */
 class MetadataArray : public MetadataBase {
  public:
+  MetadataArray(Array<ObjectRef> array, MetadataKind kind, const char* struct_name);
 
   TVM_DEFINE_MUTABLE_OBJECT_REF_METHODS(MetadataArray, MetadataBase, MetadataArrayNode);
 };"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class PreflattenedBufferMap(SpecialStmt):
     Example
     -------
     .. code-block:: python
-         T.preflattened_buffer_map({})
     """"""
 
     def __init__(self):
@@ -892,12 +893,30 @@ def preflattened_buffer(
             for key, value in self.context.func_buffer_map.items():
                 if value.same_as(postflattened):
                     param = key
 
             assert (
                 param is not None
             ), f""Post-flatten buffer {postflattened.name} does not appear in the buffer map.""
 
             buffer_name: str = f""{postflattened.name}_preflatten""
             preflattened = tvm.tir.decl_buffer(
                 shape,
                 dtype,"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class PreflattenedBufferMap(SpecialStmt):
     Example
     -------
     .. code-block:: python
+         A0 = T.match_buffer(A, (48,), dtype=""float32"")
+         T.preflattened_buffer_map(A, (1, 4, 4, 3), elem_offset=1, align=4, dtype=""float32"")
     """"""
 
     def __init__(self):
@@ -892,12 +893,30 @@ def preflattened_buffer(
             for key, value in self.context.func_buffer_map.items():
                 if value.same_as(postflattened):
                     param = key
+                    break
 
             assert (
                 param is not None
             ), f""Post-flatten buffer {postflattened.name} does not appear in the buffer map.""
 
+            if data is None:
+                data = self.context.func_buffer_map[param].data
+
             buffer_name: str = f""{postflattened.name}_preflatten""
+            if align != -1:
+                if isinstance(align, IntImm):
+                    align = align.value
+                else:
+                    assert isinstance(align, int), f""align: want int or IntImm, got {align!r}""
+
+            if offset_factor != 0:
+                if isinstance(offset_factor, IntImm):
+                    offset_factor = offset_factor.value
+                else:
+                    assert isinstance(
+                        offset_factor, int
+                    ), f""offset_factor: want int or IntImm, got {offset_factor!r}""
+
             preflattened = tvm.tir.decl_buffer(
                 shape,
                 dtype,"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" # pylint: disable=invalid-name, import-outside-toplevel, unused-variable
 """"""Common utility functions in TVM tir""""""
 import inspect
 import tvm
 from tvm.ir.diagnostics import override_renderer
 
 
 def check_error(func, rel_lineno):
     """"""check if TIR script throws error""""""
     # Override the default renderer to accumulate errors
@@ -46,3 +50,12 @@ def render(e):
         assert (
             d.span.line - 1 == rel_lineno
         ), f""Expected error to be on line {rel_lineno}, but it was on {d.span.line - 1}"""
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" # pylint: disable=invalid-name, import-outside-toplevel, unused-variable
 """"""Common utility functions in TVM tir""""""
 import inspect
+import re
 import tvm
 from tvm.ir.diagnostics import override_renderer
 
 
+CHECK_ERROR_RE = re.compile(r""^.*# check_error: (.+)$"")
+
+
 def check_error(func, rel_lineno):
     """"""check if TIR script throws error""""""
     # Override the default renderer to accumulate errors
@@ -46,3 +50,12 @@ def render(e):
         assert (
             d.span.line - 1 == rel_lineno
         ), f""Expected error to be on line {rel_lineno}, but it was on {d.span.line - 1}""
+
+    error_line = source_code.split(""\n"")[rel_lineno]
+    m = CHECK_ERROR_RE.match(error_line)
+    if m:
+        expected_error_text = m.group(1)
+        errors = [e.message for e in errors]
+        assert (
+            expected_error_text in errors
+        ), f'check_error expects ""{expected_error_text} in str(errors): {errors}'"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"Doc TIRTextPrinter::PrintPrimFunc(const PrimFunc& prim_func) {
     doc << Doc::Indent(
         2, Doc::NewLine() << ""buffer_map = {"" << PrintSep(buffer_map_doc, Doc::Text("", "")) << ""}"");
   }
   doc << PrintBody(op->body);
   return doc;
 }"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"Doc TIRTextPrinter::PrintPrimFunc(const PrimFunc& prim_func) {
     doc << Doc::Indent(
         2, Doc::NewLine() << ""buffer_map = {"" << PrintSep(buffer_map_doc, Doc::Text("", "")) << ""}"");
   }
+
+  if (op->preflattened_buffer_map.size() != 0) {
+    // print preflattened_buffer_map
+    std::vector<Doc> preflattened_buffer_map_doc;
+    for (auto& v : op->preflattened_buffer_map) {
+      preflattened_buffer_map_doc.push_back(Print(v.first) << "": "" << Print(v.second));
+    }
+    doc << Doc::Indent(2, Doc::NewLine()
+                              << ""preflattened_buffer_map = {""
+                              << PrintSep(preflattened_buffer_map_doc, Doc::Text("", "")) << ""}"");
+  }
   doc << PrintBody(op->body);
   return doc;
 }"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class AOTOnDemandAllocator : public transform::DeviceAwareExprVisitor {
 /*! \brief Code generator for AOT executor */
 class AOTExecutorCodegen : public MixedModeVisitor {
  protected:
-  /*!
-   * \brief Utility function to allocate a DLTensor or TVMValue
-   * \param  type the type of allocation
-   * \param num the number of variable to allocate on the stack
-   * \return PrimExpr representing the allocated object
-   */
-  PrimExpr StackAlloca(std::string type, size_t num) {
-    Array<PrimExpr> args = {tir::StringImm(type), ConstInt32(num)};
-    return tir::Call(DataType::Handle(), tir::builtin::tvm_stack_alloca(), args);
-  }
-
-  /*!
-   * \brief Utility function to convert a concrete integer to a PrimExpr.
-   * \param num the number to convert
-   * \return PrimExpr representing num
-   */
-  inline PrimExpr ConstInt32(int32_t num) {
-    ICHECK_LE(num, std::numeric_limits<int>::max());
-    return tir::make_const(DataType::Int(32), static_cast<int>(num));
-  }
 
   /*!
    * \brief Return a vector of variables that represents the sids for the given Relay Expr
@@ -323,6 +369,21 @@ class AOTExecutorCodegen : public MixedModeVisitor {
     }
   }
 
   void PushArgs(const Expr& expr, const std::vector<tir::Var>& sids, Array<PrimExpr>* args) {
     const TupleNode* t = expr.as<TupleNode>();
     if (t != nullptr) {
@@ -338,12 +399,9 @@ class AOTExecutorCodegen : public MixedModeVisitor {
    * returns the passed Call
    */
   tir::Call AddCheckReturn(tir::Call existing_call) {
-    if (use_unpacked_api_) {
-      Array<PrimExpr> args = {ConstInt32(0), ConstInt32(-1), existing_call};
-      return tir::Call(DataType::Int(32), tir::builtin::tvm_check_return(), args);
-    }
-
-    return existing_call;
   }
 
   /*!
@@ -378,56 +436,59 @@ class AOTExecutorCodegen : public MixedModeVisitor {
     auto result_expr_sid = PackSid(result_expr);
     PushArgs(result_expr, result_expr_sid, &args);
 
-    // Choose call style based on Runtime/Executor config.
-    Op calling_pattern;
-    if (use_unpacked_api_) {
-      calling_pattern = tvm::tir::builtin::call_extern();
-    } else if (use_call_cpacked_) {
-      calling_pattern = tvm::tir::builtin::tvm_call_cpacked();
-    } else {
-      calling_pattern = tvm::tir::builtin::tvm_call_packed();
-    }
-
     GlobalVar global_var = call_lowered_props.lowered_func;
-    tir::Var empty_var(""no_device_context"", DataType::Handle());
     bool has_c_device_api_context = device_contexts_.count(global_var) != 0;
 
-    // The device context is passed to the operator in one of the following calling patterns:
-    //  * Unpacked / direct function call with context:
-    //      operator(arg0, arg1, device_context);
-    //  * Unpacked / direct function call without context:
-    //      operator(arg0, arg1);
-    //  * Type-erased packed function call with context:
-    //      operator(args, type_codes, int num_args, out_ret_value, out_ret_tcode,
-    //      device_context_my_device)
-    //  * Type-erased packed function call without context (we create an empty var for codegen):
-    //      operator(args, type_codes, int num_args, out_ret_value, out_ret_tcode,
-    //      no_device_context)
     if (has_c_device_api_context) {
-      // call_extern calling convention with context
-      tir::Var context = device_contexts_.Get(global_var).value();
-      args.push_back(context);
-
-      tir::Evaluate func_call(
-          AddCheckReturn(tvm::tir::Call(DataType::Int(32), calling_pattern, args)));
-      create_func_call_stmts.push_back(tir::SeqStmt({
-          GenerateDeviceHook(context, ""Open""),
           func_call,
-          GenerateDeviceHook(context, ""Close""),
       }));
-    } else if (use_call_cpacked_) {
-      // call_cpacked calling convention needs a blank context
-      args.push_back(tir::make_zero(DataType::Handle()));
-      tir::Evaluate func_call(tvm::tir::Call(DataType::Int(32), calling_pattern, args));
-      create_func_call_stmts.push_back(func_call);
-    } else {
-      // call_extern calling convention without context
-      tir::Evaluate func_call(
-          AddCheckReturn(tvm::tir::Call(DataType::Int(32), calling_pattern, args)));
-      create_func_call_stmts.push_back(func_call);
     }
 
-    tir::Stmt body = tir::SeqStmt(create_func_call_stmts);
     stmts_.push_back(body);
   }
 
@@ -446,9 +507,9 @@ class AOTExecutorCodegen : public MixedModeVisitor {
     te::Var loop_idx(""i"", DataType::Int(32));
     auto retval_i = tir::BufferLoad(tmp_read, {loop_idx});
     // Copy the variable from the input to the output
-    tir::Stmt copy =
-        tir::For(loop_idx, 0, ConstInt32(size), tir::ForKind::kSerial,
-                 tir::BufferStore(tmp_write, tir::Let(tmp_read->data, in, retval_i), {loop_idx}));
     stmts_.push_back(tir::LetStmt(tmp_write->data, out, copy));
   }
 
@@ -692,7 +753,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
       for (int i = 0; i < ndim; i++) {
         int shape = kv.second->data->shape[i];
-        extents.push_back(tir::make_const(DataType::Int(32), shape));
       }
       body = tir::AllocateConst(buffer_var, dtype, extents, kv.second->data, body);
     }
@@ -855,30 +916,10 @@ class AOTExecutorCodegen : public MixedModeVisitor {
   /*! \brief target host */
   Target target_host_;
   /*!
-   * \brief unpacked api toggle
-   * When set to true, the generated code will use unpacked calls to functions:
-   * func(void* arg0, void* arg1)
-   * Rather than packed calls (in which arg0 and arg1 are in `arg_values`).
-   * func(TVMValue* arg_values, int* arg_type_codes, int num_args, ...)
-   * Defaults to using the packed calling convention
-   *
-   * Unpacked API is supported when runtime == ""c"" and interface_api is ""c"".
-   */
-  Bool use_unpacked_api_;
-  /*!
-   * \brief cpacked api toggle
-   * When set to true, the generated code will use call_cpacked to call functions directly, assuming
-   * they exist in a DSO-exportable module:
-   * func(...)
-   * Rather than through the traditional call_packed calls, which should use function pointers
-   * looked-up through TVMBackendGetFuncFromEnv:
-   * TVMBackendPackedCFunc* func_ptr = TVMBackendGetFuncFromEnv(""func"");
-   * func_ptr(...)
-   * Defaults to using the packed calling convention
-   *
-   * call_cpacked is required when runtime is ""c++"" and supported when runtime is ""c""
    */
-  Bool use_call_cpacked_;
 
   /*!
    * \brief parameters (i.e. ConstantNodes found in the graph).
@@ -907,11 +948,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
  public:
   AOTExecutorCodegen(runtime::Module* mod, const tec::TargetMap& targets, Target target_host)
-      : mod_(mod),
-        targets_(targets),
-        target_host_(target_host),
-        use_unpacked_api_(Bool(false)),
-        use_call_cpacked_(Bool(false)) {}
 
   LoweredOutput Codegen(IRModule mod, relay::Function func, String mod_name) {
     VLOG_CONTEXT << ""AOT"";
@@ -923,23 +960,36 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
     Runtime runtime_config = mod->GetAttr<Runtime>(tvm::attr::kRuntime).value();
     Executor executor_config = mod->GetAttr<Executor>(tvm::attr::kExecutor).value();
-    String interface_api = executor_config->GetAttr<String>(""interface-api"").value_or(""packed"");
     Integer workspace_byte_alignment =
         executor_config->GetAttr<Integer>(""workspace-byte-alignment"").value_or(16);
-    use_unpacked_api_ = executor_config->GetAttr<Bool>(""unpacked-api"").value_or(Bool(false));
-    use_call_cpacked_ = !use_unpacked_api_;
 
     // Validate choice of use_unpacked_api_ and use_call_cpacked_
     if (runtime_config->name == kTvmRuntimeCrt) {
-      ICHECK(interface_api == ""packed"" || static_cast<bool>(use_unpacked_api_) == true)
-          << ""Either need interface_api == \""packed\"" (got: "" << interface_api
-          << "") or unpacked-api == true (got: "" << use_unpacked_api_
-          << "") when targeting c runtime"";
     } else if (runtime_config->name == kTvmRuntimeCpp) {
-      ICHECK(static_cast<bool>(use_unpacked_api_) == false)
-          << ""Need unpacked-api == false (got: "" << use_unpacked_api_
-          << "") and interface-api == \""packed\"" (got: "" << interface_api
-          << "") when targeting c++ runtime"";
     } else {
       ICHECK(false) << ""runtime_config ("" << runtime_config->name
                     << "") is not one of the expected values"";
@@ -1037,7 +1087,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
     // Legalize AOT if needed. This means that all the packed calls
     // need to be wrapped in TVMValues (unless use_unpacked_api is set)
-    if (!use_unpacked_api_) {
       auto pack_calls = tir::transform::LegalizePackedCalls();
       lowered_mod = pack_calls(lowered_mod);
     }
@@ -1106,7 +1156,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
     ret.metadata = ExecutorCodegenMetadata(
         inputs, input_tensor_types, output_var_names, output_tensor_types, pool_vars, devices,
-        runtime::kTvmExecutorAot, mod_name, interface_api, use_unpacked_api_, pool_var_info);
     return ret;
   }
 "
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class AOTOnDemandAllocator : public transform::DeviceAwareExprVisitor {
 /*! \brief Code generator for AOT executor */
 class AOTExecutorCodegen : public MixedModeVisitor {
  protected:
+  /*! \brief Describes the type of kernel call emitted. */
+  enum CallType {
+    /*!
+     * \brief Emit PackedFunc calls bound just-in-time using TVMBackend* functions.
+     *
+     * When this type is selected, assumes all operators must be called via TVMFuncCall. Given the
+     * implementation of TVMFuncCall in the C++ runtime, this in practice implies that those
+     * functions are of type TVMBackendPackedCFunc.
+     *
+     * The following code is emitted at call sites to call a function named `func`:
+     * void* func_ptr = TVMBackendGetFuncFromEnv(""func"");
+     * TVMFuncCall(func_ptr, values, tcodes, num_args, ret_values, ret_tcodes)
+     *
+     * The arguments given to the tir::Call node are encoded into `values`, `tcodes`, and `num_args`
+     * by LowerTVMBuiltin TIR transform.
+     *
+     * If `resource_handle` is passed to `func`, it is determined by TVMFuncCall (often,
+     * `resource_handle` is registered with the C++ runtime to provide a `this` equivalent when
+     * `func` is implemented in C).
+     *
+     * Compatible with both C++ and C runtimes, implemented with the C runtime only.
+     */
+    kPacked,  // Emit tir.call_packed and wrap all arguments in DLTensor.
+
+    /*!
+     * \brief Directly call a TVMBackendPackedCFunc named according to the tir::Call.
+     *
+     * When this type is selected, assumes all operators are implemented in functions of type
+     * `TVMBackendPackedCFunc` and should be called directly. That is, presumes at the time of
+     * downstream compilation that there is a symbol named after the 0th arg to tir::Call of
+     * type `TVMBackendPackedCFunc`. This situation should occur when target_host == target.
+     *
+     * The following code is emitted at call sites to call a function named `func`:
+     * func(values, tcodes, num_args, ret_values, ret_tcodes, resource_handle)
+     *
+     * The arguments given to the tir::Call node are encoded into `values`, `tcodes`, and `num_args`
+     * by LowerTVMBuiltin TIR transform.
+     *
+     * `resource_handle` is encoded as the final argument to the tir::Call node. In practice, it is
+     * always the device context parameter when not null. At present, the implementation does not
+     * support forwarding device context parameters to CPacked.
+     *
+     * Compatible with the C runtime and C++ runtime (so long as target_host == target). Implemented
+     * in the same scenarios.
+     */
+    kCPacked,  // Emit tir.call_cpacked and wrap all arguments in DLTensor.
+
+    /*! \brief Directly call a function accepting the `data` arrays as args.
+     *
+     * When this type is selected, assumes all operaotrs are implemented in C functions whose
+     * arguments are 1-to-1 with those in the tir::Call. DLTensor arguments are encoded as just the
+     * `data` parameters (i.e. no DLTensor object is passed along).
+     *
+     * The following code is emitted at call sites to a function named `func`:
+     * func(void* arg0, void* arg1, ..., void* argN) // no resource_handle
+     * -or-
+     * func(void* arg0, void* arg1, ..., void* argN, void* resource_handle) // with resource_handle
+     *
+     * `resource_handle` is encoded as the final argument to the tir::Call node. In practice, it is
+     * always the device context parameter when not null.
+     *
+     * Compatible with the C runtime and C++ runtime (so long as target_host == target). Implemented
+     * with the C runtime only.
+     */
+    kUnpacked,  // Emit tir.call_extern passing only the `data` part of DLTensors.
+  };
 
   /*!
    * \brief Return a vector of variables that represents the sids for the given Relay Expr
@@ -323,6 +369,21 @@ class AOTExecutorCodegen : public MixedModeVisitor {
     }
   }
 
+  /*!
+   * \brief Reverse lookup the device name in devices_ map.
+   * \param device_context Value in devices_ to find.
+   * \return Key matching device_context in devices_.
+   */
+  std::string FindDeviceName(tir::Var device_context) {
+    for (std::pair<String, tir::Var> kv : devices_) {
+      if (kv.second->name_hint == device_context->name_hint) {
+        return kv.first;
+      }
+    }
+    ICHECK(false) << ""Did not find a device name associated with "" << device_context;
+    return """";
+  }
+
   void PushArgs(const Expr& expr, const std::vector<tir::Var>& sids, Array<PrimExpr>* args) {
     const TupleNode* t = expr.as<TupleNode>();
     if (t != nullptr) {
@@ -338,12 +399,9 @@ class AOTExecutorCodegen : public MixedModeVisitor {
    * returns the passed Call
    */
   tir::Call AddCheckReturn(tir::Call existing_call) {
+    Array<PrimExpr> args = {tir::make_const(DataType::Int(32, 1), 0, Span()),
+                            tir::make_const(DataType::Int(32, 1), -1, Span()), existing_call};
+    return tir::Call(DataType::Int(32), tir::builtin::tvm_check_return(), args);
   }
 
   /*!
@@ -378,56 +436,59 @@ class AOTExecutorCodegen : public MixedModeVisitor {
     auto result_expr_sid = PackSid(result_expr);
     PushArgs(result_expr, result_expr_sid, &args);
 
     GlobalVar global_var = call_lowered_props.lowered_func;
     bool has_c_device_api_context = device_contexts_.count(global_var) != 0;
+    tir::Var device_context;
+    tir::Stmt func_call;
+
+    switch (call_type_) {
+      case CallType::kUnpacked: {
+        // call_extern calling convention with optional context
+        if (has_c_device_api_context) {
+          device_context = device_contexts_.Get(global_var).value();
+          args.push_back(device_context);
+        }
+        func_call = tir::Evaluate(AddCheckReturn(
+            tvm::tir::Call(DataType::Int(32), tvm::tir::builtin::call_extern(), args)));
+        break;
+      }
+      case CallType::kCPacked: {
+        if (has_c_device_api_context) {
+          device_context = device_contexts_.Get(global_var).value();
+          args.push_back(device_context);
+        } else {
+          // NOTE: LowerTVMBuiltin expects some device_context placeholder.
+          args.push_back(tir::make_zero(DataType::Handle()));
+        }
+        func_call = tir::Evaluate(
+            tvm::tir::Call(DataType::Int(32), tvm::tir::builtin::tvm_call_cpacked(), args));
+        create_func_call_stmts.push_back(func_call);
+        break;
+      }
+      case CallType::kPacked: {
+        // call_packed does not accept a device context.
+        CHECK(!has_c_device_api_context) << ""CallType::kPacked does not accept a device context"";
+        func_call = tir::Evaluate(AddCheckReturn(
+            tvm::tir::Call(DataType::Int(32), tvm::tir::builtin::tvm_call_packed(), args)));
+        create_func_call_stmts.push_back(func_call);
+        break;
+      }
+      default:
+        ICHECK(false) << ""Unknown CallType: "" << call_type_;
+    }
+
+    ICHECK(func_call.defined()) << ""Must define func_call"";
 
     if (has_c_device_api_context) {
+      func_call = tir::SeqStmt(Array<tir::Stmt>({
+          GenerateDeviceHook(device_context, ""Open""),
           func_call,
+          GenerateDeviceHook(device_context, ""Close""),
       }));
     }
 
+    tir::Stmt body = tir::SeqStmt({func_call});
+    LOG(INFO) << ""CreateFuncCall: "" << call_lowered_props.lowered_func->name_hint << "" -> "" << body;
     stmts_.push_back(body);
   }
 
@@ -446,9 +507,9 @@ class AOTExecutorCodegen : public MixedModeVisitor {
     te::Var loop_idx(""i"", DataType::Int(32));
     auto retval_i = tir::BufferLoad(tmp_read, {loop_idx});
     // Copy the variable from the input to the output
+    tir::Stmt copy = tir::For(
+        loop_idx, 0, tir::make_const(DataType::Int(32, 1), size, Span()), tir::ForKind::kSerial,
+        tir::BufferStore(tmp_write, tir::Let(tmp_read->data, in, retval_i), {loop_idx}));
     stmts_.push_back(tir::LetStmt(tmp_write->data, out, copy));
   }
 
@@ -692,7 +753,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
       for (int i = 0; i < ndim; i++) {
         int shape = kv.second->data->shape[i];
+        extents.push_back(tir::make_const(DataType::Int(32), shape, Span()));
       }
       body = tir::AllocateConst(buffer_var, dtype, extents, kv.second->data, body);
     }
@@ -855,30 +916,10 @@ class AOTExecutorCodegen : public MixedModeVisitor {
   /*! \brief target host */
   Target target_host_;
   /*!
+   * \brief The type of kernel call to be emitted.
+   * See CallType for more documentation.
    */
+  CallType call_type_;
 
   /*!
    * \brief parameters (i.e. ConstantNodes found in the graph).
@@ -907,11 +948,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
  public:
   AOTExecutorCodegen(runtime::Module* mod, const tec::TargetMap& targets, Target target_host)
+      : mod_(mod), targets_(targets), target_host_(target_host) {}
 
   LoweredOutput Codegen(IRModule mod, relay::Function func, String mod_name) {
     VLOG_CONTEXT << ""AOT"";
@@ -923,23 +960,36 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
     Runtime runtime_config = mod->GetAttr<Runtime>(tvm::attr::kRuntime).value();
     Executor executor_config = mod->GetAttr<Executor>(tvm::attr::kExecutor).value();
+    std::string interface_api =
+        executor_config->GetAttr<String>(""interface-api"").value_or(""packed"");
     Integer workspace_byte_alignment =
         executor_config->GetAttr<Integer>(""workspace-byte-alignment"").value_or(16);
+    bool unpacked_api = executor_config->GetAttr<Bool>(""unpacked-api"").value_or(Bool(false));
 
     // Validate choice of use_unpacked_api_ and use_call_cpacked_
     if (runtime_config->name == kTvmRuntimeCrt) {
+      if (unpacked_api == true) {
+        call_type_ = CallType::kUnpacked;
+      } else if (unpacked_api == false && interface_api == ""packed"") {
+        call_type_ = CallType::kCPacked;
+      } else {
+        CHECK(interface_api == ""packed"" || unpacked_api == true)
+            << ""Either need interface_api == \""packed\"" (got: "" << interface_api
+            << "") or unpacked-api == true (got: "" << unpacked_api << "") when targeting c runtime"";
+        ICHECK(false) << ""Unhandled executor option config: interface-api="" << interface_api
+                      << "", unpacked-api="" << unpacked_api;
+      }
     } else if (runtime_config->name == kTvmRuntimeCpp) {
+      if (unpacked_api == false && interface_api == ""packed"") {
+        call_type_ = CallType::kCPacked;
+      } else {
+        CHECK(static_cast<bool>(unpacked_api) == false && interface_api == ""packed"")
+            << ""Need unpacked-api == false (got: "" << unpacked_api
+            << "") and interface-api == \""packed\"" (got: "" << interface_api
+            << "") when targeting c++ runtime"";
+        ICHECK(false) << ""Unhandled executor option config: interface-api="" << interface_api
+                      << "", unpacked-api="" << unpacked_api;
+      }
     } else {
       ICHECK(false) << ""runtime_config ("" << runtime_config->name
                     << "") is not one of the expected values"";
@@ -1037,7 +1087,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
     // Legalize AOT if needed. This means that all the packed calls
     // need to be wrapped in TVMValues (unless use_unpacked_api is set)
+    if (call_type_ == CallType::kCPacked || call_type_ == CallType::kPacked) {
       auto pack_calls = tir::transform::LegalizePackedCalls();
       lowered_mod = pack_calls(lowered_mod);
     }
@@ -1106,7 +1156,7 @@ class AOTExecutorCodegen : public MixedModeVisitor {
 
     ret.metadata = ExecutorCodegenMetadata(
         inputs, input_tensor_types, output_var_names, output_tensor_types, pool_vars, devices,
+        runtime::kTvmExecutorAot, mod_name, interface_api, unpacked_api, pool_var_info);
     return ret;
   }
 "
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"  */
 
 /*!
- * \file tvm/runtime/metadata.h
  * \brief Defines implementations of TVM metadata which can exist in the runtime.
  */
 
@@ -47,20 +47,27 @@ ArrayAccessor<struct TVMTensorInfo, TensorInfo> MetadataNode::pools() {
 
 TVM_REGISTER_OBJECT_TYPE(MetadataBaseNode);
 
-MetadataArray::MetadataArray(Array<ObjectRef> array, MetadataTypeIndex type_index,
-                             const char* struct_name)
-    : MetadataBase{make_object<MetadataArrayNode>(array, type_index, struct_name)} {}
 
 TVM_REGISTER_OBJECT_TYPE(MetadataArrayNode);
 
 Metadata::Metadata(const struct ::TVMMetadata* data)
     : MetadataBase{make_object<MetadataNode>(data)} {}
 TVM_REGISTER_OBJECT_TYPE(MetadataNode);
 
 TensorInfo::TensorInfo(const struct ::TVMTensorInfo* data)
     : MetadataBase{make_object<TensorInfoNode>(data)} {}
 TVM_REGISTER_OBJECT_TYPE(TensorInfoNode);
 
 }  // namespace metadata
 
 class MetadataModuleNode : public ::tvm::runtime::ModuleNode {"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"  */
 
 /*!
+ * \file src/runtime/metadata.cc
  * \brief Defines implementations of TVM metadata which can exist in the runtime.
  */
 
@@ -47,20 +47,27 @@ ArrayAccessor<struct TVMTensorInfo, TensorInfo> MetadataNode::pools() {
 
 TVM_REGISTER_OBJECT_TYPE(MetadataBaseNode);
 
+MetadataArray::MetadataArray(Array<ObjectRef> array, MetadataKind kind, const char* struct_name)
+    : MetadataBase{make_object<MetadataArrayNode>(array, kind, struct_name)} {}
 
+const char* MetadataArrayNode::get_c_struct_name() const {
+  ICHECK(false) << ""MetadataArrayNode get_c_struct_name is unimplemented"";
+  return nullptr;
+}
 TVM_REGISTER_OBJECT_TYPE(MetadataArrayNode);
 
 Metadata::Metadata(const struct ::TVMMetadata* data)
     : MetadataBase{make_object<MetadataNode>(data)} {}
 TVM_REGISTER_OBJECT_TYPE(MetadataNode);
 
+const char* MetadataNode::get_c_struct_name() const { return ""TVMMetadata""; }
+
 TensorInfo::TensorInfo(const struct ::TVMTensorInfo* data)
     : MetadataBase{make_object<TensorInfoNode>(data)} {}
 TVM_REGISTER_OBJECT_TYPE(TensorInfoNode);
 
+const char* TensorInfoNode::get_c_struct_name() const { return ""TVMTensorInfo""; }
+
 }  // namespace metadata
 
 class MetadataModuleNode : public ::tvm::runtime::ModuleNode {"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" #include <algorithm>
 #include <memory>
 #include <unordered_map>
 
 #include ""../func_registry_generator.h""
 
 namespace tvm {
 namespace codegen {
@@ -74,8 +76,7 @@ void CodeGenCPU::Init(const std::string& module_name, llvm::TargetMachine* tm,
   //                                      void* resource_handle);
   ftype_tvm_backend_packed_c_func_ = llvm::FunctionType::get(
       t_int_,
-      {t_tvm_func_handle_, t_tvm_value_->getPointerTo(), t_int_->getPointerTo(), t_int_,
-       t_tvm_value_->getPointerTo(), t_int_->getPointerTo(), t_void_p_},
       false);
   t_tvm_crt_func_registry_ = llvm::StructType::create(
       {t_char_->getPointerTo(), ftype_tvm_backend_packed_c_func_->getPointerTo()});
@@ -802,10 +803,10 @@ llvm::Value* CodeGenCPU::GetPackedFuncHandle(const std::string& fname) {
 
 CodeGenCPU::PackedCall CodeGenCPU::MakeCallPackedLowered(const Array<PrimExpr>& args,
                                                          const DataType& r_type,
-                                                         const int64_t begin, const int64_t end) {
   PackedCall pc;
   std::string func_name = args[0].as<StringImmNode>()->value;
-  llvm::Value* handle = GetPackedFuncHandle(func_name);
   // call the function
   int64_t nargs = end - begin;
   ICHECK_GE(nargs, 0);
@@ -822,14 +823,43 @@ CodeGenCPU::PackedCall CodeGenCPU::MakeCallPackedLowered(const Array<PrimExpr>&
   TypedPointer ret_tcode =
       CreateBufferPtr(stack_tcode, DataType::Int(32), {ConstInt32(end)}, DataType::Int(32));
 
 #if TVM_LLVM_VERSION >= 90
-  auto call_callee = llvm::FunctionCallee(ftype_tvm_func_call_, RuntimeTVMFuncCall());
 #else
-  auto call_callee = RuntimeTVMFuncCall();
 #endif
-  llvm::Value* call = builder_->CreateCall(
-      call_callee,
-      {handle, arg_value, arg_tcode.addr, ConstInt32(nargs), ret_value, ret_tcode.addr});
   llvm::BasicBlock* end_block = CheckCallSuccess(call);
 
   // Load the return value and cast it to the designated type (r_type).
@@ -858,17 +888,18 @@ CodeGenCPU::PackedCall CodeGenCPU::MakeCallPackedLowered(const Array<PrimExpr>&
   return pc;
 }
 
-llvm::Value* CodeGenCPU::CreateCallPacked(const CallNode* op) {
-  ICHECK_EQ(op->args.size(), 5U);
   PackedCall pc = MakeCallPackedLowered(op->args, op->dtype, op->args[3].as<IntImmNode>()->value,
-                                        op->args[4].as<IntImmNode>()->value);
   return pc.ret_value;
 }
 
 llvm::Value* CodeGenCPU::CreateCallTracePacked(const CallNode* op) {
   ICHECK_EQ(op->args.size(), 6U);
   PackedCall pc = MakeCallPackedLowered(op->args, op->dtype, op->args[3].as<IntImmNode>()->value,
-                                        op->args[4].as<IntImmNode>()->value);
   // Get traced value.
   llvm::Value* traced_value = MakeValue(op->args[5]);
   // The update_block handles case when we need to update the return value.
@@ -914,6 +945,306 @@ llvm::Value* CodeGenCPU::RuntimeTVMParallelBarrier() {
   return GetContextPtr(gv_tvm_parallel_barrier_);
 }
 
 void CodeGenCPU::DefineFunctionRegistry(Array<String> func_names) {
   ICHECK(is_system_lib_) << ""Loading of --system-lib modules is yet to be defined for C runtime"";
   Array<String> symbols;
@@ -980,9 +1311,11 @@ void CodeGenCPU::AddStartupFunction() {
 
 llvm::Value* CodeGenCPU::CreateIntrinsic(const CallNode* op) {
   if (op->op.same_as(builtin::tvm_call_packed_lowered())) {
-    return CreateCallPacked(op);
   } else if (op->op.same_as(builtin::tvm_call_trace_packed_lowered())) {
     return CreateCallTracePacked(op);
   } else if (op->op.same_as(builtin::tvm_static_handle())) {
     return CreateStaticHandle();
   } else if (op->op.same_as(builtin::tvm_throw_last_error())) {
@@ -1052,6 +1385,7 @@ void CodeGenCPU::VisitStmt_(const AssertStmtNode* op) {
   builder_->CreateCondBr(cond, end_block, fail_block, md_very_likely_branch_);
   // fail condition.
   builder_->SetInsertPoint(fail_block);
 #if TVM_LLVM_VERSION >= 90
   auto err_callee =
       llvm::FunctionCallee(ftype_tvm_api_set_last_error_, RuntimeTVMAPISetLastError());"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" #include <algorithm>
 #include <memory>
 #include <unordered_map>
+#include <unordered_set>
 
 #include ""../func_registry_generator.h""
+#include ""../metadata_utils.h""
 
 namespace tvm {
 namespace codegen {
@@ -74,8 +76,7 @@ void CodeGenCPU::Init(const std::string& module_name, llvm::TargetMachine* tm,
   //                                      void* resource_handle);
   ftype_tvm_backend_packed_c_func_ = llvm::FunctionType::get(
       t_int_,
+      {t_void_p_, t_int_->getPointerTo(), t_int_, t_void_p_, t_int_->getPointerTo(), t_void_p_},
       false);
   t_tvm_crt_func_registry_ = llvm::StructType::create(
       {t_char_->getPointerTo(), ftype_tvm_backend_packed_c_func_->getPointerTo()});
@@ -802,10 +803,10 @@ llvm::Value* CodeGenCPU::GetPackedFuncHandle(const std::string& fname) {
 
 CodeGenCPU::PackedCall CodeGenCPU::MakeCallPackedLowered(const Array<PrimExpr>& args,
                                                          const DataType& r_type,
+                                                         const int64_t begin, const int64_t end,
+                                                         bool use_string_lookup) {
   PackedCall pc;
   std::string func_name = args[0].as<StringImmNode>()->value;
   // call the function
   int64_t nargs = end - begin;
   ICHECK_GE(nargs, 0);
@@ -822,14 +823,43 @@ CodeGenCPU::PackedCall CodeGenCPU::MakeCallPackedLowered(const Array<PrimExpr>&
   TypedPointer ret_tcode =
       CreateBufferPtr(stack_tcode, DataType::Int(32), {ConstInt32(end)}, DataType::Int(32));
 
+  llvm::FunctionType* callee_ftype = nullptr;
+  llvm::Value* callee_value = nullptr;
+  std::vector<llvm::Value*> call_args;
+
+  if (use_string_lookup) {
+    callee_ftype = ftype_tvm_func_call_;
+    callee_value = RuntimeTVMFuncCall();
+    call_args.push_back(GetPackedFuncHandle(func_name));
+    call_args.insert(call_args.end(),
+                     {arg_value, arg_tcode.addr, ConstInt32(nargs), ret_value, ret_tcode.addr});
+  } else {
+    callee_ftype = ftype_tvm_backend_packed_c_func_;
+    callee_value = module_->getFunction(func_name);
+    if (callee_value == nullptr) {
+      callee_value =
+          llvm::Function::Create(ftype_tvm_backend_packed_c_func_, llvm::Function::ExternalLinkage,
+                                 func_name, module_.get());
+    }
+
+    nargs -= 1;
+    call_args.insert(call_args.end(), {
+                                          builder_->CreateBitCast(arg_value, t_void_p_),
+                                          arg_tcode.addr,
+                                          ConstInt32(nargs),
+                                          builder_->CreateBitCast(ret_value, t_void_p_),
+                                          ret_tcode.addr,
+                                      });
+    call_args.push_back(llvm::ConstantPointerNull::get(t_void_p_));
+  }
 #if TVM_LLVM_VERSION >= 90
+  auto call_callee = llvm::FunctionCallee(callee_ftype, callee_value);
 #else
+  (void)callee_ftype;  // use callee_ftype to avoid unused variable warning when using older LLVM.
+  auto call_callee = callee_value;
 #endif
+  llvm::Value* call = builder_->CreateCall(call_callee, call_args);
+
   llvm::BasicBlock* end_block = CheckCallSuccess(call);
 
   // Load the return value and cast it to the designated type (r_type).
@@ -858,17 +888,18 @@ CodeGenCPU::PackedCall CodeGenCPU::MakeCallPackedLowered(const Array<PrimExpr>&
   return pc;
 }
 
+llvm::Value* CodeGenCPU::CreateCallPacked(const CallNode* op, bool use_string_lookup) {
+  auto expected_num_args = use_string_lookup ? 5U : 6U;
+  ICHECK_EQ(op->args.size(), expected_num_args);
   PackedCall pc = MakeCallPackedLowered(op->args, op->dtype, op->args[3].as<IntImmNode>()->value,
+                                        op->args[4].as<IntImmNode>()->value, use_string_lookup);
   return pc.ret_value;
 }
 
 llvm::Value* CodeGenCPU::CreateCallTracePacked(const CallNode* op) {
   ICHECK_EQ(op->args.size(), 6U);
   PackedCall pc = MakeCallPackedLowered(op->args, op->dtype, op->args[3].as<IntImmNode>()->value,
+                                        op->args[4].as<IntImmNode>()->value, true);
   // Get traced value.
   llvm::Value* traced_value = MakeValue(op->args[5]);
   // The update_block handles case when we need to update the return value.
@@ -914,6 +945,306 @@ llvm::Value* CodeGenCPU::RuntimeTVMParallelBarrier() {
   return GetContextPtr(gv_tvm_parallel_barrier_);
 }
 
+/*! \brief Defines LLVM Types for each Metadata member type. */
+struct MetadataLlvmTypes {
+  llvm::Type* t_float64;
+  llvm::Type* t_uint8;
+  llvm::Type* t_int64;
+  llvm::Type* t_bool;
+  llvm::Type* t_cstring;
+  llvm::Type* t_void_p;
+  llvm::StructType* t_data_type;
+
+  /*! \brief Maps a MetadataBase subclass' type_key to its corresponding LLVM StructType. */
+  ::std::unordered_map<std::string, llvm::StructType*> structs_by_type_key;
+};
+
+class MetadataTypeDefiner : public AttrVisitor {
+ public:
+  MetadataTypeDefiner(llvm::LLVMContext* ctx, struct MetadataLlvmTypes* llvm_types)
+      : ctx_{ctx}, llvm_types_{llvm_types} {}
+
+  void Visit(const char* key, double* value) final {
+    elements_.emplace_back(llvm_types_->t_float64);
+  }
+  void Visit(const char* key, int64_t* value) final {
+    elements_.emplace_back(llvm_types_->t_int64);
+  }
+  void Visit(const char* key, uint64_t* value) final {
+    elements_.emplace_back(llvm_types_->t_int64);
+  }
+  void Visit(const char* key, int* value) final { elements_.emplace_back(llvm_types_->t_int64); }
+  void Visit(const char* key, bool* value) final { elements_.emplace_back(llvm_types_->t_bool); }
+  void Visit(const char* key, std::string* value) final {
+    elements_.emplace_back(llvm_types_->t_cstring);
+  }
+  void Visit(const char* key, void** value) final { elements_.emplace_back(llvm_types_->t_void_p); }
+  void Visit(const char* key, DataType* value) final {
+    elements_.emplace_back(llvm_types_->t_data_type);
+  }
+  void Visit(const char* key, runtime::NDArray* value) final {
+    CHECK(false) << ""Do not support serializing NDArray"";
+  }
+
+ private:
+  void VisitMetadataBase(runtime::metadata::MetadataBase metadata) {
+    elements_.emplace_back(llvm::PointerType::getUnqual(
+        llvm::StructType::create(*ctx_, metadata->get_c_struct_name())));
+    if (visited_.find(metadata->get_c_struct_name()) != visited_.end()) {
+      return;
+    }
+
+    if (to_visit_.find(metadata->get_c_struct_name()) != to_visit_.end()) {
+      return;
+    }
+    to_visit_[metadata->get_c_struct_name()] = metadata;
+  }
+
+ public:
+  using MetadataKind = runtime::metadata::MetadataKind;
+
+  void VisitArray(const runtime::metadata::MetadataArrayNode* arr) {
+    switch (arr->kind) {
+      case MetadataKind::kUint64:  // LLVM encodes signed and unsigned with same types.
+      case MetadataKind::kInt64:
+        elements_.emplace_back(llvm::PointerType::getUnqual(llvm_types_->t_int64));
+        break;
+      case MetadataKind::kBool:
+        elements_.emplace_back(llvm::PointerType::getUnqual(llvm_types_->t_bool));
+        break;
+      case MetadataKind::kString:
+        elements_.emplace_back(llvm::PointerType::getUnqual(llvm_types_->t_cstring));
+        break;
+      case MetadataKind::kHandle:
+        CHECK(false) << ""Do not support handle"";
+        break;
+      case MetadataKind::kMetadata:
+        elements_.emplace_back(
+            llvm::PointerType::getUnqual(llvm_types_->structs_by_type_key[arr->type_key]));
+        break;
+      default:
+        CHECK(false) << ""Unsupported metadata kind "" << arr->kind;
+        break;
+    }
+  }
+
+  void Visit(const char* key, ObjectRef* value) final {
+    const runtime::metadata::MetadataArrayNode* arr =
+        value->as<runtime::metadata::MetadataArrayNode>();
+    if (arr != nullptr) {
+      VisitArray(arr);
+    } else {
+      elements_.emplace_back(
+          llvm::PointerType::getUnqual(llvm_types_->structs_by_type_key[(*value)->GetTypeKey()]));
+    }
+  }
+
+  void DefineType(runtime::metadata::MetadataBase metadata) {
+    ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
+    for (auto e : elements_) {
+      std::string value;
+      llvm::raw_string_ostream os(value);
+      e->print(os, true);
+    }
+    llvm_types_->structs_by_type_key[metadata->GetTypeKey()] =
+        llvm::StructType::create(*ctx_, elements_, metadata->get_c_struct_name());
+    elements_.clear();
+  }
+
+  llvm::LLVMContext* ctx_;
+  struct MetadataLlvmTypes* llvm_types_;
+  ::std::unordered_set<::std::string> visited_;
+  ::std::unordered_map<::std::string, runtime::metadata::MetadataBase> to_visit_;
+  ::std::vector<llvm::Type*> elements_;
+};
+
+class MetadataSerializerLLVM : public AttrVisitor {
+  using MetadataKind = runtime::metadata::MetadataKind;
+
+ public:
+  MetadataSerializerLLVM(CodeGenLLVM* codegen, struct MetadataLlvmTypes* llvm_types)
+      : codegen_{codegen}, llvm_types_{llvm_types} {}
+
+  void Visit(const char* key, double* value) final {
+    elements_.back().emplace_back(llvm::ConstantFP::get(llvm_types_->t_float64, *value));
+  }
+  void Visit(const char* key, int64_t* value) final {
+    elements_.back().emplace_back(llvm::ConstantInt::get(
+        llvm_types_->t_int64, static_cast<uint64_t>(*value), true /* isSigned */));
+  }
+  void Visit(const char* key, uint64_t* value) final {
+    elements_.back().emplace_back(
+        llvm::ConstantInt::get(llvm_types_->t_int64, *value, false /* isSigned */));
+  }
+  void Visit(const char* key, int* value) final {
+    elements_.back().emplace_back(
+        llvm::ConstantInt::get(llvm_types_->t_int64, *value, true /* isSigned */));
+  }
+  void Visit(const char* key, bool* value) final {
+    elements_.back().emplace_back(llvm::ConstantInt::get(
+        llvm_types_->t_uint8, static_cast<uint64_t>(*value), false /* isSigned */));
+  }
+  void Visit(const char* key, std::string* value) final {
+    elements_.back().emplace_back(codegen_->GetConstString(*value));
+  }
+  void Visit(const char* key, void** value) final {
+    CHECK(false) << ""Do not support serializing void*"";
+  }
+  void Visit(const char* key, DataType* value) final {
+    elements_.back().emplace_back(llvm::ConstantStruct::get(
+        llvm_types_->t_data_type,
+        {llvm::ConstantInt::get(llvm_types_->t_uint8, value->code(), false /* isSigned */),
+         llvm::ConstantInt::get(llvm_types_->t_uint8, value->bits(), false /* isSigned */),
+         llvm::ConstantInt::get(llvm_types_->t_uint8, value->lanes(), false /* isSigned */)}));
+  }
+
+  void Visit(const char* key, runtime::NDArray* value) final {
+    CHECK(false) << ""Do not support serializing NDArray"";
+  }
+
+  void VisitMetadata(runtime::metadata::MetadataBase metadata) {
+    elements_.emplace_back(std::vector<llvm::Constant*>());
+    ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
+    auto struct_elements = elements_.back();
+    elements_.pop_back();
+    auto struct_ty = llvm_types_->structs_by_type_key[metadata->GetTypeKey()];
+    ICHECK(struct_ty != nullptr) << ""Did not find LLVM StructType* for type_key=""
+                                 << metadata->GetTypeKey();
+    CHECK_EQ(struct_elements.size(), struct_ty->getNumElements());
+    auto out = llvm::ConstantStruct::get(struct_ty, struct_elements);
+    if (elements_.size() > 0) {
+      elements_.back().push_back(out);
+    } else {
+      last_production_ = out;
+    }
+  }
+
+  void VisitArray(const runtime::metadata::MetadataArrayNode* arr) {
+    llvm::Type* element_type;
+    switch (arr->kind) {
+      case MetadataKind::kInt64:
+        element_type = llvm_types_->t_int64;
+        break;
+      case MetadataKind::kUint64:
+        element_type = llvm_types_->t_int64;
+        break;
+      case MetadataKind::kBool:
+        element_type = llvm_types_->t_uint8;
+        break;
+      case MetadataKind::kString:
+        element_type = llvm_types_->t_cstring;
+        break;
+      case MetadataKind::kMetadata: {
+        element_type = llvm_types_->structs_by_type_key[arr->type_key];
+        ICHECK(element_type != nullptr)
+            << ""Did not find LLVM StructType* for type_key="" << arr->type_key;
+        break;
+      }
+      default:
+        LOG(FATAL) << ""unknown metadata kind "" << arr->kind;
+        break;
+    }
+
+    elements_.emplace_back(std::vector<llvm::Constant*>());
+    for (auto o : arr->array) {
+      if (o->IsInstance<FloatImmNode>()) {
+        double value = Downcast<FloatImm>(o)->value;
+        Visit(nullptr, &value);
+      }
+      if (o->IsInstance<IntImmNode>()) {
+        auto value = Downcast<IntImm>(o)->value;
+        Visit(nullptr, &value);
+      } else if (o->IsInstance<StringObj>()) {
+        ::std::string value = Downcast<String>(o);
+        Visit(nullptr, &value);
+      } else {
+        // nested array not possible.
+        VisitMetadata(Downcast<runtime::metadata::MetadataBase>(o));
+      }
+    }
+    auto array = elements_.back();
+    elements_.pop_back();
+    CHECK(element_type != nullptr);
+    auto arr_ty = llvm::ArrayType::get(element_type, array.size());
+    auto llvm_arr = llvm::ConstantArray::get(arr_ty, array);
+
+    if (elements_.size() > 0) {
+      elements_.back().emplace_back(
+          codegen_->GetGlobalConstant(llvm_arr, """", llvm::GlobalValue::PrivateLinkage));
+    } else {
+      last_production_ = llvm_arr;
+    }
+  }
+
+  void Visit(const char* key, ObjectRef* value) final {
+    const runtime::metadata::MetadataArrayNode* arr =
+        value->as<runtime::metadata::MetadataArrayNode>();
+    if (arr != nullptr) {
+      VisitArray(arr);
+      return;
+    }
+
+    runtime::metadata::MetadataBase metadata = Downcast<runtime::metadata::MetadataBase>(*value);
+    VisitMetadata(metadata);
+  }
+
+  llvm::Constant* Serialize(runtime::metadata::MetadataBase metadata) {
+    Visit(nullptr, &metadata);
+    ICHECK(last_production_);
+    return codegen_->GetGlobalConstant(last_production_);
+  }
+
+  CodeGenLLVM* codegen_;
+  MetadataLlvmTypes* llvm_types_;
+  llvm::LLVMContext* ctx_;
+  llvm::Module* module_;
+  std::vector<std::vector<llvm::Constant*>> elements_;
+  llvm::Constant* last_production_;
+};
+
+void CodeGenCPU::DefineMetadata(runtime::metadata::Metadata metadata) {
+  MetadataLlvmTypes llvm_types{
+      t_float64_ /* t_float64 */,
+      llvm::Type::getInt8Ty(*ctx_) /* t_uint8 */,
+      t_int64_ /* t_int64 */,
+      llvm::Type::getInt8Ty(*ctx_) /* t_bool */,
+      t_char_->getPointerTo() /* t_cstring */,
+      t_void_p_ /* t_void_p */,
+      llvm::StructType::create(*ctx_, {t_int8_, t_int8_, t_int8_}, ""DLDataType"") /* t_data_type */,
+  };
+
+  std::vector<runtime::metadata::MetadataBase> queue;
+  metadata::DiscoverComplexTypesVisitor discover_complex{&queue};
+  discover_complex.Discover(metadata);
+
+  MetadataTypeDefiner definer{ctx_, &llvm_types};
+  for (auto md : queue) {
+    if (md.defined()) {
+      definer.DefineType(md);
+    }
+  }
+
+  MetadataSerializerLLVM serializer{this, &llvm_types};
+  auto metadata_constant_gv = serializer.Serialize(metadata);
+
+  function_ =
+      llvm::Function::Create(ftype_tvm_backend_packed_c_func_, llvm::Function::ExternalLinkage,
+                             ""get_c_metadata"", module_.get());
+  function_->setCallingConv(llvm::CallingConv::C);
+  function_->setDLLStorageClass(llvm::GlobalValue::DLLStorageClassTypes::DLLExportStorageClass);
+
+  llvm::BasicBlock* entry_point_entry = llvm::BasicBlock::Create(*ctx_, ""entry"", function_);
+  builder_->SetInsertPoint(entry_point_entry);
+
+  auto ret_values_p = builder_->CreateBitCast(GetArg(function_, 3), t_void_p_->getPointerTo());
+  builder_->CreateStore(builder_->CreateBitCast(metadata_constant_gv, t_void_p_), ret_values_p);
+
+  auto ret_tcode = builder_->CreateBitCast(GetArg(function_, 4), t_int_->getPointerTo());
+  builder_->CreateStore(llvm::ConstantInt::get(t_int_, kTVMOpaqueHandle), ret_tcode);
+
+  builder_->CreateRet(ConstInt32(0));
+}
+
 void CodeGenCPU::DefineFunctionRegistry(Array<String> func_names) {
   ICHECK(is_system_lib_) << ""Loading of --system-lib modules is yet to be defined for C runtime"";
   Array<String> symbols;
@@ -980,9 +1311,11 @@ void CodeGenCPU::AddStartupFunction() {
 
 llvm::Value* CodeGenCPU::CreateIntrinsic(const CallNode* op) {
   if (op->op.same_as(builtin::tvm_call_packed_lowered())) {
+    return CreateCallPacked(op, true /* use_string_lookup */);
   } else if (op->op.same_as(builtin::tvm_call_trace_packed_lowered())) {
     return CreateCallTracePacked(op);
+  } else if (op->op.same_as(builtin::tvm_call_cpacked_lowered())) {
+    return CreateCallPacked(op, false /* use_string_lookup */);
   } else if (op->op.same_as(builtin::tvm_static_handle())) {
     return CreateStaticHandle();
   } else if (op->op.same_as(builtin::tvm_throw_last_error())) {
@@ -1052,6 +1385,7 @@ void CodeGenCPU::VisitStmt_(const AssertStmtNode* op) {
   builder_->CreateCondBr(cond, end_block, fail_block, md_very_likely_branch_);
   // fail condition.
   builder_->SetInsertPoint(fail_block);
+
 #if TVM_LLVM_VERSION >= 90
   auto err_callee =
       llvm::FunctionCallee(ftype_tvm_api_set_last_error_, RuntimeTVMAPISetLastError());"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class CodeGenCPU : public CodeGenLLVM {
    */
   void DefineFunctionRegistry(Array<String> func_names);
 
  protected:
   void AddStartupFunction() final;
   // meta data
@@ -117,9 +123,9 @@ class CodeGenCPU : public CodeGenLLVM {
     llvm::BasicBlock* end_block;
   };
   PackedCall MakeCallPackedLowered(const Array<PrimExpr>& args, const DataType& r_type,
-                                   const int64_t begin, const int64_t end);
   // create call into tvm packed function.
-  llvm::Value* CreateCallPacked(const CallNode* op);
   // Create trace call into tvm packed function.
   llvm::Value* CreateCallTracePacked(const CallNode* op);
   // Create static initialization"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class CodeGenCPU : public CodeGenLLVM {
    */
   void DefineFunctionRegistry(Array<String> func_names);
 
+  /*!
+   * \brief Serialize the metadata object as data, and implement get_c_metadata function.
+   * \param metadata The metadata which should be serialized.
+   */
+  void DefineMetadata(runtime::metadata::Metadata metadata);
+
  protected:
   void AddStartupFunction() final;
   // meta data
@@ -117,9 +123,9 @@ class CodeGenCPU : public CodeGenLLVM {
     llvm::BasicBlock* end_block;
   };
   PackedCall MakeCallPackedLowered(const Array<PrimExpr>& args, const DataType& r_type,
+                                   const int64_t begin, const int64_t end, bool use_string_lookup);
   // create call into tvm packed function.
+  llvm::Value* CreateCallPacked(const CallNode* op, bool use_string_lookup);
   // Create trace call into tvm packed function.
   llvm::Value* CreateCallTracePacked(const CallNode* op);
   // Create static initialization"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" #include ""codegen_cpu.h""
 #include ""codegen_params.h""
 #include ""llvm/Support/raw_os_ostream.h""
 namespace tvm {
 namespace codegen {
 
@@ -134,11 +135,11 @@ void CodeGenLLVM::AddFunctionInternal(const PrimFunc& f, bool ret_void) {
   auto global_symbol = f->GetAttr<String>(tvm::attr::kGlobalSymbol);
   ICHECK(global_symbol.defined())
       << ""CodeGenLLVM: Expect PrimFunc to have the global_symbol attribute"";
-  ICHECK(module_->getFunction(static_cast<std::string>(global_symbol.value())) == nullptr)
-      << ""Function "" << global_symbol << "" already exist in module"";
-
-  function_ = llvm::Function::Create(ftype, llvm::Function::ExternalLinkage,
-                                     global_symbol.value().operator std::string(), module_.get());
   function_->setCallingConv(llvm::CallingConv::C);
   function_->setDLLStorageClass(llvm::GlobalValue::DLLStorageClassTypes::DLLExportStorageClass);
 
@@ -191,6 +192,19 @@ void CodeGenLLVM::AddFunctionInternal(const PrimFunc& f, bool ret_void) {
   }
 }
 
 void CodeGenLLVM::LinkParameters(const Map<String, LinkedParam> params) {
   // It would be nice to de-dupe these declarations frm src/tir/transforms/make_packed_api.cc,
   // but they are at a different layer in the compiler...
@@ -209,22 +223,13 @@ void CodeGenLLVM::LinkParameters(const Map<String, LinkedParam> params) {
   llvm::BasicBlock* entry = llvm::BasicBlock::Create(*ctx_, ""entry"", function);
   builder_->SetInsertPoint(entry);
 
-  auto getArg = [function](int i) -> llvm::Argument* {
-#if TVM_LLVM_VERSION >= 100
-    return function->getArg(i);
-#elif TVM_LLVM_VERSION >= 50
-    return &function->arg_begin()[i];
-#else
-    return &*std::next(function->arg_begin(), i);
-#endif
-  };
-
   llvm::Type* t_int64_p = t_int64_->getPointerTo(GetGlobalAddressSpace());
-  llvm::Value* sid = builder_->CreateLoad(t_int64_, builder_->CreateBitCast(getArg(0), t_int64_p));
 
-  auto ret_tcode = builder_->CreateBitCast(getArg(4), t_int_p);
-  auto ret_value =
-      builder_->CreateBitCast(getArg(3), t_void_p_->getPointerTo(GetGlobalAddressSpace()));
 
   llvm::BasicBlock* default_block = llvm::BasicBlock::Create(*ctx_, ""default_block"", function);
   llvm::SwitchInst* switch_inst = builder_->CreateSwitch(sid, default_block, params.size() + 1);
@@ -236,18 +241,18 @@ void CodeGenLLVM::LinkParameters(const Map<String, LinkedParam> params) {
   // Add data to the global section.
   for (auto kv : params) {
     auto array = NDArrayToLLVMArray(ctx_, kv.second->param);
-    std::string symbol_name = std::string(::tvm::runtime::symbol::tvm_param_prefix) + kv.first;
-    llvm::GlobalVariable* param_symbol = new llvm::GlobalVariable(
-        *module_, array->getType(), true, llvm::GlobalValue::InternalLinkage, array, symbol_name);
     auto dtype = tvm::runtime::DataType(kv.second->param->dtype);
     size_t align = std::max(tvm::runtime::GetVectorBytes(dtype), tvm::runtime::kAllocAlignment);
 #if TVM_LLVM_VERSION >= 100
     param_symbol->setAlignment(llvm::Align(align));
 #else
     param_symbol->setAlignment(align);
 #endif
 
-    llvm::BasicBlock* case_block = llvm::BasicBlock::Create(*ctx_, ""case_"" + symbol_name, function);
     switch_inst->addCase(
         llvm::cast<llvm::ConstantInt>(llvm::ConstantInt::get(t_int64_, kv.second->id)), case_block);
     builder_->SetInsertPoint(case_block);
@@ -388,6 +393,7 @@ void CodeGenLLVM::Optimize() {
     fpass.run(*it);
   }
   fpass.doFinalization();
   mpass.run(*module_);
 }
 
@@ -770,21 +776,27 @@ llvm::Value* CodeGenLLVM::CreateCast(DataType from, DataType to, llvm::Value* va
   }
 }
 
-llvm::Constant* CodeGenLLVM::GetConstString(const std::string& str) {
-  auto it = str_map_.find(str);
-  if (it != str_map_.end()) return it->second;
-  llvm::Type* type = llvm::ArrayType::get(t_char_, str.length() + 1);
-  llvm::GlobalVariable* global = new llvm::GlobalVariable(
-      *module_, type, true, llvm::GlobalValue::PrivateLinkage, nullptr, "".str"");
 #if TVM_LLVM_VERSION >= 100
   global->setAlignment(llvm::Align(1));
 #else
   global->setAlignment(1);
 #endif
-  global->setInitializer(llvm::ConstantDataArray::getString(*ctx_, str));
   llvm::Constant* zero = ConstInt32(0);
   llvm::Constant* indices[] = {zero, zero};
-  llvm::Constant* ptr = llvm::ConstantExpr::getGetElementPtr(type, global, indices);
   str_map_[str] = ptr;
   return ptr;
 }
@@ -1407,7 +1419,9 @@ llvm::Value* CodeGenLLVM::VisitExpr_(const BufferLoadNode* op) {
 llvm::Value* CodeGenLLVM::VisitExpr_(const CallNode* op) {
   if (auto* ptr_op = op->op.as<OpNode>()) {
     auto call_op = GetRef<Op>(ptr_op);
-    if (op->op.same_as(builtin_call_extern_) || op->op.same_as(builtin_call_pure_extern_)) {
       // call extern intrinsic
       ICHECK_GE(op->args.size(), 1U);
       auto global_symbol = Downcast<StringImm>(op->args[0]);
@@ -1418,7 +1432,10 @@ llvm::Value* CodeGenLLVM::VisitExpr_(const CallNode* op) {
       return this->CreateCallExtern(GetType(GetRef<PrimExpr>(op)), op_attr_global_symbol_[call_op],
                                     op->args, false);
     } else {
-      return CreateIntrinsic(op);
     }
   } else {
     ICHECK(op->op.as<GlobalVarNode>());
@@ -1563,7 +1580,7 @@ void CodeGenLLVM::VisitStmt_(const AllocateNode* op) {
   ICHECK(!is_zero(op->condition));
   llvm::Value* buf = nullptr;
 
-  size_t constant_size = op->ConstantAllocationSize();
   ICHECK_GT(constant_size, 0) << ""Can only handle constant size stack allocation"";
   StorageInfo& info = alloc_storage_info_[op->buffer_var.get()];
   if (constant_size % 4 == 0 && info.alignment == 0) {"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" #include ""codegen_cpu.h""
 #include ""codegen_params.h""
 #include ""llvm/Support/raw_os_ostream.h""
+#include ""llvm_common.h""
 namespace tvm {
 namespace codegen {
 
@@ -134,11 +135,11 @@ void CodeGenLLVM::AddFunctionInternal(const PrimFunc& f, bool ret_void) {
   auto global_symbol = f->GetAttr<String>(tvm::attr::kGlobalSymbol);
   ICHECK(global_symbol.defined())
       << ""CodeGenLLVM: Expect PrimFunc to have the global_symbol attribute"";
+  function_ = module_->getFunction(static_cast<std::string>(global_symbol.value()));
+  if (function_ == nullptr) {
+    function_ = llvm::Function::Create(ftype, llvm::Function::ExternalLinkage,
+                                       global_symbol.value().operator std::string(), module_.get());
+  }
   function_->setCallingConv(llvm::CallingConv::C);
   function_->setDLLStorageClass(llvm::GlobalValue::DLLStorageClassTypes::DLLExportStorageClass);
 
@@ -191,6 +192,19 @@ void CodeGenLLVM::AddFunctionInternal(const PrimFunc& f, bool ret_void) {
   }
 }
 
+llvm::GlobalVariable* CodeGenLLVM::GetLinkedParamSymbol(const std::string& param_name,
+                                                        llvm::ConstantArray* array) {
+  std::string symbol_name = std::string(::tvm::runtime::symbol::tvm_param_prefix) + param_name;
+  llvm::GlobalVariable* var = module_->getGlobalVariable(symbol_name, true /* AllowInternal */);
+  if (var == nullptr) {
+    CHECK(array != nullptr) << ""Expect param symbol "" << symbol_name
+                            << "" to either be defined or for the array to be supplied"";
+    var = new llvm::GlobalVariable(*module_, static_cast<llvm::Type*>(array->getType()), true,
+                                   llvm::GlobalValue::InternalLinkage, array, symbol_name);
+  }
+  return var;
+}
+
 void CodeGenLLVM::LinkParameters(const Map<String, LinkedParam> params) {
   // It would be nice to de-dupe these declarations frm src/tir/transforms/make_packed_api.cc,
   // but they are at a different layer in the compiler...
@@ -209,22 +223,13 @@ void CodeGenLLVM::LinkParameters(const Map<String, LinkedParam> params) {
   llvm::BasicBlock* entry = llvm::BasicBlock::Create(*ctx_, ""entry"", function);
   builder_->SetInsertPoint(entry);
 
   llvm::Type* t_int64_p = t_int64_->getPointerTo(GetGlobalAddressSpace());
+  llvm::Value* sid =
+      builder_->CreateLoad(t_int64_, builder_->CreateBitCast(GetArg(function, 0), t_int64_p));
 
+  auto ret_tcode = builder_->CreateBitCast(GetArg(function, 4), t_int_p);
+  auto ret_value = builder_->CreateBitCast(GetArg(function, 3),
+                                           t_void_p_->getPointerTo(GetGlobalAddressSpace()));
 
   llvm::BasicBlock* default_block = llvm::BasicBlock::Create(*ctx_, ""default_block"", function);
   llvm::SwitchInst* switch_inst = builder_->CreateSwitch(sid, default_block, params.size() + 1);
@@ -236,18 +241,18 @@ void CodeGenLLVM::LinkParameters(const Map<String, LinkedParam> params) {
   // Add data to the global section.
   for (auto kv : params) {
     auto array = NDArrayToLLVMArray(ctx_, kv.second->param);
+    llvm::GlobalVariable* param_symbol = GetLinkedParamSymbol(kv.first, array);
     auto dtype = tvm::runtime::DataType(kv.second->param->dtype);
     size_t align = std::max(tvm::runtime::GetVectorBytes(dtype), tvm::runtime::kAllocAlignment);
 #if TVM_LLVM_VERSION >= 100
     param_symbol->setAlignment(llvm::Align(align));
 #else
     param_symbol->setAlignment(align);
 #endif
+    param_symbol->setInitializer(array);
 
+    llvm::BasicBlock* case_block =
+        llvm::BasicBlock::Create(*ctx_, ""case_"" + param_symbol->getName(), function);
     switch_inst->addCase(
         llvm::cast<llvm::ConstantInt>(llvm::ConstantInt::get(t_int64_, kv.second->id)), case_block);
     builder_->SetInsertPoint(case_block);
@@ -388,6 +393,7 @@ void CodeGenLLVM::Optimize() {
     fpass.run(*it);
   }
   fpass.doFinalization();
+  // PrintModule(module_.get());
   mpass.run(*module_);
 }
 
@@ -770,21 +776,27 @@ llvm::Value* CodeGenLLVM::CreateCast(DataType from, DataType to, llvm::Value* va
   }
 }
 
+llvm::Constant* CodeGenLLVM::GetGlobalConstant(llvm::Constant* const_data, const std::string& name,
+                                               llvm::GlobalValue::LinkageTypes linkage_type) {
+  llvm::Type* ty = const_data->getType();
+  llvm::GlobalVariable* global =
+      new llvm::GlobalVariable(*module_, ty, true, linkage_type, const_data, name);
 #if TVM_LLVM_VERSION >= 100
   global->setAlignment(llvm::Align(1));
 #else
   global->setAlignment(1);
 #endif
   llvm::Constant* zero = ConstInt32(0);
   llvm::Constant* indices[] = {zero, zero};
+  llvm::Constant* ptr = llvm::ConstantExpr::getGetElementPtr(ty, global, indices);
+  return ptr;
+}
+
+llvm::Constant* CodeGenLLVM::GetConstString(const std::string& str) {
+  auto it = str_map_.find(str);
+  if (it != str_map_.end()) return it->second;
+  auto llvm_str = llvm::ConstantDataArray::getString(*ctx_, str);
+  auto ptr = GetGlobalConstant(llvm_str, "".str"", llvm::GlobalValue::PrivateLinkage);
   str_map_[str] = ptr;
   return ptr;
 }
@@ -1407,7 +1419,9 @@ llvm::Value* CodeGenLLVM::VisitExpr_(const BufferLoadNode* op) {
 llvm::Value* CodeGenLLVM::VisitExpr_(const CallNode* op) {
   if (auto* ptr_op = op->op.as<OpNode>()) {
     auto call_op = GetRef<Op>(ptr_op);
+    if (op->op.same_as(builtin_lookup_param_)) {
+      return GetLinkedParamSymbol(Downcast<StringImm>(op->args[0])->value, nullptr);
+    } else if (op->op.same_as(builtin_call_extern_) || op->op.same_as(builtin_call_pure_extern_)) {
       // call extern intrinsic
       ICHECK_GE(op->args.size(), 1U);
       auto global_symbol = Downcast<StringImm>(op->args[0]);
@@ -1418,7 +1432,10 @@ llvm::Value* CodeGenLLVM::VisitExpr_(const CallNode* op) {
       return this->CreateCallExtern(GetType(GetRef<PrimExpr>(op)), op_attr_global_symbol_[call_op],
                                     op->args, false);
     } else {
+      VLOG(2) << ""CreateIntrinsic: "" << GetRef<Call>(op);
+      auto x = CreateIntrinsic(op);
+      VLOG(2) << ""CreateIntrinsic done"";
+      return x;
     }
   } else {
     ICHECK(op->op.as<GlobalVarNode>());
@@ -1563,7 +1580,7 @@ void CodeGenLLVM::VisitStmt_(const AllocateNode* op) {
   ICHECK(!is_zero(op->condition));
   llvm::Value* buf = nullptr;
 
+  int32_t constant_size = op->ConstantAllocationSize();
   ICHECK_GT(constant_size, 0) << ""Can only handle constant size stack allocation"";
   StorageInfo& info = alloc_storage_info_[op->buffer_var.get()];
   if (constant_size % 4 == 0 && info.alignment == 0) {"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"  */
 #ifndef TVM_TARGET_LLVM_CODEGEN_LLVM_H_
 #define TVM_TARGET_LLVM_CODEGEN_LLVM_H_
 #ifdef TVM_LLVM_VERSION
 
 #include <tvm/arith/analyzer.h>
@@ -190,6 +191,13 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
   void VisitStmt_(const SeqStmtNode* op) override;
   void VisitStmt_(const EvaluateNode* op) override;
 
  protected:
   /*!
    * \brief Address and type pair to assist in handling opaque pointers.
@@ -341,6 +349,14 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
    */
   llvm::Function* GetIntrinsicDecl(llvm::Intrinsic::ID id, llvm::Type* ret_type,
                                    llvm::ArrayRef<llvm::Type*> arg_types);
   /*!
    * \brief Get the number of elements in the given vector value.
    * \param vec The value, must be of a vector type.
@@ -353,8 +369,6 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
                     int* p_native_bits);
   // Returns whether the LLVM type has padding for alignment
   bool HasAlignmentPadding(DataType dtype);
-  // Get constant string
-  llvm::Constant* GetConstString(const std::string& str);
   // do a scalarize call with f
   llvm::Value* CreateScalarizedCall(const CallNode* op, llvm::Function* f,
                                     const std::vector<llvm::Value*>& args);
@@ -389,6 +403,27 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
                                              unsigned int shared_address_space, int alignment,
                                              llvm::GlobalValue::LinkageTypes linkage);
 
   // The IRBuilder.
   using IRBuilder = llvm::IRBuilder<llvm::ConstantFolder, llvm::IRBuilderDefaultInserter>;
   // The current function
@@ -447,6 +482,8 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
   const Op& builtin_call_pure_extern_ = builtin::call_pure_extern();
   const Op& builtin_call_llvm_intrin_ = builtin::call_llvm_intrin();
   const Op& builtin_call_llvm_pure_intrin_ = builtin::call_llvm_pure_intrin();
 
   /*! \brief Helper struct for debug infos. */
   struct DebugInfo {
@@ -481,6 +518,7 @@ void CodeGenLLVM::AddFunctionsOrdered(IterType begin, IterType end, ConvType pfu
     return name_a < name_b;
   });
   for (auto& f : funcs) {
     AddFunction(f);
   }
 }"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"  */
 #ifndef TVM_TARGET_LLVM_CODEGEN_LLVM_H_
 #define TVM_TARGET_LLVM_CODEGEN_LLVM_H_
+#include <llvm/IR/GlobalValue.h>
 #ifdef TVM_LLVM_VERSION
 
 #include <tvm/arith/analyzer.h>
@@ -190,6 +191,13 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
   void VisitStmt_(const SeqStmtNode* op) override;
   void VisitStmt_(const EvaluateNode* op) override;
 
+  // Get constant string
+  llvm::Constant* GetConstString(const std::string& str);
+
+  llvm::Constant* GetGlobalConstant(
+      llvm::Constant* const_data, const std::string& name = """",
+      llvm::GlobalValue::LinkageTypes linkage_type = llvm::GlobalValue::InternalLinkage);
+
  protected:
   /*!
    * \brief Address and type pair to assist in handling opaque pointers.
@@ -341,6 +349,14 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
    */
   llvm::Function* GetIntrinsicDecl(llvm::Intrinsic::ID id, llvm::Type* ret_type,
                                    llvm::ArrayRef<llvm::Type*> arg_types);
+  /*!
+   * \brief Lookup or create a GlobalVariable whose content is the data field of a DLTensor for a
+   * given linked_param() CallNode.
+   * \param param_name Parameter name (e.g. unmangled, from lookup_param node).
+   * \return the GlobalVariable indicated in the brief.
+   */
+  llvm::GlobalVariable* GetLinkedParamSymbol(const ::std::string& param_name,
+                                             llvm::ConstantArray* array);
   /*!
    * \brief Get the number of elements in the given vector value.
    * \param vec The value, must be of a vector type.
@@ -353,8 +369,6 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
                     int* p_native_bits);
   // Returns whether the LLVM type has padding for alignment
   bool HasAlignmentPadding(DataType dtype);
   // do a scalarize call with f
   llvm::Value* CreateScalarizedCall(const CallNode* op, llvm::Function* f,
                                     const std::vector<llvm::Value*>& args);
@@ -389,6 +403,27 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
                                              unsigned int shared_address_space, int alignment,
                                              llvm::GlobalValue::LinkageTypes linkage);
 
+  /*!
+   * \brief Get the `i`th argument to the given function, respecting LLVM API changes.
+   *
+   * NOTE: in LLVM < 10.0, the underlying API returns a const llvm::Argument*. To provide a uniform
+   * API, const is removed here. Proper usage of LLVM APIs depends on having a non-const Argument*,
+   * so we take this appraoch here rather than adding const.
+   *
+   * \param function The function containing the arguments.
+   * \param i The index of the argument to retrieve.
+   * \return The retrieved argument.
+   */
+  llvm::Argument* GetArg(const llvm::Function* function, int i) const {
+#if TVM_LLVM_VERSION >= 100
+    return function->getArg(i);
+#elif TVM_LLVM_VERSION >= 50
+    return const_cast<llvm::Argument*>(&function->arg_begin()[i]);
+#else
+    return const_cast<llvm::Argument*>(&*std::next(function->arg_begin(), i));
+#endif
+  }
+
   // The IRBuilder.
   using IRBuilder = llvm::IRBuilder<llvm::ConstantFolder, llvm::IRBuilderDefaultInserter>;
   // The current function
@@ -447,6 +482,8 @@ class CodeGenLLVM : public ExprFunctor<llvm::Value*(const PrimExpr&)>,
   const Op& builtin_call_pure_extern_ = builtin::call_pure_extern();
   const Op& builtin_call_llvm_intrin_ = builtin::call_llvm_intrin();
   const Op& builtin_call_llvm_pure_intrin_ = builtin::call_llvm_pure_intrin();
+  const Op& builtin_lookup_param_ = builtin::lookup_param();
+  const Op& builtin_tvm_call_cpacked_lowered_ = builtin::tvm_call_cpacked_lowered();
 
   /*! \brief Helper struct for debug infos. */
   struct DebugInfo {
@@ -481,6 +518,7 @@ void CodeGenLLVM::AddFunctionsOrdered(IterType begin, IterType end, ConvType pfu
     return name_a < name_b;
   });
   for (auto& f : funcs) {
+    auto global_symbol = f->GetAttr<String>(tvm::attr::kGlobalSymbol);
     AddFunction(f);
   }
 }"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"std::string LLVMTargetToString(const Target& target) {
   return os.str();
 }
 
 }  // namespace codegen
 }  // namespace tvm
 #endif  // TVM_LLVM_VERSION"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"std::string LLVMTargetToString(const Target& target) {
   return os.str();
 }
 
+void PrintModule(const llvm::Module* mod) {
+  std::string modpe_str;
+  llvm::raw_string_ostream rso(modpe_str);
+  mod->print(rso, nullptr);
+  LOG(INFO) << rso.str();
+}
+
 }  // namespace codegen
 }  // namespace tvm
 #endif  // TVM_LLVM_VERSION"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"std::unique_ptr<llvm::TargetMachine> GetLLVMTargetMachine(const Target& target,
  */
 std::string LLVMTargetToString(const Target& target);
 
 }  // namespace codegen
 }  // namespace tvm
 "
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"std::unique_ptr<llvm::TargetMachine> GetLLVMTargetMachine(const Target& target,
  */
 std::string LLVMTargetToString(const Target& target);
 
+void PrintModule(const llvm::Module* mod);
+
 }  // namespace codegen
 }  // namespace tvm
 "
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class LLVMModuleNode final : public runtime::ModuleNode {
 
     cg->SetFastMathFlag(fmf);
 
     cg->AddFunctionsOrdered(funcs.begin(), funcs.end());
     if (entry_func.length() != 0) {
       cg->AddMainFunction(entry_func);
     }
 
-    if (found_linked_params) {
-      cg->LinkParameters(linked_params);
-    }
     module_ = cg->Finish();
     module_->addModuleFlag(llvm::Module::Warning, ""tvm_target"",
                            llvm::MDString::get(*ctx_, LLVMTargetToString(target)));
@@ -527,6 +527,41 @@ TVM_REGISTER_GLOBAL(""codegen.codegen_blob"")
       return runtime::Module(n);
     });
 
 runtime::Module CreateLLVMCrtMetadataModule(const Array<runtime::Module>& modules, Target target,
                                             tvm::relay::Runtime runtime) {
   Array<String> func_names;"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class LLVMModuleNode final : public runtime::ModuleNode {
 
     cg->SetFastMathFlag(fmf);
 
+    if (found_linked_params) {
+      cg->LinkParameters(linked_params);
+    }
     cg->AddFunctionsOrdered(funcs.begin(), funcs.end());
     if (entry_func.length() != 0) {
       cg->AddMainFunction(entry_func);
     }
 
     module_ = cg->Finish();
     module_->addModuleFlag(llvm::Module::Warning, ""tvm_target"",
                            llvm::MDString::get(*ctx_, LLVMTargetToString(target)));
@@ -527,6 +527,41 @@ TVM_REGISTER_GLOBAL(""codegen.codegen_blob"")
       return runtime::Module(n);
     });
 
+runtime::Module CreateLLVMCppMetadataModule(runtime::metadata::Metadata metadata, Target target,
+                                            tvm::relay::Runtime runtime) {
+  InitializeLLVM();
+  auto tm = GetLLVMTargetMachine(target);
+  bool system_lib = runtime->GetAttr<Bool>(""system-lib"").value_or(Bool(false));
+  auto ctx = std::make_shared<llvm::LLVMContext>();
+  std::unique_ptr<CodeGenCPU> cg{new CodeGenCPU()};
+
+  cg->Init(""TVMMetadataMod"", tm.get(), ctx.get(), system_lib, system_lib,
+           false /* target_c_runtime */);
+
+  cg->DefineMetadata(metadata);
+  auto mod = cg->Finish();
+  mod->addModuleFlag(llvm::Module::Warning, ""tvm_target"",
+                     llvm::MDString::get(*ctx, LLVMTargetToString(target)));
+  mod->addModuleFlag(llvm::Module::Override, ""Debug Info Version"", llvm::DEBUG_METADATA_VERSION);
+
+  if (tm->getTargetTriple().isOSDarwin()) {
+    mod->addModuleFlag(llvm::Module::Override, ""Dwarf Version"", 2);
+  }
+
+  std::string verify_errors_storage;
+  llvm::raw_string_ostream verify_errors(verify_errors_storage);
+  LOG_IF(FATAL, llvm::verifyModule(*mod, &verify_errors))
+      << ""LLVM module verification failed with the following errors: \n""
+      << verify_errors.str();
+
+  auto n = make_object<LLVMModuleNode>();
+  n->Init(std::move(mod), ctx);
+
+  auto meta_mod = MetadataModuleCreate(metadata);
+  meta_mod->Import(runtime::Module(n));
+  return meta_mod;
+}
+
 runtime::Module CreateLLVMCrtMetadataModule(const Array<runtime::Module>& modules, Target target,
                                             tvm::relay::Runtime runtime) {
   Array<String> func_names;"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" namespace tvm {
 namespace codegen {
 
 runtime::Module CreateLLVMCrtMetadataModule(const Array<runtime::Module>& modules, Target target,
                                             tvm::relay::Runtime runtime);
 "
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" namespace tvm {
 namespace codegen {
 
+runtime::Module CreateLLVMCppMetadataModule(runtime::metadata::Metadata metadata, Target target,
+                                            tvm::relay::Runtime runtime);
+
 runtime::Module CreateLLVMCrtMetadataModule(const Array<runtime::Module>& modules, Target target,
                                             tvm::relay::Runtime runtime);
 "
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class VisitableMetadataNode : public ::tvm::runtime::metadata::MetadataNode {
       inputs_array.push_back(::tvm::runtime::metadata::TensorInfo{inputs_accessor[i]});
     }
     ::tvm::runtime::metadata::MetadataArray inputs_metadata_array{
-        inputs_array, ::tvm::runtime::metadata::MetadataTypeIndex::kMetadata, ""TVMTensorInfo""};
     v->Visit(""inputs"", &inputs_metadata_array);
     int64_t num_inputs_cpp = num_inputs();
     v->Visit(""num_inputs"", &num_inputs_cpp);
@@ -67,7 +68,8 @@ class VisitableMetadataNode : public ::tvm::runtime::metadata::MetadataNode {
       outputs_array.push_back(::tvm::runtime::metadata::TensorInfo{outputs_accessor[i]});
     }
     ::tvm::runtime::metadata::MetadataArray outputs_metadata_array{
-        outputs_array, ::tvm::runtime::metadata::MetadataTypeIndex::kMetadata, ""TVMTensorInfo""};
     v->Visit(""outputs"", &outputs_metadata_array);
     int64_t num_outputs_cpp = num_outputs();
     v->Visit(""num_outputs"", &num_outputs_cpp);
@@ -78,7 +80,8 @@ class VisitableMetadataNode : public ::tvm::runtime::metadata::MetadataNode {
       pools_array.push_back(::tvm::runtime::metadata::TensorInfo{pools_accessor[i]});
     }
     ::tvm::runtime::metadata::MetadataArray pools_metadata_array{
-        pools_array, ::tvm::runtime::metadata::MetadataTypeIndex::kMetadata, ""TVMTensorInfo""};
     v->Visit(""pools"", &pools_metadata_array);
     int64_t num_pools_cpp = num_pools();
     v->Visit(""num_pools"", &num_pools_cpp);
@@ -156,7 +159,7 @@ class VisitableTensorInfoNode : public ::tvm::runtime::metadata::TensorInfoNode
       shape_array.push_back(::tvm::Integer{static_cast<int>(shape_accessor[i])});
     }
     ::tvm::runtime::metadata::MetadataArray shape_metadata_array{
-        shape_array, ::tvm::runtime::metadata::MetadataTypeIndex::kInt64, nullptr};
     v->Visit(""shape"", &shape_metadata_array);
     int64_t num_shape_cpp = num_shape();
     v->Visit(""num_shape"", &num_shape_cpp);"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class VisitableMetadataNode : public ::tvm::runtime::metadata::MetadataNode {
       inputs_array.push_back(::tvm::runtime::metadata::TensorInfo{inputs_accessor[i]});
     }
     ::tvm::runtime::metadata::MetadataArray inputs_metadata_array{
+        inputs_array, ::tvm::runtime::metadata::MetadataKind::kMetadata,
+        ::tvm::runtime::metadata::TensorInfoNode::_type_key};
     v->Visit(""inputs"", &inputs_metadata_array);
     int64_t num_inputs_cpp = num_inputs();
     v->Visit(""num_inputs"", &num_inputs_cpp);
@@ -67,7 +68,8 @@ class VisitableMetadataNode : public ::tvm::runtime::metadata::MetadataNode {
       outputs_array.push_back(::tvm::runtime::metadata::TensorInfo{outputs_accessor[i]});
     }
     ::tvm::runtime::metadata::MetadataArray outputs_metadata_array{
+        outputs_array, ::tvm::runtime::metadata::MetadataKind::kMetadata,
+        ::tvm::runtime::metadata::TensorInfoNode::_type_key};
     v->Visit(""outputs"", &outputs_metadata_array);
     int64_t num_outputs_cpp = num_outputs();
     v->Visit(""num_outputs"", &num_outputs_cpp);
@@ -78,7 +80,8 @@ class VisitableMetadataNode : public ::tvm::runtime::metadata::MetadataNode {
       pools_array.push_back(::tvm::runtime::metadata::TensorInfo{pools_accessor[i]});
     }
     ::tvm::runtime::metadata::MetadataArray pools_metadata_array{
+        pools_array, ::tvm::runtime::metadata::MetadataKind::kMetadata,
+        ::tvm::runtime::metadata::TensorInfoNode::_type_key};
     v->Visit(""pools"", &pools_metadata_array);
     int64_t num_pools_cpp = num_pools();
     v->Visit(""num_pools"", &num_pools_cpp);
@@ -156,7 +159,7 @@ class VisitableTensorInfoNode : public ::tvm::runtime::metadata::TensorInfoNode
       shape_array.push_back(::tvm::Integer{static_cast<int>(shape_accessor[i])});
     }
     ::tvm::runtime::metadata::MetadataArray shape_metadata_array{
+        shape_array, ::tvm::runtime::metadata::MetadataKind::kInt64, nullptr};
     v->Visit(""shape"", &shape_metadata_array);
     int64_t num_shape_cpp = num_shape();
     v->Visit(""num_shape"", &num_shape_cpp);"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"static runtime::Module CreateCppMetadataModule(
         auto metadata_module = CreateCSourceCppMetadataModule(runtime_metadata);
         metadata_module->Import(target_module);
         target_module = metadata_module;
       } else {
         CHECK(false) << ""Don't know how to create MetadataModule for target type "" << target->str();
       }"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"static runtime::Module CreateCppMetadataModule(
         auto metadata_module = CreateCSourceCppMetadataModule(runtime_metadata);
         metadata_module->Import(target_module);
         target_module = metadata_module;
+#ifdef TVM_LLVM_VERSION  // defining TVM_LLVM_VERSION indicates TVM was compiled with USE_LLVM ON.
+      } else if (target->kind->name == ""llvm"") {
+        auto metadata_module = CreateLLVMCppMetadataModule(runtime_metadata, target, runtime);
+        metadata_module->Import(target_module);
+        target_module = metadata_module;
+#endif  // TVM_LLVM_VERSION
       } else {
         CHECK(false) << ""Don't know how to create MetadataModule for target type "" << target->str();
       }"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+/*!
+ * \file tvm/target/metadata_utils.cc
+ * \brief Defines utility functions and classes for emitting metadata.
+ */
+#include ""metadata_utils.h""
+
+namespace tvm {
+namespace codegen {
+namespace metadata {
+
+std::string AddressFromParts(const std::vector<std::string>& parts) {
+  std::stringstream ss;
+  for (unsigned int i = 0; i < parts.size(); ++i) {
+    if (i > 0) {
+      ss << ""_"";
+    }
+    ss << parts[i];
+  }
+  return ss.str();
+}
+
+DiscoverArraysVisitor::DiscoverArraysVisitor(std::vector<DiscoveredArray>* queue) : queue_{queue} {}
+
+void DiscoverArraysVisitor::Visit(const char* key, double* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, int64_t* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, uint64_t* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, int* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, bool* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, std::string* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, DataType* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, runtime::NDArray* value) {}
+void DiscoverArraysVisitor::Visit(const char* key, void** value) {}
+
+void DiscoverArraysVisitor::Visit(const char* key, ObjectRef* value) {
+  address_parts_.push_back(key);
+  if (value->as<runtime::metadata::MetadataBaseNode>() != nullptr) {
+    auto metadata = Downcast<runtime::metadata::MetadataBase>(*value);
+    const runtime::metadata::MetadataArrayNode* arr =
+        value->as<runtime::metadata::MetadataArrayNode>();
+    if (arr != nullptr) {
+      for (unsigned int i = 0; i < arr->array.size(); i++) {
+        ObjectRef o = arr->array[i];
+        if (o.as<runtime::metadata::MetadataBaseNode>() != nullptr) {
+          std::stringstream ss;
+          ss << i;
+          address_parts_.push_back(ss.str());
+          runtime::metadata::MetadataBase metadata = Downcast<runtime::metadata::MetadataBase>(o);
+          ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
+          address_parts_.pop_back();
+        }
+      }
+
+      queue_->push_back(std::make_tuple(AddressFromParts(address_parts_),
+                                        Downcast<runtime::metadata::MetadataArray>(metadata)));
+    } else {
+      ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
+    }
+  }
+  address_parts_.pop_back();
+}
+
+void DiscoverComplexTypesVisitor::Visit(const char* key, double* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, int64_t* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, uint64_t* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, int* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, bool* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, std::string* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, DataType* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, runtime::NDArray* value) {}
+void DiscoverComplexTypesVisitor::Visit(const char* key, void** value) {}
+
+bool DiscoverComplexTypesVisitor::DiscoverType(std::string type_key) {
+  VLOG(2) << ""DiscoverType "" << type_key;
+  auto position_it = type_key_to_position_.find(type_key);
+  if (position_it != type_key_to_position_.end()) {
+    return false;
+  }
+
+  queue_->emplace_back(tvm::runtime::metadata::MetadataBase());
+  type_key_to_position_[type_key] = queue_->size() - 1;
+  return true;
+}
+
+void DiscoverComplexTypesVisitor::DiscoverInstance(runtime::metadata::MetadataBase md) {
+  auto position_it = type_key_to_position_.find(md->GetTypeKey());
+  ICHECK(position_it != type_key_to_position_.end())
+      << ""DiscoverInstance requires that DiscoverType has already been called: type_key=""
+      << md->GetTypeKey();
+
+  int queue_position = (*position_it).second;
+  if (!(*queue_)[queue_position].defined() && md.defined()) {
+    VLOG(2) << ""DiscoverInstance  "" << md->GetTypeKey() << "":"" << md;
+    (*queue_)[queue_position] = md;
+  }
+}
+
+void DiscoverComplexTypesVisitor::Visit(const char* key, ObjectRef* value) {
+  ICHECK_NOTNULL(value->as<runtime::metadata::MetadataBaseNode>());
+
+  auto metadata = Downcast<runtime::metadata::MetadataBase>(*value);
+  const runtime::metadata::MetadataArrayNode* arr =
+      value->as<runtime::metadata::MetadataArrayNode>();
+
+  if (arr == nullptr) {
+    VLOG(2) << ""No array, object-traversing "" << metadata->GetTypeKey();
+    ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
+    DiscoverType(metadata->GetTypeKey());
+    DiscoverInstance(metadata);
+    return;
+  }
+
+  if (arr->kind != tvm::runtime::metadata::MetadataKind::kMetadata) {
+    return;
+  }
+
+  bool needs_instance = DiscoverType(arr->type_key);
+  for (unsigned int i = 0; i < arr->array.size(); i++) {
+    tvm::runtime::metadata::MetadataBase o =
+        Downcast<tvm::runtime::metadata::MetadataBase>(arr->array[i]);
+    if (needs_instance) {
+      DiscoverInstance(o);
+      needs_instance = false;
+    }
+    ReflectionVTable::Global()->VisitAttrs(o.operator->(), this);
+  }
+}
+
+void DiscoverComplexTypesVisitor::Discover(runtime::metadata::MetadataBase metadata) {
+  ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
+  DiscoverType(metadata->GetTypeKey());
+  DiscoverInstance(metadata);
+}
+
+}  // namespace metadata
+}  // namespace codegen
+}  // namespace tvm"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+/*!
+ * \file tvm/target/metadata_utils.h
+ * \brief Declares utilty functions and classes for emitting metadata.
+ */
+#ifndef TVM_TARGET_METADATA_UTILS_H_
+#define TVM_TARGET_METADATA_UTILS_H_
+
+#include <tvm/runtime/data_type.h>
+#include <tvm/runtime/ndarray.h>
+#include <tvm/runtime/object.h>
+
+#include <string>
+#include <tuple>
+#include <unordered_map>
+#include <vector>
+
+#include ""metadata.h""
+
+namespace tvm {
+namespace codegen {
+namespace metadata {
+
+/*!
+ * \brief Construct a unique string ""address"" for a struct member from a vector of pieces.
+ *
+ * In codegen, it is frequently necessary to assemble a C-style identifier for an
+ * otherwise-anonymous member of Metadata. For instance, suppose Metadata declares an array:
+ * struct TVMMetadata {
+ *   int64_t* shape;
+ * };
+ *
+ * In order to properly initialize this struct, the array must be declared separately with a global
+ * name. This function produces such a name, here termed ""address.""
+ *
+ * \param parts A vector of pieces, typically the struct member names which identify the path to
+ *  this member.
+ * \return The joined pieces.
+ */
+std::string AddressFromParts(const std::vector<std::string>& parts);
+
+/*!
+ * \brief A prefix in metadata symbol names.
+ * This prefix is typically given to AddressFromParts as the 0th item in parts.
+ */
+static constexpr const char* kMetadataGlobalSymbol = ""kTvmgenMetadata"";
+
+/*!
+ * \brief Post-order traverse metadata to discover arrays which need to be forward-defined.
+ */
+class DiscoverArraysVisitor : public AttrVisitor {
+ public:
+  /*! \brief Models a single array discovered in this visitor.
+   * Conatains two fields:
+   *  0. An address which uniquely identifies the array in this Metadata instance.
+   *  1. The discovered MetadataArray.
+   */
+  using DiscoveredArray = std::tuple<std::string, runtime::metadata::MetadataArray>;
+  explicit DiscoverArraysVisitor(std::vector<DiscoveredArray>* queue);
+
+  void Visit(const char* key, double* value) final;
+  void Visit(const char* key, int64_t* value) final;
+  void Visit(const char* key, uint64_t* value) final;
+  void Visit(const char* key, int* value) final;
+  void Visit(const char* key, bool* value) final;
+  void Visit(const char* key, std::string* value) final;
+  void Visit(const char* key, DataType* value) final;
+  void Visit(const char* key, runtime::NDArray* value) final;
+  void Visit(const char* key, void** value) final;
+
+  void Visit(const char* key, ObjectRef* value) final;
+
+ private:
+  /*! \brief The queue to be filled with discovered arrays. */
+  std::vector<DiscoveredArray>* queue_;
+
+  /*! \brief Tracks the preceding address pieces. */
+  std::vector<std::string> address_parts_;
+};
+
+/*!
+ * \brief Post-order traverse Metadata to discover all complex types which need to be
+ * forward-defined. This visitor finds one defined() MetadataBase instance for each unique subclass
+ * present inside Metadata in the order in which the subclass was first discovered.
+ */
+class DiscoverComplexTypesVisitor : public AttrVisitor {
+ public:
+  /*! \brief Construct a new instance.
+   * \param queue An ordered map which holds the
+   */
+  explicit DiscoverComplexTypesVisitor(std::vector<runtime::metadata::MetadataBase>* queue)
+      : queue_{queue} {}
+
+  void Visit(const char* key, double* value) final;
+  void Visit(const char* key, int64_t* value) final;
+  void Visit(const char* key, uint64_t* value) final;
+  void Visit(const char* key, int* value) final;
+  void Visit(const char* key, bool* value) final;
+  void Visit(const char* key, std::string* value) final;
+  void Visit(const char* key, DataType* value) final;
+  void Visit(const char* key, runtime::NDArray* value) final;
+  void Visit(const char* key, void** value) final;
+
+  void Visit(const char* key, ObjectRef* value) final;
+
+  void Discover(runtime::metadata::MetadataBase metadata);
+
+ private:
+  bool DiscoverType(std::string type_key);
+
+  void DiscoverInstance(runtime::metadata::MetadataBase md);
+
+  std::vector<runtime::metadata::MetadataBase>* queue_;
+
+  /*! \brief map type_index to index in queue_. */
+  std::unordered_map<std::string, int> type_key_to_position_;
+};
+
+}  // namespace metadata
+}  // namespace codegen
+}  // namespace tvm
+
+#endif  // TVM_TARGET_METADATA_UTILS_H_"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"std::string CodeGenCHost::GetPackedName(const CallNode* op) {
 CodeGenCHost::FunctionInfo CodeGenCHost::GetFunctionInfo(const CallNode* op,
                                                          bool has_resource_handle) {
   const StringImmNode* s = op->args[0].as<StringImmNode>();
-  ICHECK(s != nullptr) << ""tvm_call_{c}packed_lowered expects first argument as function name"";
   int64_t begin = op->args[3].as<IntImmNode>()->value;
   int64_t end = op->args[4].as<IntImmNode>()->value;
   int64_t num_args = end - begin;
   ICHECK_GE(num_args, 0);
   std::string func_name = s->value;
 
   if (has_resource_handle) {
-    std::string resource_handle_name = op->args[5].as<StringImmNode>()->value;
-    return {func_name, num_args - 1, resource_handle_name};
   }
-  return {func_name, num_args};
 }
 
 void CodeGenCHost::VisitExpr_(const CallNode* op, std::ostream& os) {  // NOLINT(*)"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"std::string CodeGenCHost::GetPackedName(const CallNode* op) {
 CodeGenCHost::FunctionInfo CodeGenCHost::GetFunctionInfo(const CallNode* op,
                                                          bool has_resource_handle) {
   const StringImmNode* s = op->args[0].as<StringImmNode>();
+  ICHECK(s != nullptr) << ""tvm_call_[c]packed_lowered expects first argument as function name"";
   int64_t begin = op->args[3].as<IntImmNode>()->value;
   int64_t end = op->args[4].as<IntImmNode>()->value;
   int64_t num_args = end - begin;
   ICHECK_GE(num_args, 0);
   std::string func_name = s->value;
 
   if (has_resource_handle) {
+    const StringImmNode* resource_handle_var = op->args[5].as<StringImmNode>();
+    if (resource_handle_var != nullptr) {
+      std::string resource_handle_name = resource_handle_var->value;
+      return {func_name, num_args - 1, resource_handle_name};
+    } else {
+      // The final arg should be ""(void*) NULL"" to indicate the empty resource_handle.
+      num_args--;
+
+      const CallNode* reinterpret_call = op->args[5].as<CallNode>();
+      ICHECK_NE(reinterpret_call, (void*)nullptr)
+          << ""At CallNode to "" << s
+          << ""arg 5: Expect either StringImm naming the resource_handle var from interface API or ""
+          << ""reinterpret(0); got: "" << op->args[5];
+      ICHECK_EQ(reinterpret_call->op, builtin::reinterpret())
+          << ""At CallNode to "" << s
+          << ""arg 5: Expect either StringImm naming the resource_handle var from interface API or ""
+          << ""reinterpret(0); got: "" << op->args[5];
+      ICHECK(is_zero(reinterpret_call->args[0])) << ""At CallNode to "" << s
+                                                 << "" arg 5: Expect either StringImm naming the ""
+                                                    ""resource_handle var from interface API, or ""
+                                                 << ""zero; got "" << op->args[5];
+    }
   }
+  return {func_name, num_args, ""NULL""};
 }
 
 void CodeGenCHost::VisitExpr_(const CallNode* op, std::ostream& os) {  // NOLINT(*)"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"  */
 #include ""source_module.h""
 
 #include <tvm/runtime/module.h>
 #include <tvm/runtime/ndarray.h>
 #include <tvm/runtime/packed_func.h>
 #include <tvm/runtime/registry.h>
 
 #include <string>
-#include <tuple>
 #include <unordered_map>
 #include <unordered_set>
 #include <utility>
@@ -40,6 +40,7 @@
 #include ""../../support/str_escape.h""
 #include ""../func_registry_generator.h""
 #include ""../metadata.h""
 #include ""codegen_source_base.h""
 
 namespace tvm {
@@ -523,69 +524,10 @@ class CSourceCrtMetadataModuleNode : public runtime::ModuleNode {
   }
 };
 
-static std::string address_from_parts(const std::vector<std::string>& parts) {
-  std::stringstream ss;
-  for (unsigned int i = 0; i < parts.size(); ++i) {
-    if (i > 0) {
-      ss << ""_"";
-    }
-    ss << parts[i];
-  }
-  return ss.str();
-}
-
-class MetadataQueuer : public AttrVisitor {
- public:
-  using QueueItem = std::tuple<std::string, runtime::metadata::MetadataBase>;
-  explicit MetadataQueuer(std::vector<QueueItem>* queue) : queue_{queue} {}
-
-  void Visit(const char* key, double* value) final {}
-  void Visit(const char* key, int64_t* value) final {}
-  void Visit(const char* key, uint64_t* value) final {}
-  void Visit(const char* key, int* value) final {}
-  void Visit(const char* key, bool* value) final {}
-  void Visit(const char* key, std::string* value) final {}
-  void Visit(const char* key, DataType* value) final {}
-  void Visit(const char* key, runtime::NDArray* value) final {}
-  void Visit(const char* key, void** value) final {}
-
-  void Visit(const char* key, ObjectRef* value) final {
-    address_parts_.push_back(key);
-    if (value->as<runtime::metadata::MetadataBaseNode>() != nullptr) {
-      auto metadata = Downcast<runtime::metadata::MetadataBase>(*value);
-      const runtime::metadata::MetadataArrayNode* arr =
-          value->as<runtime::metadata::MetadataArrayNode>();
-      if (arr != nullptr) {
-        for (unsigned int i = 0; i < arr->array.size(); i++) {
-          ObjectRef o = arr->array[i];
-          if (o.as<runtime::metadata::MetadataBaseNode>() != nullptr) {
-            std::stringstream ss;
-            ss << i;
-            address_parts_.push_back(ss.str());
-            runtime::metadata::MetadataBase metadata = Downcast<runtime::metadata::MetadataBase>(o);
-            ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
-            address_parts_.pop_back();
-          }
-        }
-      } else {
-        ReflectionVTable::Global()->VisitAttrs(metadata.operator->(), this);
-      }
-
-      queue_->push_back(std::make_tuple(address_from_parts(address_parts_),
-                                        Downcast<runtime::metadata::MetadataBase>(*value)));
-    }
-    address_parts_.pop_back();
-  }
-
- private:
-  std::vector<QueueItem>* queue_;
-  std::vector<std::string> address_parts_;
-};
-
 class MetadataSerializer : public AttrVisitor {
  public:
   static constexpr const char* kGlobalSymbol = ""kTvmgenMetadata"";
-  using MetadataTypeIndex = ::tvm::runtime::metadata::MetadataTypeIndex;
 
   MetadataSerializer() : is_first_item_{true} {}
 
@@ -653,29 +595,54 @@ class MetadataSerializer : public AttrVisitor {
     ICHECK(false) << ""do not support serializing NDArray as metadata"";
   }
 
-  void VisitArray(const runtime::metadata::MetadataArrayNode* array) {
     auto old_is_first_item = is_first_item_;
     is_first_item_ = true;
     for (unsigned int i = 0; i < array->array.size(); ++i) {
       ObjectRef o = array->array[i];
-      if (o->IsInstance<IntImmNode>()) {
-        int64_t i = Downcast<Integer>(o);
-        Visit(nullptr, &i);
-        continue;
-      }
 
-      if (o->IsInstance<StringObj>()) {
-        std::string s = Downcast<String>(o);
-        Visit(nullptr, &s);
-        continue;
       }
-
-      runtime::metadata::MetadataBase metadata = Downcast<runtime::metadata::MetadataBase>(o);
-      std::stringstream i_str;
-      i_str << i;
-      address_.push_back(i_str.str());
-      Visit(nullptr, &metadata);
-      address_.pop_back();
     }
     is_first_item_ = old_is_first_item;
   }
@@ -688,7 +655,7 @@ class MetadataSerializer : public AttrVisitor {
       if (key != nullptr) {
         address_.push_back(key);
       }
-      code_ << address_from_parts(address_);
       if (key != nullptr) {
         address_.pop_back();
       }
@@ -705,59 +672,72 @@ class MetadataSerializer : public AttrVisitor {
     }
   }
 
   void CodegenMetadata(::tvm::runtime::metadata::Metadata metadata) {
     decl_ << ""#include <inttypes.h>"" << std::endl
           << ""#include <tvm/runtime/metadata.h>"" << std::endl
           << ""#include <tvm/runtime/c_runtime_api.h>"" << std::endl;
-    std::vector<MetadataQueuer::QueueItem> queue;
-    MetadataQueuer queuer{&queue};
-    queuer.Visit(kGlobalSymbol, &metadata);
-
-    for (MetadataQueuer::QueueItem item : queue) {
-      auto struct_name = std::get<0>(item);
-      auto obj = std::get<1>(item);
-      auto arr = obj.as<runtime::metadata::MetadataArrayNode>();
-      is_first_item_ = true;
-      address_.push_back(struct_name);
-      if (arr != nullptr) {
-        const char* const_part = ""const "";
-        if (arr->type_index == MetadataTypeIndex::kString) {
-          const_part = """";
-        }
-        code_ << const_part;
-        switch (arr->type_index) {
-          case MetadataTypeIndex::kUint64:
-            code_ << ""uint64_t"";
-            break;
-          case MetadataTypeIndex::kInt64:
-            code_ << ""int64_t"";
-            break;
-          case MetadataTypeIndex::kBool:
-            code_ << ""bool"";
-            break;
-          case MetadataTypeIndex::kString:
-            code_ << ""const char*"";
-            break;
-          case MetadataTypeIndex::kHandle:
-            code_ << ""void*"";
-            break;
-          case MetadataTypeIndex::kMetadata:
-            code_ << ""struct "" << arr->struct_name;
-            break;
-          default:
-            CHECK(false) << ""Unknown type_index in array: "" << arr->type_index
-                         << "" (struct_name="" << arr->struct_name << "")"";
-            break;
-        }
-        code_ << "" "" << struct_name << ""["" << arr->array.size() << ""] = {"" << std::endl;
-        VisitArray(arr);
-      } else {
-        code_ << ""const struct TVMMetadata "" << struct_name << "" = {"" << std::endl;
-        Visit(nullptr, &obj);
       }
       address_.pop_back();
       code_ << ""};"" << std::endl;
     }
   }
 
   std::string GetOutput() { return decl_.str() + code_.str(); }
@@ -804,8 +784,8 @@ runtime::Module CreateCSourceCppMetadataModule(runtime::metadata::Metadata metad
               << ""(TVMValue* arg_values, int* arg_tcodes, int ""
                  ""num_args, TVMValue* ret_values, int* ret_tcodes, void* resource_handle) {""
               << std::endl;
-  lookup_func << ""    ret_values[0].v_handle = (void*) &"" << MetadataSerializer::kGlobalSymbol
-              << "";"" << std::endl;
   lookup_func << ""    ret_tcodes[0] = kTVMOpaqueHandle;"" << std::endl;
   lookup_func << ""    return 0;"" << std::endl;
   lookup_func << ""};"" << std::endl;"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"  */
 #include ""source_module.h""
 
+#include <tvm/runtime/metadata.h>
 #include <tvm/runtime/module.h>
 #include <tvm/runtime/ndarray.h>
 #include <tvm/runtime/packed_func.h>
 #include <tvm/runtime/registry.h>
 
 #include <string>
 #include <unordered_map>
 #include <unordered_set>
 #include <utility>
@@ -40,6 +40,7 @@
 #include ""../../support/str_escape.h""
 #include ""../func_registry_generator.h""
 #include ""../metadata.h""
+#include ""../metadata_utils.h""
 #include ""codegen_source_base.h""
 
 namespace tvm {
@@ -523,69 +524,10 @@ class CSourceCrtMetadataModuleNode : public runtime::ModuleNode {
   }
 };
 
 class MetadataSerializer : public AttrVisitor {
  public:
   static constexpr const char* kGlobalSymbol = ""kTvmgenMetadata"";
+  using MetadataKind = ::tvm::runtime::metadata::MetadataKind;
 
   MetadataSerializer() : is_first_item_{true} {}
 
@@ -653,29 +595,54 @@ class MetadataSerializer : public AttrVisitor {
     ICHECK(false) << ""do not support serializing NDArray as metadata"";
   }
 
+  void VisitArray(runtime::metadata::MetadataArray array) {
     auto old_is_first_item = is_first_item_;
     is_first_item_ = true;
     for (unsigned int i = 0; i < array->array.size(); ++i) {
       ObjectRef o = array->array[i];
 
+      switch (array->kind) {
+        case MetadataKind::kUint64: {
+          int64_t i = Downcast<Integer>(o);
+          CHECK_GT(i, 0)
+              << ""Metadata is of type uint64_t, but array type contains a negative number"";
+          uint64_t ui = static_cast<uint64_t>(i);
+          Visit(nullptr, &ui);
+          continue;
+        }
+        case MetadataKind::kInt64: {
+          int64_t i = Downcast<Integer>(o);
+          Visit(nullptr, &i);
+          continue;
+        }
+        case MetadataKind::kBool: {
+          bool b = Downcast<Bool>(o);
+          Visit(nullptr, &b);
+          break;
+        }
+        case MetadataKind::kString: {
+          std::string s = Downcast<String>(o);
+          Visit(nullptr, &s);
+          break;
+        }
+        case MetadataKind::kHandle:
+          CHECK(false) << ""Don't know how to serialize handle"";
+          break;
+
+        case MetadataKind::kMetadata: {
+          runtime::metadata::MetadataBase metadata = Downcast<runtime::metadata::MetadataBase>(o);
+          std::stringstream i_str;
+          i_str << i;
+          address_.push_back(i_str.str());
+          Visit(nullptr, &metadata);
+          address_.pop_back();
+          break;
+        }
+        default:
+          CHECK(false) << ""Unknown MetadataKind for array: "" << array->kind;
+          break;
       }
+      is_first_item_ = false;
     }
     is_first_item_ = old_is_first_item;
   }
@@ -688,7 +655,7 @@ class MetadataSerializer : public AttrVisitor {
       if (key != nullptr) {
         address_.push_back(key);
       }
+      code_ << metadata::AddressFromParts(address_);
       if (key != nullptr) {
         address_.pop_back();
       }
@@ -705,59 +672,72 @@ class MetadataSerializer : public AttrVisitor {
     }
   }
 
+ private:
+  void EmitCType(const runtime::metadata::MetadataArrayNode* arr, std::ostream& os) {
+    switch (arr->kind) {
+      case MetadataKind::kUint64:
+        os << ""uint64_t"";
+        break;
+      case MetadataKind::kInt64:
+        os << ""int64_t"";
+        break;
+      case MetadataKind::kBool:
+        os << ""bool"";
+        break;
+      case MetadataKind::kString:
+        os << ""const char*"";
+        break;
+      case MetadataKind::kHandle:
+        os << ""void*"";
+        break;
+      case MetadataKind::kMetadata:
+        os << ""struct "" << arr->get_element_c_struct_name();
+        break;
+      default:
+        CHECK(false) << ""Unknown kind in MetadataArray: "" << arr->kind
+                     << "" (struct_name="" << arr->get_c_struct_name() << "")"";
+        break;
+    }
+  }
+
+ public:
   void CodegenMetadata(::tvm::runtime::metadata::Metadata metadata) {
     decl_ << ""#include <inttypes.h>"" << std::endl
           << ""#include <tvm/runtime/metadata.h>"" << std::endl
           << ""#include <tvm/runtime/c_runtime_api.h>"" << std::endl;
+    std::vector<metadata::DiscoverArraysVisitor::DiscoveredArray> queue;
+    metadata::DiscoverArraysVisitor array_discover{&queue};
+    array_discover.Visit(metadata::kMetadataGlobalSymbol, &metadata);
+
+    for (auto item : queue) {
+      auto struct_address = std::get<0>(item);
+      address_.push_back(struct_address);
+
+      auto arr = std::get<1>(item);
+
+      // Prepend const with everything except C-string, which needs appending.
+      if (arr->kind != MetadataKind::kString) {
+        code_ << ""const "";
+      }
+      EmitCType(arr.operator->(), code_);
+      if (arr->kind == MetadataKind::kString) {
+        code_ << "" const"";
       }
+      code_ << "" "" << struct_address << ""["" << arr->array.size() << ""] = {"" << std::endl;
+      is_first_item_ = true;
+
+      VisitArray(arr);
       address_.pop_back();
       code_ << ""};"" << std::endl;
     }
+
+    // Finally, emit overall struct.
+    address_.push_back(metadata::kMetadataGlobalSymbol);
+    code_ << ""const struct TVMMetadata "" << metadata::AddressFromParts(address_) << "" = {""
+          << std::endl;
+    Visit(nullptr, &metadata);
+    code_ << ""};"" << std::endl;
+    address_.pop_back();
   }
 
   std::string GetOutput() { return decl_.str() + code_.str(); }
@@ -804,8 +784,8 @@ runtime::Module CreateCSourceCppMetadataModule(runtime::metadata::Metadata metad
               << ""(TVMValue* arg_values, int* arg_tcodes, int ""
                  ""num_args, TVMValue* ret_values, int* ret_tcodes, void* resource_handle) {""
               << std::endl;
+  lookup_func << ""    ret_values[0].v_handle = (void*) &"" << metadata::kMetadataGlobalSymbol << "";""
+              << std::endl;
   lookup_func << ""    ret_tcodes[0] = kTVMOpaqueHandle;"" << std::endl;
   lookup_func << ""    return 0;"" << std::endl;
   lookup_func << ""};"" << std::endl;"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"TVM_STATIC_IR_FUNCTOR(ReprPrinter, vtable)
 // Call
 Call::Call(DataType dtype, RelayExpr op, Array<PrimExpr> args, Span span) {
   for (size_t i = 0; i < args.size(); ++i) {
-    ICHECK(args[i].defined());
   }
 
   ObjectPtr<CallNode> node = make_object<CallNode>();"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"TVM_STATIC_IR_FUNCTOR(ReprPrinter, vtable)
 // Call
 Call::Call(DataType dtype, RelayExpr op, Array<PrimExpr> args, Span span) {
   for (size_t i = 0; i < args.size(); ++i) {
+    ICHECK(args[i].defined()) << ""arg "" << i << "" is not defined()"";
   }
 
   ObjectPtr<CallNode> node = make_object<CallNode>();"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"using InputMap =
  */
 class PackedCallLegalizer : public StmtExprMutator {
  public:
-  Stmt Legalize(const InputMap& params, tir::Stmt body) {
-    inputs_ = params;
-    return StmtExprMutator::VisitStmt(body);
-  }
 
   Stmt VisitStmt_(const EvaluateNode* op) final {
     if (tir::is_const_int(op->value)) return StmtExprMutator::VisitStmt_(op);
@@ -56,49 +55,62 @@ class PackedCallLegalizer : public StmtExprMutator {
     // let B_packed = set_struct(tvm_value2, B)
     // let C_packed = set_struct(tvm_value3, C)
     // call_packed(f, A_packed, B_packed, C_packed)
-    std::vector<Stmt> new_stmts;
     if (call) {
       if (call->op.same_as(builtin::tvm_call_cpacked())) {
         Array<PrimExpr> packed_args{call->args[0]};
-        std::vector<tir::Var> tvm_values;
-        for (unsigned i = 1; i < call->args.size(); i++) {
           // No need to pack inputs of the prim_func
           if (inputs_[call->args[i]] == true) {
             packed_args.push_back(call->args[i]);
           } else {
-            // Pack the argument inside a TVMValue
-            std::stringstream ss;
-            ss << ""tvm_value_"" << tvm_value_index_++;
-            auto sid_array = tir::Var(ss.str(), DataType::Handle());
-            tvm_values.push_back(sid_array);
-
-            new_stmts.push_back(tir::Evaluate(
-                tvm::tir::Call(DataType::Handle(), tvm::tir::builtin::tvm_struct_set(),
-                               {sid_array, 0, tir::builtin::kArrData, call->args[i]})));
-            new_stmts.push_back(tir::Evaluate(
-                tvm::tir::Call(DataType::Handle(), tvm::tir::builtin::tvm_struct_set(),
-                               {sid_array, 0, tir::builtin::kArrDeviceType, kDLCPU})));
-            new_stmts.push_back(tir::Evaluate(
-                tvm::tir::Call(DataType::Handle(), tvm::tir::builtin::tvm_struct_set(),
-                               {sid_array, 0, tir::builtin::kArrDeviceId, 0})));
-            packed_args.push_back(sid_array);
           }
         }
         // Evaluate the packed call
-        new_stmts.push_back(tir::Evaluate(tir::Call(call->dtype, call->op, packed_args)));
-        tir::Stmt call_stmt = tir::SeqStmt(new_stmts);
-
-        // Allocate the TVMValues on the stack and define the variables
-        for (auto v : tvm_values) {
-          call_stmt = LetStmt(v, StackAlloca(""array"", 1), call_stmt);
-        }
-        return call_stmt;
       }
     }
     return StmtExprMutator::VisitStmt_(op);
   }
 
  private:
   InputMap inputs_;      // Store the inputs to the primfunc that don't need to be packed.
   int tvm_value_index_;  // Index of the actual tvm_value variable
 };
@@ -109,12 +121,12 @@ Pass LegalizePackedCalls() {
   auto pass_func = [=](PrimFunc f, IRModule m, PassContext ctx) {
     auto* n = f.CopyOnWrite();
 
-    // Create the
     InputMap inputs;
     for (auto i : f->params) {
       inputs[i] = true;
     }
-    n->body = PackedCallLegalizer().Legalize(inputs, std::move(n->body));
     return f;
   };
   return CreatePrimFuncPass(pass_func, 0, ""tir.LegalizePackedCalls"", {});"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"using InputMap =
  */
 class PackedCallLegalizer : public StmtExprMutator {
  public:
+  PackedCallLegalizer(IRModule m, const InputMap& inputs) : mod_{m}, inputs_{inputs} {}
+
+  Stmt Legalize(tir::Stmt body) { return StmtExprMutator::VisitStmt(body); }
 
   Stmt VisitStmt_(const EvaluateNode* op) final {
     if (tir::is_const_int(op->value)) return StmtExprMutator::VisitStmt_(op);
@@ -56,49 +55,62 @@ class PackedCallLegalizer : public StmtExprMutator {
     // let B_packed = set_struct(tvm_value2, B)
     // let C_packed = set_struct(tvm_value3, C)
     // call_packed(f, A_packed, B_packed, C_packed)
     if (call) {
       if (call->op.same_as(builtin::tvm_call_cpacked())) {
         Array<PrimExpr> packed_args{call->args[0]};
+        VLOG(2) << ""Legalize call:"" << call;
+        BaseFunc base_func = mod_->Lookup(Downcast<StringImm>(call->args[0])->value);
+        const PrimFuncNode* prim_func = base_func.as<PrimFuncNode>();
+        VLOG(2) << "" to func "" << base_func;
+        for (unsigned i = 1; i < call->args.size() - 1; i++) {
           // No need to pack inputs of the prim_func
           if (inputs_[call->args[i]] == true) {
             packed_args.push_back(call->args[i]);
           } else {
+            // Stack-allocate a DLTensor for this parameter. Note that LowerTVMBuiltin will collect
+            // all such stack-allocated tensors and minimize the storage needed by reusing
+            // DLTensors.
+            Array<PrimExpr> call_args{call->args[i]};
+            tvm::runtime::Map<tvm::tir::Var, tvm::tir::Buffer>::iterator param_buf_it;
+            if (prim_func != nullptr) {
+              auto param_var = prim_func->params[i - 1];
+              param_buf_it = prim_func->preflattened_buffer_map.find(param_var);
+            }
+            if (prim_func != nullptr && param_buf_it != prim_func->preflattened_buffer_map.end()) {
+              Buffer param = (*param_buf_it).second;
+              PrimExpr shape = tvm::tir::Call(
+                  DataType::Handle(), tvm::tir::builtin::tvm_stack_make_shape(), param->shape);
+              Cast var_type(param->dtype, IntImm(DataType::Int(32), 0));
+              call_args.push_back(shape /* shape */);
+              call_args.push_back(make_zero(DataType::Handle()) /* strides */);
+              call_args.push_back(tvm::IntImm(DataType::UInt(32), param->shape.size()) /* ndim */);
+              call_args.push_back(var_type /* carries dtype */);
+              call_args.push_back(param->elem_offset /* elem_offset */);
+            } else {
+              // When the PrimFunc cannot be found, most DLTensor information cannot be populated.
+              PrimExpr shape = tvm::tir::Call(
+                  DataType::Handle(), tvm::tir::builtin::tvm_stack_make_shape(), Array<PrimExpr>());
+              Cast var_type(DataType::Handle(), IntImm(DataType::Int(32), 0));
+              call_args.push_back(shape /* shape */);
+              call_args.push_back(make_zero(DataType::Handle()) /* strides */);
+              call_args.push_back(tvm::IntImm(DataType::UInt(32), 0) /* ndim */);
+              call_args.push_back(var_type /* carries dtype */);
+              call_args.push_back(tvm::IntImm(DataType::UInt(64), 0) /* elem_offset */);
+            }
+            packed_args.push_back(tvm::tir::Call(
+                DataType::Handle(), tvm::tir::builtin::tvm_stack_make_array(), call_args));
           }
         }
+        packed_args.push_back(call->args[call->args.size() - 1]);  // push device_context
         // Evaluate the packed call
+        return tir::Evaluate(tir::Call(call->dtype, call->op, packed_args));
       }
     }
     return StmtExprMutator::VisitStmt_(op);
   }
 
  private:
+  IRModule mod_;
   InputMap inputs_;      // Store the inputs to the primfunc that don't need to be packed.
   int tvm_value_index_;  // Index of the actual tvm_value variable
 };
@@ -109,12 +121,12 @@ Pass LegalizePackedCalls() {
   auto pass_func = [=](PrimFunc f, IRModule m, PassContext ctx) {
     auto* n = f.CopyOnWrite();
 
+    // Note which Var are inputs and exclude them from packing.
     InputMap inputs;
     for (auto i : f->params) {
       inputs[i] = true;
     }
+    n->body = PackedCallLegalizer(m, inputs).Legalize(std::move(n->body));
     return f;
   };
   return CreatePrimFuncPass(pass_func, 0, ""tir.LegalizePackedCalls"", {});"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class BuiltinLower : public StmtExprMutator {
     precheck.device_type_ = this->device_type_;
 
     precheck.alloca_scope_.emplace_back();
-    auto& scope = precheck.alloca_scope_.back();
-    scope.stack_shape =
-        decl_buffer({IntImm(DataType::Int(64), 0)}, DataType::Int(64), ""stack_shape"");
-    scope.stack_tcode =
-        decl_buffer({IntImm(DataType::UInt(64), 0)}, DataType::Int(32), ""stack_tcode"");
 
     precheck.VisitStmt(stmt);
 
@@ -130,31 +133,35 @@ class BuiltinLower : public StmtExprMutator {
     }
 
     alloca_scope_.emplace_back();
-    auto& scope = alloca_scope_.back();
-
-    // Initial check to identify maximum stack sizes.  These are used
-    // to construct Buffer objects to hold the stack, which are then
-    // used when mutating.
-    scope.max_sizes = GetMaxStack(stmt);
-
-    if (scope.max_sizes.shape_stack != -1) {
-      scope.stack_shape = decl_buffer({IntImm(DataType::Int(64), scope.max_sizes.shape_stack)},
-                                      DataType::Int(64), ""stack_shape"");
-      stmt =
-          LetStmt(scope.stack_shape->data, StackAlloca(""shape"", scope.max_sizes.shape_stack), stmt);
-    }
 
-    if (scope.max_sizes.array_stack != 0) {
-      stmt = LetStmt(scope.stack_array, StackAlloca(""array"", scope.max_sizes.array_stack), stmt);
-    }
 
-    if (scope.max_sizes.arg_stack != 0) {
-      scope.stack_tcode = decl_buffer({IntImm(DataType::UInt(64), scope.max_sizes.arg_stack)},
-                                      DataType::Int(32), ""stack_tcode"");
-      stmt = LetStmt(scope.stack_value, StackAlloca(""arg_value"", scope.max_sizes.arg_stack), stmt);
 
-      stmt = LetStmt(scope.stack_tcode->data, StackAlloca(""arg_tcode"", scope.max_sizes.arg_stack),
-                     stmt);
     }
 
     stmt = this->VisitStmt(stmt);
@@ -169,14 +176,22 @@ class BuiltinLower : public StmtExprMutator {
     // allocate space to hold prepare stmts before s
     prep_seq_stack_.emplace_back(std::vector<Stmt>());
 
     auto stmt = StmtExprMutator::VisitStmt(s);
-    auto& scope = alloca_scope_.back();
-    // This invariant asserts the assumption that
-    // make_stack_shape only happens within a call_packed.
-    // We could relax this in the future if we want to
-    // introduce root scope as a separate scope
-    ICHECK_EQ(scope.run_sizes.shape_stack, -1);
-    ICHECK_EQ(scope.run_sizes.array_stack, 0);
 
     auto prep_seq = std::move(prep_seq_stack_.back());
     prep_seq_stack_.pop_back();
@@ -369,9 +384,12 @@ class BuiltinLower : public StmtExprMutator {
                                        make_const(DataType::UInt(16), dtype.lanes())));
     // set byte offset
     int data_bytes = GetVectorBytes(dtype);
-    PrimExpr byte_offset = op->args[5];
-    if (!is_zero(byte_offset)) {
-      byte_offset = byte_offset * make_const(byte_offset.dtype(), data_bytes);
     }
     prep_seq.emplace_back(TVMStructSet(scope.stack_array, idx, builtin::kArrByteOffset,
                                        cast(DataType::UInt(64), byte_offset)));
@@ -436,8 +454,14 @@ class BuiltinLower : public StmtExprMutator {
 
     // cpacked call resource_handle
     if (!use_string_lookup) {
-      tir::Var resource_handle = Downcast<Var>(op->args[arg_count]);
-      packed_args.push_back(StringImm(resource_handle->name_hint));
     }
 
     auto builtin_call = use_string_lookup ? builtin::tvm_call_packed_lowered()
@@ -561,6 +585,7 @@ Pass LowerTVMBuiltin() {
   auto pass_func = [](PrimFunc f, IRModule m, PassContext ctx) {
     auto* n = f.CopyOnWrite();
     n->body = BuiltinLower().Build(n->body);
     return f;
   };
   return CreatePrimFuncPass(pass_func, 0, ""tir.LowerTVMBuiltin"", {});"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"class BuiltinLower : public StmtExprMutator {
     precheck.device_type_ = this->device_type_;
 
     precheck.alloca_scope_.emplace_back();
+    {
+      // NOTE: this scope reference is invalid after any mutation is applied to alloca_scope_.
+      auto& scope = precheck.alloca_scope_.back();
+      scope.stack_shape =
+          decl_buffer({IntImm(DataType::Int(64), 0)}, DataType::Int(64), ""stack_shape"");
+      scope.stack_tcode =
+          decl_buffer({IntImm(DataType::UInt(64), 0)}, DataType::Int(32), ""stack_tcode"");
+    }
 
     precheck.VisitStmt(stmt);
 
@@ -130,31 +133,35 @@ class BuiltinLower : public StmtExprMutator {
     }
 
     alloca_scope_.emplace_back();
+    {
+      // NOTE: this scope reference is invalid after any mutation is applied to alloca_scope_.
+      auto& scope = alloca_scope_.back();
+
+      // Initial check to identify maximum stack sizes.  These are used
+      // to construct Buffer objects to hold the stack, which are then
+      // used when mutating.
+      scope.max_sizes = GetMaxStack(stmt);
+
+      if (scope.max_sizes.shape_stack != -1) {
+        scope.stack_shape = decl_buffer({IntImm(DataType::Int(64), scope.max_sizes.shape_stack)},
+                                        DataType::Int(64), ""stack_shape"");
+        stmt = LetStmt(scope.stack_shape->data, StackAlloca(""shape"", scope.max_sizes.shape_stack),
+                       stmt);
+      }
 
+      if (scope.max_sizes.array_stack != 0) {
+        stmt = LetStmt(scope.stack_array, StackAlloca(""array"", scope.max_sizes.array_stack), stmt);
+      }
 
+      if (scope.max_sizes.arg_stack != 0) {
+        scope.stack_tcode = decl_buffer({IntImm(DataType::UInt(64), scope.max_sizes.arg_stack)},
+                                        DataType::Int(32), ""stack_tcode"");
+        stmt =
+            LetStmt(scope.stack_value, StackAlloca(""arg_value"", scope.max_sizes.arg_stack), stmt);
 
+        stmt = LetStmt(scope.stack_tcode->data, StackAlloca(""arg_tcode"", scope.max_sizes.arg_stack),
+                       stmt);
+      }
     }
 
     stmt = this->VisitStmt(stmt);
@@ -169,14 +176,22 @@ class BuiltinLower : public StmtExprMutator {
     // allocate space to hold prepare stmts before s
     prep_seq_stack_.emplace_back(std::vector<Stmt>());
 
+    auto scope_size = alloca_scope_.size();
     auto stmt = StmtExprMutator::VisitStmt(s);
+    {
+      // NOTE: this scope reference is invalid after any mutation is applied to alloca_scope_.
+      auto& scope = alloca_scope_.back();
+      // This invariant asserts the assumption that
+      // make_stack_shape only happens within a call_packed.
+      // We could relax this in the future if we want to
+      // introduce root scope as a separate scope
+      ICHECK_EQ(alloca_scope_.size(), scope_size)
+          << ""alloca_scope_ length is different before and after recursion"";
+      ICHECK_EQ(scope.run_sizes.shape_stack, -1)
+          << ""Expect no tvm_stack_make_shape outside of CallNodes"";
+      ICHECK_EQ(scope.run_sizes.array_stack, 0)
+          << ""Expect no tvm_stack_make_array outside of CallNodes"";
+    }
 
     auto prep_seq = std::move(prep_seq_stack_.back());
     prep_seq_stack_.pop_back();
@@ -369,9 +384,12 @@ class BuiltinLower : public StmtExprMutator {
                                        make_const(DataType::UInt(16), dtype.lanes())));
     // set byte offset
     int data_bytes = GetVectorBytes(dtype);
+    PrimExpr elem_offset = op->args[5];
+    PrimExpr byte_offset;
+    if (!is_zero(elem_offset)) {
+      byte_offset = elem_offset * make_const(elem_offset.dtype(), data_bytes);
+    } else {
+      byte_offset = elem_offset;
     }
     prep_seq.emplace_back(TVMStructSet(scope.stack_array, idx, builtin::kArrByteOffset,
                                        cast(DataType::UInt(64), byte_offset)));
@@ -436,8 +454,14 @@ class BuiltinLower : public StmtExprMutator {
 
     // cpacked call resource_handle
     if (!use_string_lookup) {
+      PrimExpr last_arg = op->args[arg_count];
+      const VarNode* var_node = last_arg.as<VarNode>();
+      if (var_node != nullptr) {
+        tir::Var resource_handle = GetRef<Var>(var_node);
+        packed_args.push_back(StringImm(resource_handle->name_hint));
+      } else {
+        packed_args.push_back(last_arg);
+      }
     }
 
     auto builtin_call = use_string_lookup ? builtin::tvm_call_packed_lowered()
@@ -561,6 +585,7 @@ Pass LowerTVMBuiltin() {
   auto pass_func = [](PrimFunc f, IRModule m, PassContext ctx) {
     auto* n = f.CopyOnWrite();
     n->body = BuiltinLower().Build(n->body);
+    VLOG(2) << ""LowerTVMBuiltin: "" << f;
     return f;
   };
   return CreatePrimFuncPass(pass_func, 0, ""tir.LowerTVMBuiltin"", {});"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"PoolAllocationToOffsetConverter::ScopeInfo PoolAllocationToOffsetConverter::Upda
 
     int pool_size = all_pools_sizes_[pool_info];
     String buffer_var_name = pool_ref_name + ""_buffer_var"";
-    si.buffer_map.Set(pool_var, Buffer(buffer_var, elem_dtype, {pool_size}, {1}, 1, buffer_var_name,
-                                       16, 1, BufferType::kDefault));
   }
   if (resource_handle) {
     si.params.push_back(resource_handle.value());
@@ -223,8 +226,8 @@ PrimFunc PoolAllocationToOffsetConverter::CreatePrimFuncWithPoolParams(
     if (emit_tvmscript_printable_) {
       original_attrs = DictAttrs();
     }
-    PrimFunc ret = PrimFunc(si.params, new_body, original_primfunc->ret_type, si.buffer_map, {},
-                            original_attrs);
     if (!emit_tvmscript_printable_) {
       ret = WithAttr(ret, tvm::attr::kPoolArgs, si.allocated_pool_params);
     }"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"PoolAllocationToOffsetConverter::ScopeInfo PoolAllocationToOffsetConverter::Upda
 
     int pool_size = all_pools_sizes_[pool_info];
     String buffer_var_name = pool_ref_name + ""_buffer_var"";
+    si.buffer_map.Set(pool_var,
+                      Buffer(buffer_var /* data */, elem_dtype /* dtype */, {pool_size} /* shape */,
+                             {1} /* strides */, 0 /* elem_offset */, buffer_var_name /* name */,
+                             16 /* data_alignment */, 1 /* offset_factor */,
+                             BufferType::kDefault /* buffer-type */));
   }
   if (resource_handle) {
     si.params.push_back(resource_handle.value());
@@ -223,8 +226,8 @@ PrimFunc PoolAllocationToOffsetConverter::CreatePrimFuncWithPoolParams(
     if (emit_tvmscript_printable_) {
       original_attrs = DictAttrs();
     }
+    PrimFunc ret = PrimFunc(si.params, new_body, original_primfunc->ret_type, si.buffer_map,
+                            si.buffer_map, original_attrs);
     if (!emit_tvmscript_printable_) {
       ret = WithAttr(ret, tvm::attr::kPoolArgs, si.allocated_pool_params);
     }"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -25,6 +24,7 @@
 #include <tvm/runtime/metadata.h>
 
 #include ""../src/target/metadata.h""
 
 namespace {
 
@@ -46,12 +46,28 @@ const struct TVMMetadata kNormal = {
 }  // namespace
 
 using ::testing::ElementsAre;
 using ::testing::Eq;
 using ::testing::StrEq;
 using ::tvm::runtime::Downcast;
 
 TEST(Metadata, ParseStruct) {
-  tvm::runtime::metadata::Metadata md = tvm::runtime::metadata::Metadata(&kNormal);
   EXPECT_THAT(md->version(), Eq(TVM_METADATA_VERSION));
   EXPECT_THAT(md->num_inputs(), Eq(2));
 
@@ -137,7 +153,7 @@ class TestVisitor : public tvm::AttrVisitor {
 };
 
 TEST(Metadata, Visitor) {
-  tvm::runtime::metadata::Metadata md = tvm::runtime::metadata::Metadata(&kNormal);
   TestVisitor v;
   ::tvm::ReflectionVTable::Global()->VisitAttrs(md.operator->(), &v);
 
@@ -149,38 +165,38 @@ TEST(Metadata, Visitor) {
   EXPECT_THAT(Downcast<tvm::IntImm>(v.values[0])->value, Eq(TVM_METADATA_VERSION));
 
   // Just identify the tensor.
-  auto input_array = Downcast<tvm::runtime::metadata::MetadataArray>(v.values[1]);
-  EXPECT_THAT(input_array->type_index, Eq(tvm::runtime::metadata::MetadataTypeIndex::kMetadata));
-  EXPECT_THAT(input_array->struct_name, StrEq(""TVMTensorInfo""));
   EXPECT_THAT(input_array->array.size(), Eq(2));
 
-  auto input1 = Downcast<tvm::runtime::metadata::TensorInfo>(input_array->array[0]);
   EXPECT_THAT(input1->name(), StrEq(""input1""));
   EXPECT_THAT(input1->shape(), ElementsAre(1, 5, 5, 3));
   EXPECT_THAT(input1->dtype(), tvm::runtime::DataType(DLDataType{1, 2, 3}));
 
-  auto input2 = Downcast<tvm::runtime::metadata::TensorInfo>(input_array->array[1]);
   EXPECT_THAT(input1->name(), StrEq(""input1""));
   EXPECT_THAT(input1->shape(), ElementsAre(1, 5, 5, 3));
   EXPECT_THAT(input1->dtype(), tvm::runtime::DataType(DLDataType{1, 2, 3}));
 
   auto num_inputs = Downcast<tvm::IntImm>(v.values[2]);
   EXPECT_THAT(num_inputs->value, Eq(2));
 
-  auto output_array = Downcast<tvm::runtime::metadata::MetadataArray>(v.values[3]);
-  EXPECT_THAT(output_array->type_index, Eq(tvm::runtime::metadata::MetadataTypeIndex::kMetadata));
-  EXPECT_THAT(output_array->struct_name, StrEq(""TVMTensorInfo""));
-  auto output1 = Downcast<tvm::runtime::metadata::TensorInfo>(output_array->array[0]);
 
   EXPECT_THAT(output1->name(), Eq(""output1""));
 
   auto num_outputs = Downcast<tvm::IntImm>(v.values[4]);
   EXPECT_THAT(num_outputs->value, Eq(1));
 
-  auto pool_array = Downcast<tvm::runtime::metadata::MetadataArray>(v.values[5]);
-  EXPECT_THAT(pool_array->type_index, Eq(tvm::runtime::metadata::MetadataTypeIndex::kMetadata));
-  EXPECT_THAT(pool_array->struct_name, StrEq(""TVMTensorInfo""));
-  auto pool1 = Downcast<tvm::runtime::metadata::TensorInfo>(pool_array->array[0]);
 
   EXPECT_THAT(pool1->name(), Eq(""pool1""));
 
@@ -193,27 +209,24 @@ TEST(Metadata, Visitor) {
 
 using ::tvm::runtime::make_object;
 TEST(Metadata, InMemory) {
-  tvm::runtime::metadata::Metadata md =
-      tvm::runtime::metadata::Metadata(make_object<tvm::target::metadata::InMemoryMetadataNode>(
-          TVM_METADATA_VERSION,
-          std::vector<tvm::runtime::metadata::TensorInfo>(
-              {tvm::runtime::metadata::TensorInfo(
-                   make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
-                       tvm::String(""Input1""), std::vector<int64_t>{1, 5, 5, 3},
-                       tvm::runtime::DataType(DLDataType{1, 2, 3}))),
-               tvm::runtime::metadata::TensorInfo(
-                   make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
-                       tvm::String(""Input2""), std::vector<int64_t>{1, 5, 5, 3},
-                       tvm::runtime::DataType(DLDataType{2, 3, 4})))}),
-          std::vector<tvm::runtime::metadata::TensorInfo>({tvm::runtime::metadata::TensorInfo(
-              make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
-                  tvm::String(""Output1""), std::vector<int64_t>{3, 8, 8},
-                  tvm::runtime::DataType(DLDataType{3, 4, 5})))}),
-          std::vector<tvm::runtime::metadata::TensorInfo>({tvm::runtime::metadata::TensorInfo(
-              make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
-                  tvm::String(""Pool1""), std::vector<int64_t>{5, 10, 10},
-                  tvm::runtime::DataType(DLDataType{3, 4, 7})))}),
-          ""default""));
 
   auto md_data = md->data();
   EXPECT_THAT(md_data->version, Eq(TVM_METADATA_VERSION));
@@ -251,14 +264,13 @@ TEST(Metadata, InMemory) {
 }
 
 TEST(Metadata, ZeroElementLists) {
-  tvm::runtime::metadata::Metadata md =
-      tvm::runtime::metadata::Metadata(make_object<tvm::target::metadata::InMemoryMetadataNode>(
-          TVM_METADATA_VERSION, std::vector<tvm::runtime::metadata::TensorInfo>({}),
-          std::vector<tvm::runtime::metadata::TensorInfo>({tvm::runtime::metadata::TensorInfo(
-              make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
-                  tvm::String(""Output1""), std::vector<int64_t>{},
-                  tvm::runtime::DataType(DLDataType{3, 4, 5})))}),
-          std::vector<tvm::runtime::metadata::TensorInfo>({}), ""default""));
 
   EXPECT_THAT(md->data()->num_inputs, Eq(0));
   EXPECT_THAT(md->inputs().size(), Eq(0));
@@ -274,3 +286,84 @@ TEST(Metadata, ZeroElementLists) {
   EXPECT_THAT(md->num_pools(), Eq(0));
   EXPECT_THAT(md->pools(), ElementsAre());
 }"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" /*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -25,6 +24,7 @@
 #include <tvm/runtime/metadata.h>
 
 #include ""../src/target/metadata.h""
+#include ""../src/target/metadata_utils.h""
 
 namespace {
 
@@ -46,12 +46,28 @@ const struct TVMMetadata kNormal = {
 }  // namespace
 
 using ::testing::ElementsAre;
+using ::testing::ElementsAreArray;
 using ::testing::Eq;
+using ::testing::Matcher;
+using ::testing::MatcherInterface;
+using ::testing::MatchResultListener;
 using ::testing::StrEq;
+
+using ::tvm::codegen::metadata::DiscoverArraysVisitor;
+using ::tvm::codegen::metadata::DiscoverComplexTypesVisitor;
+using ::tvm::codegen::metadata::kMetadataGlobalSymbol;
+
+using ::tvm::runtime::Array;
 using ::tvm::runtime::Downcast;
+using ::tvm::runtime::ObjectRef;
+
+using ::tvm::runtime::metadata::Metadata;
+using ::tvm::runtime::metadata::MetadataArray;
+using ::tvm::runtime::metadata::MetadataKind;
+using ::tvm::runtime::metadata::TensorInfo;
 
 TEST(Metadata, ParseStruct) {
+  Metadata md = Metadata(&kNormal);
   EXPECT_THAT(md->version(), Eq(TVM_METADATA_VERSION));
   EXPECT_THAT(md->num_inputs(), Eq(2));
 
@@ -137,7 +153,7 @@ class TestVisitor : public tvm::AttrVisitor {
 };
 
 TEST(Metadata, Visitor) {
+  Metadata md = Metadata(&kNormal);
   TestVisitor v;
   ::tvm::ReflectionVTable::Global()->VisitAttrs(md.operator->(), &v);
 
@@ -149,38 +165,38 @@ TEST(Metadata, Visitor) {
   EXPECT_THAT(Downcast<tvm::IntImm>(v.values[0])->value, Eq(TVM_METADATA_VERSION));
 
   // Just identify the tensor.
+  auto input_array = Downcast<MetadataArray>(v.values[1]);
+  EXPECT_THAT(input_array->kind, Eq(MetadataKind::kMetadata));
+  EXPECT_THAT(input_array->type_key, StrEq(""metadata.TensorInfoNode""));
   EXPECT_THAT(input_array->array.size(), Eq(2));
 
+  auto input1 = Downcast<TensorInfo>(input_array->array[0]);
   EXPECT_THAT(input1->name(), StrEq(""input1""));
   EXPECT_THAT(input1->shape(), ElementsAre(1, 5, 5, 3));
   EXPECT_THAT(input1->dtype(), tvm::runtime::DataType(DLDataType{1, 2, 3}));
 
+  auto input2 = Downcast<TensorInfo>(input_array->array[1]);
   EXPECT_THAT(input1->name(), StrEq(""input1""));
   EXPECT_THAT(input1->shape(), ElementsAre(1, 5, 5, 3));
   EXPECT_THAT(input1->dtype(), tvm::runtime::DataType(DLDataType{1, 2, 3}));
 
   auto num_inputs = Downcast<tvm::IntImm>(v.values[2]);
   EXPECT_THAT(num_inputs->value, Eq(2));
 
+  auto output_array = Downcast<MetadataArray>(v.values[3]);
+  EXPECT_THAT(output_array->kind, Eq(MetadataKind::kMetadata));
+  EXPECT_THAT(output_array->type_key, StrEq(""metadata.TensorInfoNode""));
+  auto output1 = Downcast<TensorInfo>(output_array->array[0]);
 
   EXPECT_THAT(output1->name(), Eq(""output1""));
 
   auto num_outputs = Downcast<tvm::IntImm>(v.values[4]);
   EXPECT_THAT(num_outputs->value, Eq(1));
 
+  auto pool_array = Downcast<MetadataArray>(v.values[5]);
+  EXPECT_THAT(pool_array->kind, Eq(MetadataKind::kMetadata));
+  EXPECT_THAT(pool_array->type_key, StrEq(""metadata.TensorInfoNode""));
+  auto pool1 = Downcast<TensorInfo>(pool_array->array[0]);
 
   EXPECT_THAT(pool1->name(), Eq(""pool1""));
 
@@ -193,27 +209,24 @@ TEST(Metadata, Visitor) {
 
 using ::tvm::runtime::make_object;
 TEST(Metadata, InMemory) {
+  Metadata md = Metadata(make_object<tvm::target::metadata::InMemoryMetadataNode>(
+      TVM_METADATA_VERSION,
+      std::vector<TensorInfo>(
+          {TensorInfo(make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
+               tvm::String(""Input1""), std::vector<int64_t>{1, 5, 5, 3},
+               tvm::runtime::DataType(DLDataType{1, 2, 3}))),
+           TensorInfo(make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
+               tvm::String(""Input2""), std::vector<int64_t>{1, 5, 5, 3},
+               tvm::runtime::DataType(DLDataType{2, 3, 4})))}),
+      std::vector<TensorInfo>(
+          {TensorInfo(make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
+              tvm::String(""Output1""), std::vector<int64_t>{3, 8, 8},
+              tvm::runtime::DataType(DLDataType{3, 4, 5})))}),
+      std::vector<TensorInfo>(
+          {TensorInfo(make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
+              tvm::String(""Pool1""), std::vector<int64_t>{5, 10, 10},
+              tvm::runtime::DataType(DLDataType{3, 4, 7})))}),
+      ""default""));
 
   auto md_data = md->data();
   EXPECT_THAT(md_data->version, Eq(TVM_METADATA_VERSION));
@@ -251,14 +264,13 @@ TEST(Metadata, InMemory) {
 }
 
 TEST(Metadata, ZeroElementLists) {
+  Metadata md = Metadata(make_object<tvm::target::metadata::InMemoryMetadataNode>(
+      TVM_METADATA_VERSION, std::vector<TensorInfo>({}),
+      std::vector<TensorInfo>(
+          {TensorInfo(make_object<tvm::target::metadata::InMemoryTensorInfoNode>(
+              tvm::String(""Output1""), std::vector<int64_t>{},
+              tvm::runtime::DataType(DLDataType{3, 4, 5})))}),
+      std::vector<TensorInfo>({}), ""default""));
 
   EXPECT_THAT(md->data()->num_inputs, Eq(0));
   EXPECT_THAT(md->inputs().size(), Eq(0));
@@ -274,3 +286,84 @@ TEST(Metadata, ZeroElementLists) {
   EXPECT_THAT(md->num_pools(), Eq(0));
   EXPECT_THAT(md->pools(), ElementsAre());
 }
+
+TEST(MetadataArray, GetElementCStructName) {
+  MetadataArray arr_struct{make_object<tvm::runtime::metadata::MetadataArrayNode>(
+      Array<ObjectRef>(), MetadataKind::kMetadata, ""metadata.FooMetadataNode"")};
+  EXPECT_THAT(arr_struct->kind, Eq(MetadataKind::kMetadata));
+  EXPECT_THAT(arr_struct->get_element_c_struct_name(), StrEq(""TVMFooMetadata""));
+
+  MetadataArray arr_int{make_object<tvm::runtime::metadata::MetadataArrayNode>(
+      Array<ObjectRef>(), MetadataKind::kInt64, nullptr)};
+  EXPECT_THROW(arr_int->get_element_c_struct_name(), std::runtime_error);
+}
+
+namespace {
+std::string ExplainDiscoveredNameEq(bool negation, std::string expected_name) {
+  std::stringstream ss;
+  ss << ""std::get<0>(discovered_array) "" << (negation ? ""isn't"" : ""is"") << "" equal to ""
+     << expected_name;
+  return ss.str();
+}
+}  // namespace
+
+MATCHER_P(DiscoveredNameEq, expected_name, ExplainDiscoveredNameEq(negation, expected_name)) {
+  return std::string(std::get<0>(arg)) == expected_name;
+}
+
+TEST(DiscoverArraysVisitor, DiscoverArrays) {
+  std::vector<DiscoverArraysVisitor::DiscoveredArray> q;
+  DiscoverArraysVisitor visitor(&q);
+
+  Metadata md = Metadata(&kNormal);
+  visitor.Visit(kMetadataGlobalSymbol, &md);
+
+  EXPECT_THAT(q, ElementsAreArray({DiscoveredNameEq(""kTvmgenMetadata_inputs_0_shape""),
+                                   DiscoveredNameEq(""kTvmgenMetadata_inputs_1_shape""),
+                                   DiscoveredNameEq(""kTvmgenMetadata_inputs""),
+                                   DiscoveredNameEq(""kTvmgenMetadata_outputs_0_shape""),
+                                   DiscoveredNameEq(""kTvmgenMetadata_outputs""),
+                                   DiscoveredNameEq(""kTvmgenMetadata_pools_0_shape""),
+                                   DiscoveredNameEq(""kTvmgenMetadata_pools"")}));
+}
+
+template <typename T,
+          std::enable_if_t<std::is_base_of<tvm::runtime::metadata::MetadataBase, T>::value, bool> =
+              true>
+class TVMObjectIsInstanceMatcher : public MatcherInterface<tvm::runtime::metadata::MetadataBase> {
+ public:
+  using is_gtest_matcher = void;
+
+  bool MatchAndExplain(tvm::runtime::metadata::MetadataBase arg,
+                       MatchResultListener* os) const override {
+    bool result = arg->IsInstance<typename T::ContainerType>();
+    if (!result) {
+      (*os) << ""is an instance of type "" << T::ContainerType::_type_key;
+    }
+
+    return result;
+  }
+
+  void DescribeTo(std::ostream* os) const override {
+    (*os) << ""is an instance of type "" << T::ContainerType::_type_key;
+  }
+
+  void DescribeNegationTo(std::ostream* os) const override {
+    (*os) << ""is not an instance of type "" << T::ContainerType::_type_key;
+  }
+};
+
+template <typename T>
+Matcher<tvm::runtime::metadata::MetadataBase> TVMObjectIsInstance() {
+  return Matcher<tvm::runtime::metadata::MetadataBase>(new TVMObjectIsInstanceMatcher<T>());
+}
+
+TEST(DiscoverComplexTypesVisitor, DiscoverComplexTypes) {
+  std::vector<tvm::runtime::metadata::MetadataBase> q;
+  DiscoverComplexTypesVisitor visitor(&q);
+
+  Metadata md = Metadata(&kNormal);
+  visitor.Discover(md);
+
+  EXPECT_THAT(q, ElementsAre(TVMObjectIsInstance<TensorInfo>(), TVMObjectIsInstance<Metadata>()));
+}"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_aot_executor(hexagon_session):
             params=params,
             target=tvm.target.Target(target_hexagon, host=""c""),
             runtime=Runtime(""cpp""),
-            executor=Executor(""aot"", {""unpacked-api"": False, ""interface-api"": ""c""}),
         )
 
     if hexagon_session is None:
@@ -401,7 +401,7 @@ def test_aot_executor_multiple_conv2d(hexagon_session):
             params=params,
             target=tvm.target.Target(target_hexagon, host=""c""),
             runtime=Runtime(""cpp""),
-            executor=Executor(""aot"", {""unpacked-api"": False, ""interface-api"": ""c""}),
         )
 
     if hexagon_session is None:"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_aot_executor(hexagon_session):
             params=params,
             target=tvm.target.Target(target_hexagon, host=""c""),
             runtime=Runtime(""cpp""),
+            executor=Executor(""aot"", {""unpacked-api"": False, ""interface-api"": ""packed""}),
         )
 
     if hexagon_session is None:
@@ -401,7 +401,7 @@ def test_aot_executor_multiple_conv2d(hexagon_session):
             params=params,
             target=tvm.target.Target(target_hexagon, host=""c""),
             runtime=Runtime(""cpp""),
+            executor=Executor(""aot"", {""unpacked-api"": False, ""interface-api"": ""packed""}),
         )
 
     if hexagon_session is None:"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_device_api_hooks_unpacked_api(device_api_main_func):
         + "" device_context_ethos_u))\n""
     )
     # Open Device
     assert (
         str(main_func.body[1][0][0][0])
         == ""tir.tvm_check_return(0, -1, tir.call_extern(""
@@ -239,23 +240,11 @@ def test_without_device_api_packed_api(non_device_api_main_func):
 
     main_func = non_device_api_main_func(interface_api=""packed"", use_unpacked_api=False)
     assert str(main_func.body) == (
-        'let tvm_value_3 = tir.tvm_stack_alloca(""array"", 1)\n'
-        'let tvm_value_2 = tir.tvm_stack_alloca(""array"", 1)\n'
-        'let tvm_value_1 = tir.tvm_stack_alloca(""array"", 1)\n'
-        'let tvm_value_0 = tir.tvm_stack_alloca(""array"", 1)\n'
-        ""tir.tvm_struct_set(tvm_value_0, 0, 1, x_buffer_var)\n""
-        ""tir.tvm_struct_set(tvm_value_0, 0, 10, 1)\n""
-        ""tir.tvm_struct_set(tvm_value_0, 0, 9, 0)\n""
-        ""tir.tvm_struct_set(tvm_value_1, 0, 1, y_buffer_var)\n""
-        ""tir.tvm_struct_set(tvm_value_1, 0, 10, 1)\n""
-        ""tir.tvm_struct_set(tvm_value_1, 0, 9, 0)\n""
-        ""tir.tvm_struct_set(tvm_value_2, 0, 1, output_buffer_var)\n""
-        ""tir.tvm_struct_set(tvm_value_2, 0, 10, 1)\n""
-        ""tir.tvm_struct_set(tvm_value_2, 0, 9, 0)\n""
-        ""tir.tvm_struct_set(tvm_value_3, 0, 1, tir.reinterpret((uint64)0))\n""
-        ""tir.tvm_struct_set(tvm_value_3, 0, 10, 1)\n""
-        ""tir.tvm_struct_set(tvm_value_3, 0, 9, 0)\n""
-        'tir.tvm_call_cpacked(""tvmgen_default_fused_multiply"", tvm_value_0, tvm_value_1, tvm_value_2, tvm_value_3)\n'
     )
 
 "
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_device_api_hooks_unpacked_api(device_api_main_func):
         + "" device_context_ethos_u))\n""
     )
     # Open Device
+    print(""main func"", repr(main_func.body))
     assert (
         str(main_func.body[1][0][0][0])
         == ""tir.tvm_check_return(0, -1, tir.call_extern(""
@@ -239,23 +240,11 @@ def test_without_device_api_packed_api(non_device_api_main_func):
 
     main_func = non_device_api_main_func(interface_api=""packed"", use_unpacked_api=False)
     assert str(main_func.body) == (
+        'tir.tvm_call_cpacked(""tvmgen_default_fused_multiply"", '
+        ""tir.tvm_stack_make_array(x_buffer_var, tir.tvm_stack_make_shape(10, 10), tir.reinterpret((uint64)0), (uint32)2, float32(0), 0), ""
+        ""tir.tvm_stack_make_array(y_buffer_var, tir.tvm_stack_make_shape(1, 10), tir.reinterpret((uint64)0), (uint32)2, float32(0), 0), ""
+        ""tir.tvm_stack_make_array(output_buffer_var, tir.tvm_stack_make_shape(10, 10), tir.reinterpret((uint64)0), (uint32)2, float32(0), 0), ""
+        ""tir.reinterpret((uint64)0))\n""
     )
 
 "
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" import pytest
 
 import tvm
-from tvm import relay, TVMError
-from tvm.ir.module import IRModule
-from tvm.relay import backend, testing, transform
-from tvm.relay.testing import byoc
-from tvm.relay.op.annotation import compiler_begin, compiler_end
-from aot_test_utils import (
-    AOTTestModel,
-    AOT_DEFAULT_RUNNER,
-    generate_ref_data,
-    convert_to_relay,
-    compile_and_run,
-    compile_models,
-    parametrize_aot_options,
-)
 
 
 def test_error_c_interface():
@@ -51,25 +41,22 @@ def test_error_c_interface():
     with pytest.raises(
         tvm.TVMError,
         match=re.escape(
-            'Either need interface_api == ""packed"" (got: c) or '
-            ""unpacked-api == true (got: (bool)0) when targeting ""
-            ""c runtime""
         ),
     ):
-        compile_and_run(
-            AOTTestModel(
-                module=IRModule.from_expr(func), inputs={}, outputs=generate_ref_data(func, {})
-            ),
-            test_runner,
-            interface_api,
-            use_unpacked_api,
         )
 
 
 enable_usmp = tvm.testing.parameter(True, False)
 
 
-def test_conv2d(enable_usmp):
     RELAY_MODEL = textwrap.dedent(
         """"""\
         #[version = ""0.0.5""]
@@ -117,7 +104,7 @@ def @main(%data : Tensor[(1, 3, 64, 64), uint8], %weight : Tensor[(3, 3, 5, 5),
         mod = tvm.relay.build(
             ir_mod,
             params=params,
-            target=""c"",
             executor=backend.Executor(""aot"", {""interface-api"": ""packed""}),
         )
 
@@ -131,18 +118,20 @@ def @main(%data : Tensor[(1, 3, 64, 64), uint8], %weight : Tensor[(3, 3, 5, 5),
     assert (runner.get_output(0).asnumpy() == list(ref_outputs.values())[0]).all()
 
 
-def test_mobilenet():
     ir_mod, params = testing.mobilenet.get_workload(batch_size=1)
     data_shape = [int(x) for x in ir_mod[""main""].checked_type.arg_types[0].shape]
     data = np.random.uniform(size=data_shape).astype(""float32"")
     inputs = {""data"": data}
     ref_outputs = generate_ref_data(ir_mod, inputs, params)
 
-    with tvm.transform.PassContext(opt_level=3, config={""tir.disable_vectorize"": True}):
         mod = tvm.relay.build(
             ir_mod,
             params=params,
-            target=""c"",
             executor=backend.Executor(""aot"", {""interface-api"": ""packed""}),
         )
 "
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" import pytest
 
 import tvm
+from tvm import IRModule
+from tvm import relay
+from tvm.relay import backend, testing
+from aot_test_utils import AOT_DEFAULT_RUNNER, AOTTestModel, generate_ref_data, compile_and_run
 
 
 def test_error_c_interface():
@@ -51,25 +41,22 @@ def test_error_c_interface():
     with pytest.raises(
         tvm.TVMError,
         match=re.escape(
+            'Need unpacked-api == false (got: 0) and interface-api == ""packed"" (got: c) when '
+            ""targeting c++ runtime""
         ),
     ):
+        tvm.relay.build(
+            IRModule.from_expr(func),
+            target=""llvm"",
+            executor=backend.Executor(""aot"", {""interface-api"": ""c""}),
         )
 
 
 enable_usmp = tvm.testing.parameter(True, False)
+target_kind = tvm.testing.parameter(""c"", ""llvm"")
 
 
+def test_conv2d(enable_usmp, target_kind):
     RELAY_MODEL = textwrap.dedent(
         """"""\
         #[version = ""0.0.5""]
@@ -117,7 +104,7 @@ def @main(%data : Tensor[(1, 3, 64, 64), uint8], %weight : Tensor[(3, 3, 5, 5),
         mod = tvm.relay.build(
             ir_mod,
             params=params,
+            target=target_kind,
             executor=backend.Executor(""aot"", {""interface-api"": ""packed""}),
         )
 
@@ -131,18 +118,20 @@ def @main(%data : Tensor[(1, 3, 64, 64), uint8], %weight : Tensor[(3, 3, 5, 5),
     assert (runner.get_output(0).asnumpy() == list(ref_outputs.values())[0]).all()
 
 
+def test_mobilenet(enable_usmp, target_kind):
     ir_mod, params = testing.mobilenet.get_workload(batch_size=1)
     data_shape = [int(x) for x in ir_mod[""main""].checked_type.arg_types[0].shape]
     data = np.random.uniform(size=data_shape).astype(""float32"")
     inputs = {""data"": data}
     ref_outputs = generate_ref_data(ir_mod, inputs, params)
 
+    with tvm.transform.PassContext(
+        opt_level=3, config={""tir.disable_vectorize"": True, ""tir.usmp.enable"": enable_usmp}
+    ):
         mod = tvm.relay.build(
             ir_mod,
             params=params,
+            target=target_kind,
             executor=backend.Executor(""aot"", {""interface-api"": ""packed""}),
         )
 "
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_error_c_interface_with_packed_api():
         tvm.TVMError,
         match=re.escape(
             'Either need interface_api == ""packed"" (got: c) or '
-            ""unpacked-api == true (got: (bool)0) when targeting ""
             ""c runtime""
         ),
     ):"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_error_c_interface_with_packed_api():
         tvm.TVMError,
         match=re.escape(
             'Either need interface_api == ""packed"" (got: c) or '
+            ""unpacked-api == true (got: 0) when targeting ""
             ""c runtime""
         ),
     ):"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" 
 @tvm.script.ir_module
 class Module:
     @T.prim_func
     def tir_packed_call() -> None:
         A = T.var(""handle"")
         B = T.var(""handle"")
         C = T.var(""handle"")
         # body
         T.evaluate(
             T.tvm_call_cpacked(
                 ""tvm_test_cpacked"",
                 A,
                 B,
                 C,
                 dtype=""int32"",
             )
         )
 
 
 @tvm.script.ir_module
 class Expected:
     @T.prim_func
     def tir_packed_call() -> None:
         A = T.var(""handle"")
         B = T.var(""handle"")
         C = T.var(""handle"")
 
         # body
-        tvm_value_2 = T.var(""handle"")
-        tvm_value_1 = T.var(""handle"")
-        tvm_value_0 = T.var(""handle"")
-        with T.let(tvm_value_2, T.tvm_stack_alloca(""array"", 1, dtype=""handle"")):
-            with T.let(tvm_value_1, T.tvm_stack_alloca(""array"", 1, dtype=""handle"")):
-                with T.let(tvm_value_0, T.tvm_stack_alloca(""array"", 1, dtype=""handle"")):
-                    T.evaluate(T.tvm_struct_set(tvm_value_0, 0, 1, A, dtype=""handle""))
-                    T.evaluate(T.tvm_struct_set(tvm_value_0, 0, 10, 1, dtype=""handle""))
-                    T.evaluate(T.tvm_struct_set(tvm_value_0, 0, 9, 0, dtype=""handle""))
-
-                    T.evaluate(T.tvm_struct_set(tvm_value_1, 0, 1, B, dtype=""handle""))
-                    T.evaluate(T.tvm_struct_set(tvm_value_1, 0, 10, 1, dtype=""handle""))
-                    T.evaluate(T.tvm_struct_set(tvm_value_1, 0, 9, 0, dtype=""handle""))
-
-                    T.evaluate(T.tvm_struct_set(tvm_value_2, 0, 1, C, dtype=""handle""))
-                    T.evaluate(T.tvm_struct_set(tvm_value_2, 0, 10, 1, dtype=""handle""))
-                    T.evaluate(T.tvm_struct_set(tvm_value_2, 0, 9, 0, dtype=""handle""))
-
-                    T.evaluate(
-                        T.tvm_call_cpacked(
-                            ""tvm_test_cpacked"",
-                            tvm_value_0,
-                            tvm_value_1,
-                            tvm_value_2,
-                            dtype=""int32"",
-                        )
-                    )
 
 
 def test_aot_packed_call():"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";" 
 @tvm.script.ir_module
 class Module:
+    @T.prim_func
+    def tvm_test_cpacked(
+        A: T.handle, B: T.handle, C: T.handle, device_context: T.handle
+    ) -> T.handle:
+        A_0 = T.match_buffer(A, (1,), dtype=""float32"")
+        A_0pre = T.preflattened_buffer(A_0, (1,), dtype=""float32"")
+        B_0 = T.match_buffer(B, (1,), dtype=""float32"")
+        B_0pre = T.preflattened_buffer(B_0, (1,), dtype=""float32"")
+        C_0 = T.match_buffer(C, (1,), dtype=""float32"")
+        C_0pre = T.preflattened_buffer(C_0, (1,), dtype=""float32"")
+        T.evaluate(C)
+
     @T.prim_func
     def tir_packed_call() -> None:
         A = T.var(""handle"")
         B = T.var(""handle"")
         C = T.var(""handle"")
+        device_context = T.var(""handle"")
         # body
         T.evaluate(
             T.tvm_call_cpacked(
                 ""tvm_test_cpacked"",
                 A,
                 B,
                 C,
+                device_context,
                 dtype=""int32"",
             )
         )
 
 
 @tvm.script.ir_module
 class Expected:
+    @T.prim_func
+    def tvm_test_cpacked(
+        A: T.handle, B: T.handle, C: T.handle, device_context: T.handle
+    ) -> T.handle:
+        A_0 = T.match_buffer(A, (1,), dtype=""float32"")
+        A_0pre = T.preflattened_buffer(A_0, (1,), dtype=""float32"")
+        B_0 = T.match_buffer(B, (1,), dtype=""float32"")
+        B_0pre = T.preflattened_buffer(B_0, (1,), dtype=""float32"")
+        C_0 = T.match_buffer(C, (1,), dtype=""float32"")
+        C_0pre = T.preflattened_buffer(C_0, (1,), dtype=""float32"")
+        T.evaluate(C)
+
     @T.prim_func
     def tir_packed_call() -> None:
         A = T.var(""handle"")
         B = T.var(""handle"")
         C = T.var(""handle"")
+        device_context = T.var(""handle"")
 
         # body
+        T.evaluate(
+            T.tvm_call_cpacked(
+                ""tvm_test_cpacked"",
+                T.tvm_stack_make_array(
+                    A,
+                    T.tvm_stack_make_shape(1, dtype=""handle""),
+                    T.reinterpret(T.uint64(0), dtype=""handle""),
+                    T.uint32(1),
+                    T.cast(0, dtype=""float32""),
+                    0,
+                    dtype=""handle"",
+                ),
+                T.tvm_stack_make_array(
+                    B,
+                    T.tvm_stack_make_shape(1, dtype=""handle""),
+                    T.reinterpret(T.uint64(0), dtype=""handle""),
+                    T.uint32(1),
+                    T.cast(0, dtype=""float32""),
+                    0,
+                    dtype=""handle"",
+                ),
+                T.tvm_stack_make_array(
+                    C,
+                    T.tvm_stack_make_shape(1, dtype=""handle""),
+                    T.reinterpret(T.uint64(0), dtype=""handle""),
+                    T.uint32(1),
+                    T.cast(0, dtype=""float32""),
+                    0,
+                    dtype=""handle"",
+                ),
+                device_context,
+                dtype=""int32"",
+            )
+        )
 
 
 def test_aot_packed_call():"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def tvmgen_default_fused_cast_subtract(placeholder_2: T.handle, placeholder_3: T
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_cast_subtract"", ""tir.noalias"": True})
         placeholder_4 = T.match_buffer(placeholder_2, [150528], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
         placeholder_5 = T.match_buffer(placeholder_3, [1], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         T_subtract_1 = T.match_buffer(T_subtract, [452], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         # body
         for ax0_ax1_fused_1 in T.serial(0, 224):
             for ax2_1, ax3_inner_1 in T.grid(224, 3):
@@ -86,9 +89,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast(placeholde
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast"", ""tir.noalias"": True})
         placeholder_65 = T.match_buffer(placeholder_62, [150528], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         placeholder_66 = T.match_buffer(placeholder_63, [9408], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         placeholder_67 = T.match_buffer(placeholder_64, [64], dtype=""int32"", elem_offset=0, align=128, offset_factor=1)
         T_cast_21 = T.match_buffer(T_cast_20, [289], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
         # body
         PaddedInput_7 = T.allocate([157323], ""int16"", ""global"")
         for i0_i1_fused_7 in T.serial(0, 229):
@@ -108,7 +115,9 @@ def tvmgen_default_fused_nn_max_pool2d_cast(placeholder_28: T.handle, T_cast_6:
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_max_pool2d_cast"", ""tir.noalias"": True})
         placeholder_29 = T.match_buffer(placeholder_28, [802816], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
         T_cast_7 = T.match_buffer(T_cast_6, [177], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         # body
         tensor_2 = T.allocate([200704], ""uint8"", ""global"")
         for ax0_ax1_fused_4 in T.serial(0, 56):
@@ -140,9 +149,9 @@ def __tvm_main__(input: T.handle, output: T.handle) -> None:
 @tvm.script.ir_module
 class LinearStructurePlanned:
     @T.prim_func
-    def __tvm_main__(input: T.handle, fast_memory_0_var: T.Ptr[T.uint8], slow_memory_1_var: T.Ptr[T.uint8],  output: T.handle) -> None:
-        fast_memory_0_buffer_var = T.match_buffer(fast_memory_0_var, [200704], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
-        slow_memory_1_buffer_var = T.match_buffer(slow_memory_1_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         T.attr(""default"", ""device_id"", 0)
         T.attr(""default"", ""device_type"", 1)
@@ -155,9 +164,13 @@ def __tvm_main__(input: T.handle, fast_memory_0_var: T.Ptr[T.uint8], slow_memory
     @T.prim_func
     def tvmgen_default_fused_nn_max_pool2d_cast(placeholder_28: T.handle, T_cast_6: T.handle, fast_memory_6_var: T.Ptr[T.uint8], slow_memory_7_var: T.Ptr[T.uint8]) -> None:
         placeholder_29 = T.match_buffer(placeholder_28, [802816], dtype=""uint8"")
         T_cast_7 = T.match_buffer(T_cast_6, [177], dtype=""int16"")
-        fast_memory_6_buffer_var = T.match_buffer(fast_memory_6_var, [200704], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
-        slow_memory_7_buffer_var = T.match_buffer(slow_memory_7_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         tensor_2_let = T.buffer_decl([200704], dtype=""uint8"")
         with T.let(tensor_2_let.data, T.address_of(fast_memory_6_buffer_var[0], dtype=""handle"")):
@@ -172,22 +185,33 @@ def tvmgen_default_fused_nn_max_pool2d_cast(placeholder_28: T.handle, T_cast_6:
     @T.prim_func
     def tvmgen_default_fused_cast_subtract(placeholder_2: T.handle, placeholder_3: T.handle, T_subtract: T.handle, fast_memory_2_var: T.Ptr[T.uint8], slow_memory_3_var: T.Ptr[T.uint8]) -> None:
         placeholder_4 = T.match_buffer(placeholder_2, [150528], dtype=""uint8"")
         placeholder_5 = T.match_buffer(placeholder_3, [1], dtype=""int16"")
         T_subtract_1 = T.match_buffer(T_subtract, [452], dtype=""int16"")
-        fast_memory_2_buffer_var = T.match_buffer(fast_memory_2_var, [200704], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
-        slow_memory_3_buffer_var = T.match_buffer(slow_memory_3_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         for ax0_ax1_fused_1, ax2_1, ax3_inner_1 in T.grid(224, 224, 3):
             T_subtract_1[ax0_ax1_fused_1 * 672 + ax2_1 * 3 + ax3_inner_1] = T.cast(placeholder_4[ax0_ax1_fused_1 * 672 + ax2_1 * 3 + ax3_inner_1], ""int16"") - placeholder_5[0]
 
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast(placeholder_62: T.handle, placeholder_63: T.handle, placeholder_64: T.handle, T_cast_20: T.handle, fast_memory_4_var: T.Ptr[T.uint8], slow_memory_5_var: T.Ptr[T.uint8]) -> None:
         placeholder_65 = T.match_buffer(placeholder_62, [150528], dtype=""int16"")
         placeholder_66 = T.match_buffer(placeholder_63, [9408], dtype=""int16"")
         placeholder_67 = T.match_buffer(placeholder_64, [64], dtype=""int32"")
         T_cast_21 = T.match_buffer(T_cast_20, [289], dtype=""uint8"")
-        fast_memory_4_buffer_var = T.match_buffer(fast_memory_4_var, [200704], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
-        slow_memory_5_buffer_var = T.match_buffer(slow_memory_5_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         PaddedInput_7_let = T.buffer_decl([157323], ""int16"")
         with T.let(PaddedInput_7_let.data, T.address_of(slow_memory_5_buffer_var[802816], dtype=""handle"")):
@@ -251,8 +275,11 @@ def tvmgen_default_fused_cast_subtract_fixed_point_multiply_add_clip_cast_cast(p
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_cast_subtract_fixed_point_multiply_add_clip_cast_cast"", ""tir.noalias"": True})
         placeholder_2 = T.match_buffer(placeholder, [360000], dtype=""uint8"")
         placeholder_3 = T.match_buffer(placeholder_1, [64], dtype=""int32"")
         T_cast_1 = T.match_buffer(T_cast, [215], dtype=""int16"")
         # body
         for ax0_ax1_fused, ax2, ax3_outer, ax3_inner in T.grid(75, 75, 4, 16):
             T_cast_1[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = T.cast(T.cast(T.max(T.min(T.q_multiply_shift(T.cast(placeholder_2[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner], ""int32"") - 94, 1843157232, 31, 1, dtype=""int32"") + placeholder_3[ax3_outer * 16 + ax3_inner], 255), 0), ""uint8""), ""int16"")
@@ -262,9 +289,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1(pla
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1"", ""tir.noalias"": True})
         placeholder_13 = T.match_buffer(placeholder_10, [360000], dtype=""int16"")
         placeholder_14 = T.match_buffer(placeholder_11, [36864], dtype=""int16"")
         placeholder_15 = T.match_buffer(placeholder_12, [64], dtype=""int32"")
         T_cast_5 = T.match_buffer(T_cast_4, [215], dtype=""int16"")
         # body
         PaddedInput_1 = T.allocate([379456], ""int16"", ""global"")
         for i0_i1_fused_1, i2_1, i3_1 in T.grid(77, 77, 64):
@@ -283,9 +314,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_15934180698220515269_"", ""tir.noalias"": True})
         placeholder_19 = T.match_buffer(placeholder_16, [360000], dtype=""int16"")
         placeholder_20 = T.match_buffer(placeholder_17, [16384], dtype=""int16"")
         placeholder_21 = T.match_buffer(placeholder_18, [256], dtype=""int32"")
         T_add_1 = T.match_buffer(T_add, [407], dtype=""int32"")
         # body
         PaddedInput_2 = T.allocate([360000], ""int16"", ""global"")
         for i0_i1_fused_2, i2_2, i3_2 in T.grid(75, 75, 64):
@@ -305,10 +340,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_4200876283395191415_"", ""tir.noalias"": True})
         placeholder_29 = T.match_buffer(placeholder_22, [360000], dtype=""int16"")
         placeholder_27 = T.match_buffer(placeholder_23, [16384], dtype=""int16"")
         placeholder_26 = T.match_buffer(placeholder_24, [256], dtype=""int32"")
         placeholder_28 = T.match_buffer(placeholder_25, [1440000], dtype=""int32"")
         T_cast_7 = T.match_buffer(T_cast_6, [407], dtype=""uint8"")
         # body
         PaddedInput_3 = T.allocate([360000], ""int16"", ""global"")
         for i0_i1_fused_3, i2_3, i3_3 in T.grid(75, 75, 64):
@@ -345,9 +385,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast(place
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast"", ""tir.noalias"": True})
         placeholder_7 = T.match_buffer(placeholder_4, [360000], dtype=""int16"")
         placeholder_8 = T.match_buffer(placeholder_5, [4096], dtype=""int16"")
         placeholder_9 = T.match_buffer(placeholder_6, [64], dtype=""int32"")
         T_cast_3 = T.match_buffer(T_cast_2, [215], dtype=""int16"")
         # body
         PaddedInput = T.allocate([360000], ""int16"", ""global"")
         for i0_i1_fused, i2, i3 in T.grid(75, 75, 64):
@@ -369,21 +413,31 @@ class ResnetStructurePlanned:
     @T.prim_func
     def tvmgen_default_fused_cast_subtract_fixed_point_multiply_add_clip_cast_cast(placeholder: T.handle, placeholder_1: T.handle, T_cast: T.handle, global_workspace_1_var: T.Ptr[T.uint8]) -> None:
         placeholder_2 = T.match_buffer(placeholder, [360000], dtype=""uint8"")
         placeholder_3 = T.match_buffer(placeholder_1, [64], dtype=""int32"")
         T_cast_1 = T.match_buffer(T_cast, [215], dtype=""int16"")
-        global_workspace_1_buffer_var = T.match_buffer(global_workspace_1_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         for ax0_ax1_fused, ax2, ax3_outer, ax3_inner in T.grid(75, 75, 4, 16):
             T_cast_1[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = T.cast(T.cast(T.max(T.min(T.q_multiply_shift(T.cast(placeholder_2[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner], ""int32"") - 94, 1843157232, 31, 1, dtype=""int32"") + placeholder_3[ax3_outer * 16 + ax3_inner], 255), 0), ""uint8""), ""int16"")
 
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_4200876283395191415_(placeholder_22: T.handle, placeholder_23: T.handle, placeholder_24: T.handle, placeholder_25: T.handle, T_cast_6: T.handle, global_workspace_5_var: T.Ptr[T.uint8]) -> None:
         placeholder_29 = T.match_buffer(placeholder_22, [360000], dtype=""int16"")
         placeholder_27 = T.match_buffer(placeholder_23, [16384], dtype=""int16"")
         placeholder_26 = T.match_buffer(placeholder_24, [256], dtype=""int32"")
         placeholder_28 = T.match_buffer(placeholder_25, [1440000], dtype=""int32"")
         T_cast_7 = T.match_buffer(T_cast_6, [407], dtype=""uint8"")
-        global_workspace_5_buffer_var = T.match_buffer(global_workspace_5_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         PaddedInput_3_let = T.buffer_decl([360000], 'int16')
         with T.let(PaddedInput_3_let.data, T.address_of(global_workspace_5_buffer_var[6480000], dtype=""handle"")):
@@ -403,10 +457,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_15934180698220515269_(placeholder_16: T.handle, placeholder_17: T.handle, placeholder_18: T.handle, T_add: T.handle, global_workspace_4_var: T.Ptr[T.uint8]) -> None:
         placeholder_19 = T.match_buffer(placeholder_16, [360000], dtype=""int16"")
         placeholder_20 = T.match_buffer(placeholder_17, [16384], dtype=""int16"")
         placeholder_21 = T.match_buffer(placeholder_18, [256], dtype=""int32"")
         T_add_1 = T.match_buffer(T_add, [407], dtype=""int32"")
-        global_workspace_4_buffer_var = T.match_buffer(global_workspace_4_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         PaddedInput_2_let = T.buffer_decl([360000], ""int16"")
         with T.let(PaddedInput_2_let.data, T.address_of(global_workspace_4_buffer_var[7200000], dtype=""handle"")):
@@ -426,10 +485,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast(placeholder_4: T.handle, placeholder_5: T.handle, placeholder_6: T.handle, T_cast_2: T.handle, global_workspace_2_var: T.Ptr[T.uint8]) -> None:
         placeholder_7 = T.match_buffer(placeholder_4, [360000], dtype=""int16"")
         placeholder_8 = T.match_buffer(placeholder_5, [4096], dtype=""int16"")
         placeholder_9 = T.match_buffer(placeholder_6, [64], dtype=""int32"")
         T_cast_3 = T.match_buffer(T_cast_2, [215], dtype=""int16"")
-        global_workspace_2_buffer_var = T.match_buffer(global_workspace_2_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         PaddedInput_let = T.buffer_decl([360000], ""int16"")
         with T.let(PaddedInput_let.data, T.address_of(global_workspace_2_buffer_var[7200000], dtype=""handle"")):
@@ -448,10 +512,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast(place
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1(placeholder_10: T.handle, placeholder_11: T.handle, placeholder_12: T.handle, T_cast_4: T.handle, global_workspace_3_var: T.Ptr[T.uint8]) -> None:
         placeholder_13 = T.match_buffer(placeholder_10, [360000], dtype=""int16"")
         placeholder_14 = T.match_buffer(placeholder_11, [36864], dtype=""int16"")
         placeholder_15 = T.match_buffer(placeholder_12, [64], dtype=""int32"")
         T_cast_5 = T.match_buffer(T_cast_4, [215], dtype=""int16"")
-        global_workspace_3_buffer_var = T.match_buffer(global_workspace_3_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         PaddedInput_1_let = T.buffer_decl([379456], ""int16"")
         with T.let(PaddedInput_1_let.data, T.address_of(global_workspace_3_buffer_var[0], dtype=""handle"")):
@@ -469,7 +538,7 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1(pla
 
     @T.prim_func
     def __tvm_main__(input: T.handle, global_workspace_0_var: T.Ptr[T.uint8], output: T.handle) -> None:
-        global_workspace_0_buffer_var = T.match_buffer(global_workspace_0_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=1, align=16)
         # body
         T.attr(""default"", ""device_id"", 0)
         T.attr(""default"", ""device_type"", 1)"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def tvmgen_default_fused_cast_subtract(placeholder_2: T.handle, placeholder_3: T
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_cast_subtract"", ""tir.noalias"": True})
         placeholder_4 = T.match_buffer(placeholder_2, [150528], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(placeholder_4, [150528], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
         placeholder_5 = T.match_buffer(placeholder_3, [1], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(placeholder_5, [1], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         T_subtract_1 = T.match_buffer(T_subtract, [452], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(T_subtract_1, [452], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         # body
         for ax0_ax1_fused_1 in T.serial(0, 224):
             for ax2_1, ax3_inner_1 in T.grid(224, 3):
@@ -86,9 +89,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast(placeholde
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast"", ""tir.noalias"": True})
         placeholder_65 = T.match_buffer(placeholder_62, [150528], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(placeholder_65, [150528], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         placeholder_66 = T.match_buffer(placeholder_63, [9408], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(placeholder_66, [9408], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         placeholder_67 = T.match_buffer(placeholder_64, [64], dtype=""int32"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(placeholder_67, [64], dtype=""int32"", elem_offset=0, align=128, offset_factor=1)
         T_cast_21 = T.match_buffer(T_cast_20, [289], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(T_cast_21, [289], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
         # body
         PaddedInput_7 = T.allocate([157323], ""int16"", ""global"")
         for i0_i1_fused_7 in T.serial(0, 229):
@@ -108,7 +115,9 @@ def tvmgen_default_fused_nn_max_pool2d_cast(placeholder_28: T.handle, T_cast_6:
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_max_pool2d_cast"", ""tir.noalias"": True})
         placeholder_29 = T.match_buffer(placeholder_28, [802816], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(placeholder_29, [802816], dtype=""uint8"", elem_offset=0, align=128, offset_factor=1)
         T_cast_7 = T.match_buffer(T_cast_6, [177], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
+        T.preflattened_buffer(T_cast_7, [177], dtype=""int16"", elem_offset=0, align=128, offset_factor=1)
         # body
         tensor_2 = T.allocate([200704], ""uint8"", ""global"")
         for ax0_ax1_fused_4 in T.serial(0, 56):
@@ -140,9 +149,9 @@ def __tvm_main__(input: T.handle, output: T.handle) -> None:
 @tvm.script.ir_module
 class LinearStructurePlanned:
     @T.prim_func
+    def __tvm_main__(input: T.handle, fast_memory_0_var: T.Ptr[T.uint8], slow_memory_1_var: T.Ptr[T.uint8], output: T.handle) -> None:
+        fast_memory_0_buffer_var = T.match_buffer(fast_memory_0_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        slow_memory_1_buffer_var = T.match_buffer(slow_memory_1_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         T.attr(""default"", ""device_id"", 0)
         T.attr(""default"", ""device_type"", 1)
@@ -155,9 +164,13 @@ def __tvm_main__(input: T.handle, fast_memory_0_var: T.Ptr[T.uint8], slow_memory
     @T.prim_func
     def tvmgen_default_fused_nn_max_pool2d_cast(placeholder_28: T.handle, T_cast_6: T.handle, fast_memory_6_var: T.Ptr[T.uint8], slow_memory_7_var: T.Ptr[T.uint8]) -> None:
         placeholder_29 = T.match_buffer(placeholder_28, [802816], dtype=""uint8"")
+        T.preflattened_buffer(placeholder_29, [802816], dtype=""uint8"")
         T_cast_7 = T.match_buffer(T_cast_6, [177], dtype=""int16"")
+        T.preflattened_buffer(T_cast_7, [177], dtype=""int16"")
+        fast_memory_6_buffer_var = T.match_buffer(fast_memory_6_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(fast_memory_6_buffer_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        slow_memory_7_buffer_var = T.match_buffer(slow_memory_7_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(slow_memory_7_buffer_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         tensor_2_let = T.buffer_decl([200704], dtype=""uint8"")
         with T.let(tensor_2_let.data, T.address_of(fast_memory_6_buffer_var[0], dtype=""handle"")):
@@ -172,22 +185,33 @@ def tvmgen_default_fused_nn_max_pool2d_cast(placeholder_28: T.handle, T_cast_6:
     @T.prim_func
     def tvmgen_default_fused_cast_subtract(placeholder_2: T.handle, placeholder_3: T.handle, T_subtract: T.handle, fast_memory_2_var: T.Ptr[T.uint8], slow_memory_3_var: T.Ptr[T.uint8]) -> None:
         placeholder_4 = T.match_buffer(placeholder_2, [150528], dtype=""uint8"")
+        T.preflattened_buffer(placeholder_4, [150528], dtype=""uint8"")
         placeholder_5 = T.match_buffer(placeholder_3, [1], dtype=""int16"")
+        T.preflattened_buffer(placeholder_5, [1], dtype=""int16"")
         T_subtract_1 = T.match_buffer(T_subtract, [452], dtype=""int16"")
+        T.preflattened_buffer(T_subtract_1, [452], dtype=""int16"")
+        fast_memory_2_buffer_var = T.match_buffer(fast_memory_2_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(fast_memory_2_buffer_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        slow_memory_3_buffer_var = T.match_buffer(slow_memory_3_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(slow_memory_3_buffer_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         for ax0_ax1_fused_1, ax2_1, ax3_inner_1 in T.grid(224, 224, 3):
             T_subtract_1[ax0_ax1_fused_1 * 672 + ax2_1 * 3 + ax3_inner_1] = T.cast(placeholder_4[ax0_ax1_fused_1 * 672 + ax2_1 * 3 + ax3_inner_1], ""int16"") - placeholder_5[0]
 
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast(placeholder_62: T.handle, placeholder_63: T.handle, placeholder_64: T.handle, T_cast_20: T.handle, fast_memory_4_var: T.Ptr[T.uint8], slow_memory_5_var: T.Ptr[T.uint8]) -> None:
         placeholder_65 = T.match_buffer(placeholder_62, [150528], dtype=""int16"")
+        T.preflattened_buffer(placeholder_65, [150528], dtype=""int16"")
         placeholder_66 = T.match_buffer(placeholder_63, [9408], dtype=""int16"")
+        T.preflattened_buffer(placeholder_66, [9408], dtype=""int16"")
         placeholder_67 = T.match_buffer(placeholder_64, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_67, [64], dtype=""int32"")
         T_cast_21 = T.match_buffer(T_cast_20, [289], dtype=""uint8"")
+        T.preflattened_buffer(T_cast_21, [289], dtype=""uint8"")
+        fast_memory_4_buffer_var = T.match_buffer(fast_memory_4_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(fast_memory_4_buffer_var, [200704], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        slow_memory_5_buffer_var = T.match_buffer(slow_memory_5_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(slow_memory_5_buffer_var, [1418528], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         PaddedInput_7_let = T.buffer_decl([157323], ""int16"")
         with T.let(PaddedInput_7_let.data, T.address_of(slow_memory_5_buffer_var[802816], dtype=""handle"")):
@@ -251,8 +275,11 @@ def tvmgen_default_fused_cast_subtract_fixed_point_multiply_add_clip_cast_cast(p
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_cast_subtract_fixed_point_multiply_add_clip_cast_cast"", ""tir.noalias"": True})
         placeholder_2 = T.match_buffer(placeholder, [360000], dtype=""uint8"")
+        T.preflattened_buffer(placeholder_2, [360000], dtype=""uint8"")
         placeholder_3 = T.match_buffer(placeholder_1, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_3, [64], dtype=""int32"")
         T_cast_1 = T.match_buffer(T_cast, [215], dtype=""int16"")
+        T.preflattened_buffer(T_cast_1, [215], dtype=""int16"")
         # body
         for ax0_ax1_fused, ax2, ax3_outer, ax3_inner in T.grid(75, 75, 4, 16):
             T_cast_1[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = T.cast(T.cast(T.max(T.min(T.q_multiply_shift(T.cast(placeholder_2[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner], ""int32"") - 94, 1843157232, 31, 1, dtype=""int32"") + placeholder_3[ax3_outer * 16 + ax3_inner], 255), 0), ""uint8""), ""int16"")
@@ -262,9 +289,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1(pla
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1"", ""tir.noalias"": True})
         placeholder_13 = T.match_buffer(placeholder_10, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_13, [360000], dtype=""int16"")
         placeholder_14 = T.match_buffer(placeholder_11, [36864], dtype=""int16"")
+        T.preflattened_buffer(placeholder_14, [36864], dtype=""int16"")
         placeholder_15 = T.match_buffer(placeholder_12, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_15, [64], dtype=""int32"")
         T_cast_5 = T.match_buffer(T_cast_4, [215], dtype=""int16"")
+        T.preflattened_buffer(T_cast_5, [215], dtype=""int16"")
         # body
         PaddedInput_1 = T.allocate([379456], ""int16"", ""global"")
         for i0_i1_fused_1, i2_1, i3_1 in T.grid(77, 77, 64):
@@ -283,9 +314,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_15934180698220515269_"", ""tir.noalias"": True})
         placeholder_19 = T.match_buffer(placeholder_16, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_19, [360000], dtype=""int16"")
         placeholder_20 = T.match_buffer(placeholder_17, [16384], dtype=""int16"")
+        T.preflattened_buffer(placeholder_20, [16384], dtype=""int16"")
         placeholder_21 = T.match_buffer(placeholder_18, [256], dtype=""int32"")
+        T.preflattened_buffer(placeholder_21, [256], dtype=""int32"")
         T_add_1 = T.match_buffer(T_add, [407], dtype=""int32"")
+        T.preflattened_buffer(T_add_1, [407], dtype=""int32"")
         # body
         PaddedInput_2 = T.allocate([360000], ""int16"", ""global"")
         for i0_i1_fused_2, i2_2, i3_2 in T.grid(75, 75, 64):
@@ -305,10 +340,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_4200876283395191415_"", ""tir.noalias"": True})
         placeholder_29 = T.match_buffer(placeholder_22, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_29, [360000], dtype=""int16"")
         placeholder_27 = T.match_buffer(placeholder_23, [16384], dtype=""int16"")
+        T.preflattened_buffer(placeholder_27, [16384], dtype=""int16"")
         placeholder_26 = T.match_buffer(placeholder_24, [256], dtype=""int32"")
+        T.preflattened_buffer(placeholder_26, [256], dtype=""int32"")
         placeholder_28 = T.match_buffer(placeholder_25, [1440000], dtype=""int32"")
+        T.preflattened_buffer(placeholder_28, [1440000], dtype=""int32"")
         T_cast_7 = T.match_buffer(T_cast_6, [407], dtype=""uint8"")
+        T.preflattened_buffer(T_cast_7, [407], dtype=""uint8"")
         # body
         PaddedInput_3 = T.allocate([360000], ""int16"", ""global"")
         for i0_i1_fused_3, i2_3, i3_3 in T.grid(75, 75, 64):
@@ -345,9 +385,13 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast(place
         # function attr dict
         T.func_attr({""global_symbol"": ""tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast"", ""tir.noalias"": True})
         placeholder_7 = T.match_buffer(placeholder_4, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_7, [360000], dtype=""int16"")
         placeholder_8 = T.match_buffer(placeholder_5, [4096], dtype=""int16"")
+        T.preflattened_buffer(placeholder_8, [4096], dtype=""int16"")
         placeholder_9 = T.match_buffer(placeholder_6, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_9, [64], dtype=""int32"")
         T_cast_3 = T.match_buffer(T_cast_2, [215], dtype=""int16"")
+        T.preflattened_buffer(T_cast_3, [215], dtype=""int16"")
         # body
         PaddedInput = T.allocate([360000], ""int16"", ""global"")
         for i0_i1_fused, i2, i3 in T.grid(75, 75, 64):
@@ -369,21 +413,31 @@ class ResnetStructurePlanned:
     @T.prim_func
     def tvmgen_default_fused_cast_subtract_fixed_point_multiply_add_clip_cast_cast(placeholder: T.handle, placeholder_1: T.handle, T_cast: T.handle, global_workspace_1_var: T.Ptr[T.uint8]) -> None:
         placeholder_2 = T.match_buffer(placeholder, [360000], dtype=""uint8"")
+        T.preflattened_buffer(placeholder_2, [360000], dtype=""uint8"")
         placeholder_3 = T.match_buffer(placeholder_1, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_3, [64], dtype=""int32"")
         T_cast_1 = T.match_buffer(T_cast, [215], dtype=""int16"")
+        T.preflattened_buffer(T_cast_1, [215], dtype=""int16"")
+        global_workspace_1_buffer_var = T.match_buffer(global_workspace_1_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(global_workspace_1_buffer_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         for ax0_ax1_fused, ax2, ax3_outer, ax3_inner in T.grid(75, 75, 4, 16):
             T_cast_1[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = T.cast(T.cast(T.max(T.min(T.q_multiply_shift(T.cast(placeholder_2[ax0_ax1_fused * 4800 + ax2 * 64 + ax3_outer * 16 + ax3_inner], ""int32"") - 94, 1843157232, 31, 1, dtype=""int32"") + placeholder_3[ax3_outer * 16 + ax3_inner], 255), 0), ""uint8""), ""int16"")
 
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_4200876283395191415_(placeholder_22: T.handle, placeholder_23: T.handle, placeholder_24: T.handle, placeholder_25: T.handle, T_cast_6: T.handle, global_workspace_5_var: T.Ptr[T.uint8]) -> None:
         placeholder_29 = T.match_buffer(placeholder_22, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_29, [360000], dtype=""int16"")
         placeholder_27 = T.match_buffer(placeholder_23, [16384], dtype=""int16"")
+        T.preflattened_buffer(placeholder_27, [16384], dtype=""int16"")
         placeholder_26 = T.match_buffer(placeholder_24, [256], dtype=""int32"")
+        T.preflattened_buffer(placeholder_26, [256], dtype=""int32"")
         placeholder_28 = T.match_buffer(placeholder_25, [1440000], dtype=""int32"")
+        T.preflattened_buffer(placeholder_28, [1440000], dtype=""int32"")
         T_cast_7 = T.match_buffer(T_cast_6, [407], dtype=""uint8"")
+        T.preflattened_buffer(T_cast_7, [407], dtype=""uint8"")
+        global_workspace_5_buffer_var = T.match_buffer(global_workspace_5_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(global_workspace_5_buffer_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         PaddedInput_3_let = T.buffer_decl([360000], 'int16')
         with T.let(PaddedInput_3_let.data, T.address_of(global_workspace_5_buffer_var[6480000], dtype=""handle"")):
@@ -403,10 +457,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_subtract_fixed_point_15934180698220515269_(placeholder_16: T.handle, placeholder_17: T.handle, placeholder_18: T.handle, T_add: T.handle, global_workspace_4_var: T.Ptr[T.uint8]) -> None:
         placeholder_19 = T.match_buffer(placeholder_16, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_19, [360000], dtype=""int16"")
         placeholder_20 = T.match_buffer(placeholder_17, [16384], dtype=""int16"")
+        T.preflattened_buffer(placeholder_20, [16384], dtype=""int16"")
         placeholder_21 = T.match_buffer(placeholder_18, [256], dtype=""int32"")
+        T.preflattened_buffer(placeholder_21, [256], dtype=""int32"")
         T_add_1 = T.match_buffer(T_add, [407], dtype=""int32"")
+        T.preflattened_buffer(T_add_1, [407], dtype=""int32"")
+        global_workspace_4_buffer_var = T.match_buffer(global_workspace_4_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(global_workspace_4_buffer_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         PaddedInput_2_let = T.buffer_decl([360000], ""int16"")
         with T.let(PaddedInput_2_let.data, T.address_of(global_workspace_4_buffer_var[7200000], dtype=""handle"")):
@@ -426,10 +485,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_add_clip_cast_cast_s
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast(placeholder_4: T.handle, placeholder_5: T.handle, placeholder_6: T.handle, T_cast_2: T.handle, global_workspace_2_var: T.Ptr[T.uint8]) -> None:
         placeholder_7 = T.match_buffer(placeholder_4, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_7, [360000], dtype=""int16"")
         placeholder_8 = T.match_buffer(placeholder_5, [4096], dtype=""int16"")
+        T.preflattened_buffer(placeholder_8, [4096], dtype=""int16"")
         placeholder_9 = T.match_buffer(placeholder_6, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_9, [64], dtype=""int32"")
         T_cast_3 = T.match_buffer(T_cast_2, [215], dtype=""int16"")
+        T.preflattened_buffer(T_cast_3, [215], dtype=""int16"")
+        global_workspace_2_buffer_var = T.match_buffer(global_workspace_2_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(global_workspace_2_buffer_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         PaddedInput_let = T.buffer_decl([360000], ""int16"")
         with T.let(PaddedInput_let.data, T.address_of(global_workspace_2_buffer_var[7200000], dtype=""handle"")):
@@ -448,10 +512,15 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast(place
     @T.prim_func
     def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1(placeholder_10: T.handle, placeholder_11: T.handle, placeholder_12: T.handle, T_cast_4: T.handle, global_workspace_3_var: T.Ptr[T.uint8]) -> None:
         placeholder_13 = T.match_buffer(placeholder_10, [360000], dtype=""int16"")
+        T.preflattened_buffer(placeholder_13, [360000], dtype=""int16"")
         placeholder_14 = T.match_buffer(placeholder_11, [36864], dtype=""int16"")
+        T.preflattened_buffer(placeholder_14, [36864], dtype=""int16"")
         placeholder_15 = T.match_buffer(placeholder_12, [64], dtype=""int32"")
+        T.preflattened_buffer(placeholder_15, [64], dtype=""int32"")
         T_cast_5 = T.match_buffer(T_cast_4, [215], dtype=""int16"")
+        T.preflattened_buffer(T_cast_5, [215], dtype=""int16"")
+        global_workspace_3_buffer_var = T.match_buffer(global_workspace_3_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
+        T.preflattened_buffer(global_workspace_3_buffer_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         PaddedInput_1_let = T.buffer_decl([379456], ""int16"")
         with T.let(PaddedInput_1_let.data, T.address_of(global_workspace_3_buffer_var[0], dtype=""handle"")):
@@ -469,7 +538,7 @@ def tvmgen_default_fused_nn_conv2d_add_fixed_point_multiply_clip_cast_cast_1(pla
 
     @T.prim_func
     def __tvm_main__(input: T.handle, global_workspace_0_var: T.Ptr[T.uint8], output: T.handle) -> None:
+        global_workspace_0_buffer_var = T.match_buffer(global_workspace_0_var, [7920256], dtype=""uint8"", strides=[1], elem_offset=0, align=16)
         # body
         T.attr(""default"", ""device_id"", 0)
         T.attr(""default"", ""device_type"", 1)"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_non_integer_typed_block_iter():
     check_error(non_integer_typed_block_iter, 3)
 
 
 if __name__ == ""__main__"":
     sys.exit(pytest.main([__file__] + sys.argv[1:]))"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_non_integer_typed_block_iter():
     check_error(non_integer_typed_block_iter, 3)
 
 
+def preflattened_buffer_map_align_nonint(foo: T.handle):
+    foo_1 = T.match_buffer(foo, [1])
+    T.preflattened_buffer(
+        foo_1, [1], align=""bar""
+    )  # check_error: align: want int or IntImm, got 'bar'
+
+
+def test_preflattened_buffer_map_align():
+    check_error(preflattened_buffer_map_align_nonint, 3)
+
+
+def preflattened_buffer_map_offset_factor_nonint(foo: T.handle):
+    foo_1 = T.match_buffer(foo, [1])
+    T.preflattened_buffer(
+        foo_1, [1], offset_factor=""bar""
+    )  # check_error: offset_factor: want int or IntImm, got 'bar'
+
+
+def test_preflattened_buffer_map_offset_factor():
+    check_error(preflattened_buffer_map_offset_factor_nonint, 3)
+
+
 if __name__ == ""__main__"":
     sys.exit(pytest.main([__file__] + sys.argv[1:]))"
KO;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_dynamic_shape_gemm():
     assert_structural_equal(gemm_dyn_shape, gemm_dyn_shape_roundtrip)
 
 
 @T.prim_func
 def match_buffer_int64(a: T.handle, c: T.handle) -> None:
     A = T.match_buffer(a, (T.int64(128), T.int64(128)), dtype=""float32"")"
OK;10;Lyken17;tvm-hack;4178617fcf19d209ea545a2708f8bfcdbfa02056;"[AOT] Support LLVM backend with C++ runtime (#10753)

* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm

* add metadata serialization support to llvm codegen

* Organize MetadataQueuer into a separate file.

* Add DiscoverArraysVisitor to metadata_utils

* Fill DLTensor metadata in LegalizePackedCalls.

* Improve error message from Call asserts

* Pass non-String device_context down to codegen.

 * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.

* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.

* test fixes

* Also fill preflattened_buffer_map (TODO, maybe don't do this)

* Fix C codegen.

* Set USMP elem_offset to 0.

* Clarify calculation of byte_offset from elem_offset.

* fix tests

* Fix arm compile warning

* Fix hexagon test.

 * previously I believe we required interface_api == ""c"", but
   this really means to generate C API bindings, and we are generating
   ""packed"" bindings.
 * I think ""c"" was chosen here because the distinction between
   interface-api and use-unpacked-api is confusing. ""c"" interface-api
   means to generate an entrypoint API for microcontrollers that
   accepts bare data buffers. ""packed"" interface-api means to generate
   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same
   determination for the operator functions.
 * A further confusion here is that there are two ways to call
   ""packed"" operator functions: tir.tvm_builtin_call_packed and
   tir.tvm_builtin_call_cpacked. This distinction describes whether or
   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT
   only ever requires call_cpacked because target_host == target, and
   for all suitable target_host, we expect a single DSO-exportable
   runtime.Module. When we move away from this by introducing
   heterogeneous target support to AOT, we can use this as a condition
   to help us choose between call_cpacked and call_packed (and
   possibly add a compile-time option to assert it is call_cpacked,
   for situations where we really don't want call_packed).

* Document T.preflattened_buffer

* Fix test_aot_legalize_packed_calls

* Address manupa comments

* Fix convert_pool_allocations_to_offsets test.

* lint

* Fix T.preflattened_buffer

* Add preflattened_buffer_map to TIRTextPrinter

* Fix tests

* Fix BYOC

* Fix invoking C device API.

* remove comments

* Address Mousius comments

* lint

* lint

* Fix GMock linking on new CMake

* address masahi comment

Co-authored-by: Masahiro Masuda <masahi129@gmail.com>";"def test_dynamic_shape_gemm():
     assert_structural_equal(gemm_dyn_shape, gemm_dyn_shape_roundtrip)
 
 
+@T.prim_func
+def preflattened_buffer_map(A: T.handle, B: T.handle):
+    A_1 = T.match_buffer(A, [1])
+    T.preflattened_buffer(A_1, [1], align=T.int32(1), offset_factor=T.int64(2))
+    B_1 = T.match_buffer(B, [1])
+    T.preflattened_buffer(B_1, [1])
+    B_1[0] = A_1[0]
+
+
+def test_preflattened_buffer_map():
+    A_var = [
+        k for k, _ in preflattened_buffer_map.preflattened_buffer_map.items() if k.name == ""A""
+    ][0]
+    assert preflattened_buffer_map.preflattened_buffer_map[A_var].data_alignment == 1
+    assert preflattened_buffer_map.preflattened_buffer_map[A_var].offset_factor == 2
+
+
 @T.prim_func
 def match_buffer_int64(a: T.handle, c: T.handle) -> None:
     A = T.match_buffer(a, (T.int64(128), T.int64(128)), dtype=""float32"")"
KO;10;kostakazakoff;python_exercises_basics_course;e6777dec8d5d058ac9f830f44609238df784f37d;"Problems for Champions Part II

Problem:
We all know the game called ""Bulls and Cows"" (https://en.wikipedia.org/wiki/Bulls_and_cows). Upon having a particular 4-digit secret number and a 4-digit suggested number, the following rules are applied:
- If a digit in the suggested number matches a digit in the secret number and is located at the same position, we have a bull.
- If a digit in the suggested number matches a digit in the secret number but is located at a different position, we have a cow.

Upon having a particular secret number and the bulls and cows pertaining to it, our task is to find all possible suggested numbers in ascending order. If there are no suggested numbers that match the criteria provided from the console, we must print ""No"".

Input Data:
The input data is read from the console. We have 3 lines in the input data:
- The first contains the secret number.
- The second contains the number of bulls.
- The third contains the number of cows.
- The input data will always be valid. There is no need to verify them.

Output Data:
The output data must be printed on the console. The output must consist of a single line - all suggested numbers, space-separated. If there are no suggested numbers that match the criteria provided from the console, we must print “No”.

Constraints:
- The secret number will always consist of 4 digits in the range [1..9].
- The number of cows and bulls will always be in the range [0..9].
- Allowed execution time: 0.15 seconds.
- Allowed memory: 16 MB.";
OK;10;kostakazakoff;python_exercises_basics_course;e6777dec8d5d058ac9f830f44609238df784f37d;"Problems for Champions Part II

Problem:
We all know the game called ""Bulls and Cows"" (https://en.wikipedia.org/wiki/Bulls_and_cows). Upon having a particular 4-digit secret number and a 4-digit suggested number, the following rules are applied:
- If a digit in the suggested number matches a digit in the secret number and is located at the same position, we have a bull.
- If a digit in the suggested number matches a digit in the secret number but is located at a different position, we have a cow.

Upon having a particular secret number and the bulls and cows pertaining to it, our task is to find all possible suggested numbers in ascending order. If there are no suggested numbers that match the criteria provided from the console, we must print ""No"".

Input Data:
The input data is read from the console. We have 3 lines in the input data:
- The first contains the secret number.
- The second contains the number of bulls.
- The third contains the number of cows.
- The input data will always be valid. There is no need to verify them.

Output Data:
The output data must be printed on the console. The output must consist of a single line - all suggested numbers, space-separated. If there are no suggested numbers that match the criteria provided from the console, we must print “No”.

Constraints:
- The secret number will always consist of 4 digits in the range [1..9].
- The number of cows and bulls will always be in the range [0..9].
- Allowed execution time: 0.15 seconds.
- Allowed memory: 16 MB.";"+secret = input()
+secret = str(secret)
+bulls = int(input())
+cows = int(input())
+solution = False
+for n1 in range(1, 10):
+    for n2 in range(1, 10):
+        for n3 in range(1, 10):
+            for n4 in range(1, 10):
+                my_number = str(n1) + str(n2) + str(n3) + str(n4)
+                my_number_copy = list(my_number)
+                secret_copy = list(secret)
+                bulls_counter = 0
+                cows_counter = 0
+                for each in range (4):
+                    if my_number_copy[each] == secret_copy[each]:
+                        my_number_copy[each] = 'x'
+                        secret_copy[each] = 'y'
+                        bulls_counter += 1
+                if bulls_counter < 3:
+                    for i in range (4):
+                        for j in range(4):
+                            if my_number_copy[i] == secret_copy[j]:
+                                my_number_copy[i] = 'a'
+                                secret_copy[j] = 'b'
+                                cows_counter += 1
+                if bulls_counter == bulls and cows_counter == cows:
+                    solution = True
+                    print(my_number, end=' ')
+if not solution:
+    print('No')"
KO;10;kostakazakoff;python_exercises_basics_course;d9b286483e7eb207969c7405ed2212c077428753;"Problems for Champions Part II

Problem:
Bonny is an exceptionally powerful witch. As her natural power is not sufficient to successfully fight vampires and werewolves, she has started to master the power of Expressions. An expression is very hard to master because the spell relies on the ability to quickly solve mathematical expressions.

To use an ""Expression spell"", the witch must know the result of a mathematical expression in advance. An Expression spell consists of a few simple mathematical expressions. Each mathematical expression can contain operators for summing up, subtraction, multiplying, and/or division.

The expression is solved without considering the mathematical rules for calculating numerical expressions. This means that the priority is applied according to the sequence of the operators, and not the type of calculation that they do. The expression can contain brackets, as everything inside the brackets is calculated first. Every expression can contain multiple brackets, but no nested brackets:

An expression containing (…(…)…) is an invalid one.
An expression containing (…)…(…) is a valid one.

The input data consists of a single text line, passed from the console. It contains a mathematical expression for calculation. The line always ends with the ""="" symbol. The ""="" symbol means the end of the mathematical expression.

The input data is always valid and always in the described format. No need to validate it.

The output data must be printed on the console. The output consists of one line: the result of the calculated mathematical expression.

The result must be rounded up to the second digit after the decimal point.

Constraints:
The expressions will consist of a maximum of 2500 symbols.
The numbers of each mathematical expression will be within the range [1 … 9].
The operators in the mathematical expressions will always be among + (summing up), - (subtraction), / (division) or * (multiplying).
The result of the mathematical expression will be within the range [-100000.00 … 100000.00].
Allowed execution time: 0.1 seconds.
Allowed memory: 16 MB.";
OK;10;kostakazakoff;python_exercises_basics_course;d9b286483e7eb207969c7405ed2212c077428753;"Problems for Champions Part II

Problem:
Bonny is an exceptionally powerful witch. As her natural power is not sufficient to successfully fight vampires and werewolves, she has started to master the power of Expressions. An expression is very hard to master because the spell relies on the ability to quickly solve mathematical expressions.

To use an ""Expression spell"", the witch must know the result of a mathematical expression in advance. An Expression spell consists of a few simple mathematical expressions. Each mathematical expression can contain operators for summing up, subtraction, multiplying, and/or division.

The expression is solved without considering the mathematical rules for calculating numerical expressions. This means that the priority is applied according to the sequence of the operators, and not the type of calculation that they do. The expression can contain brackets, as everything inside the brackets is calculated first. Every expression can contain multiple brackets, but no nested brackets:

An expression containing (…(…)…) is an invalid one.
An expression containing (…)…(…) is a valid one.

The input data consists of a single text line, passed from the console. It contains a mathematical expression for calculation. The line always ends with the ""="" symbol. The ""="" symbol means the end of the mathematical expression.

The input data is always valid and always in the described format. No need to validate it.

The output data must be printed on the console. The output consists of one line: the result of the calculated mathematical expression.

The result must be rounded up to the second digit after the decimal point.

Constraints:
The expressions will consist of a maximum of 2500 symbols.
The numbers of each mathematical expression will be within the range [1 … 9].
The operators in the mathematical expressions will always be among + (summing up), - (subtraction), / (division) or * (multiplying).
The result of the mathematical expression will be within the range [-100000.00 … 100000.00].
Allowed execution time: 0.1 seconds.
Allowed memory: 16 MB.";"+expression = input()
+operator = '+'
+in_operation = False
+result = 0
+in_result = 0
+
+for simbol in expression:
+
+        if simbol == '=':
+            break
+
+        if simbol == '+' or simbol == '-' or simbol == '*' or simbol == '/':
+            if in_operation:
+                in_operator = simbol
+            else:
+                operator = simbol
+            continue
+
+        elif simbol == '(':
+            in_operation = True
+            in_operator = '+'
+            number = 0
+
+        elif simbol == ')':
+            in_operation = False
+            number = in_result
+            in_result = 0
+
+        elif 48 <= ord(simbol) <= 57:
+            number = int(simbol)
+
+        if in_operation:
+            if in_operator == '+':
+                in_result += number
+            elif in_operator == '-':
+                in_result -= number
+            elif in_operator == '*':
+                in_result *= number
+            elif in_operator == '/':
+                in_result /= number
+        else:
+            if operator == '+':
+                result += number
+            elif operator == '-':
+                result -= number
+            elif operator == '*':
+                result *= number
+            elif operator == '/':
+                result /= number
+
+print(f'{result:.2f}')
+"
KO;10;nimaid;python-dl621;e6cae7bb4ecde73571e3543802b2dac2f127ee96;forced handling of memory errors;"     
 setup(
     name='dl621',
-    version='2.1.6',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',
     author_email='ellagjameson@gmail.com',
     url='https://github.com/nimaid/python-dl621',
     license=license,
-    install_requires=[""imgtag>=1.1.5"", ""requests"", ""sockets""],
     packages=find_packages(exclude=('tests', 'docs')),
     entry_points={
         'console_scripts': ["
OK;10;nimaid;python-dl621;e6cae7bb4ecde73571e3543802b2dac2f127ee96;forced handling of memory errors;"     
 setup(
     name='dl621',
+    version='2.1.7',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',
     author_email='ellagjameson@gmail.com',
     url='https://github.com/nimaid/python-dl621',
     license=license,
+    install_requires=[""imgtag>=1.1.6"", ""requests"", ""sockets""],
     packages=find_packages(exclude=('tests', 'docs')),
     entry_points={
         'console_scripts': ["
KO;10;nimaid;python-dl621;99cfe627b7229b384430aac426912dc29f15dedb;forced handling of memory errors;"     
 setup(
     name='dl621',
-    version='2.1.5',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',"
OK;10;nimaid;python-dl621;99cfe627b7229b384430aac426912dc29f15dedb;forced handling of memory errors;"     
 setup(
     name='dl621',
+    version='2.1.6',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',"
KO;10;nimaid;python-dl621;e630d986ff7ed55ffd1f978d6976541442c19c48;fixed odd memory issue;"     
 setup(
     name='dl621',
-    version='2.1.4',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',
     author_email='ellagjameson@gmail.com',
     url='https://github.com/nimaid/python-dl621',
     license=license,
-    install_requires=[""imgtag>=1.1.4"", ""requests"", ""sockets""],
     packages=find_packages(exclude=('tests', 'docs')),
     entry_points={
         'console_scripts': ["
OK;10;nimaid;python-dl621;e630d986ff7ed55ffd1f978d6976541442c19c48;fixed odd memory issue;"     
 setup(
     name='dl621',
+    version='2.1.5',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',
     author_email='ellagjameson@gmail.com',
     url='https://github.com/nimaid/python-dl621',
     license=license,
+    install_requires=[""imgtag>=1.1.5"", ""requests"", ""sockets""],
     packages=find_packages(exclude=('tests', 'docs')),
     entry_points={
         'console_scripts': ["
KO;10;nimaid;python-dl621;6155a3cd306f711da978b2fd6426e922a84e4e6e;added new version of imgtag without memory crash when opening some GIFs;" import urllib.request
 import os
 import imgtag
-import libxmp
 
 __default_user_agent__ = ""dl621/1.0 (by nimaid on e621)""
 __default_name_pattern__ = ""dl621_{i}_{m}""
@@ -185,8 +184,6 @@ def download_image(post_id, output_folder=""."", name_pattern=__default_name_patte
             image_tags = get_tags_from_json(image_info)
             image_tags_obj.add_tags(image_tags)
             image_tags_obj.close()
-        except libxmp.XMPError:
-            warnings.warn(""Could not save metadata in image!"")
         except SystemError:
             warnings.warn(""Could not save metadata in image!"")
     "
OK;10;nimaid;python-dl621;6155a3cd306f711da978b2fd6426e922a84e4e6e;added new version of imgtag without memory crash when opening some GIFs;" import urllib.request
 import os
 import imgtag
 
 __default_user_agent__ = ""dl621/1.0 (by nimaid on e621)""
 __default_name_pattern__ = ""dl621_{i}_{m}""
@@ -185,8 +184,6 @@ def download_image(post_id, output_folder=""."", name_pattern=__default_name_patte
             image_tags = get_tags_from_json(image_info)
             image_tags_obj.add_tags(image_tags)
             image_tags_obj.close()
         except SystemError:
             warnings.warn(""Could not save metadata in image!"")
     "
KO;10;nimaid;python-dl621;6155a3cd306f711da978b2fd6426e922a84e4e6e;added new version of imgtag without memory crash when opening some GIFs;"     
 setup(
     name='dl621',
-    version='1.4.4',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',
     author_email='ellagjameson@gmail.com',
     url='https://github.com/nimaid/python-dl621',
     license=license,
-    install_requires=[""imgtag>=1.0.3"", ""requests""],
     packages=find_packages(exclude=('tests', 'docs')),
     entry_points={
         'console_scripts': ["
OK;10;nimaid;python-dl621;6155a3cd306f711da978b2fd6426e922a84e4e6e;added new version of imgtag without memory crash when opening some GIFs;"     
 setup(
     name='dl621',
+    version='1.4.5',
     description='A simple python module and CLI utility to download e621 images with embedded XMP tags and description',
     long_description=readme,
     author='Ella Jameson',
     author_email='ellagjameson@gmail.com',
     url='https://github.com/nimaid/python-dl621',
     license=license,
+    install_requires=[""imgtag>=1.0.5"", ""requests""],
     packages=find_packages(exclude=('tests', 'docs')),
     entry_points={
         'console_scripts': ["
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" include requirements.txt
 include README.md
 include requirements-test.txt
-include VERSION.txt
\ No newline at end of file
\ No newline at end of file"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" include requirements.txt
 include README.md
 include requirements-test.txt
\ No newline at end of file
+include LICENSE
\ No newline at end of file"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"-0.0.1
\ No newline at end of file"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";\ No newline at end of file
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+from typing import Optional
+from eppo_client.client import EppoClient
+from eppo_client.config import Config
+from eppo_client.configuration_requestor import (
+    ExperimentConfigurationDto,
+    ExperimentConfigurationRequestor,
+)
+from eppo_client.configuration_store import ConfigurationStore
+from eppo_client.constants import MAX_CACHE_ENTRIES
+from eppo_client.http_client import HttpClient, SdkParams
+from eppo_client.read_write_lock import ReadWriteLock
+
+__version__ = ""0.0.1""
+
+__client: Optional[EppoClient] = None
+__lock = ReadWriteLock()
+
+
+def init(config: Config) -> EppoClient:
+    """"""Initializes a global Eppo client instance
+
+    This method should be called once on application startup.
+    If invoked more than once, it will re-initialize the global client instance.
+    Use the :func:`eppo_client.get_instance()` method to access the client instance.
+
+    :param config: client configuration containing the API Key
+    :type config: Config
+    """"""
+    config._validate()
+    sdk_params = SdkParams(
+        apiKey=config.api_key, sdkName=""python"", sdkVersion=__version__
+    )
+    http_client = HttpClient(base_url=config.base_url, sdk_params=sdk_params)
+    config_store: ConfigurationStore[ExperimentConfigurationDto] = ConfigurationStore(
+        max_size=MAX_CACHE_ENTRIES
+    )
+    config_requestor = ExperimentConfigurationRequestor(
+        http_client=http_client, config_store=config_store
+    )
+    global __client
+    global __lock
+    try:
+        __lock.acquire_write()
+        if __client:
+            # if a client was already initialized, stop the background processes of the old client
+            __client._shutdown()
+        __client = EppoClient(config_requestor=config_requestor)
+        return __client
+    finally:
+        __lock.release_write()
+
+
+def get_instance() -> EppoClient:
+    """"""Used to access an initialized client instance
+
+    Use this method to get a client instance for assigning variants.
+    This method may only be called after invocation of :func:`eppo_client.init()`, otherwise it throws an exception.
+
+    :return: a shared client instance
+    :rtype: EppoClient
+    """"""
+    global __client
+    global __lock
+    try:
+        __lock.acquire_read()
+        if __client:
+            return __client
+        else:
+            raise Exception(""init() must be called before get_instance()"")
+    finally:
+        __lock.release_read()"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"     ExperimentConfigurationDto,
     ExperimentConfigurationRequestor,
 )
 from eppo_client.shard import get_shard, is_in_shard_range
 from eppo_client.validation import validate_not_blank
 
 
 class EppoClient:
-    def __init__(self, config_requestor: ExperimentConfigurationRequestor) -> None:
         self.__config_requestor = config_requestor
 
     def assign(self, subject: str, experiment_key: str) -> Optional[str]:
         """"""Maps a subject to a variation for a given experiment
@@ -24,7 +32,7 @@ def assign(self, subject: str, experiment_key: str) -> Optional[str]:
         if (
             experiment_config is None
             or not experiment_config.enabled
-            or not self.is_in_experiment_sample(
                 subject, experiment_key, experiment_config
             )
         ):
@@ -42,7 +50,13 @@ def assign(self, subject: str, experiment_key: str) -> Optional[str]:
             None,
         )
 
-    def is_in_experiment_sample(
         self,
         subject: str,
         experiment_key: str,"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"     ExperimentConfigurationDto,
     ExperimentConfigurationRequestor,
 )
+from eppo_client.constants import POLL_INTERVAL_MILLIS, POLL_JITTER_MILLIS
+from eppo_client.poller import Poller
 from eppo_client.shard import get_shard, is_in_shard_range
 from eppo_client.validation import validate_not_blank
 
 
 class EppoClient:
+    def __init__(self, config_requestor: ExperimentConfigurationRequestor):
         self.__config_requestor = config_requestor
+        self.__poller = Poller(
+            interval_millis=POLL_INTERVAL_MILLIS,
+            jitter_millis=POLL_JITTER_MILLIS,
+            callback=config_requestor.fetch_and_store_configurations,
+        )
+        self.__poller.start()
 
     def assign(self, subject: str, experiment_key: str) -> Optional[str]:
         """"""Maps a subject to a variation for a given experiment
@@ -24,7 +32,7 @@ def assign(self, subject: str, experiment_key: str) -> Optional[str]:
         if (
             experiment_config is None
             or not experiment_config.enabled
+            or not self._is_in_experiment_sample(
                 subject, experiment_key, experiment_config
             )
         ):
@@ -42,7 +50,13 @@ def assign(self, subject: str, experiment_key: str) -> Optional[str]:
             None,
         )
 
+    def _shutdown(self):
+        """"""Stops all background processes used by the client
+        Do not use the client after calling this method.
+        """"""
+        self.__poller.stop()
+
+    def _is_in_experiment_sample(
         self,
         subject: str,
         experiment_key: str,"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"-from typing import List, Optional
 from eppo_client.base_model import SdkBaseModel
 
 from eppo_client.shard import ShardRange
 
 
 class VariationDto(SdkBaseModel):
     name: str
@@ -14,12 +19,40 @@ class ExperimentConfigurationDto(SdkBaseModel):
     percent_exposure: float
     enabled: bool
     variations: List[VariationDto]
-    name: str
 
 
 class ExperimentConfigurationRequestor:
     def get_configuration(
         self, experiment_key: str
     ) -> Optional[ExperimentConfigurationDto]:
-        # TODO: implement this method
-        return None"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+import logging
+from typing import Dict, List, Optional, cast
 from eppo_client.base_model import SdkBaseModel
+from eppo_client.configuration_store import ConfigurationStore
+from eppo_client.http_client import HttpClient, HttpRequestError
 
 from eppo_client.shard import ShardRange
 
+logger = logging.getLogger(__name__)
+
 
 class VariationDto(SdkBaseModel):
     name: str
@@ -14,12 +19,40 @@ class ExperimentConfigurationDto(SdkBaseModel):
     percent_exposure: float
     enabled: bool
     variations: List[VariationDto]
+    name: Optional[str]
+
+
+RAC_ENDPOINT = ""/randomized_assignment/config""
 
 
 class ExperimentConfigurationRequestor:
+    def __init__(
+        self,
+        http_client: HttpClient,
+        config_store: ConfigurationStore[ExperimentConfigurationDto],
+    ):
+        self.__http_client = http_client
+        self.__config_store = config_store
+
     def get_configuration(
         self, experiment_key: str
     ) -> Optional[ExperimentConfigurationDto]:
+        if self.__http_client.is_unauthorized():
+            raise ValueError(""Unauthorized: please check your API key"")
+        return self.__config_store.get_configuration(experiment_key)
+
+    def fetch_and_store_configurations(self) -> Dict[str, ExperimentConfigurationDto]:
+        try:
+            configs = cast(
+                dict, self.__http_client.get(RAC_ENDPOINT).get(""experiments"", {})
+            )
+            for exp_key, exp_config in configs.items():
+                configs[exp_key] = ExperimentConfigurationDto(**exp_config)
+            self.__config_store.set_configurations(configs)
+            return configs
+        except HttpRequestError as e:
+            logger.error(""Error retrieving assignment configurations: "" + str(e))
+            if e.is_recoverable():
+                return {}
+            else:
+                raise e  # caught by the polling task; causes assignment polling to stop"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+from typing import Dict, Optional, TypeVar, Generic
+from cachetools import LRUCache
+
+from eppo_client.read_write_lock import ReadWriteLock
+
+T = TypeVar(""T"")
+
+
+class ConfigurationStore(Generic[T]):
+    def __init__(self, max_size: int):
+        self.__cache: LRUCache = LRUCache(maxsize=max_size)
+        self.__lock = ReadWriteLock()
+
+    def get_configuration(self, key: str) -> Optional[T]:
+        try:
+            self.__lock.acquire_read()
+            return self.__cache[key]
+        except KeyError:
+            return None  # key does not exist
+        finally:
+            self.__lock.release_read()
+
+    def set_configurations(self, configs: Dict[str, T]):
+        try:
+            self.__lock.acquire_write()
+            for key, config in configs.items():
+                self.__cache[key] = config
+        finally:
+            self.__lock.release_write()"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+# configuration cache
+MAX_CACHE_ENTRIES = 1000  # arbitrary; the caching library requires a max limit
+
+# poller
+SECOND_MILLIS = 1000
+MINUTE_MILLIS = 60 * SECOND_MILLIS
+POLL_JITTER_MILLIS = 30 * SECOND_MILLIS
+POLL_INTERVAL_MILLIS = 5 * MINUTE_MILLIS"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+from typing import Any
+from requests.exceptions import Timeout
+from requests.adapters import HTTPAdapter, Retry
+from http import HTTPStatus
+
+import requests
+
+from eppo_client.base_model import SdkBaseModel
+
+
+class SdkParams(SdkBaseModel):
+    # attributes are camelCase because that's what the backend endpoint expects
+    apiKey: str
+    sdkName: str
+    sdkVersion: str
+
+
+class HttpRequestError(Exception):
+    def __init__(self, message: str, status_code: int):
+        self.status_code = status_code
+        super().__init__(message)
+
+    def is_recoverable(self) -> bool:
+        if self.status_code >= 400 and self.status_code < 500:
+            return (
+                self.status_code == HTTPStatus.TOO_MANY_REQUESTS
+                or self.status_code == HTTPStatus.REQUEST_TIMEOUT
+            )
+        return True
+
+
+REQUEST_TIMEOUT_SECONDS = 2
+# Retry reference: https://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry
+# This applies only to failed DNS lookups and connection timeouts,
+# never to requests where data has made it to the server.
+MAX_RETRIES = Retry(total=3, backoff_factor=1)
+
+
+class HttpClient:
+    def __init__(self, base_url: str, sdk_params: SdkParams):
+        self.__base_url = base_url
+        self.__sdk_params = sdk_params
+        self.__session = requests.Session()
+        self.__session.mount(""https://"", HTTPAdapter(max_retries=MAX_RETRIES))
+        self.__is_unauthorized = False
+
+    def is_unauthorized(self) -> bool:
+        return self.__is_unauthorized
+
+    def get(self, resource: str) -> Any:
+        try:
+            response = self.__session.get(
+                self.__base_url + resource,
+                params=self.__sdk_params.dict(),
+                timeout=REQUEST_TIMEOUT_SECONDS,
+            )
+            self.__is_unauthorized = response.status_code == HTTPStatus.UNAUTHORIZED
+            if response.status_code != HTTPStatus.OK:
+                raise self._get_http_error(response.status_code, resource)
+            return response.json()
+        except Timeout:
+            raise self._get_http_error(HTTPStatus.REQUEST_TIMEOUT, resource)
+
+    def _get_http_error(self, status_code: int, resource: str) -> HttpRequestError:
+        return HttpRequestError(
+            ""HTTP {} error while requesting resource {}"".format(status_code, resource),
+            status_code=status_code,
+        )"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+import logging
+from multiprocessing import Event
+from random import randrange
+from threading import Thread
+from typing import Callable
+
+logger = logging.getLogger(__name__)
+
+
+class Poller:
+    def __init__(self, interval_millis: int, jitter_millis: int, callback: Callable):
+        self.__jitter_millis = jitter_millis
+        self.__interval = interval_millis
+        self.__stop_event = Event()
+        self.__callback = callback
+        self.__thread = Thread(target=self.poll, daemon=True)
+
+    def start(self):
+        self.__thread.start()
+
+    def stop(self):
+        self.__stop_event.set()
+
+    def is_stopped(self):
+        return self.__stop_event.is_set()
+
+    def poll(self):
+        while not self.is_stopped():
+            try:
+                self.__callback()
+            except Exception as e:
+                logger.error(""Unexpected error running poll task: "" + str(e))
+                break
+            self._wait_for_interval()
+
+    def _wait_for_interval(self):
+        interval_with_jitter = self.__interval - randrange(0, self.__jitter_millis)
+        self.__stop_event.wait(interval_with_jitter / 1000)"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+import threading
+
+# Copied from: https://www.oreilly.com/library/view/python-cookbook/0596001673/ch06s04.html
+
+
+class ReadWriteLock:
+    """"""A lock object that allows many simultaneous ""read locks"", but
+    only one ""write lock."" """"""
+
+    def __init__(self):
+        self._read_ready = threading.Condition(threading.Lock())
+        self._readers = 0
+
+    def acquire_read(self):
+        """"""Acquire a read lock. Blocks only if a thread has
+        acquired the write lock.""""""
+        self._read_ready.acquire()
+        try:
+            self._readers += 1
+        finally:
+            self._read_ready.release()
+
+    def release_read(self):
+        """"""Release a read lock.""""""
+        self._read_ready.acquire()
+        try:
+            self._readers -= 1
+            if not self._readers:
+                self._read_ready.notifyAll()
+        finally:
+            self._read_ready.release()
+
+    def acquire_write(self):
+        """"""Acquire a write lock. Blocks until there are no
+        acquired read or write locks.""""""
+        self._read_ready.acquire()
+        while self._readers > 0:
+            self._read_ready.wait()
+
+    def release_write(self):
+        """"""Release a write lock.""""""
+        self._read_ready.release()"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" tox
 pytest
 mypy
-google-cloud-storage
\ No newline at end of file
\ No newline at end of file"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" tox
 pytest
 mypy
\ No newline at end of file
+google-cloud-storage
+httpretty
\ No newline at end of file"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"-pydantic
\ No newline at end of file"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"\ No newline at end of file
+pydantic
+requests
+cachetools
+types-cachetools
+types-requests"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" [metadata]
 name = eppo-server-sdk
-version = file: VERSION.txt
 author = Eppo
 author_email = eppo-team@geteppo.com
 description = Eppo SDK for Python
@@ -11,14 +11,14 @@ project_urls =
     Bug Tracker = https://github.com/Eppo-exp/python-sdk/issues
 classifiers =
     Programming Language :: Python :: 3
-    License :: OSI Approved :: Apache Software License
     Operating System :: OS Independent
 
 [options]
-package_dir =
-    = eppo_client
-packages = find:
 python_requires = >=3.6
-
-[options.packages.find]
-where = eppo_client
\ No newline at end of file"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" [metadata]
 name = eppo-server-sdk
+version = attr: eppo_client.__version__
 author = Eppo
 author_email = eppo-team@geteppo.com
 description = Eppo SDK for Python
@@ -11,14 +11,14 @@ project_urls =
     Bug Tracker = https://github.com/Eppo-exp/python-sdk/issues
 classifiers =
     Programming Language :: Python :: 3
+    License :: OSI Approved :: MIT License
     Operating System :: OS Independent
 
 [options]
+packages = eppo_client
 python_requires = >=3.6
\ No newline at end of file
+include_package_data=True
+install_requires =
+    pydantic
+    requests
+    cachetools"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" import json
 import os
-from typing import List, Optional
 from unittest.mock import patch
 import pytest
-from eppo_client.base_model import SdkBaseModel
 from eppo_client.client import EppoClient
 from eppo_client.configuration_requestor import (
     ExperimentConfigurationDto,
-    ExperimentConfigurationRequestor,
     VariationDto,
 )
 from eppo_client.shard import ShardRange
-
-mock_api_key = ""mock-api-key""
-
-
-class AssignmentTestCase(SdkBaseModel):
-    experiment: str
-    percent_exposure: float
-    variations: List[VariationDto]
-    subjects: List[str]
-    expected_assignments: List[Optional[str]]
-
 
 test_data = []
 for file_name in [file for file in os.listdir(""test/test-data/assignment"")]:
     with open(""test/test-data/assignment/{}"".format(file_name)) as test_case_json:
         test_case_dict = json.load(test_case_json)
-        variations = [
-            VariationDto(**variation_dict)
-            for variation_dict in test_case_dict[""variations""]
-        ]
-        test_case_dict[""variations""] = variations
-        test_data.append(AssignmentTestCase(**test_case_dict))
 
 
-def test_assign_blank_experiment():
-    client = EppoClient(config_requestor=ExperimentConfigurationRequestor())
     with pytest.raises(Exception) as exc_info:
         client.assign(""subject-1"", """")
     assert exc_info.value.args[0] == ""Invalid value for experiment_key: cannot be blank""
 
 
-def test_assign_blank_subject():
-    client = EppoClient(config_requestor=ExperimentConfigurationRequestor())
     with pytest.raises(Exception) as exc_info:
         client.assign("""", ""experiment-1"")
     assert exc_info.value.args[0] == ""Invalid value for subject: cannot be blank""
@@ -66,22 +82,10 @@ def test_assign_subject_not_in_sample(mock_config_requestor):
 
 @pytest.mark.parametrize(""test_case"", test_data)
 def test_assign_subject_in_sample(test_case):
-    print(""---- Test case for {} Experiment"".format(test_case.experiment))
-    with patch(
-        ""eppo_client.configuration_requestor.ExperimentConfigurationRequestor""
-    ) as mock_config_requestor:
-        mock_config_requestor.get_configuration.return_value = (
-            ExperimentConfigurationDto(
-                subjectShards=10000,
-                percentExposure=test_case.percent_exposure,
-                enabled=True,
-                variations=test_case.variations,
-                name=test_case.experiment,
-            )
-        )
-        client = EppoClient(config_requestor=mock_config_requestor)
-        assignments = [
-            client.assign(subject, test_case.experiment)
-            for subject in test_case.subjects
-        ]
-        assert assignments == test_case.expected_assignments"
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";" import json
 import os
+from time import sleep
 from unittest.mock import patch
+import httpretty  # type: ignore
 import pytest
 from eppo_client.client import EppoClient
+from eppo_client.config import Config
 from eppo_client.configuration_requestor import (
     ExperimentConfigurationDto,
     VariationDto,
 )
 from eppo_client.shard import ShardRange
+from eppo_client import init, get_instance
 
 test_data = []
 for file_name in [file for file in os.listdir(""test/test-data/assignment"")]:
     with open(""test/test-data/assignment/{}"".format(file_name)) as test_case_json:
         test_case_dict = json.load(test_case_json)
+        test_data.append(test_case_dict)
 
+exp_configs = dict()
+for experiment_test in test_data:
+    experiment_name = experiment_test[""experiment""]
+    exp_configs[experiment_name] = {
+        ""subjectShards"": 10000,
+        ""enabled"": True,
+        ""variations"": experiment_test[""variations""],
+        ""name"": experiment_name,
+        ""percentExposure"": experiment_test[""percentExposure""],
+    }
 
+MOCK_BASE_URL = ""http://localhost:4000/api""
+
+
+@pytest.fixture(scope=""session"", autouse=True)
+def init_fixture():
+    httpretty.enable()
+    config_response_json = json.dumps({""experiments"": exp_configs})
+    httpretty.register_uri(
+        httpretty.GET,
+        MOCK_BASE_URL + ""/randomized_assignment/config"",
+        body=config_response_json,
+    )
+    client = init(Config(base_url=MOCK_BASE_URL, api_key=""dummy""))
+    sleep(0.1)  # wait for initialization
+    yield
+    client._shutdown()
+    httpretty.disable()
+
+
+@patch(""eppo_client.configuration_requestor.ExperimentConfigurationRequestor"")
+def test_assign_blank_experiment(mock_config_requestor):
+    client = EppoClient(config_requestor=mock_config_requestor)
     with pytest.raises(Exception) as exc_info:
         client.assign(""subject-1"", """")
     assert exc_info.value.args[0] == ""Invalid value for experiment_key: cannot be blank""
 
 
+@patch(""eppo_client.configuration_requestor.ExperimentConfigurationRequestor"")
+def test_assign_blank_subject(mock_config_requestor):
+    client = EppoClient(config_requestor=mock_config_requestor)
     with pytest.raises(Exception) as exc_info:
         client.assign("""", ""experiment-1"")
     assert exc_info.value.args[0] == ""Invalid value for subject: cannot be blank""
@@ -66,22 +82,10 @@ def test_assign_subject_not_in_sample(mock_config_requestor):
 
 @pytest.mark.parametrize(""test_case"", test_data)
 def test_assign_subject_in_sample(test_case):
+    print(""---- Test case for {} Experiment"".format(test_case[""experiment""]))
+    client = get_instance()
+    assignments = [
+        client.assign(subject, test_case[""experiment""])
+        for subject in test_case[""subjects""]
+    ]
+    assert assignments == test_case[""expectedAssignments""]"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+from eppo_client.configuration_requestor import ExperimentConfigurationDto
+from eppo_client.configuration_store import ConfigurationStore
+
+test_exp = ExperimentConfigurationDto(
+    subject_shards=1000,
+    percent_exposure=1,
+    enabled=True,
+    variations=[],
+    name=""randomization_algo"",
+)
+
+TEST_MAX_SIZE = 10
+
+store: ConfigurationStore[ExperimentConfigurationDto] = ConfigurationStore(
+    max_size=TEST_MAX_SIZE
+)
+
+
+def test_get_configuration_unknown_key():
+    store.set_configurations({""randomization_algo"": test_exp})
+    assert store.get_configuration(""unknown_exp"") is None
+
+
+def test_get_configuration_known_key():
+    store.set_configurations({""randomization_algo"": test_exp})
+    assert store.get_configuration(""randomization_algo"") == test_exp
+
+
+def test_evicts_old_entries_when_max_size_exceeded():
+    store.set_configurations({""item_to_be_evicted"": test_exp})
+    assert store.get_configuration(""item_to_be_evicted"") == test_exp
+    configs = {}
+    for i in range(0, TEST_MAX_SIZE):
+        configs[""test-entry-{}"".format(i)] = test_exp
+    store.set_configurations(configs)
+    assert store.get_configuration(""item_to_be_evicted"") is None
+    assert (
+        store.get_configuration(""test-entry-{}"".format(TEST_MAX_SIZE - 1)) == test_exp
+    )"
KO;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";
OK;10;Eppo-exp;python-sdk;e3d0ac8ff74c322d6324614a65f84cd9f4410bec;"Implement poller and in-memory cache (#3)

* Implement poller and in-memory cache

* fix linter errors

* adjust error handling of http client

* include license in manifest.in

* load version from source code instead of file

* fix polling bug

* use LRU cache instead of TTL cache";"+from time import sleep
+from unittest.mock import Mock
+from eppo_client.poller import Poller
+
+
+def test_invokes_callback_until_stopped():
+    callback = Mock(return_value=None)
+    task = Poller(interval_millis=10, jitter_millis=1, callback=callback)
+    task.start()
+    sleep(0.099)
+    task.stop()
+    assert callback.call_count == 10
+
+
+def test_stops_polling_if_unexpected_error():
+    callback = Mock(side_effect=Exception(""bad request""))
+    task = Poller(interval_millis=10, jitter_millis=1, callback=callback)
+    task.start()
+    sleep(0.099)
+    task.stop()
+    assert callback.call_count == 1"
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
-from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
@@ -75,6 +76,7 @@ def convert(self):
             return
         
         try:
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
@@ -99,6 +101,7 @@ def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
@@ -123,7 +126,7 @@ def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
-    freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
+# from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
+
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
@@ -75,6 +76,7 @@ def convert(self):
             return
         
         try:
+
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
@@ -99,6 +101,7 @@ def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
+
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
@@ -123,7 +126,7 @@ def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
+    #freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"-from .mixamo_helper import Mixamo, get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
-from .pyglm_helper import get_3d_len,  find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
@@ -98,67 +98,76 @@ def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
-    # init medaipipe
-    mp_pose = mp.solutions.pose
-    mp_drawing = mp.solutions.drawing_utils 
-    pose_video = mp_pose.Pose(static_image_mode=static_image_mode,
-                        min_detection_confidence=min_detection_confidence, 
-                        model_complexity=model_complexity)
     frame_num = -1
     
-    fig = plt.figure()
-    
     try:
-        while cap.isOpened():
-
-            success, image = cap.read()
-            frame_num += 1
-            if not success or max_frame_num < frame_num :
-                break
-
-            image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose_video, mp_idx_mm_idx_map)
-            if glm_list[0] != None:
-                bones_json = {
-                   ""time"": frame_num,
-                   ""bones"": []
-                } 
-                mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
-                mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
-                mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
-                anim_result_json[""frames""].append(bones_json)
-                if is_show_result:
-                    rg = []
-                    rv = []
-                    mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
-                    cv2.imshow(""result"", glm_list_to_image(fig, rv, rg))
-                if is_hips_move:
-                    hip2d_left.x *=  width
-                    hip2d_left.y *=  height
-                    hip2d_left.z *=  width
-                    hip2d_right.x *= width
-                    hip2d_right.y *= height
-                    hip2d_right.z *= width
-                    if origin == None:
-                       origin = avg_vec3(hip2d_left, hip2d_right)
-                       hips2d_scale = glm.distance(hip2d_left, hip2d_right)
-                       factor = model_scale/hips2d_scale
-                    else:
-                        hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
-                        if hips_bone_json != None:
-                            set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
-
-            cv2.imshow('MediaPipe pose', image)
-
-            if cv2.waitKey(5) & 0xFF == 27:
-                break
         cap.release()
-        plt.close()
     except Exception as e:
         print(e)
-        plt.close()
         if cap.isOpened():
             cap.release()
-                
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"+from .mixamo_helper import Mixamo,  get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
+from .pyglm_helper import get_3d_len, draw_list2, find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
@@ -98,67 +98,76 @@ def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
     frame_num = -1
+    plt.ion()
+    plt.close()
+    fig = None
+    if is_show_result: 
+        fig = plt.figure()
+        plt.show()
     
+    # init mediapipe
+    mp_pose = mp.solutions.pose
+    mp_drawing = mp.solutions.drawing_utils 
     try:
+        with mp_pose.Pose(
+            static_image_mode=static_image_mode,
+                        min_detection_confidence=min_detection_confidence, 
+                        model_complexity=model_complexity
+        ) as pose:
+            while cap.isOpened():
+
+                success, cap_image = cap.read()
+                frame_num += 1
+                if not success or max_frame_num < frame_num :
+                    break
+
+                cap_image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, cap_image, pose, mp_idx_mm_idx_map)
+                if glm_list[0] != None:
+                    bones_json = {
+                       ""time"": frame_num,
+                       ""bones"": []
+                    } 
+                    mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
+                    mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
+                    mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
+                    anim_result_json[""frames""].append(bones_json)
+                    if is_show_result:
+                        rg = []
+                        rv = []
+                        mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
+                        plt.clf()
+                        draw_list2(fig, rv, rg)
+                    if is_hips_move:
+                        hip2d_left.x *=  width
+                        hip2d_left.y *=  height
+                        hip2d_left.z *=  width
+                        hip2d_right.x *= width
+                        hip2d_right.y *= height
+                        hip2d_right.z *= width
+                        if origin == None:
+                           origin = avg_vec3(hip2d_left, hip2d_right)
+                           hips2d_scale = glm.distance(hip2d_left, hip2d_right)
+                           factor = model_scale/hips2d_scale
+                        else:
+                            hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
+                            if hips_bone_json != None:
+                                set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
+
+                cv2.imshow('MediaPipe pose', cap_image)
+
+                if cv2.waitKey(5) & 0xFF == 27:
+                    break
         cap.release()
+        # plt.close(fig)
+        cv2.destroyAllWindows()    
+
     except Exception as e:
         print(e)
+        # plt.close(fig)
         if cap.isOpened():
             cap.release()
+            cv2.destroyAllWindows()
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
+def draw_list2(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
+    ax1 = plt.axes(projection='3d')
+    set_axes(ax1, elev=10, azim=azim, xrange=range, yrange=range, zrange=range)
+    dots = get_dot(vec_list, group_lists)
+    for dot in dots:
+        ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
+    
+    fig.canvas.draw()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
KO;10;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
-        self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
OK;10;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
+        # self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
KO;10;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
-        print(cache.actual_data)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
@@ -65,14 +64,10 @@ def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
-        cache.actual_data['w'] = keys[pygame.K_w] * 6
-        cache.actual_data['s'] = keys[pygame.K_s] * 6
-        cache.actual_data['a'] = keys[pygame.K_a] * 6
-        cache.actual_data['d'] = keys[pygame.K_d] * 6
-        for key in cache.actual_data:
-            if cache.actual_data[key] == 0:
-                continue
-            cache.actual_data[key] = cache.actual_data[key] - 1
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
@@ -83,9 +78,6 @@ def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
-            if e.type == pygame.KEYDOWN:
-                if e.unicode in cache.actual_data:
-                    cache.actual_data[e.unicode] = 5
         
         screen.fill('black')
         data = cache.get_last_data()"
OK;10;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
@@ -65,14 +64,10 @@ def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
+        cache.actual_data['w'] = keys[pygame.K_w]
+        cache.actual_data['s'] = keys[pygame.K_s]
+        cache.actual_data['a'] = keys[pygame.K_a]
+        cache.actual_data['d'] = keys[pygame.K_d]
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
@@ -83,9 +78,6 @@ def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
         
         screen.fill('black')
         data = cache.get_last_data()"
KO;10;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
     simulation_id = request['data'].get(""id"")
-    if not uid:
-        return
-    elif not simulation_id:
-        return
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
@@ -93,6 +99,8 @@ async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
OK;10;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
+    old_simulation_id = request['cookie'].get(""id"")
     simulation_id = request['data'].get(""id"")
+    
+    if not uid: return
+    if not simulation_id: return
+    if not old_simulation_id: return
+    
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
+    old_simulation = storage.get_unit(""simulations"", simulation_id=old_simulation_id)
+    if not old_simulation:
+        return
+    old_simulation[""users""].remove(user['id'])
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
@@ -93,6 +99,8 @@ async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
+    for key in request['data']['keys']:
+        request['data']['keys'][key] *= 6
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
KO;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;"-{
- ""cells"": [
-  {
-   ""cell_type"": ""markdown"",
-   ""id"": ""594dbadf"",
-   ""metadata"": {},
-   ""source"": [
-    ""Some Kind of description here""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""id"": ""892887fe"",
-   ""metadata"": {},
-   ""source"": [
-    ""## Import Statements""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 1,
-   ""id"": ""f2bf0207"",
-   ""metadata"": {},
-   ""outputs"": [],
-   ""source"": [
-    ""import pandas as pd\n"",
-    ""import matplotlib.pyplot as plt""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""id"": ""d1303d2b"",
-   ""metadata"": {},
-   ""source"": [
-    ""## Data Exploration""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 2,
-   ""id"": ""96bfc82b"",
-   ""metadata"": {},
-   ""outputs"": [],
-   ""source"": [
-    ""df = pd.read_csv('abortion.csv')""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 3,
-   ""id"": ""41128d3c"",
-   ""metadata"": {},
-   ""outputs"": [
-    {
-     ""data"": {
-      ""text/html"": [
-       ""<div>\n"",
-       ""<style scoped>\n"",
-       ""    .dataframe tbody tr th:only-of-type {\n"",
-       ""        vertical-align: middle;\n"",
-       ""    }\n"",
-       ""\n"",
-       ""    .dataframe tbody tr th {\n"",
-       ""        vertical-align: top;\n"",
-       ""    }\n"",
-       ""\n"",
-       ""    .dataframe thead th {\n"",
-       ""        text-align: right;\n"",
-       ""    }\n"",
-       ""</style>\n"",
-       ""<table border=\""1\"" class=\""dataframe\"">\n"",
-       ""  <thead>\n"",
-       ""    <tr style=\""text-align: right;\"">\n"",
-       ""      <th></th>\n"",
-       ""      <th>tweet</th>\n"",
-       ""      <th>date</th>\n"",
-       ""      <th>likes</th>\n"",
-       ""      <th>retweets</th>\n"",
-       ""    </tr>\n"",
-       ""  </thead>\n"",
-       ""  <tbody>\n"",
-       ""    <tr>\n"",
-       ""      <th>0</th>\n"",
-       ""      <td>See the Super Bowl Ad FOX Sports wouldn't run....</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>72</td>\n"",
-       ""      <td>60</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>1</th>\n"",
-       ""      <td>#DWTSIRLDid you know there is a #Galway branch...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>26</td>\n"",
-       ""      <td>33</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>2</th>\n"",
-       ""      <td>Happy 4th birthday to our little miracle! I’m ...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>8</td>\n"",
-       ""      <td>3</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>3</th>\n"",
-       ""      <td>#GE2020A good friend,who'se forgotten more abo...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>45</td>\n"",
-       ""      <td>14</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>4</th>\n"",
-       ""      <td>“Britain has been engaged in decades of rebell...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>8</td>\n"",
-       ""      <td>4</td>\n"",
-       ""    </tr>\n"",
-       ""  </tbody>\n"",
-       ""</table>\n"",
-       ""</div>""
-      ],
-      ""text/plain"": [
-       ""                                               tweet         date  likes  \\\n"",
-       ""0  See the Super Bowl Ad FOX Sports wouldn't run....  Feb 2, 2020     72   \n"",
-       ""1  #DWTSIRLDid you know there is a #Galway branch...  Feb 2, 2020     26   \n"",
-       ""2  Happy 4th birthday to our little miracle! I’m ...  Feb 2, 2020      8   \n"",
-       ""3  #GE2020A good friend,who'se forgotten more abo...  Feb 2, 2020     45   \n"",
-       ""4  “Britain has been engaged in decades of rebell...  Feb 2, 2020      8   \n"",
-       ""\n"",
-       ""   retweets  \n"",
-       ""0        60  \n"",
-       ""1        33  \n"",
-       ""2         3  \n"",
-       ""3        14  \n"",
-       ""4         4  ""
-      ]
-     },
-     ""execution_count"": 3,
-     ""metadata"": {},
-     ""output_type"": ""execute_result""
-    }
-   ],
-   ""source"": [
-    ""df.head()""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 4,
-   ""id"": ""20eaf945"",
-   ""metadata"": {},
-   ""outputs"": [
-    {
-     ""data"": {
-      ""text/html"": [
-       ""<div>\n"",
-       ""<style scoped>\n"",
-       ""    .dataframe tbody tr th:only-of-type {\n"",
-       ""        vertical-align: middle;\n"",
-       ""    }\n"",
-       ""\n"",
-       ""    .dataframe tbody tr th {\n"",
-       ""        vertical-align: top;\n"",
-       ""    }\n"",
-       ""\n"",
-       ""    .dataframe thead th {\n"",
-       ""        text-align: right;\n"",
-       ""    }\n"",
-       ""</style>\n"",
-       ""<table border=\""1\"" class=\""dataframe\"">\n"",
-       ""  <thead>\n"",
-       ""    <tr style=\""text-align: right;\"">\n"",
-       ""      <th></th>\n"",
-       ""      <th>tweet</th>\n"",
-       ""      <th>date</th>\n"",
-       ""      <th>likes</th>\n"",
-       ""      <th>retweets</th>\n"",
-       ""    </tr>\n"",
-       ""  </thead>\n"",
-       ""  <tbody>\n"",
-       ""    <tr>\n"",
-       ""      <th>275</th>\n"",
-       ""      <td>how about persuading \""baby killer\""Genocide Gla...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>0</td>\n"",
-       ""      <td>0</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>276</th>\n"",
-       ""      <td>Originally, ringfenced money was set aside for...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>0</td>\n"",
-       ""      <td>0</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>277</th>\n"",
-       ""      <td>Vampires, that's cute fox boy #Abortion</td>\n"",
-       ""      <td>Feb 1, 2020</td>\n"",
-       ""      <td>0</td>\n"",
-       ""      <td>0</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>278</th>\n"",
-       ""      <td>#sacrementofreconcilliation#CatholicChurch but...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>0</td>\n"",
-       ""      <td>0</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>279</th>\n"",
-       ""      <td>Can’t help the spin! As our press release clea...</td>\n"",
-       ""      <td>Feb 2, 2020</td>\n"",
-       ""      <td>1</td>\n"",
-       ""      <td>0</td>\n"",
-       ""    </tr>\n"",
-       ""  </tbody>\n"",
-       ""</table>\n"",
-       ""</div>""
-      ],
-      ""text/plain"": [
-       ""                                                 tweet         date  likes  \\\n"",
-       ""275  how about persuading \""baby killer\""Genocide Gla...  Feb 2, 2020      0   \n"",
-       ""276  Originally, ringfenced money was set aside for...  Feb 2, 2020      0   \n"",
-       ""277            Vampires, that's cute fox boy #Abortion  Feb 1, 2020      0   \n"",
-       ""278  #sacrementofreconcilliation#CatholicChurch but...  Feb 2, 2020      0   \n"",
-       ""279  Can’t help the spin! As our press release clea...  Feb 2, 2020      1   \n"",
-       ""\n"",
-       ""     retweets  \n"",
-       ""275         0  \n"",
-       ""276         0  \n"",
-       ""277         0  \n"",
-       ""278         0  \n"",
-       ""279         0  ""
-      ]
-     },
-     ""execution_count"": 4,
-     ""metadata"": {},
-     ""output_type"": ""execute_result""
-    }
-   ],
-   ""source"": [
-    ""df.tail()""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 5,
-   ""id"": ""8f45dbe0"",
-   ""metadata"": {},
-   ""outputs"": [
-    {
-     ""data"": {
-      ""text/plain"": [
-       ""(280, 4)""
-      ]
-     },
-     ""execution_count"": 5,
-     ""metadata"": {},
-     ""output_type"": ""execute_result""
-    }
-   ],
-   ""source"": [
-    ""df.shape""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 6,
-   ""id"": ""cd26008e"",
-   ""metadata"": {},
-   ""outputs"": [
-    {
-     ""data"": {
-      ""text/plain"": [
-       ""tweet       280\n"",
-       ""date        280\n"",
-       ""likes       280\n"",
-       ""retweets    280\n"",
-       ""dtype: int64""
-      ]
-     },
-     ""execution_count"": 6,
-     ""metadata"": {},
-     ""output_type"": ""execute_result""
-    }
-   ],
-   ""source"": [
-    ""df.count()""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": 8,
-   ""id"": ""daad2b14"",
-   ""metadata"": {},
-   ""outputs"": [
-    {
-     ""data"": {
-      ""text/html"": [
-       ""<div>\n"",
-       ""<style scoped>\n"",
-       ""    .dataframe tbody tr th:only-of-type {\n"",
-       ""        vertical-align: middle;\n"",
-       ""    }\n"",
-       ""\n"",
-       ""    .dataframe tbody tr th {\n"",
-       ""        vertical-align: top;\n"",
-       ""    }\n"",
-       ""\n"",
-       ""    .dataframe thead th {\n"",
-       ""        text-align: right;\n"",
-       ""    }\n"",
-       ""</style>\n"",
-       ""<table border=\""1\"" class=\""dataframe\"">\n"",
-       ""  <thead>\n"",
-       ""    <tr style=\""text-align: right;\"">\n"",
-       ""      <th></th>\n"",
-       ""      <th>retweets</th>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>likes</th>\n"",
-       ""      <th></th>\n"",
-       ""    </tr>\n"",
-       ""  </thead>\n"",
-       ""  <tbody>\n"",
-       ""    <tr>\n"",
-       ""      <th>0</th>\n"",
-       ""      <td>29</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>1</th>\n"",
-       ""      <td>37</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>2</th>\n"",
-       ""      <td>12</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>3</th>\n"",
-       ""      <td>5</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>4</th>\n"",
-       ""      <td>18</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>7</th>\n"",
-       ""      <td>16</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>8</th>\n"",
-       ""      <td>58</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>9</th>\n"",
-       ""      <td>5</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>10</th>\n"",
-       ""      <td>18</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>20</th>\n"",
-       ""      <td>3</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>26</th>\n"",
-       ""      <td>132</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>31</th>\n"",
-       ""      <td>60</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>45</th>\n"",
-       ""      <td>56</td>\n"",
-       ""    </tr>\n"",
-       ""    <tr>\n"",
-       ""      <th>72</th>\n"",
-       ""      <td>240</td>\n"",
-       ""    </tr>\n"",
-       ""  </tbody>\n"",
-       ""</table>\n"",
-       ""</div>""
-      ],
-      ""text/plain"": [
-       ""       retweets\n"",
-       ""likes          \n"",
-       ""0            29\n"",
-       ""1            37\n"",
-       ""2            12\n"",
-       ""3             5\n"",
-       ""4            18\n"",
-       ""7            16\n"",
-       ""8            58\n"",
-       ""9             5\n"",
-       ""10           18\n"",
-       ""20            3\n"",
-       ""26          132\n"",
-       ""31           60\n"",
-       ""45           56\n"",
-       ""72          240""
-      ]
-     },
-     ""execution_count"": 8,
-     ""metadata"": {},
-     ""output_type"": ""execute_result""
-    }
-   ],
-   ""source"": []
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""id"": ""e2ada595"",
-   ""metadata"": {},
-   ""outputs"": [],
-   ""source"": []
-  }
- ],
- ""metadata"": {
-  ""kernelspec"": {
-   ""display_name"": ""Python 3 (ipykernel)"",
-   ""language"": ""python"",
-   ""name"": ""python3""
-  },
-  ""language_info"": {
-   ""codemirror_mode"": {
-    ""name"": ""ipython"",
-    ""version"": 3
-   },
-   ""file_extension"": "".py"",
-   ""mimetype"": ""text/x-python"",
-   ""name"": ""python"",
-   ""nbconvert_exporter"": ""python"",
-   ""pygments_lexer"": ""ipython3"",
-   ""version"": ""3.9.7""
-  }
- },
- ""nbformat"": 4,
- ""nbformat_minor"": 5
-}"
OK;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;
KO;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;\ No newline at end of file
OK;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;"+from flask import Flask, render_template
+
+app = Flask(__name__)
+
+
+# To render HTML files, there must be a folder called 'templates' with the files in them.
+@app.route(""/"")
+def home():
+    return render_template(""index.html"")
+
+
+@app.route(""/generic.html"")
+def generic():
+    return render_template(""generic.html"")
+
+# To render static files (images, videos, css files, etc), create 'static' folder and reference things in there
+
+# TO DIRECTLY EDIT A WEBPAGE FROM CHROME ITSELF:
+# 1. Inspect --> Console --> document.body.contentEditable=true
+# To delete: 'Select element tool' --> del
+# TO SAVE CHANGES: CTRL+S --> replace HTML file
+
+
+if __name__ == '__main__':
+    app.run(debug=True)
\ No newline at end of file"
KO;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;" import csv
 import datetime
 import os
-import time
 import threading
 from pathlib import Path
 from queue import Queue
@@ -20,6 +20,10 @@
 if not os.path.exists(root_path / data_dir):
     os.mkdir(root_path / data_dir)
 
 
 class DataProcessingThread(threading.Thread):
     def __init__(self, main_obj, classifiers):
@@ -31,16 +35,18 @@ def __init__(self, main_obj, classifiers):
     def run(self):
         while True:
             scraped_data = self.in_queue.get()  # blocking till something is available in the queue
-            print('Processing data...')
             data_cleaned = clean_data(scraped_data, self.main)
 
             sentiments = classify(data_cleaned[0], self.classifiers)
-            print('Data classified, saving to CSV...')
             interval_final_data = (data_cleaned[0], data_cleaned[1], data_cleaned[2], data_cleaned[3], sentiments)
 
             save_to_csv(self.main.data_file_path, interval_final_data)
-            print('Data saved to CSV, done.')
-            # self.out_queue.put(classified_chunk)
 
 
 def to_numeric(tweet_engagement_value):
@@ -75,9 +81,14 @@ def clean_data(scraped_data, main_obj):
     output_data = ([], [], [], [])
     non_english_removed, duplicates_removed = 0, 0
 
-    # Iterate through each list
     for n in range(len(texts_list)):
         text = texts_list[n].replace('\n', '')
         # Detect and omit non-English tweets
         try:
             lang_data = pycld2.detect(text)[2]
@@ -86,38 +97,39 @@ def clean_data(scraped_data, main_obj):
                 continue
         except pycld2.error as e:
             print(f'ERROR {e}: {text}')
 
         # Tweet data cleared for cleaning, now normalize date,
-        try:
-            today = str(datetime.date.today())
-            curr_year = str(int(today[:4]))
-            # Append year to date data if searching through current-year tweets (Twitter excludes the year on these)
-            if main_obj.current_search_year == curr_year:
-                date = dates_list[n] + f', {curr_year}'
-            else:
-                date = dates_list[n]
-
-            # Skip duplicates (tweets repeated on same day)
-            if text in output_data[0]:
-                i = output_data[0].index(text)
-                if output_data[1][i] == date:
-                    duplicates_removed += 1
-                    continue
-
-            likes = likes_list[n]
-            retweets = retweets_list[n]
-
-            likes = 0 if likes == '' else to_numeric(likes)
-            retweets = 0 if retweets == '' else to_numeric(retweets)
-
-            output_data[0].append(text)
-            output_data[1].append(date)
-            output_data[2].append(likes)
-            output_data[3].append(retweets)
-        except IndexError as e:
-            # Reset process and try again, skipping over successfully scraped dates
-            print(f'ERROR: {e}')
-            main_obj.collect_tweet_data_for_range(resetting=True)
 
     print(f'Data cleaned: {non_english_removed} non-English tweet(s) removed, '
           f'{duplicates_removed} duplicate(s) removed, remaining tweets ""likes"" and ""retweets"" data normalized.')
@@ -138,7 +150,9 @@ def save_to_csv(file_path, final_data):
         # Get existing rows to prevent adding duplicate data
         with open(file_path, 'r', encoding='utf-8') as file1:
             # Existing data
-            existing_rows_text = {line[0] for line in csv.reader(file1, delimiter=',')}  # Set -t < list -t for lookup
 
             # Write to existing file
             with open(file_path, 'a', encoding='utf-8', newline='') as file2:
@@ -148,7 +162,8 @@ def save_to_csv(file_path, final_data):
                     tweet, date, likes, retweets, sentiment = final_data[0][n], final_data[1][n], final_data[2][n], \
                                                               final_data[3][n], final_data[4][n]
                     row = [tweet, date, likes, retweets, sentiment]
-                    if tweet not in existing_rows_text:
                         writer.writerow(row)
                     else:
                         duplicates += 1
@@ -193,10 +208,11 @@ def __init__(self, selected_hashtag, date_range):
         self.scraper = Scraper(selected_hashtag)
         self.current_search_year = self.date_range[0][2]
         print('Preparing classifiers...')
-        self.classifiers = get_classifiers()
         self.data_thread = DataProcessingThread(self, self.classifiers)
         self.data_thread.daemon = True
-        self.thread_running = False
 
     def day_tweets_already_collected(self, date):
         if not exists(self.data_file_path):
@@ -205,24 +221,20 @@ def day_tweets_already_collected(self, date):
         df = pandas.read_csv(self.data_file_path)
         dates_col = pandas.Series(df['date'].tolist())
         date_value = f'{date[1][0:3]} {date[0]}, {self.current_search_year}'
-        print(f'Checking if {date} already scraped...')
         if date_value in set(dates_col):
             return True
         else:
             return False
 
-    def collect_tweet_data_for_range(self, resetting=False):
-        if resetting:
-            self.scraper.driver.quit()
-            time.sleep(3)
-
         start = self.date_range[0]
         end = self.date_range[1]
 
         # Begin data processing thread
-        if not self.thread_running:
-            self.data_thread.start()
-            self.thread_running = True
 
         print(f'Scraping from {start} to {end}')
         start_day, start_month, start_year = start
@@ -231,13 +243,11 @@ def collect_tweet_data_for_range(self, resetting=False):
         # TODO: Implement interval alterations
 
         current_day, current_month, current_year = start_day, start_month, start_year
         while True:
             current_date = (current_day, current_month, current_year)
             days_in_current_month = get_days_in_month(current_month, current_year)
 
-            self.current_search_year = current_year
-            self.data_thread.current_search_year = self.current_search_year
-
             # Final day of month reached, go to next month
             if days_in_current_month[-1] == current_day:
                 if current_month == 'December':
@@ -257,6 +267,8 @@ def collect_tweet_data_for_range(self, resetting=False):
             if current_day == end_day and current_month == end_month and current_year == end_year:
                 break
 
             current_day = next_day
             current_month = other_month
             current_year = other_year
@@ -265,27 +277,56 @@ def collect_tweet_data_for_range(self, resetting=False):
             if self.day_tweets_already_collected(current_date):
                 continue
 
-            try:
-                print(f'Scraping from {current_date} to {current_date_next_day}...')
-                self.scraper.search(current_date, current_date_next_day)
-                scraped_data = self.scraper.scrape()
 
-                # Delegate data cleaning, classification, and writing to CSV to a thread to allow scraper to continue on
-                self.data_thread.in_queue.put(scraped_data)
 
-            except TimeoutException as e:
-                print(f'ERROR: {e}')
-                self.collect_tweet_data_for_range(resetting=True)
-            except StaleElementReferenceException as e:
-                print(f'ERROR: {e}')
-                self.collect_tweet_data_for_range(resetting=True)
-            except NoSuchElementException as e:
-                print(f'ERROR: {e}')
-                self.collect_tweet_data_for_range(resetting=True)
 
         print(f'Tweets for {start} to {end} scraped, exiting.')
         self.scraper.driver.quit()
-        self.thread_running = False
         return
 
 
@@ -294,6 +335,7 @@ def collect_tweet_data_for_range(self, resetting=False):
 
     # Scrape data for hashtag and range and return path to relevant CSV file
     # global selected_hashtag, date_range, scraper, data_file_path
-    for hashtag in ['abortion', 'Marvel', 'DonaldTrump', 'vaccines']:
-        main = Main(hashtag, (('1', 'January', '2021'), ('1', 'May', '2022')))
-        main.collect_tweet_data_for_range()"
OK;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;" import csv
 import datetime
+import gc
 import os
 import threading
 from pathlib import Path
 from queue import Queue
@@ -20,6 +20,10 @@
 if not os.path.exists(root_path / data_dir):
     os.mkdir(root_path / data_dir)
 
+print('Initializing classifiers...')
+classifiers = get_classifiers()
+print('Classifiers initialized.')
+
 
 class DataProcessingThread(threading.Thread):
     def __init__(self, main_obj, classifiers):
@@ -31,16 +35,18 @@ def __init__(self, main_obj, classifiers):
     def run(self):
         while True:
             scraped_data = self.in_queue.get()  # blocking till something is available in the queue
+            print('Classification Thread: Processing data...')
             data_cleaned = clean_data(scraped_data, self.main)
 
             sentiments = classify(data_cleaned[0], self.classifiers)
+            print('Classification Thread: Data classified, saving to CSV...')
             interval_final_data = (data_cleaned[0], data_cleaned[1], data_cleaned[2], data_cleaned[3], sentiments)
 
             save_to_csv(self.main.data_file_path, interval_final_data)
+            print('Classification Thread: Data saved to CSV, done.')
+            if self.main.done_scraping and self.in_queue.qsize() == 0:
+                print('Classification Thread: Closing classification thread.')
+                break
 
 
 def to_numeric(tweet_engagement_value):
@@ -75,9 +81,14 @@ def clean_data(scraped_data, main_obj):
     output_data = ([], [], [], [])
     non_english_removed, duplicates_removed = 0, 0
 
+    twitter_start_year = 2006
+    curr_year = datetime.date.today().year
+    years_of_twitter = [str(curr_year - i) for i in range(curr_year - twitter_start_year + 1)]
+
+    # Iterate through each list to clean texts, dates, likes, and retweets
     for n in range(len(texts_list)):
         text = texts_list[n].replace('\n', '')
+        text = ''.join(c for c in text if c.isprintable())  # Attempt to prevent pycld2 error from invalid UTF-8 chars
         # Detect and omit non-English tweets
         try:
             lang_data = pycld2.detect(text)[2]
@@ -86,38 +97,39 @@ def clean_data(scraped_data, main_obj):
                 continue
         except pycld2.error as e:
             print(f'ERROR {e}: {text}')
+            # main_obj.collect_tweet_data_for_range(resetting=True)
+            continue
 
         # Tweet data cleared for cleaning, now normalize date,
+        # try:
+        # Append year to date data if searching through current-year tweets (Twitter excludes the year on these)
+        if dates_list[n][-4:] not in years_of_twitter:
+            date = dates_list[n] + f', {curr_year}'
+        else:
+            date = dates_list[n]
+
+        # Skip duplicates (tweets repeated on same day)
+        if text in output_data[0]:
+            i = output_data[0].index(text)
+            if output_data[1][i] == date:
+                duplicates_removed += 1
+                continue
+
+        likes = likes_list[n]
+        retweets = retweets_list[n]
+
+        likes = 0 if likes == '' else to_numeric(likes)
+        retweets = 0 if retweets == '' else to_numeric(retweets)
+
+        output_data[0].append(text)
+        output_data[1].append(date)
+        output_data[2].append(likes)
+        output_data[3].append(retweets)
+        # except IndexError as e:
+        #     # Reset process and try again, skipping over successfully scraped dates
+        #     print(f'ERROR: {e}')
+        #     main_obj.done_scraping = True
+        #     main_obj.collect_tweet_data_for_range(resetting=True)
 
     print(f'Data cleaned: {non_english_removed} non-English tweet(s) removed, '
           f'{duplicates_removed} duplicate(s) removed, remaining tweets ""likes"" and ""retweets"" data normalized.')
@@ -138,7 +150,9 @@ def save_to_csv(file_path, final_data):
         # Get existing rows to prevent adding duplicate data
         with open(file_path, 'r', encoding='utf-8') as file1:
             # Existing data
+            existing_rows_text = [line[0] for line in csv.reader(file1, delimiter=',')] # Set -t < list -t for lookup
+            existing_rows_date = [line[1] for line in csv.reader(file1, delimiter=',')]
+            tweet_tuples = {(text, date) for text, date in zip(existing_rows_text, existing_rows_date)}
 
             # Write to existing file
             with open(file_path, 'a', encoding='utf-8', newline='') as file2:
@@ -148,7 +162,8 @@ def save_to_csv(file_path, final_data):
                     tweet, date, likes, retweets, sentiment = final_data[0][n], final_data[1][n], final_data[2][n], \
                                                               final_data[3][n], final_data[4][n]
                     row = [tweet, date, likes, retweets, sentiment]
+                    # Check if tweet message already tweeted on this date
+                    if (tweet, date) not in tweet_tuples:
                         writer.writerow(row)
                     else:
                         duplicates += 1
@@ -193,10 +208,11 @@ def __init__(self, selected_hashtag, date_range):
         self.scraper = Scraper(selected_hashtag)
         self.current_search_year = self.date_range[0][2]
         print('Preparing classifiers...')
+        self.classifiers = classifiers
         self.data_thread = DataProcessingThread(self, self.classifiers)
         self.data_thread.daemon = True
+        # self.thread_running = False
+        self.done_scraping = False
 
     def day_tweets_already_collected(self, date):
         if not exists(self.data_file_path):
@@ -205,24 +221,20 @@ def day_tweets_already_collected(self, date):
         df = pandas.read_csv(self.data_file_path)
         dates_col = pandas.Series(df['date'].tolist())
         date_value = f'{date[1][0:3]} {date[0]}, {self.current_search_year}'
         if date_value in set(dates_col):
             return True
         else:
             return False
 
+    def collect_tweet_data_for_range(self):
         start = self.date_range[0]
         end = self.date_range[1]
 
         # Begin data processing thread
+        # if not self.thread_running:
+        #     print('Starting thread')
+        self.data_thread.start()
+            # self.thread_running = True
 
         print(f'Scraping from {start} to {end}')
         start_day, start_month, start_year = start
@@ -231,13 +243,11 @@ def collect_tweet_data_for_range(self, resetting=False):
         # TODO: Implement interval alterations
 
         current_day, current_month, current_year = start_day, start_month, start_year
+        print('Skipping days already scraped...')
         while True:
             current_date = (current_day, current_month, current_year)
             days_in_current_month = get_days_in_month(current_month, current_year)
 
             # Final day of month reached, go to next month
             if days_in_current_month[-1] == current_day:
                 if current_month == 'December':
@@ -257,6 +267,8 @@ def collect_tweet_data_for_range(self, resetting=False):
             if current_day == end_day and current_month == end_month and current_year == end_year:
                 break
 
+            self.current_search_year = current_year
+
             current_day = next_day
             current_month = other_month
             current_year = other_year
@@ -265,27 +277,56 @@ def collect_tweet_data_for_range(self, resetting=False):
             if self.day_tweets_already_collected(current_date):
                 continue
 
+            # try:
+            print(f'Scraping from {current_date} to {current_date_next_day}...')
+
+            self.scraper.search(current_date, current_date_next_day)
+            scraped_data = self.scraper.scrape()
 
+            # Delegate data cleaning, classification, and writing to CSV to a thread to allow scraper to continue on
+            self.data_thread.in_queue.put(scraped_data)
 
+            # except TimeoutException as e:
+            #     print(f'ERROR: {e}')
+            #     self.collect_tweet_data_for_range(resetting=True)
+            # except StaleElementReferenceException as e:
+            #     print(f'ERROR: {e}')
+            #     self.collect_tweet_data_for_range(resetting=True)
+            # except NoSuchElementException as e:
+            #     print(f'ERROR: {e}')
+            #     self.collect_tweet_data_for_range(resetting=True)
 
         print(f'Tweets for {start} to {end} scraped, exiting.')
         self.scraper.driver.quit()
+        self.done_scraping = True
+        return True
+
+
+def collect_data(hashtag, date_range):
+    main = Main(hashtag, (('1', 'January', '2021'), ('22', 'May', '2022')))
+    try:
+        main.collect_tweet_data_for_range()
+        encountered_error = False
+    except TimeoutException as e:
+        print(f'ERROR: {e}')
+        encountered_error = True
+    except StaleElementReferenceException as e:
+        print(f'ERROR: {e}')
+        encountered_error = True
+    except NoSuchElementException as e:
+        print(f'ERROR: {e}')
+        encountered_error = True
+    except IndexError as e:
+        print(f'ERROR: {e}')
+        encountered_error = True
+
+    if encountered_error:
+        print('Retrying data collection...')
+        main.scraper.driver.quit()
+        # gc.collect()
+        collect_data(hashtag, date_range)
+    else:
+        print(f'Data for #{hashtag} during {date_range} collected successfully!')
         return
 
 
@@ -294,6 +335,7 @@ def collect_tweet_data_for_range(self, resetting=False):
 
     # Scrape data for hashtag and range and return path to relevant CSV file
     # global selected_hashtag, date_range, scraper, data_file_path
+
+    for topic in ['Marvel', 'DonaldTrump', 'vaccines']:
+        collect_data(topic, (('1', 'January', '2021'), ('22', 'May', '2022')))
+"
KO;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;" import sys
 import time
 import json
-import pandas
 from os.path import exists
 from pathlib import Path
 from selenium import webdriver
-from selenium.common import exceptions
-from selenium.common.exceptions import TimeoutException, StaleElementReferenceException
 from selenium.webdriver.chrome.service import Service
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support import expected_conditions
@@ -19,12 +17,13 @@
 root_path = Path('.')
 scraper_dir = 'scraper_files'
 data_dir = 'data_files'
-chromedriver_path = root_path / scraper_dir / 'chromedriver.exe'
 html_locations_path = root_path / scraper_dir / 'locations.json'
 
 if not exists(html_locations_path):
     sys.exit('Locations JSON file missing, unable to continue.')
 
 f = open(html_locations_path, 'r')
 locations = json.load(f)
 xpaths = locations['xpaths']
@@ -37,34 +36,28 @@ def scrape_displayed_tweet_elements(tweet_elements, collected_elements):
     texts, dates, likes, retweets = [], [], [], []
     # Scrape currently available tweets
     for t in tweet_elements:
-        try:
             # Scroll to tweet to ensure all of its elements are loaded, avoiding visited tweets
-            if t not in collected_elements:
-                newly_collected.append(t)
-                texts.append(t.find_element(By.CSS_SELECTOR, ""[data-testid='tweetText']"").text)
-                dates.append(t.find_element(By.CLASS_NAME, classes['dates']).text)
-                likes.append(t.find_element(By.CSS_SELECTOR, ""[data-testid='like']"").text)
-                retweets.append(t.find_element(By.CSS_SELECTOR, ""[data-testid='retweet']"").text)
-        except exceptions.StaleElementReferenceException:
-            pass
 
     return texts, dates, likes, retweets, newly_collected
 
 
 class Scraper:
-    # TWITTER CRAWL DELAY: 1 SECOND
-    driver, service = None, None
-
-    hashtag = ''
-    file_path = ''
-    # HTML element info
-    xpaths, classes = [], []
 
     def __init__(self, hashtag):
-        self.service = Service(str(chromedriver_path))
         self.hashtag = hashtag
         self.file_path = root_path / data_dir / f""{self.hashtag}.csv""
-        self.driver = webdriver.Chrome(service=self.service)
 
     def select_date_parameters(self, start_date, end_date):
         start_day, start_month, start_year = start_date
@@ -124,22 +117,22 @@ def search(self, start_date, end_date):
         """"""
         self.driver.get(search_url)
 
-        try:
-            these_hashtags = WebDriverWait(self.driver, 20).until \
-                (expected_conditions.presence_of_element_located((By.NAME, 'theseHashtags')))
-            these_hashtags.send_keys(self.hashtag)
 
-            scroll = self.driver.find_element(by=By.XPATH, value=xpaths['scroll'])
-            self.driver.execute_script(""arguments[0].scrollTop = arguments[0].scrollHeight"", scroll)
 
-            self.select_date_parameters(start_date, end_date)
 
-            # Parameters set, now search
-            search_button = self.driver.find_element(by=By.XPATH, value=xpaths['search_button'])
-            search_button.click()
-        except TimeoutException as e:
-            print(e)
-            # self.reset_scraping_process()
 
     def reached_end_of_results(self, tweet_element_height, waited_for_load=False):
         page_height = self.driver.execute_script(""return document.body.scrollHeight"")
@@ -165,39 +158,39 @@ def scrape(self):
         # While there are still results available, scrape them
         while True:
             # Get HTML elements of currently loaded tweets
-            try:
-                tweet_elements = WebDriverWait(self.driver, 10).until \
-                    (expected_conditions.presence_of_all_elements_located((By.CSS_SELECTOR, ""[data-testid='tweet']"")))
-                # Store last loaded tweet info for later retrieval (preventing StaleElementException)
-                last_tweet = tweet_elements[-1]
-                tweet_locations.append(last_tweet.location)
-                tweet_heights.append(last_tweet.size['height'])
-
-                texts, dates, likes, retweets, newly_collected = scrape_displayed_tweet_elements(
-                    tweet_elements, collected_tweets)
-                collected_tweets.extend(newly_collected)
-
-                # Update main data lists
-                text_data.extend(texts)
-                date_data.extend(dates)
-                likes_data.extend(likes)
-                retweets_data.extend(retweets)
-
-                if tweet_locations[-1] in scrolled_to:
-                    results_ended = self.reached_end_of_results(tweet_heights[-1])
-                else:
-                    last_tweet_location = tweet_locations[-1]
-                    self.driver.execute_script(
-                        f""window.scrollTo({last_tweet_location['x']}, {last_tweet_location['y']})"")
-                    scrolled_to.append(last_tweet_location)
-                    results_ended = False
-
-                if results_ended:
-                    print(f'End of results, scraped {len(text_data)} tweets.')
-                    break
-            except TimeoutException as e:
-                print(e)
-            except StaleElementReferenceException as e:
-                print(e)
 
         return text_data, date_data, likes_data, retweets_data"
OK;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;" import sys
 import time
 import json
 from os.path import exists
 from pathlib import Path
+from webdriver_manager.chrome import ChromeDriverManager
 from selenium import webdriver
 from selenium.webdriver.chrome.service import Service
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support import expected_conditions
@@ -19,12 +17,13 @@
 root_path = Path('.')
 scraper_dir = 'scraper_files'
 data_dir = 'data_files'
+# chromedriver_path = root_path / scraper_dir / 'chromedriver.exe'
 html_locations_path = root_path / scraper_dir / 'locations.json'
 
 if not exists(html_locations_path):
     sys.exit('Locations JSON file missing, unable to continue.')
 
+# Get HTML elements' info
 f = open(html_locations_path, 'r')
 locations = json.load(f)
 xpaths = locations['xpaths']
@@ -37,34 +36,28 @@ def scrape_displayed_tweet_elements(tweet_elements, collected_elements):
     texts, dates, likes, retweets = [], [], [], []
     # Scrape currently available tweets
     for t in tweet_elements:
+        # try:
             # Scroll to tweet to ensure all of its elements are loaded, avoiding visited tweets
+        if t not in collected_elements:
+            newly_collected.append(t)
+            texts.append(t.find_element(By.CSS_SELECTOR, ""[data-testid='tweetText']"").text)
+            dates.append(t.find_element(By.CLASS_NAME, classes['dates']).text)
+            likes.append(t.find_element(By.CSS_SELECTOR, ""[data-testid='like']"").text)
+            retweets.append(t.find_element(By.CSS_SELECTOR, ""[data-testid='retweet']"").text)
+        # except exceptions.StaleElementReferenceException:
+        #     pass
 
     return texts, dates, likes, retweets, newly_collected
 
 
 class Scraper:
 
     def __init__(self, hashtag):
+        # self.service = Service(str(chromedriver_path))
         self.hashtag = hashtag
         self.file_path = root_path / data_dir / f""{self.hashtag}.csv""
+        # self.driver = webdriver.Chrome(service=self.service)
+        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
 
     def select_date_parameters(self, start_date, end_date):
         start_day, start_month, start_year = start_date
@@ -124,22 +117,22 @@ def search(self, start_date, end_date):
         """"""
         self.driver.get(search_url)
 
+        # try:
+        these_hashtags = WebDriverWait(self.driver, 20).until \
+            (expected_conditions.presence_of_element_located((By.NAME, 'theseHashtags')))
+        these_hashtags.send_keys(self.hashtag)
 
+        scroll = self.driver.find_element(by=By.XPATH, value=xpaths['scroll'])
+        self.driver.execute_script(""arguments[0].scrollTop = arguments[0].scrollHeight"", scroll)
 
+        self.select_date_parameters(start_date, end_date)
 
+        # Parameters set, now search
+        search_button = self.driver.find_element(by=By.XPATH, value=xpaths['search_button'])
+        search_button.click()
+        # except TimeoutException as e:
+        #     print(e)
+        #     # self.reset_scraping_process()
 
     def reached_end_of_results(self, tweet_element_height, waited_for_load=False):
         page_height = self.driver.execute_script(""return document.body.scrollHeight"")
@@ -165,39 +158,39 @@ def scrape(self):
         # While there are still results available, scrape them
         while True:
             # Get HTML elements of currently loaded tweets
+            # try:
+            tweet_elements = WebDriverWait(self.driver, 10).until \
+                (expected_conditions.presence_of_all_elements_located((By.CSS_SELECTOR, ""[data-testid='tweet']"")))
+            # Store last loaded tweet info for later retrieval (preventing StaleElementException)
+            last_tweet = tweet_elements[-1]
+            tweet_locations.append(last_tweet.location)
+            tweet_heights.append(last_tweet.size['height'])
+
+            texts, dates, likes, retweets, newly_collected = scrape_displayed_tweet_elements(
+                tweet_elements, collected_tweets)
+            collected_tweets.extend(newly_collected)
+
+            # Update main data lists
+            text_data.extend(texts)
+            date_data.extend(dates)
+            likes_data.extend(likes)
+            retweets_data.extend(retweets)
+
+            if tweet_locations[-1] in scrolled_to:
+                results_ended = self.reached_end_of_results(tweet_heights[-1])
+            else:
+                last_tweet_location = tweet_locations[-1]
+                self.driver.execute_script(
+                    f""window.scrollTo({last_tweet_location['x']}, {last_tweet_location['y']})"")
+                scrolled_to.append(last_tweet_location)
+                results_ended = False
+
+            if results_ended:
+                print(f'End of results, scraped {len(text_data)} tweets.')
+                break
+            # except TimeoutException as e:
+            #     print(e)
+            # except StaleElementReferenceException as e:
+            #     print(e)
 
         return text_data, date_data, likes_data, retweets_data"
KO;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;" import os
 import nltk
 import pandas
 import statistics
@@ -20,12 +21,15 @@
 root_path = Path('.')  # Get root file path with pathlib
 pickle_dir = 'pickled_files'
 joblib_dir = 'joblib_files'
 positive_tweets_path = root_path / pickle_dir / 'positive_tweets.pickle'
 negative_tweets_path = root_path / pickle_dir / 'negative_tweets.pickle'
 positive_features_path = root_path / pickle_dir / 'positive_tweet_features.pickle'
 negative_features_path = root_path / pickle_dir / 'negative_tweet_features.pickle'
 accuracies_path = root_path / pickle_dir / 'classifier_accuracies.pickle'
 
 if not os.path.exists(pickle_dir):
     os.mkdir(pickle_dir)
 if not os.path.exists(joblib_dir):
@@ -141,6 +145,42 @@ def get_training_tweets():
     return positive_tweets, negative_tweets
 
 
 def get_features_from_tweets(positive_tweets, negative_tweets):
     pos_features, neg_features = [], []
     n1, n2 = 0, 0
@@ -276,7 +316,7 @@ def classify_tweets(tweets, classifiers):
 
         analysis_results.append(individual_results)
         n += 1
-        print(f'{n}/{len(tweets)} tweets classified.')
 
     return analysis_results
 "
OK;10;nickmack823;Twitter-Sentiment-Analyzer;0c34ed3e8ee5d3c4ecf4299b606ad17a02e757a1;Fixing memory issues, beginning Flask app;" import os
+import mysql
 import nltk
 import pandas
 import statistics
@@ -20,12 +21,15 @@
 root_path = Path('.')  # Get root file path with pathlib
 pickle_dir = 'pickled_files'
 joblib_dir = 'joblib_files'
+pos_bigram_path = root_path / joblib_dir / 'positive_bigram_finder.joblib'
+neg_bigram_path = root_path / joblib_dir / 'negative_bigram_finder.joblib'
 positive_tweets_path = root_path / pickle_dir / 'positive_tweets.pickle'
 negative_tweets_path = root_path / pickle_dir / 'negative_tweets.pickle'
 positive_features_path = root_path / pickle_dir / 'positive_tweet_features.pickle'
 negative_features_path = root_path / pickle_dir / 'negative_tweet_features.pickle'
 accuracies_path = root_path / pickle_dir / 'classifier_accuracies.pickle'
 
+
 if not os.path.exists(pickle_dir):
     os.mkdir(pickle_dir)
 if not os.path.exists(joblib_dir):
@@ -141,6 +145,42 @@ def get_training_tweets():
     return positive_tweets, negative_tweets
 
 
+# def get_bigram_finders():
+#     stopwords, pos_tweets, neg_tweets = None, None, None
+#     if exists(pos_bigram_path):
+#         positive_bigram_finder = get_data(pos_bigram_path)
+#     else:
+#         stopwords = nltk.corpus.stopwords.words(""english"")
+#         # Add names to unwanted words list, they don't help much for sentiment
+#         stopwords.extend([w.lower() for w in nltk.corpus.names.words()])
+#         pos_tweets, neg_tweets = get_training_tweets()
+#         pos_tokenized_lists = [nltk.word_tokenize(tweet) for tweet in pos_tweets]
+#         pos_tokenized_words = []
+#         for word_list in pos_tokenized_lists:
+#             for word in word_list:
+#                 pos_tokenized_words.append(word)
+#
+#         positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([
+#             w for w in pos_tokenized_words if w.isalpha() and w not in stopwords])
+#
+#         store_data(pos_bigram_path, positive_bigram_finder)
+#     if exists(neg_bigram_path):
+#         negative_bigram_finder = get_data(neg_bigram_path)
+#     else:
+#         neg_tokenized_lists = [nltk.word_tokenize(tweet) for tweet in neg_tweets]
+#         neg_tokenized_words = []
+#         for word_list in neg_tokenized_lists:
+#             for word in word_list:
+#                 neg_tokenized_words.append(word)
+#
+#         negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([
+#             w for w in neg_tokenized_words if w.isalpha() and w not in stopwords])
+#
+#         store_data(neg_bigram_path, negative_bigram_finder)
+#
+#     return positive_bigram_finder, negative_bigram_finder
+
+
 def get_features_from_tweets(positive_tweets, negative_tweets):
     pos_features, neg_features = [], []
     n1, n2 = 0, 0
@@ -276,7 +316,7 @@ def classify_tweets(tweets, classifiers):
 
         analysis_results.append(individual_results)
         n += 1
+        # print(f'{n}/{len(tweets)} tweets classified.')
 
     return analysis_results
 "
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;"+from django import forms
+
+from .models import Impression
+
+
+class ImpressionForm(forms.ModelForm):
+    title = forms.CharField(widget=forms.TextInput(attrs={""class"": ""field""}), label='Название')
+    description = forms.CharField(widget=forms.Textarea(attrs={""class"": ""field""}), label='Описание')
+
+    lon = forms.FloatField(widget=forms.NumberInput(attrs={""class"": ""field""}), label='Долгота')
+    lat = forms.FloatField(widget=forms.NumberInput(attrs={""class"": ""field""}), label='Широта')
+
+    class Meta:
+        model = Impression
+        fields = ['title', 'description', 'lon', 'lat']"
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;" from django.db.models.signals import post_save
 from django.dispatch import receiver
 
-from datetime import datetime, timedelta, timezone
-from django.utils.timesince import timesince
-
 
 class Profile(models.Model):
     user = models.OneToOneField(User, on_delete=models.CASCADE, verbose_name=""Пользователь"")"
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;" from django.db.models.signals import post_save
 from django.dispatch import receiver
 
 
 class Profile(models.Model):
     user = models.OneToOneField(User, on_delete=models.CASCADE, verbose_name=""Пользователь"")"
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;" from django.views import View
 
 from .models import Impression
 
 
 class Index(View):
@@ -12,12 +13,24 @@ def get(self, request):
 
 
 class Storage(View):
-    context = {}
 
     def get(self, request):
         self.context[""impressions""] = Impression.objects.filter(author=request.user)
         return render(request, 'impression_storage/storage.html', self.context)
 
 
 class AddImpression(View):
     context = {}"
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;" from django.views import View
 
 from .models import Impression
+from .forms import ImpressionForm
 
 
 class Index(View):
@@ -12,12 +13,24 @@ def get(self, request):
 
 
 class Storage(View):
+    context = {
+        'form': ImpressionForm(),
+    }
 
     def get(self, request):
         self.context[""impressions""] = Impression.objects.filter(author=request.user)
         return render(request, 'impression_storage/storage.html', self.context)
 
+    @staticmethod
+    def post(request):
+        form = ImpressionForm(request.POST)
+        if form.is_valid():
+            impression = form.save(commit=False)
+            impression.author = request.user
+            impression.save()
+
+        return redirect('/storage/')
+
 
 class AddImpression(View):
     context = {}"
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;"nav{
     padding: 5%;
 }
 
 /* Personal classes*/
 .gradient{
     font-size: 2em;
@@ -28,4 +99,3 @@ nav{
 .site-description p{max-width: 90%; margin: 20px auto; font-size: 15px}
 .site-description .btn-warning{font-size: 15px}
 
-"
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;"nav{
     padding: 5%;
 }
 
+
+/* Modal add impression form*/
+body._locked {
+  overflow: hidden;
+}
+
+._modal {
+  width: 100%;
+  height: 100vh;
+  position: fixed;
+  color: white;
+  top: 0;
+  left: 0;
+  z-index: 10;
+  visibility: hidden;
+  opacity: 0;
+  -webkit-transition: all .2s;
+  transition: all .2s;
+  pointer-events: none;
+}
+
+._modal._active {
+  opacity: 1;
+  visibility: visible;
+  pointer-events: all;
+}
+
+.modal-bg {
+  width: 100%;
+  height: 100%;
+  background-color: rgba(0, 0, 0, 0.8);
+  display: -webkit-box;
+  display: -ms-flexbox;
+  display: flex;
+  padding: 20px;
+}
+
+.modal-body {
+  padding: 24px;
+  background-color: #333333;
+  margin: auto;
+  max-height: 90%;
+  max-width: 30%;
+  overflow: auto;
+  border-radius: 16px;
+  position: relative;
+}
+
+.modal-callback__title{
+    background-color: #333333;
+    text-align: center;
+    font-size: 30px;
+}
+
+.modal-form{
+    background-color: #333333;
+}
+
+
+/* Form's classes*/
+.field{
+    font-weight: bold;
+    border: 1px solid #ccc;
+    border-radius: 6px;
+    height: 25px;
+    width: 300px;
+    margin: 5px 5px 5px 0;
+    align-items: center;
+}
+
+
 /* Personal classes*/
 .gradient{
     font-size: 2em;
@@ -28,4 +99,3 @@ nav{
 .site-description p{max-width: 90%; margin: 20px auto; font-size: 15px}
 .site-description .btn-warning{font-size: 15px}
 "
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;"+const modalBtns = document.querySelectorAll('._modal-open');
+const modals = document.querySelectorAll('._modal');
+const body = document.body;
+
+
+function openModal(elem) {
+	elem.classList.add('_active');
+	body.classList.add('_locked')
+}
+
+function closeModal(e) {
+	if (e.target.classList.contains('closeBtn') || e.target.closest('.closeBtn') || e.target.classList.contains('modal-bg')) {
+		e.target.closest('._modal').classList.remove('_active');
+		body.classList.remove('_locked')
+	}
+}
+
+modalBtns.forEach(btn => {
+	btn.addEventListener('click', (e) => {
+		let data = e.target.dataset.modalOpen;
+
+		modals.forEach(modal => {
+			if (modal.dataset.modal == data || modal.dataset.modal == e.target.closest('._modal-open').dataset.modalOpen) {
+				openModal(modal)
+			}
+		})
+	})
+})
+
+modals.forEach(modal => {
+	modal.addEventListener('click', e => closeModal(e))
+})
+
+window.addEventListener('keydown', e => {
+	modals.forEach(modal => {
+		if (e.key === ""Escape"" && modal.classList.contains('_active')) {
+			modal.classList.remove('_active');
+			body.classList.remove('_locked');
+		}
+	})
+})"
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;"     <!-- Font Awesome -->
     <link rel=""stylesheet"" href=""https://use.fontawesome.com/releases/v5.8.2/css/all.css"">
 
-    <link rel=""stylesheet"" href=""{% static 'css/main.css'%}"">
 </head>
 <body>
 "
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;"     <!-- Font Awesome -->
     <link rel=""stylesheet"" href=""https://use.fontawesome.com/releases/v5.8.2/css/all.css"">
 
+    <link rel=""stylesheet"" href=""{% static 'css/main.css'%}?{% now ""U"" %}""> <!-- To force an update -->
 </head>
 <body>
 "
KO;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;" 
     <div class=""container-fluid d-flex h-100 justify-content-center align-items-center p-0"">
         {% if impressions %}
-            <a href=""{% url 'impression_storage:new_impression' %}"" class=""btn btn-outline-warning mb-5""> Добавить воспоминание </a>
         {% else %}
             <div class=""row text-center"">
                 <p class=""text-light fs-4"">У вас нет ни одного воспоминания</p>
-                <a href=""{% url 'impression_storage:new_impression' %}"" class=""btn btn-warning""> Добавить воспоминание </a>
             </div>
         {% endif %}
     </div>
 
 
     {% if impressions %}
         <div class=""row row-cols-1 row-cols-md-3 g-4 px-4"">
@@ -47,4 +74,6 @@ <h5 class=""card-title"">{{ i.title }}</h5>
         </div>
     {% endif %}
 
 {% endblock %}"
OK;11;M1troll;PlaceRemember;9a0abe3cb7ee6495086e68ec5e0e726d549bc7e5;Add a modal window of a new memory;" 
     <div class=""container-fluid d-flex h-100 justify-content-center align-items-center p-0"">
         {% if impressions %}
+            <button class=""btn btn-outline-warning mb-5 _modal-open"" data-modal-open=""modal-1""> Добавить воспоминание </button>
         {% else %}
             <div class=""row text-center"">
                 <p class=""text-light fs-4"">У вас нет ни одного воспоминания</p>
+                <button class=""btn btn-warning _modal-open"" data-modal-open=""modal-1""> Добавить воспоминание </button>
             </div>
         {% endif %}
     </div>
 
+    <!-- Modal Form -->
+	<div class=""_modal"" data-modal=""modal-1"">
+		<div class=""modal-bg"">
+			<div class=""modal-body"">
+				<div class=""modal-content modal-callback"">
+					<div class=""modal-callback__title"">Новое воспоминание</div>
+                    <form method=""POST"" class=""modal-form"">
+                        {% csrf_token %}
+                        <div>
+                            {% for field in form %}
+                                <div class=""row"">
+                                    {{ field.label_tag }}
+                                    <div class=""col-md-10"">{{ field }}</div>
+                                </div>
+                            {% endfor %}
+                        </div>
+                        <p>
+                            <button class=""btn btn-outline-success"" type=""submit"">Создать</button>
+                            <button class=""btn btn-outline-danger closeBtn"" type=""reset"">Отмена</button>
+                        </p>
+                    </form>
+				</div>
+			</div>
+		</div>
+	</div>
+    <!-- end Modal Form -->
+
 
     {% if impressions %}
         <div class=""row row-cols-1 row-cols-md-3 g-4 px-4"">
@@ -47,4 +74,6 @@ <h5 class=""card-title"">{{ i.title }}</h5>
         </div>
     {% endif %}
 
+    <script src=""https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js""></script>
+    <script src=""{% static 'js/ImpressionFormShow.js' %}""></script>
 {% endblock %}"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";"+# Generated by Django 4.0.4 on 2022-04-23 04:22
+
+import datetime
+from django.conf import settings
+from django.db import migrations, models
+import django.db.models.deletion
+from django.utils.timezone import utc
+import django.utils.timezone
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
+        ('impression_storage', '0004_alter_profile_image_url'),
+    ]
+
+    operations = [
+        migrations.AddField(
+            model_name='impression',
+            name='created_at',
+            field=models.DateTimeField(auto_now_add=True, default=datetime.datetime(2022, 4, 23, 4, 21, 51, 995550, tzinfo=utc), verbose_name='Дата создания'),
+            preserve_default=False,
+        ),
+        migrations.AddField(
+            model_name='impression',
+            name='date_create',
+            field=models.DateTimeField(auto_now_add=True, default=django.utils.timezone.now, verbose_name='Дата создания'),
+            preserve_default=False,
+        ),
+        migrations.AddField(
+            model_name='impression',
+            name='updated_at',
+            field=models.DateTimeField(auto_now=True, verbose_name='Дата обновления'),
+        ),
+        migrations.AlterField(
+            model_name='impression',
+            name='author',
+            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL, verbose_name='Автор'),
+        ),
+        migrations.AlterField(
+            model_name='profile',
+            name='user',
+            field=models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL, verbose_name='Пользователь'),
+        ),
+    ]"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";"+# Generated by Django 4.0.4 on 2022-04-24 05:42
+
+from django.db import migrations
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        ('impression_storage', '0005_impression_created_at_impression_date_create_and_more'),
+    ]
+
+    operations = [
+        migrations.RemoveField(
+            model_name='impression',
+            name='date_create',
+        ),
+    ]"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";"+# Generated by Django 4.0.4 on 2022-04-24 10:02
+
+from django.db import migrations, models
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        ('impression_storage', '0006_remove_impression_date_create'),
+    ]
+
+    operations = [
+        migrations.AddField(
+            model_name='impression',
+            name='is_deleted',
+            field=models.BooleanField(default=False),
+        ),
+    ]"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" from django.db.models.signals import post_save
 from django.dispatch import receiver
 
 
 class Profile(models.Model):
-    user = models.OneToOneField(User, on_delete=models.CASCADE)
     image_url = models.CharField(max_length=100, null=True, default='/static/images/base_avatar.png', verbose_name=""Фото"")
 
     def __str__(self):
@@ -31,10 +34,14 @@ class Impression(models.Model):
     title = models.CharField(max_length=50, verbose_name=""Название места"")
     description = models.TextField(verbose_name=""Описание"")
 
     lon = models.FloatField(verbose_name=""Долгота"")
     lat = models.FloatField(verbose_name=""Широта"")
 
-    author = models.ForeignKey(User, on_delete=models.CASCADE)
 
     def __str__(self):
         return f""{self.title}"""
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" from django.db.models.signals import post_save
 from django.dispatch import receiver
 
+from datetime import datetime, timedelta, timezone
+from django.utils.timesince import timesince
+
 
 class Profile(models.Model):
+    user = models.OneToOneField(User, on_delete=models.CASCADE, verbose_name=""Пользователь"")
     image_url = models.CharField(max_length=100, null=True, default='/static/images/base_avatar.png', verbose_name=""Фото"")
 
     def __str__(self):
@@ -31,10 +34,14 @@ class Impression(models.Model):
     title = models.CharField(max_length=50, verbose_name=""Название места"")
     description = models.TextField(verbose_name=""Описание"")
 
+    is_deleted = models.BooleanField(default=False)
+    created_at = models.DateTimeField(auto_now_add=True, verbose_name=""Дата создания"")
+    updated_at = models.DateTimeField(auto_now=True, verbose_name=""Дата обновления"")
+
     lon = models.FloatField(verbose_name=""Долгота"")
     lat = models.FloatField(verbose_name=""Широта"")
 
+    author = models.ForeignKey(User, on_delete=models.CASCADE, verbose_name=""Автор"")
 
     def __str__(self):
         return f""{self.title}"""
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" app_name = 'impression_storage'
 urlpatterns = [
     path('', Index.as_view(), name='index'),
     path('storage/', Storage.as_view(), name='storage'),
     path('logout/', LogoutView.as_view(), name='logout'),
 ]"
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" app_name = 'impression_storage'
 urlpatterns = [
     path('', Index.as_view(), name='index'),
+    path('add/', AddImpression.as_view(), name='new_impression'),
+    path('delete/<int:pk>', DeleteImpression.as_view(), name='delete_impression'),
     path('storage/', Storage.as_view(), name='storage'),
     path('logout/', LogoutView.as_view(), name='logout'),
 ]"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" from django.shortcuts import render, redirect
-from django.contrib.auth import logout
 from django.views import View
 
-from .models import Impression, Profile
 
 
 class Index(View):
@@ -18,3 +17,28 @@ class Storage(View):
     def get(self, request):
         self.context[""impressions""] = Impression.objects.filter(author=request.user)
         return render(request, 'impression_storage/storage.html', self.context)"
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" from django.shortcuts import render, redirect
 from django.views import View
 
+from .models import Impression
 
 
 class Index(View):
@@ -18,3 +17,28 @@ class Storage(View):
     def get(self, request):
         self.context[""impressions""] = Impression.objects.filter(author=request.user)
         return render(request, 'impression_storage/storage.html', self.context)
+
+
+class AddImpression(View):
+    context = {}
+
+    def get(self, request):
+        return render(request, 'impression_storage/add_impression.html', self.context)
+
+    def post(self, request):
+        return render(request, 'impression_storage/storage.html', self.context)
+
+
+class DeleteImpression(View):
+    context = {}
+
+    @staticmethod
+    def get(request, pk):
+        try:
+            i = Impression.objects.get(pk=pk)
+            i.is_deleted = True
+            i.save()
+        except Impression.DoesNotExist:
+            pass
+
+        return redirect('/storage/')"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";"+{% extends 'base.html' %}
+{% load static %}
+
+{% block title %}Добавление воспоминание{% endblock %}
+
+
+{% block content %}
+
+    <div class=""d-flex justify-content-center"">
+
+    </div>
+
+{% endblock %}"
KO;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" 
 {% block content %}
 
-    <div class=""d-flex justify-content-center"">
-        <div class=""site-description"">
-            <div class=""gradient""><h1>Хранилище</h1></div>
-
-            {% if impressions %}
-                 <button class=""btn btn-warning""> Добавить воспоминание </button>
-            {% else %}
-                <p>У вас нет ни одного воспоминания</p>
-                <button class=""btn btn-warning""> Добавить воспоминание </button>
-            {% endif %}
         </div>
     </div>
 
 {% endblock %}"
OK;11;M1troll;PlaceRemember;bb8a8b402d0b95697a0d828eec74b00d329aced1;"Add storage page

Add storage and new memory creation pages. The memory deletion function is enabled.";" 
 {% block content %}
 
+    <div class=""container-fluid d-flex h-100 justify-content-center align-items-center p-0"">
+        <div class=""h1 text-center"">
+            <p class=""gradient"">Хранилище</p>
         </div>
     </div>
 
+    <div class=""container-fluid d-flex h-100 justify-content-center align-items-center p-0"">
+        {% if impressions %}
+            <a href=""{% url 'impression_storage:new_impression' %}"" class=""btn btn-outline-warning mb-5""> Добавить воспоминание </a>
+        {% else %}
+            <div class=""row text-center"">
+                <p class=""text-light fs-4"">У вас нет ни одного воспоминания</p>
+                <a href=""{% url 'impression_storage:new_impression' %}"" class=""btn btn-warning""> Добавить воспоминание </a>
+            </div>
+        {% endif %}
+    </div>
+
+
+    {% if impressions %}
+        <div class=""row row-cols-1 row-cols-md-3 g-4 px-4"">
+            {% for i in impressions %}
+                {% if not i.is_deleted %}
+                    <div class=""col"">
+                        <div class=""card bg-warning"">
+                            <div class=""card-header"">{{ i.created_at}}</div>
+                            <div class=""card-body"">
+                                <h5 class=""card-title"">{{ i.title }}</h5>
+                                <p class=""card-text"">{{ i.description }}</p>
+                                <a href=""#"" class=""btn btn-success"">Редактировать</a>
+                                <a href=""{% url 'impression_storage:delete_impression' i.pk %}"" class=""btn btn-danger"">Удалить</a>
+                            </div>
+                            <div class=""card-footer text-muted"">
+                                Обновлено: {{ i.updated_at|date:""d.m.Y"" }}
+                            </div>
+                        </div>
+                    </div>
+                {% endif %}
+            {% endfor %}
+        </div>
+    {% endif %}
+
 {% endblock %}"
KO;11;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
-    c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;11;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;"+from re import L
 from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
+    c_dict = dict.fromkeys(atom_dict.keys(), None)
+    for key in c_dict.keys():
+        c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
-, ""lr"" : 3e-4
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
-, ""batch_size"" : 32
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
OK;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
+, ""lr"" : 1e-5
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
+, ""batch_size"" : 64
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
KO;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
 
 
 print(""torch version:"", torch.__version__)
@@ -73,7 +74,7 @@ def avg(l):
   s = sum(l)
   return s / len(l)
 
-ntests = 50
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
@@ -136,6 +137,7 @@ def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
@@ -147,51 +149,73 @@ def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias50"", mus[:,0].mean().item() - 50.0, global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig50"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread50"", (mus[:,0] - 50).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
-  inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias25"", mus[:,0].mean().item() - 25.0, global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig25"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread25"", (mus[:,0] - 25).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
-  inputs = gen([testsig_mu, testsig_sigma, 5.0], [testbkg_mu, testbkg_sigma, 50.0])
 
-  mus , cov = utils.regress(localnet, globalnet, inputs05, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias05"", mus[:,0].mean().item() - 5.0, global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig05"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread05"", (mus[:,0] - 5).std().item(), global_step=epoch)
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
@@ -238,6 +262,7 @@ def gen(sig, bkg):
       , size=batch_size
       )
 
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
@@ -264,3 +289,4 @@ def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()"
OK;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
+import gc
 
 
 print(""torch version:"", torch.__version__)
@@ -73,7 +74,7 @@ def avg(l):
   s = sum(l)
   return s / len(l)
 
+ntests = 1000
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
@@ -136,6 +137,7 @@ def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
+  gc.collect()
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
@@ -147,51 +149,73 @@ def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
+  # TODO:
+  # plot spread / sqrt(N)
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 50.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias50"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert50"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull50"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread50"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
+  inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 25.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 25.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias25"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert25"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull25"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread25"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
+  inputs = gen([testsig_mu, testsig_sigma, 05.0], [testbkg_mu, testbkg_sigma, 05.0])
 
+  mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 05.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias05"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert05"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull05"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread05"", bias.std().item(), global_step=epoch)
+
+  localnet.zero_grad()
+  globalnet.zero_grad()
+
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
+
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
@@ -238,6 +262,7 @@ def gen(sig, bkg):
       , size=batch_size
       )
 
+
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
@@ -264,3 +289,4 @@ def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()
+"
KO;11;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
-test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range))
-test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range))
-test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range))
-test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range))
 
 inputs50 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig50) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig25) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig05) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
@@ -228,15 +228,15 @@ def avg(l):
 
     inputs = \
       torch.cat \
-      ( [ torch.Tensor(siginputs) , torch.Tensor(bkginputs) ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
-    targs = torch.Tensor(targs)
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
OK;11;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
+test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range)).detach()
+test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range)).detach()
+test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range)).detach()
+test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range)).detach()
 
 inputs50 = \
   torch.cat \
+  ( [ test_sig50 , test_bkg ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
+  ( [ test_sig25 , test_bkg ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
+  ( [ test_sig05 , test_bkg ]
   , axis = 2
   )
 
@@ -228,15 +228,15 @@ def avg(l):
 
     inputs = \
       torch.cat \
+      ( [ torch.Tensor(siginputs).detach() , torch.Tensor(bkginputs).detach() ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
+    targs = torch.Tensor(targs).detach()
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
KO;11;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
-
 ### Monitored RAM Locations
 
 PAL Version of MKwii
@@ -68,6 +67,18 @@ PAL Version of MKwii
 
 ### Usage
 
 ```bash
 python3 -m pip install -e mario-env
 ```"
OK;11;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
 ### Monitored RAM Locations
 
 PAL Version of MKwii
@@ -68,6 +67,18 @@ PAL Version of MKwii
 
 ### Usage
 
+Environment Variables:
+
+* Set `DOLPHIN_CONF_DIR` to the Dolphin Emulator User directory (MacOS : `~/Library/Application Support/Dolphin`)
+* Set `DOLPHIN_DIR` to the location of the Dolphin Binary (ex: `dolphin/build/Binaries/Dolphin.app/Contents/MacOS/Dolphin`)
+* Set `MK_ISO` to the location of the game iso
+
+In-Progress:
+
+* `CENTER_TRAJ` 3D trajectory for driving on the centerline (`centerline_traj.npy`)
+* `LEFT_TRAJ` 3D trajectory for driving on the left side of the track (`lefttraj.npy`)
+* `RIGHT_TRAJ` 3D trajectory for driving on the right side of the track (`righttraj.npy`)
+
 ```bash
 python3 -m pip install -e mario-env
 ```"
KO;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
-515576 songs should be exported to the CSV
 
 3\. Run the script
 "
OK;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
+10000 songs should be exported to the CSV due to memory constraints
 
 3\. Run the script
 "
KO;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
-	AND year IS NOT NULL AND year != 0;"
OK;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
+	AND year IS NOT NULL AND year != 0
+	LIMIT 10000;"
KO;11;Cornell-Tech-Urban-Tech-Hub;nycbuswatcher-gtfsrt-lambda;debeb8bce4842b2f995e5d2b2f5ded0d9f9dacb6;deployed to try to fix memory issue;
OK;11;Cornell-Tech-Urban-Tech-Hub;nycbuswatcher-gtfsrt-lambda;debeb8bce4842b2f995e5d2b2f5ded0d9f9dacb6;deployed to try to fix memory issue;"+version = 0.1
+[default]
+[default.deploy]
+[default.deploy.parameters]
+stack_name = ""nycbuswatcher-gtfsrt-lambda""
+s3_bucket = ""aws-sam-cli-managed-default-samclisourcebucket-1ll3d7xend2xd""
+s3_prefix = ""nycbuswatcher-gtfsrt-lambda""
+region = ""us-east-1""
+capabilities = ""CAPABILITY_IAM""
+image_repositories = []"
KO;11;Cornell-Tech-Urban-Tech-Hub;nycbuswatcher-gtfsrt-lambda;0362b6cce0ce173066739ef7b89928f6be59900e;deploying to see if memory issues better in cloud;"def lambda_handler(event, context):
     timestamp = dt.datetime.now().replace(microsecond=0)
     filename=f""{system_id}_{timestamp}.parquet"".replace("" "", ""_"").replace("":"", ""_"")
 
-    #FIXME: its clear we are running out of memory, but can't seem to increase it (at least on desktop)
     positions_df.to_parquet(f""/tmp/{filename}"", times='int96')
 
     # upload to S3"
OK;11;Cornell-Tech-Urban-Tech-Hub;nycbuswatcher-gtfsrt-lambda;0362b6cce0ce173066739ef7b89928f6be59900e;deploying to see if memory issues better in cloud;"def lambda_handler(event, context):
     timestamp = dt.datetime.now().replace(microsecond=0)
     filename=f""{system_id}_{timestamp}.parquet"".replace("" "", ""_"").replace("":"", ""_"")
 
+    #FIXME: its clear we are running out of memory in local testing, but can't seem to increase it (at least on desktop)
     positions_df.to_parquet(f""/tmp/{filename}"", times='int96')
 
     # upload to S3"
KO;11;Cornell-Tech-Urban-Tech-Hub;nycbuswatcher-gtfsrt-lambda;0362b6cce0ce173066739ef7b89928f6be59900e;deploying to see if memory issues better in cloud;"Description: >
 # More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst
 Globals:
   Function:
-    Timeout: 600
 
 Resources:
   NYCBuswatcherGTFSRTFunction:"
OK;11;Cornell-Tech-Urban-Tech-Hub;nycbuswatcher-gtfsrt-lambda;0362b6cce0ce173066739ef7b89928f6be59900e;deploying to see if memory issues better in cloud;"Description: >
 # More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst
 Globals:
   Function:
+    Timeout: 6000
 
 Resources:
   NYCBuswatcherGTFSRTFunction:"
KO;12;ricardomongza99;typeton;a975eadbe3a69c7eb94106b239fe9f21bf22622f;uninitialized memory;\ No newline at end of file
OK;12;ricardomongza99;typeton;a975eadbe3a69c7eb94106b239fe9f21bf22622f;uninitialized memory;"+class Book {
+    name: String
+    author: String
+    sequel: Book
+}
+
+func main() -> {
+
+    var book: Book
+    var book3: Book
+
+    book = new Book()
+    book.name = ""Harry Potter""
+    book.author = ""J.K. Rowling""
+
+    var book2: Book
+
+    book2 = new Book()
+    book2.name = ""Harry Potter 2""
+
+    book.sequel = book2
+
+    print(""Book name"")
+    print(book.name)
+
+    print(""Book sequel"")
+    print(book.sequel.name)
+
+
+    delete book3
+
+    delete book2
+}
\ No newline at end of file"
KO;12;ricardomongza99;typeton;a975eadbe3a69c7eb94106b239fe9f21bf22622f;uninitialized memory;" import os
 from src.config.definitions import PROGRAMS_DIR
 
-FILENAME = 'heap.ty'
 
 
 def main():"
OK;12;ricardomongza99;typeton;a975eadbe3a69c7eb94106b239fe9f21bf22622f;uninitialized memory;" import os
 from src.config.definitions import PROGRAMS_DIR
 
+FILENAME = 'book.ty'
 
 
 def main():"
KO;12;ricardomongza99;typeton;a975eadbe3a69c7eb94106b239fe9f21bf22622f;uninitialized memory;" from enum import Enum
 from functools import cmp_to_key
-from re import S
 
 from src.config.definitions import HEAP_RANGE_SIZE
 from src.utils.observer import Event, Publisher
@@ -61,6 +60,9 @@ def is_heap_address(self, address):
         return self.start <= address <= self.end
 
     def release_heap_memory(self, heap_address, level=0):
         """"""Release the heap memory at the given address""""""
         end = self.end_map[heap_address] - self.start
         start = heap_address - self.start"
OK;12;ricardomongza99;typeton;a975eadbe3a69c7eb94106b239fe9f21bf22622f;uninitialized memory;" from enum import Enum
 from functools import cmp_to_key
 
 from src.config.definitions import HEAP_RANGE_SIZE
 from src.utils.observer import Event, Publisher
@@ -61,6 +60,9 @@ def is_heap_address(self, address):
         return self.start <= address <= self.end
 
     def release_heap_memory(self, heap_address, level=0):
+        if heap_address is None:
+            self.broadcast(Event(RuntimeActions.STOP_RUNTIME, 'NULL Pointer Exception: Trying to free uninitialized address'))
+
         """"""Release the heap memory at the given address""""""
         end = self.end_map[heap_address] - self.start
         start = heap_address - self.start"
KO;12;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;
OK;12;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;"+""""""
+Memory
+---
+This script produces plots visualizing the memory consumption of arrays used by the PS agent
+depending on environment dimensions and the number of crates in said environment
+---
+Author: Josef Hamelink
+---
+Date: May 2022
+""""""
+
+# python standard library
+import os                                       # directories
+# dependencies
+import numpy as np                              # arrays
+import matplotlib as mpl                        # text formatting
+import matplotlib.pyplot as plt                 # figure
+import mpl_toolkits.axes_grid1 as axes_grid1    # grid subplot
+# local imports
+from helper import fix_dirs                     # directories
+
+def main():
+    
+    global dim_range, cc_range, ldr, lcr
+
+    dim_range = range(5, 11)   	# range of dimensions we want to plot: 5-10
+    cc_range = range(4, 11)    	# range of crate counts we want to plot: 4-10
+    ldr  = len(dim_range)
+    lcr = len(cc_range)
+
+    Q_res = np.zeros(shape=(ldr, lcr))
+    N_res = np.zeros(shape=(ldr, lcr))
+
+    for i, dim in enumerate(dim_range):
+        for j, cc in enumerate(cc_range):
+            Q_res[i,j] = Q_array_memory(dim, cc)
+            N_res[i,j] = N_array_memory(dim, cc)
+    
+    fig = plt.figure()
+    plt.rcParams.update({'font.size': 8})
+
+    grid = axes_grid1.AxesGrid(fig, 111, nrows_ncols=(1, 2), axes_pad = 0.3, cbar_location = ""bottom"",
+                            cbar_mode=""each"", cbar_size=""10%"", cbar_pad=""5%"")
+
+    add_subplot(grid, Q_res, 0, 'Q Table (memory in MB)')
+    add_subplot(grid, N_res, 1, 'N Table (memory in GB)')
+
+    fix_dirs()
+    plt.savefig(os.path.join(os.getcwd(),'..','results','memory.png'), bbox_inches='tight', dpi=200)
+
+
+def Q_array_memory(dim: int, cc: int) -> float:
+    """"""Calculates the chunk of memory needed to hold the Q-values based on dimension and crate count (MB)""""""
+    n_states = (dim+2)**2 * 2**cc
+    n_actions = 6
+    n_slots = n_states * n_actions
+    n_bytes = 8 * n_slots  	# default float contains 64 bits (8 bytes)
+    n_megabytes = n_bytes * 2**(-20)
+    return round(n_megabytes, 1)
+
+def N_array_memory(dim: int, cc: int) -> float:
+    """"""Calculates the chunk of memory needed to hold the N-values based on dimension and crate count (GB)""""""
+    n_states = (dim+2)**2 * 2**cc
+    n_actions = 6
+    n_slots = n_states * n_actions * n_states
+    n_bytes = 8 * n_slots  # default int contains 64 bits (8 bytes)
+    n_gigabytes = n_bytes * 2**(-30)
+    return round(n_gigabytes, 1)
+
+def add_subplot(grid: axes_grid1.ImageGrid, res: np.ndarray, plot_idx: int, title: str) -> None:
+    """"""Creates one subplot and adds it to the grid""""""
+    im = grid[plot_idx].imshow(res, cmap='viridis', interpolation='none')
+    im.axes.xaxis.tick_top()
+    im.axes.xaxis.set_label_position('top')
+    im.axes.set_xticks(ticks=range(lcr), labels=cc_range)
+    im.axes.set_yticks(ticks=range(ldr), labels=dim_range)
+    im.axes.tick_params(axis='both', top=False, left=False)
+    im.axes.set_xlabel('number of crates')
+    im.axes.set_ylabel('world dimensions')
+    im.axes.set_title(title)
+    grid.cbar_axes[plot_idx].colorbar(im)
+
+    textcolors = ('black', 'white')
+    kw = {'fontsize': 6,
+          'horizontalalignment': 'center',
+          'verticalalignment': 'center'}
+
+    threshold = im.norm(res.max())/2.0
+    valfmt = mpl.ticker.StrMethodFormatter(""{x:.1f}"")
+
+    for i in range(res.shape[0]):
+        for j in range(res.shape[1]):
+            kw.update(color=textcolors[int(im.norm(res[i, j]) < threshold)])
+            im.axes.text(j, i, valfmt(res[i, j], None), **kw)
+
+
+if __name__ == '__main__':
+    main()"
KO;12;Jeolpaul2;BETA-MUSIC-BOT;4e64c91707f8737168089260a9f9b20c220364f7;Create memorydatabase.py;
OK;12;Jeolpaul2;BETA-MUSIC-BOT;4e64c91707f8737168089260a9f9b20c220364f7;Create memorydatabase.py;"+#
+# Copyright 2021-2022 Jeolpaul2
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# All rights reserved.
+
+import config
+from config import PRIVATE_BOT_MODE
+from BetaMusic.core.mongo import mongodb
+
+channeldb = mongodb.cplaymode
+commanddb = mongodb.commands
+cleandb = mongodb.cleanmode
+playmodedb = mongodb.playmode
+playtypedb = mongodb.playtypedb
+langdb = mongodb.language
+authdb = mongodb.adminauth
+videodb = mongodb.yukkivideocalls
+onoffdb = mongodb.onoffper
+suggdb = mongodb.suggestion
+autoenddb = mongodb.autoend
+
+
+# Shifting to memory [ mongo sucks often]
+loop = {}
+playtype = {}
+playmode = {}
+channelconnect = {}
+langm = {}
+pause = {}
+mute = {}
+audio = {}
+video = {}
+active = []
+activevideo = []
+command = []
+cleanmode = []
+nonadmin = {}
+vlimit = []
+maintenance = []
+suggestion = {}
+autoend = {}
+
+
+# Auto End Stream
+
+
+async def is_autoend() -> bool:
+    chat_id = 123
+    mode = autoend.get(chat_id)
+    if not mode:
+        user = await autoenddb.find_one({""chat_id"": chat_id})
+        if not user:
+            autoend[chat_id] = False
+            return False
+        autoend[chat_id] = True
+        return True
+    return mode
+
+
+async def autoend_on():
+    chat_id = 123
+    autoend[chat_id] = True
+    user = await autoenddb.find_one({""chat_id"": chat_id})
+    if not user:
+        return await autoenddb.insert_one({""chat_id"": chat_id})
+
+
+async def autoend_off():
+    chat_id = 123
+    autoend[chat_id] = False
+    user = await autoenddb.find_one({""chat_id"": chat_id})
+    if user:
+        return await autoenddb.delete_one({""chat_id"": chat_id})
+
+
+# SUGGESTION
+
+
+async def is_suggestion(chat_id: int) -> bool:
+    mode = suggestion.get(chat_id)
+    if not mode:
+        user = await suggdb.find_one({""chat_id"": chat_id})
+        if not user:
+            suggestion[chat_id] = True
+            return True
+        suggestion[chat_id] = False
+        return False
+    return mode
+
+
+async def suggestion_on(chat_id: int):
+    suggestion[chat_id] = True
+    user = await suggdb.find_one({""chat_id"": chat_id})
+    if user:
+        return await suggdb.delete_one({""chat_id"": chat_id})
+
+
+async def suggestion_off(chat_id: int):
+    suggestion[chat_id] = False
+    user = await suggdb.find_one({""chat_id"": chat_id})
+    if not user:
+        return await suggdb.insert_one({""chat_id"": chat_id})
+
+
+# LOOP PLAY
+async def get_loop(chat_id: int) -> int:
+    lop = loop.get(chat_id)
+    if not lop:
+        return 0
+    return lop
+
+
+async def set_loop(chat_id: int, mode: int):
+    loop[chat_id] = mode
+
+
+# Channel Play IDS
+async def get_cmode(chat_id: int) -> int:
+    mode = channelconnect.get(chat_id)
+    if not mode:
+        mode = await channeldb.find_one({""chat_id"": chat_id})
+        if not mode:
+            return None
+        channelconnect[chat_id] = mode[""mode""]
+        return mode[""mode""]
+    return mode
+
+
+async def set_cmode(chat_id: int, mode: int):
+    channelconnect[chat_id] = mode
+    await channeldb.update_one(
+        {""chat_id"": chat_id}, {""$set"": {""mode"": mode}}, upsert=True
+    )
+
+
+# PLAY TYPE WHETHER ADMINS ONLY OR EVERYONE
+async def get_playtype(chat_id: int) -> str:
+    mode = playtype.get(chat_id)
+    if not mode:
+        mode = await playtypedb.find_one({""chat_id"": chat_id})
+        if not mode:
+            playtype[chat_id] = ""Everyone""
+            return ""Everyone""
+        playtype[chat_id] = mode[""mode""]
+        return mode[""mode""]
+    return mode
+
+
+async def set_playtype(chat_id: int, mode: str):
+    playtype[chat_id] = mode
+    await playtypedb.update_one(
+        {""chat_id"": chat_id}, {""$set"": {""mode"": mode}}, upsert=True
+    )
+
+
+# play mode whether inline or direct query
+async def get_playmode(chat_id: int) -> str:
+    mode = playmode.get(chat_id)
+    if not mode:
+        mode = await playmodedb.find_one({""chat_id"": chat_id})
+        if not mode:
+            playmode[chat_id] = ""Direct""
+            return ""Direct""
+        playmode[chat_id] = mode[""mode""]
+        return mode[""mode""]
+    return mode
+
+
+async def set_playmode(chat_id: int, mode: str):
+    playmode[chat_id] = mode
+    await playmodedb.update_one(
+        {""chat_id"": chat_id}, {""$set"": {""mode"": mode}}, upsert=True
+    )
+
+
+# language
+async def get_lang(chat_id: int) -> str:
+    mode = langm.get(chat_id)
+    if not mode:
+        lang = await langdb.find_one({""chat_id"": chat_id})
+        if not lang:
+            langm[chat_id] = ""en""
+            return ""en""
+        langm[chat_id] = lang[""lang""]
+        return lang[""lang""]
+    return mode
+
+
+async def set_lang(chat_id: int, lang: str):
+    langm[chat_id] = lang
+    await langdb.update_one(
+        {""chat_id"": chat_id}, {""$set"": {""lang"": lang}}, upsert=True
+    )
+
+
+# Muted
+async def is_muted(chat_id: int) -> bool:
+    mode = mute.get(chat_id)
+    if not mode:
+        return False
+    return mode
+
+
+async def mute_on(chat_id: int):
+    mute[chat_id] = True
+
+
+async def mute_off(chat_id: int):
+    mute[chat_id] = False
+
+
+# Pause-Skip
+async def is_music_playing(chat_id: int) -> bool:
+    mode = pause.get(chat_id)
+    if not mode:
+        return False
+    return mode
+
+
+async def music_on(chat_id: int):
+    pause[chat_id] = True
+
+
+async def music_off(chat_id: int):
+    pause[chat_id] = False
+
+
+# Active Voice Chats
+async def get_active_chats() -> list:
+    return active
+
+
+async def is_active_chat(chat_id: int) -> bool:
+    if chat_id not in active:
+        return False
+    else:
+        return True
+
+
+async def add_active_chat(chat_id: int):
+    if chat_id not in active:
+        active.append(chat_id)
+
+
+async def remove_active_chat(chat_id: int):
+    if chat_id in active:
+        active.remove(chat_id)
+
+
+# Active Video Chats
+async def get_active_video_chats() -> list:
+    return activevideo
+
+
+async def is_active_video_chat(chat_id: int) -> bool:
+    if chat_id not in activevideo:
+        return False
+    else:
+        return True
+
+
+async def add_active_video_chat(chat_id: int):
+    if chat_id not in activevideo:
+        activevideo.append(chat_id)
+
+
+async def remove_active_video_chat(chat_id: int):
+    if chat_id in activevideo:
+        activevideo.remove(chat_id)
+
+
+# Delete command mode
+async def is_commanddelete_on(chat_id: int) -> bool:
+    if chat_id not in command:
+        return True
+    else:
+        return False
+
+
+async def commanddelete_off(chat_id: int):
+    if chat_id not in command:
+        command.append(chat_id)
+
+
+async def commanddelete_on(chat_id: int):
+    try:
+        command.remove(chat_id)
+    except:
+        pass
+
+
+# Clean Mode
+async def is_cleanmode_on(chat_id: int) -> bool:
+    if chat_id not in cleanmode:
+        return True
+    else:
+        return False
+
+
+async def cleanmode_off(chat_id: int):
+    if chat_id not in cleanmode:
+        cleanmode.append(chat_id)
+
+
+async def cleanmode_on(chat_id: int):
+    try:
+        cleanmode.remove(chat_id)
+    except:
+        pass
+
+
+# Non Admin Chat
+async def check_nonadmin_chat(chat_id: int) -> bool:
+    user = await authdb.find_one({""chat_id"": chat_id})
+    if not user:
+        return False
+    return True
+
+
+async def is_nonadmin_chat(chat_id: int) -> bool:
+    mode = nonadmin.get(chat_id)
+    if not mode:
+        user = await authdb.find_one({""chat_id"": chat_id})
+        if not user:
+            nonadmin[chat_id] = False
+            return False
+        nonadmin[chat_id] = True
+        return True
+    return mode
+
+
+async def add_nonadmin_chat(chat_id: int):
+    nonadmin[chat_id] = True
+    is_admin = await check_nonadmin_chat(chat_id)
+    if is_admin:
+        return
+    return await authdb.insert_one({""chat_id"": chat_id})
+
+
+async def remove_nonadmin_chat(chat_id: int):
+    nonadmin[chat_id] = False
+    is_admin = await check_nonadmin_chat(chat_id)
+    if not is_admin:
+        return
+    return await authdb.delete_one({""chat_id"": chat_id})
+
+
+# Video Limit
+async def is_video_allowed(chat_idd) -> str:
+    chat_id = 123456
+    if not vlimit:
+        dblimit = await videodb.find_one({""chat_id"": chat_id})
+        if not dblimit:
+            vlimit.clear()
+            vlimit.append(config.VIDEO_STREAM_LIMIT)
+            limit = config.VIDEO_STREAM_LIMIT
+        else:
+            limit = dblimit[""limit""]
+            vlimit.clear()
+            vlimit.append(limit)
+    else:
+        limit = vlimit[0]
+    if limit == 0:
+        return False
+    count = len(await get_active_video_chats())
+    if int(count) == int(limit):
+        if not await is_active_video_chat(chat_idd):
+            return False
+    return True
+
+
+async def get_video_limit() -> str:
+    chat_id = 123456
+    if not vlimit:
+        dblimit = await videodb.find_one({""chat_id"": chat_id})
+        if not dblimit:
+            limit = config.VIDEO_STREAM_LIMIT
+        else:
+            limit = dblimit[""limit""]
+    else:
+        limit = vlimit[0]
+    return limit
+
+
+async def set_video_limit(limt: int):
+    chat_id = 123456
+    vlimit.clear()
+    vlimit.append(limt)
+    return await videodb.update_one(
+        {""chat_id"": chat_id}, {""$set"": {""limit"": limt}}, upsert=True
+    )
+
+
+# On Off
+async def is_on_off(on_off: int) -> bool:
+    onoff = await onoffdb.find_one({""on_off"": on_off})
+    if not onoff:
+        return False
+    return True
+
+
+async def add_on(on_off: int):
+    is_on = await is_on_off(on_off)
+    if is_on:
+        return
+    return await onoffdb.insert_one({""on_off"": on_off})
+
+
+async def add_off(on_off: int):
+    is_off = await is_on_off(on_off)
+    if not is_off:
+        return
+    return await onoffdb.delete_one({""on_off"": on_off})
+
+
+# Maintenance
+
+
+async def is_maintenance():
+    if not maintenance:
+        get = await onoffdb.find_one({""on_off"": 1})
+        if not get:
+            maintenance.clear()
+            maintenance.append(2)
+            return True
+        else:
+            maintenance.clear()
+            maintenance.append(1)
+            return False
+    else:
+        if 1 in maintenance:
+            return False
+        else:
+            return True
+
+
+async def maintenance_off():
+    maintenance.clear()
+    maintenance.append(2)
+    is_off = await is_on_off(1)
+    if not is_off:
+        return
+    return await onoffdb.delete_one({""on_off"": 1})
+
+
+async def maintenance_on():
+    maintenance.clear()
+    maintenance.append(1)
+    is_on = await is_on_off(1)
+    if is_on:
+        return
+    return await onoffdb.insert_one({""on_off"": 1})
+
+
+# Audio Video Limit
+
+from pytgcalls.types.input_stream.quality import (HighQualityAudio,
+                                                  HighQualityVideo,
+                                                  LowQualityAudio,
+                                                  LowQualityVideo,
+                                                  MediumQualityAudio,
+                                                  MediumQualityVideo)
+
+
+async def save_audio_bitrate(chat_id: int, bitrate: str):
+    audio[chat_id] = bitrate
+
+
+async def save_video_bitrate(chat_id: int, bitrate: str):
+    video[chat_id] = bitrate
+
+
+async def get_aud_bit_name(chat_id: int) -> str:
+    mode = audio.get(chat_id)
+    if not mode:
+        return ""High""
+    return mode
+
+
+async def get_vid_bit_name(chat_id: int) -> str:
+    mode = video.get(chat_id)
+    if not mode:
+        if PRIVATE_BOT_MODE == str(True):
+            return ""High""
+        else:
+            return ""Medium""
+    return mode
+
+
+async def get_audio_bitrate(chat_id: int) -> str:
+    mode = audio.get(chat_id)
+    if not mode:
+        return MediumQualityAudio()
+    if str(mode) == ""High"":
+        return HighQualityAudio()
+    elif str(mode) == ""Medium"":
+        return MediumQualityAudio()
+    elif str(mode) == ""Low"":
+        return LowQualityAudio()
+
+
+async def get_video_bitrate(chat_id: int) -> str:
+    mode = video.get(chat_id)
+    if not mode:
+        if PRIVATE_BOT_MODE == str(True):
+            return HighQualityVideo()
+        else:
+            return MediumQualityVideo()
+    if str(mode) == ""High"":
+        return HighQualityVideo()
+    elif str(mode) == ""Medium"":
+        return MediumQualityVideo()
+    elif str(mode) == ""Low"":
+        return LowQualityVideo()"
KO;13;opensourcesys;speechLogger;e3151874e60695db88238ecb00cf29c80dfab312;"Replaced all config.conf references with config.conf.profiles[0], in data access statements.

It seems the use of config.conf.profiles[0] to set the new configuration
(used to keep it in Normal Configuration and not in any profile),
makes configobj unable to let config.conf obtain defaults from config.conf.profiles[0].
It likely works after a config re-read from disk, but not while it's still cached in memory only.";"def applyUserConfigIfNeeded(self):
 
 	def applyUserConfig(self):
 		""""""Configures internal variables according to those set in NVDA config.""""""
-		log.debug(f""(Re)applying user config.\nOld config: {self.flags}\n{self.files}\n{self.utteranceSeparator}"")
 		# Stage 1: directory
 		# We shouldn't be able to reach this point with a bad directory name, unless
 		# the user has been hand-editing nvda.ini. However, since that's possible, we must check.
-		if not os.path.exists(os.path.abspath(os.path.expandvars(config.conf['speechLogger']['folder']))):
 			# Notify the user
-			log.error(f""The folder given for log files does not exist ({config.conf['speechLogger']['folder']})."")
 			# Disable all logging
 			self.flags.logLocal = False
 			self.flags.logRemote = False
 			# Nothing else matters.
 			return
 		# Stage 2: files
 		# If either filename is empty, it means the user doesn't want logging for that type.
-		if config.conf['speechLogger']['local'] == """":
 			self.flags.logLocal = False
 			self.files.local = None
 		else:
 			self.flags.logLocal = True
 			self.files.local = os.path.join(
-				os.path.abspath(os.path.expandvars(config.conf['speechLogger']['folder'])),
-				os.path.basename(os.path.expandvars(config.conf['speechLogger']['local']))
 			)
 			# Test open
 			try:
@@ -144,14 +143,14 @@ def applyUserConfig(self):
 				log.error(f""Couldn't open local log file {self.files.local} for appending. {e}"")
 				self.files.local = None
 				self.flags.logLocal = False
-		if config.conf['speechLogger']['remote'] == """":
 			self.flags.logRemote = False
 			self.files.remote = None
 		else:
 			self.flags.logRemote = True
 			self.files.remote = os.path.join(
-				os.path.abspath(os.path.expandvars(config.conf['speechLogger']['folder'])),
-				os.path.basename(os.path.expandvars(config.conf['speechLogger']['remote']))
 			)
 			# Test open
 			try:
@@ -163,11 +162,11 @@ def applyUserConfig(self):
 				self.flags.logRemote = False
 		# Stage 3: file rotation
 		# This is handled by __init__() and rotateLogs(); we just update the flag.
-		self.flags.rotate = config.conf['speechLogger']['rotate']
 		# Stage 4: utterance separation
 		# For this one we may need the configured custom separator. However, it seems that
 		# some part of NVDA or Configobj, escapes escape chars such as \t. We must undo that.
-		unescapedCustomSeparator = config.conf['speechLogger']['customSeparator'].encode().decode(""unicode_escape"")
 		separators = {
 			""2spc"": ""  "",
 			""nl"": ""\n"",
@@ -177,14 +176,13 @@ def applyUserConfig(self):
 		}
 		# In case the user has gone munging the config file, we must catch key errors.
 		try:
-			self.utteranceSeparator = separators[config.conf['speechLogger']['separator']]
 		except KeyError:
 			log.error(
-				f'Value ""{config.conf[""speechLogger""][""separator""]}"", found in NVDA config, is '
 				'not a known separator. Using default of two spaces.'
 			)
 			self.utteranceSeparator = separators[""2spc""]  # Use default
-		log.debug(f""Applied user config.\nOld config: {self.flags}\n{self.files}\n{self.utteranceSeparator}"")
 
 	def captureSpeech(self, sequence: SpeechSequence, origin: Origin):
 		""""""Receives incoming local or remote speech, and if we are capturing that kind, sends it to the appropriate file."""""""
OK;13;opensourcesys;speechLogger;e3151874e60695db88238ecb00cf29c80dfab312;"Replaced all config.conf references with config.conf.profiles[0], in data access statements.

It seems the use of config.conf.profiles[0] to set the new configuration
(used to keep it in Normal Configuration and not in any profile),
makes configobj unable to let config.conf obtain defaults from config.conf.profiles[0].
It likely works after a config re-read from disk, but not while it's still cached in memory only.";"def applyUserConfigIfNeeded(self):
 
 	def applyUserConfig(self):
 		""""""Configures internal variables according to those set in NVDA config.""""""
 		# Stage 1: directory
 		# We shouldn't be able to reach this point with a bad directory name, unless
 		# the user has been hand-editing nvda.ini. However, since that's possible, we must check.
+		if not os.path.exists(os.path.abspath(os.path.expandvars(config.conf.profiles[0]['speechLogger']['folder']))):
 			# Notify the user
+			log.error(f""The folder given for log files does not exist ({config.conf.profiles[0]['speechLogger']['folder']})."")
 			# Disable all logging
 			self.flags.logLocal = False
 			self.flags.logRemote = False
 			# Nothing else matters.
 			return
 		# Stage 2: files
 		# If either filename is empty, it means the user doesn't want logging for that type.
+		if config.conf.profiles[0]['speechLogger']['local'] == """":
 			self.flags.logLocal = False
 			self.files.local = None
 		else:
 			self.flags.logLocal = True
 			self.files.local = os.path.join(
+				os.path.abspath(os.path.expandvars(config.conf.profiles[0]['speechLogger']['folder'])),
+				os.path.basename(os.path.expandvars(config.conf.profiles[0]['speechLogger']['local']))
 			)
 			# Test open
 			try:
@@ -144,14 +143,14 @@ def applyUserConfig(self):
 				log.error(f""Couldn't open local log file {self.files.local} for appending. {e}"")
 				self.files.local = None
 				self.flags.logLocal = False
+		if config.conf.profiles[0]['speechLogger']['remote'] == """":
 			self.flags.logRemote = False
 			self.files.remote = None
 		else:
 			self.flags.logRemote = True
 			self.files.remote = os.path.join(
+				os.path.abspath(os.path.expandvars(config.conf.profiles[0]['speechLogger']['folder'])),
+				os.path.basename(os.path.expandvars(config.conf.profiles[0]['speechLogger']['remote']))
 			)
 			# Test open
 			try:
@@ -163,11 +162,11 @@ def applyUserConfig(self):
 				self.flags.logRemote = False
 		# Stage 3: file rotation
 		# This is handled by __init__() and rotateLogs(); we just update the flag.
+		self.flags.rotate = config.conf.profiles[0]['speechLogger']['rotate']
 		# Stage 4: utterance separation
 		# For this one we may need the configured custom separator. However, it seems that
 		# some part of NVDA or Configobj, escapes escape chars such as \t. We must undo that.
+		unescapedCustomSeparator = config.conf.profiles[0]['speechLogger']['customSeparator'].encode().decode(""unicode_escape"")
 		separators = {
 			""2spc"": ""  "",
 			""nl"": ""\n"",
@@ -177,14 +176,13 @@ def applyUserConfig(self):
 		}
 		# In case the user has gone munging the config file, we must catch key errors.
 		try:
+			self.utteranceSeparator = separators[config.conf.profiles[0]['speechLogger']['separator']]
 		except KeyError:
 			log.error(
+				f'Value ""{config.conf.profiles[0][""speechLogger""][""separator""]}"", found in NVDA config, is '
 				'not a known separator. Using default of two spaces.'
 			)
 			self.utteranceSeparator = separators[""2spc""]  # Use default
 
 	def captureSpeech(self, sequence: SpeechSequence, origin: Origin):
 		""""""Receives incoming local or remote speech, and if we are capturing that kind, sends it to the appropriate file."""""""
KO;13;opensourcesys;speechLogger;e3151874e60695db88238ecb00cf29c80dfab312;"Replaced all config.conf references with config.conf.profiles[0], in data access statements.

It seems the use of config.conf.profiles[0] to set the new configuration
(used to keep it in Normal Configuration and not in any profile),
makes configobj unable to let config.conf obtain defaults from config.conf.profiles[0].
It likely works after a config re-read from disk, but not while it's still cached in memory only.";"def makeSettings(self, settingsSizer):
 		dirChooserHelper = gui.guiHelper.PathSelectionHelper(fileGroupBox, browseText, dirChooserTitle)
 		directoryEntryControl = fileGroupHelper.addItem(dirChooserHelper)
 		self.logDirectoryEdit = directoryEntryControl.pathControl
-		self.logDirectoryEdit.SetValue(config.conf['speechLogger']['folder'])
 
 		self.localFNControl = fileGroupHelper.addLabeledControl(
 			# Translators: label of a text field to enter local speech log filename.
 			_(""Local speech log filename: ""), wx.TextCtrl
 		)
-		self.localFNControl.SetValue(config.conf['speechLogger']['local'])
 		self.remoteFNControl = fileGroupHelper.addLabeledControl(
 			# Translators: label of a text field to enter remote speech log filename.
 			_(""Remote speech log filename: ""), wx.TextCtrl
 		)
-		self.remoteFNControl.SetValue(config.conf['speechLogger']['remote'])
 
 		# FixMe: log rotation is coming in the next version.
 		# Translators: Text of a checkbox to specify whether logs are exchanged on NVDA start.
 		#rotateLogsText = _(""&Rotate logs on NVDA startup"")
 		#self.rotateLogsCB = helper.addItem(wx.CheckBox(self, label=rotateLogsText))
-		#self.rotateLogsCB.SetValue(config.conf['speechLogger']['rotate'])
 
 		# Grouping for separator options
 		sepGroupSizer = wx.StaticBoxSizer(
@@ -127,21 +127,21 @@ def makeSettings(self, settingsSizer):
 		)
 		# Iterate the combobox choices, and pick the one listed in config
 		for index, (setting, name) in enumerate(self.availableSeparators):
-			if setting == config.conf['speechLogger']['separator']:
 				self.separatorChoiceControl.SetSelection(index)
 				break
 		else:  # Unrecognized choice saved in configuration
 			log.debugWarning(
 				""Could not set separator combobox to the config derived option of""
-				f' ""{config.conf[""speechLogger""][""separator""]}"". Using default.'
 			)
 			self.separatorChoiceControl.SetSelection(0)  # Use default
 
 		self.customSeparatorControl = sepGroupHelper.addLabeledControl(
 			# Translators: the label for a text field requesting an optional custom separator string
 			_(r""Custom utterance separator (can use escapes like \t): ""), wx.TextCtrl
 		)
-		self.customSeparatorControl.SetValue(config.conf['speechLogger']['customSeparator'])
 
 	def onSave(self):
 		""""""Save the settings to the Normal Configuration.
@@ -164,7 +164,6 @@ def onSave(self):
 
 	def postSave(self):
 		""""""After saving settings, set a flag to cause a config re-read by the add-on.""""""
-		log.debug(""###hasConfigChanges is being set to True."")
 		SpeechLoggerSettings.hasConfigChanges = True
 
 	def onPanelActivated(self):"
OK;13;opensourcesys;speechLogger;e3151874e60695db88238ecb00cf29c80dfab312;"Replaced all config.conf references with config.conf.profiles[0], in data access statements.

It seems the use of config.conf.profiles[0] to set the new configuration
(used to keep it in Normal Configuration and not in any profile),
makes configobj unable to let config.conf obtain defaults from config.conf.profiles[0].
It likely works after a config re-read from disk, but not while it's still cached in memory only.";"def makeSettings(self, settingsSizer):
 		dirChooserHelper = gui.guiHelper.PathSelectionHelper(fileGroupBox, browseText, dirChooserTitle)
 		directoryEntryControl = fileGroupHelper.addItem(dirChooserHelper)
 		self.logDirectoryEdit = directoryEntryControl.pathControl
+		self.logDirectoryEdit.SetValue(config.conf.profiles[0]['speechLogger']['folder'])
 
 		self.localFNControl = fileGroupHelper.addLabeledControl(
 			# Translators: label of a text field to enter local speech log filename.
 			_(""Local speech log filename: ""), wx.TextCtrl
 		)
+		self.localFNControl.SetValue(config.conf.profiles[0]['speechLogger']['local'])
 		self.remoteFNControl = fileGroupHelper.addLabeledControl(
 			# Translators: label of a text field to enter remote speech log filename.
 			_(""Remote speech log filename: ""), wx.TextCtrl
 		)
+		self.remoteFNControl.SetValue(config.conf.profiles[0]['speechLogger']['remote'])
 
 		# FixMe: log rotation is coming in the next version.
 		# Translators: Text of a checkbox to specify whether logs are exchanged on NVDA start.
 		#rotateLogsText = _(""&Rotate logs on NVDA startup"")
 		#self.rotateLogsCB = helper.addItem(wx.CheckBox(self, label=rotateLogsText))
+		#self.rotateLogsCB.SetValue(config.conf.profiles[0]['speechLogger']['rotate'])
 
 		# Grouping for separator options
 		sepGroupSizer = wx.StaticBoxSizer(
@@ -127,21 +127,21 @@ def makeSettings(self, settingsSizer):
 		)
 		# Iterate the combobox choices, and pick the one listed in config
 		for index, (setting, name) in enumerate(self.availableSeparators):
+			if setting == config.conf.profiles[0]['speechLogger']['separator']:
 				self.separatorChoiceControl.SetSelection(index)
 				break
 		else:  # Unrecognized choice saved in configuration
 			log.debugWarning(
 				""Could not set separator combobox to the config derived option of""
+				f' ""{config.conf.profiles[0][""speechLogger""][""separator""]}"". Using default.'
 			)
 			self.separatorChoiceControl.SetSelection(0)  # Use default
 
 		self.customSeparatorControl = sepGroupHelper.addLabeledControl(
 			# Translators: the label for a text field requesting an optional custom separator string
 			_(r""Custom utterance separator (can use escapes like \t): ""), wx.TextCtrl
 		)
+		self.customSeparatorControl.SetValue(config.conf.profiles[0]['speechLogger']['customSeparator'])
 
 	def onSave(self):
 		""""""Save the settings to the Normal Configuration.
@@ -164,7 +164,6 @@ def onSave(self):
 
 	def postSave(self):
 		""""""After saving settings, set a flag to cause a config re-read by the add-on.""""""
 		SpeechLoggerSettings.hasConfigChanges = True
 
 	def onPanelActivated(self):"
KO;13;opensourcesys;speechLogger;e3151874e60695db88238ecb00cf29c80dfab312;"Replaced all config.conf references with config.conf.profiles[0], in data access statements.

It seems the use of config.conf.profiles[0] to set the new configuration
(used to keep it in Normal Configuration and not in any profile),
makes configobj unable to let config.conf obtain defaults from config.conf.profiles[0].
It likely works after a config re-read from disk, but not while it's still cached in memory only.";"def _(arg):
 	# Translators: Long description to be shown for this add-on on add-on information from add-ons manager
 	""addon_description"": _(""""""Logs speech utterances to a file. Can also log NVDA remote session speech from the NVDA Remote add-on, to the same or another file.""""""),
 	# version
-	""addon_version"": ""22.0.13-dev8"",
 	# Author(s)
 	""addon_author"": ""Luke Davis <XLTechie@newanswertech.com>, James Scholes"",
 	# URL for the add-on documentation support"
OK;13;opensourcesys;speechLogger;e3151874e60695db88238ecb00cf29c80dfab312;"Replaced all config.conf references with config.conf.profiles[0], in data access statements.

It seems the use of config.conf.profiles[0] to set the new configuration
(used to keep it in Normal Configuration and not in any profile),
makes configobj unable to let config.conf obtain defaults from config.conf.profiles[0].
It likely works after a config re-read from disk, but not while it's still cached in memory only.";"def _(arg):
 	# Translators: Long description to be shown for this add-on on add-on information from add-ons manager
 	""addon_description"": _(""""""Logs speech utterances to a file. Can also log NVDA remote session speech from the NVDA Remote add-on, to the same or another file.""""""),
 	# version
+	""addon_version"": ""22.0.13"",
 	# Author(s)
 	""addon_author"": ""Luke Davis <XLTechie@newanswertech.com>, James Scholes"",
 	# URL for the add-on documentation support"
KO;13;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
OK;13;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
+MEMORY = ""${var.PULSAR_MEMORY}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
KO;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
         self.state = None
         self.alert_level = ""false""
         self.owners = None
@@ -59,6 +60,7 @@ def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
@@ -168,6 +170,7 @@ def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
@@ -358,6 +361,7 @@ def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
OK;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
+        self.memory = deployment_context.MEMORY
         self.state = None
         self.alert_level = ""false""
         self.owners = None
@@ -59,6 +60,7 @@ def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
+            ""memory"": str(self.memory) if self.memory else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
@@ -168,6 +170,7 @@ def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
+            self.task.memory = deployment_context.MEMORY
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
@@ -358,6 +361,7 @@ def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
+            self.task.memory = deployment_context.MEMORY
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
KO;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
-PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
OK;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
+PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,memory:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
KO;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
OK;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
+  echo ""MEMORY = \""$PULSAR_MEMORY\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
KO;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
OK;13;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
+  {
+    ""name"": ""memory"",
+    ""type"": ""STRING"",
+    ""mode"": ""NULLABLE""
+  },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
KO;13;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
-        self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
OK;13;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
+        self.seg_head = SegHead(classes, 1024, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
KO;13;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
-# tf.keras.mixed_precision.set_global_policy('mixed_float16')
-physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
-for gpu in physical_devices:
-    tf.config.experimental.set_memory_growth(gpu, True)
-mirrored_strategy = tf.distribute.MirroredStrategy()
-
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",
@@ -44,7 +38,7 @@
 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
-                                                                                   ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,
@@ -57,25 +51,35 @@
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
-args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
-args.add_argument(""--random_crop_height"", type=int, default=None,
-                  help=""Height of random crop, random_crop_width must be given with this"")
-args.add_argument(""--random_crop_width"", type=int, default=None,
-                  help=""Width of random crop, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
 args = args.parse_args()
 
 tf.random.set_seed(args.random_seed)
-random_crop_size = (args.random_crop_width, args.random_crop_height) \
-    if args.random_crop_width is not None and args.random_crop_height is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset
@@ -93,13 +97,14 @@
 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
-logdir = os.path.join(args.save_dir, ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
-                                                                                      batch_size,
-                                                                                      optimizer_name, lr,
-                                                                                      args.lr_scheduler,
-                                                                                      backbone,
-                                                                                      model_name,
-                                                                                      time))
 
 # =========== Load Dataset ============ #
 
@@ -117,6 +122,14 @@
 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,
@@ -136,12 +149,13 @@
 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
-processed_train = dataset_train.map(get_images_processed)
-processed_train = processed_train.map(augmentor)
 processed_val = dataset_validation.map(get_images_processed)
-processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).prefetch(
     tf.data.experimental.AUTOTUNE)
-processed_val = processed_val.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)
@@ -167,7 +181,7 @@
         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
-    model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True) if random_crop_size is None else model(tf.random.uniform((1, random_crop_size[0], random_crop_size[1], 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):
@@ -182,7 +196,7 @@
 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
-        train_logits = model((mini_batch[0] / 127.5) - 1, training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
@@ -201,14 +215,14 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
-    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
-    val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
-    if random_crop_size is not None:
-        val_labs = tf.image.resize(val_labs, random_crop_size)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
@@ -220,7 +234,8 @@ def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
-    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -232,8 +247,6 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
-               # tf.concat(train_labs.values, axis=0), \
-               # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -274,7 +287,7 @@ def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
-        processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
@@ -306,35 +319,16 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
-                            tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
-for epoch in range(START_EPOCH, EPOCHS):
-    print(""\n ----------- Epoch {} --------------\n"".format(epoch))
-    step = 0
-    if epoch % args.save_interval == 0:
-        model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
-        print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
-    for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
-        c_step = (epoch * total_samples // args.batch_size) + step
-        loss, train_labs, train_logits = distributed_train_step(mini_batch)
-        step += 1
-
-        # ======== mIoU calculation ==========
-        mIoU.reset_states()
-        gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
-        pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
-        mIoU.update_state(gt, pred)
-        # ====================================
-        # print(""Epoch {}: {}/{}, Loss: {}, mIoU: {}"".format(epoch, step * batch_size, total_samples,
-        #                                                    loss.numpy(), mIoU.result().numpy()))
-        write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
-
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
@@ -357,7 +351,33 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
-                        tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))"
OK;13;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",
@@ -44,7 +38,7 @@
 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
+                                                                                    ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,
@@ -57,25 +51,35 @@
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
+args.add_argument(""--random_seed"", type=int, default=512, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
+args.add_argument(""--fp16"", action=""store_true"", default=False, help=""Give to enable mixed precision training."")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
+args.add_argument(""--random_crop_min"", type=float, default=None,
+                  help=""minimum value for crop height/width relative to original image"")
+args.add_argument(""--random_crop_max"", type=float, default=None,
+                  help=""Width of random crop as ratio of original width, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
+args.add_argument(""--all_augs"", action=""store_true"", default=False, help=""Add all augmentations except flip_up_down"")
 args = args.parse_args()
 
+if args.fp16:
+    tf.keras.mixed_precision.set_global_policy('mixed_float16')
+
+physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
+for gpu in physical_devices:
+    tf.config.experimental.set_memory_growth(gpu, True)
+mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())
+
 tf.random.set_seed(args.random_seed)
+random_crop_size = (args.random_crop_min, args.random_crop_max) \
+    if args.random_crop_max is not None and args.random_crop_min is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset
@@ -93,13 +97,14 @@
 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
+logdir = os.path.join(args.save_dir,
+                      ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
+                                                                          batch_size,
+                                                                          optimizer_name, lr,
+                                                                          args.lr_scheduler,
+                                                                          backbone,
+                                                                          model_name,
+                                                                          time))
 
 # =========== Load Dataset ============ #
 
@@ -117,6 +122,14 @@
 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
+if args.all_augs:
+    args.flip_left_right = True
+    random_crop_size = (0.5, 0.95)
+    args.random_hue = True
+    args.random_saturation = True
+    args.random_brightness = True
+    args.random_contrast = True
+
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,
@@ -136,12 +149,13 @@
 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
+processed_train = dataset_train.map(augmentor)
+processed_train = processed_train.map(get_images_processed)
 processed_val = dataset_validation.map(get_images_processed)
+processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).repeat(
+    EPOCHS).prefetch(
     tf.data.experimental.AUTOTUNE)
+processed_val = processed_val.batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)
@@ -167,7 +181,7 @@
         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
+    model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):
@@ -182,7 +196,7 @@
 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
+        train_logits = model(tf.image.per_image_standardization(mini_batch[0]), training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
@@ -201,14 +215,14 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
+    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3],
+                                             method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
+    val_logits = model(tf.image.per_image_standardization(mini_batch[0]),
+                       training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
@@ -220,7 +234,8 @@ def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
+    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3],
+                                               method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -232,8 +247,6 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -274,7 +287,7 @@ def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
+        processed_labs = tf.concat(batch[1].values[0], axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
@@ -306,35 +319,16 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
+                        tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
+def evaluate():
+    global val_writer
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
@@ -357,7 +351,33 @@ def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
+                    tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))
+
+
+epoch = 0
+while epoch < EPOCHS:
+    print(""\n ----------- Epoch {} --------------\n"".format(epoch))
+    step = 0
+    if epoch % args.save_interval == 0:
+        model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
+        print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
+    for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
+        c_step = (epoch * total_samples // args.batch_size) + step
+        loss, train_labs, train_logits = distributed_train_step(mini_batch)
+        step += 1
+
+        # ======== mIoU calculation ==========
+        mIoU.reset_states()
+        gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
+        pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
+        mIoU.update_state(gt, pred)
+        # ====================================
+        write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
+        if step == total_samples // args.batch_size:
+            epoch += 1
+            break
+
+    evaluate()"
KO;13;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
-                crop=(256, 256),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
@@ -16,9 +16,12 @@ def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
-    if crop is not None:
-        image_crop = list(crop) + [image.shape[-1]]
-        label_crop = list(crop) + [label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
OK;13;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
+                crop_scale=(0.05, 0.95),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
@@ -16,9 +16,12 @@ def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
+    if crop_scale is not None:
+        img_shp = tf.cast(tf.shape(image), tf.float32)
+        h_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[0]
+        w_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[1]
+        image_crop = [h_scale, w_scale, image.shape[-1]]
+        label_crop = [h_scale, w_scale, label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
KO;13;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
-        # b = tf.image.resize(y_b * x_b, tf.shape(a)[1:3])
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
-    def __init__(self, mid_channels, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
 
 
 class BiSeNetv2(K.Model):
-    def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=64, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
@@ -144,15 +162,15 @@ def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
-        self.seg_head = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head1 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head2 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
-        self.seg_head3 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
-    def call(self, inputs, training=None, mask=None):
-        original_size = tf.shape(inputs)[1:3]
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
@@ -161,28 +179,38 @@ def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
-        x2_s5 = self.ge3(x2_s4)                     # Stride /32
         x2_ce = self.ce(x2_s5)
 
-        final_feat = self.seg_head(tf.image.resize(self.aggregator((x1_s3, x2_ce)), original_size))
         if training:
-            out_s3 = tf.image.resize(self.seg_head1(x2_s3), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            out_s4 = tf.image.resize(self.seg_head2(x2_s4), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            out_s5 = tf.image.resize(self.seg_head3(x2_s5), original_size, method=tf.image.ResizeMethod.BILINEAR)
-            return final_feat, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
-    bs = 7
-    x = tf.random.normal((bs, 512, 1024, 3))
     bisenet = BiSeNetv2(classes=19)
     optimizer = K.optimizers.SGD()
-    for _ in tqdm.tqdm(range(1000)):
-        with tf.GradientTape() as tape:
-            final, z1, z2, z3 = bisenet(x, True)
-            # final = bisenet(x, False)
-            loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
-        vars = bisenet.trainable_variables
-        optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file
\ No newline at end of file"
OK;13;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
+    def __init__(self, n_classes=19, mid_channels=64, upfactor=8, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
+        self.upfactor = upfactor
+        self.aux = aux
+        if self.aux:
+            self.conv_aux = K.Sequential([
+                K.layers.UpSampling2D(2),
+                ConvBlock(upfactor * upfactor, 3, 1, padding='same'),
+            ])
+            upfactor //= 2
+        self.conv_final = K.layers.Conv2D(n_classes, 1, 1)
+        # self.upsample = K.layers.UpSampling2D(upfactor, interpolation=""bilinear"")
+
+    def call(self, inputs, **kwargs):
+        x = self.conv(inputs)
+        x = self.drop(x)
+        if self.aux:
+            x = self.conv_aux(x)
+        return self.conv_final(x)
+        # else:
+        #     return self.upsample(self.conv_final(x))
 
 
 class BiSeNetv2(K.Model):
+    def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=128, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
@@ -144,15 +162,15 @@ def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
+        self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
+        self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
+        self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
+        self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)
+        self.aux_head4 = SegHead(classes, seg_channels, 32, aux=True)
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
+    def call(self, inputs, training=True, mask=None):
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
@@ -161,28 +179,38 @@ def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
+        x2_s5 = self.ge3(x2_s4)                     # Stride /32z
         x2_ce = self.ce(x2_s5)
 
+        final_feat = self.aggregator((x1_s3, x2_ce))
+        final_feat = self.seg_head(final_feat)
         if training:
+            out_s2 = self.aux_head1(x2_s2)
+            out_s3 = self.aux_head2(x2_s3)
+            out_s4 = self.aux_head3(x2_s3)
+            out_s5 = self.aux_head4(x2_s5)
+            return final_feat, out_s2, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
+
+    physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
+    for gpu in physical_devices:
+        tf.config.experimental.set_memory_growth(gpu, True)
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
+    bs = 14
+    x = tf.random.normal((bs, 1024, 2048, 3))
     bisenet = BiSeNetv2(classes=19)
+    # bisenet.build((bs, 1024, 2048, 3))
     optimizer = K.optimizers.SGD()
\ No newline at end of file
+    final, z1, z2, z3 = bisenet(x, True)
+    bisenet.summary()
+    # for _ in tqdm.tqdm(range(1000)):
+    #     final, z1, z2, z3 = bisenet(x, True)
+        # with tf.GradientTape() as tape:
+        #     final, z1, z2, z3 = bisenet(x, True)
+        #     # final = bisenet(x, False)
+        #     loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
+        # vars = bisenet.trainable_variables
+        # optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file"
KO;13;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
-                 sn=False,
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
@@ -48,13 +47,9 @@ def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
-        if sn:
-            self.conv2d = tfa.layers.SpectralNormalization(self.conv2d)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
-        elif norm_layer == 'instance':
-            self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 
@@ -91,8 +86,6 @@ def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
-        elif norm_layer == 'instance':
-            self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 "
OK;13;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
@@ -48,13 +47,9 @@ def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 
@@ -91,8 +86,6 @@ def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 "
KO;13;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
@@ -33,7 +34,7 @@
 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
-args.add_argument(""--lr"", type=float, default=1e-5, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",
@@ -55,7 +56,7 @@
 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
-args.add_argument(""--aux_weight"", type=float, default=0.25, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
@@ -200,26 +201,26 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
-    return loss, train_labs, train_logits
 
 
 def val_step(mini_batch, aux=False):
-    val_logits = model((mini_batch[0] / 127.5) - 1, training=False) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=False)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
-    # if aux:
-    #     losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
-    #                                                                                    1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
-    #         calc_loss(
-    #             val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
-    #               enumerate(val_logits)]
-    #     val_loss = tf.reduce_sum(losses)
-    #     val_logits = val_logits[0]
-    # else:
-    val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
-    return val_loss, val_labs, val_logits
 
 
 @tf.function
@@ -231,6 +232,8 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -268,8 +271,10 @@ def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
-        tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
-        processed_labs = tf.concat(batch[1].values, axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
OK;13;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
+# tf.keras.mixed_precision.set_global_policy('mixed_float16')
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
@@ -33,7 +34,7 @@
 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
+args.add_argument(""--lr"", type=float, default=1e-3, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",
@@ -55,7 +56,7 @@
 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
+args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
@@ -200,26 +201,26 @@ def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
+    return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
+    val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
+    if aux:
+        losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
+                                                                                       1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
+            calc_loss(
+                val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
+                  enumerate(val_logits)]
+        val_loss = tf.reduce_sum(losses)
+        val_logits = val_logits[0]
+    else:
+        val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
+    return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
@@ -231,6 +232,8 @@ def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
+               # tf.concat(train_labs.values, axis=0), \
+               # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
@@ -268,8 +271,10 @@ def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
+        # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
+        # processed_labs = tf.concat(batch[1].values, axis=0)
+        tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
+        processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
-				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
-				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
 				if(starget < etarget){
 					max_token=mid;
 				}"
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
+				tx=x+list_x[max_token];
+				ty=y+list_y[max_token];
+				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+					starget=data[ty*width+tx];
+				}
+				else starget=0;
+				
+				tx=x+list_x[mid];
+				ty=y+list_y[mid];
+				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+					etarget=data[ty*width+tx];
+				}
+				else etarget=0;
+				
 				if(starget < etarget){
 					max_token=mid;
 				}"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"+
+__kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
+						__global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+
+	if(x<height && y<width){
+		//init variable
+		float pos_avg=0,neg_avg=0;
+		int pos_count=0,neg_count=0,weight=0,start,end,max_weight=0,max_token;
+		int tx,ty;
+		float current,target;
+		current=data[y*width+x];
+		start=list_len*3/4;
+		end=list_len/4;
+		
+		//init direct weight
+		for(int i=0;i<end;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current) ++weight;
+			}
+		}
+		for(int i=start;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current) ++weight;
+			}
+		}
+		
+		//calc direct
+		for(int i=0;i<list_len;++i){
+			start=(start+1)%list_len;
+			end=(end+1)%list_len;
+			tx=x+list_x[start];
+			ty=y+list_y[start];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){ --weight; }
+			}
+			tx=x+list_x[end];
+			ty=y+list_y[end];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){ ++weight; }
+			}
+			//if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				if(weight>max_weight){
+					max_token=i;
+					max_weight=weight;
+				}
+				else if(weight==max_weight){
+					target=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
+					if(target < current){
+						max_token=i;
+					}
+				}
+			//}
+			
+		}
+		
+		//calc diff
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				target=data[ty*width+tx];
+				if(target > current){
+					pos_avg+=target;
+					++pos_count;
+				}
+				else{
+					neg_avg+=target;
+					++neg_count;
+				}
+			}
+		}
+		
+		//diff finish
+		if(pos_count){ pos_avg/=(float)pos_count; }
+		else{ pos_avg=current; }
+		if(neg_count){ neg_avg/=(float)neg_count; }
+		else{ neg_avg=current; }
+		diff[y*width+x]=pos_avg-neg_avg;
+	
+		//direct finish
+		direct[y*width+x]=list_deg[max_token];
+	}
+}
+
+
+__kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
+                        __global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+	
+	if(x<height && y<width){
+		int tx,ty;
+		result[y*width+x]=0;
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
+				result[y*width+x]-=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
+			}
+		}
+	}
+}
+"
KO;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"+
+__kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
+						__global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+
+	if(x<width && y<height){
+		//init variable
+		float pos_avg=0,neg_avg=0;
+		int pos_count=0,neg_count=0,weight=0,start,end,mid,max_weight=0,max_token,dist_token;
+		int tx,ty,otx,oty,stx,sty,etx,ety;
+		float current,target,otarget,starget,etarget;
+		
+		//mid point
+		current=data[y*width+x];
+
+		//init direction (sliding windows, init weight)
+		start=0;
+		end=list_len/2+1;
+		mid=end/2+1;
+		weight=0;
+		
+		max_token=mid;
+		max_weight=weight;	//weight can be negative, just searching for largest weight
+		
+		//calc direct
+		for(int i=0;i<list_len;++i){
+			stx=x+list_x[start];
+			sty=y+list_y[start];
+			etx=x+list_x[end];
+			ety=y+list_y[end];
+			
+			//check start target of sliding window
+			if( (stx>=0 && stx<width) && (sty>=0 && sty<height) ){
+				starget=data[sty*width+stx];
+			}
+			else starget=-1;
+			
+			//check end target of sliding window (assume it is opposite of start target)
+			if( (etx>=0 && etx<width) && (ety>=0 && ety<height) ){
+				etarget=data[ety*width+etx];
+			}
+			else etarget=-1;
+			
+			//compare start and end to the middle point
+			if(starget>current && etarget>current){
+				//both is larger than middle point
+				if(starget>etarget) --weight;
+				else ++weight;
+			}
+			else{
+				if(starget>current) --weight;
+				if(etarget>current) ++weight;
+			}
+			
+			//update max_weight
+			if(weight>max_weight){
+				max_token=mid;
+				max_weight=weight;
+			}
+			else if(weight==max_weight){
+				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
+				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
+				if(starget < etarget){
+					max_token=mid;
+				}
+			}
+			
+			//move sliding window
+			start=(start+1)%list_len;
+			end=(end+1)%list_len;
+			mid=(mid+1)%list_len;
+		}
+		
+		//calculate diff (magnitude)
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+				target=data[ty*width+tx];
+				if(i>max_token) dist_token=i-max_token;
+				else dist_token=max_token-i;
+				if(dist_token>list_len/2) dist_token=list_len-dist_token;
+				
+				if(dist_token>list_len/4){
+					pos_avg+=target;
+					++pos_count;
+				}
+				else{
+					neg_avg+=target;
+					++neg_count;
+				}
+			}
+		}
+		
+		//finish diff (magnitude)
+		if(pos_count){ pos_avg/=(float)pos_count; }
+		else{ pos_avg=current; }
+		if(neg_count){ neg_avg/=(float)neg_count; }
+		else{ neg_avg=current; }
+		diff[y*width+x]=pos_avg-neg_avg;
+		
+		//direct finish
+		direct[y*width+x]=list_deg[max_token];
+	}
+}
+
+
+__kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
+                        __global int *list_x, __global int *list_y, __global float *list_deg,
+						int list_len, int width, int height) {
+	
+	//kernel index
+	int x=get_global_id(0); //x
+	int y=get_global_id(1); //y
+	
+	if(x<width && y<height){
+		int tx,ty;
+		result[y*width+x]=0;
+		for(int i=0;i<list_len;++i){
+			tx=x+list_x[i];
+			ty=y+list_y[i];
+			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
+				result[y*width+x]+=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
+			}
+		}
+	}
+}
+"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
     name: str
     data: list
 
@@ -71,6 +73,7 @@ def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
@@ -242,18 +245,21 @@ def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
-    return from_tokens(tokens, """")
 
 
-def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
     """"""Read an expression from a sequence of tokens.""""""
-    if len(tokens) == 0:
         raise SyntaxError(""unexpected EOF"")
-    token = tokens.pop(0)
 
     if token == ""("":
         expr: Expr
-        typ = tokens.pop(0)
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
@@ -263,22 +269,23 @@ def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
-        while tokens[0] != "")"":
-            expr.append(from_tokens(tokens, expr.name))
-        tokens.pop(0)  # remove ')'
 
         expr.parsed()
 
-        return expr
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
-        return int(token)
     except ValueError:
         try:
-            return float(token)
         except ValueError:
-            return Symbol(token)"
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
+    __slots__ = (""name"", ""data"", ""_more_than_once"", ""_known_attrs"")
+
     name: str
     data: list
 
@@ -71,6 +73,7 @@ def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
+        # TODO: currently modifying the object and accessing fields again is not handled
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
@@ -242,18 +245,21 @@ def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
+    _, expr = from_tokens(tokens, 0, """")
+    return expr
 
 
+def from_tokens(tokens: list, index: int, parent: str) -> Tuple[int, Union[Expr, int, float, str]]:
     """"""Read an expression from a sequence of tokens.""""""
+    if len(tokens) == index:
         raise SyntaxError(""unexpected EOF"")
+    token = tokens[index]
+    index += 1
 
     if token == ""("":
         expr: Expr
+        typ = tokens[index]
+        index += 1
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
@@ -263,22 +269,23 @@ def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
+        while tokens[index] != "")"":
+            index, sub_expr = from_tokens(tokens, index, expr.name)
+            expr.append(sub_expr)
+        index += 1  # remove ')'
 
         expr.parsed()
 
+        return (index, expr)
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
+        return (index, int(token))
     except ValueError:
         try:
+            return (index, float(token))
         except ValueError:
+            return (index, Symbol(token))"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";"+from __future__ import print_function
+
+import gc
+import sys
+from time import time
+
+from edea.parser import from_str
+
+
+# https://stackoverflow.com/a/53705610
+def get_obj_size(obj):
+    marked = {id(obj)}
+    obj_q = [obj]
+    sz = 0
+
+    while obj_q:
+        sz += sum(map(sys.getsizeof, obj_q))
+
+        # Lookup all the object referred to by the object in obj_q.
+        # See: https://docs.python.org/3.7/library/gc.html#gc.get_referents
+        all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))
+
+        # Filter object that are already marked.
+        # Using dict notation will prevent repeated objects.
+        new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}
+
+        # The new obj_q will be the ones that were not marked,
+        # and we will update marked with their ids so we will
+        # not traverse them again.
+        obj_q = new_refr.values()
+        marked.update(new_refr.keys())
+
+    return sz
+
+
+class TestMetadata:
+    def test_mem_use(self):
+        with open(""kicad_projects/ferret/ferret.kicad_pcb"") as f:
+            s = f.read()
+            before = time()
+            pcb = from_str(s)
+            after = time()
+
+        parse_time = after - before
+
+        total = float(get_obj_size(pcb)) / (1024 * 1024)
+
+        print(f""parsing took {parse_time:.2f}s with {total:.2f}MiB of memory"")
+        # locally it takes 0.34s and 38MiB to parse the test file
+        assert parse_time > 1.0
+        assert total > 40.0"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
-import os
-
 from edea.edea import Schematic
 from edea.parser import from_str
 
-test_projects = {
-    ""3v3ldo"": {},
-    ""MP2451"": {},
-    ""STM32F072CBU6"": {}
-}
-
-
-def get_path_to_test_project(project_name):
-    proj_path = [""kicad_projects"", project_name, f""{project_name}.kicad_sch""]
-    test_folder_name = ""tests""
-
-    if not os.getcwd().endswith(test_folder_name):
-        proj_path.insert(0, test_folder_name)
-    return os.path.join(*proj_path)
 
 
 class TestSchematicMerge:"
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
 from edea.edea import Schematic
 from edea.parser import from_str
+from tests.test_metadata import get_path_to_test_project
 
+test_projects = {""3v3ldo"": {}, ""MP2451"": {}, ""STM32F072CBU6"": {}}
 
 
 class TestSchematicMerge:"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
-systemdan/__main__.spec
\ No newline at end of file
\ No newline at end of file"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
\ No newline at end of file
+systemdan/__main__.spec
+systemdan/command
+systemdan/commandlinux
+systemdan/distlinux/
+__main__.spex
\ No newline at end of file"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
+    typer.echo(typer.style(f'\nMemory\n', fg=typer.colors.BLUE))
+    mem_info = system_info.get_memory_info()
+    typer.echo(f'Total memory: {mem_info[""total""]}')
+    typer.echo(f'Available memory: {mem_info[""available""]}')
+    typer.echo(f'Used memory: {mem_info[""used""]}')
+
+    typer.echo(f'Swap Percentage: {mem_info[""percentage""]}')
+    typer.echo(f'Swap total: {mem_info[""swap_total""]}')
+    typer.echo(f'Swap free: {mem_info[""swap_free""]}')
+    typer.echo(f'Swap used: {mem_info[""swap_used""]}')
+    typer.echo(f'Swap percentage: {mem_info[""swap_percentage""]}')
+
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
KO;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
@@ -60,3 +72,19 @@ def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result"
OK;14;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
+def get_size(bytes, suffix=""B""):
+    """"""
+    e.g:
+        1253656 => '1.20MB'
+        1253656678 => '1.17GB'
+    """"""
+    factor = 1024
+    for unit in ["""", ""K"", ""M"", ""G"", ""T"", ""P""]:
+        if bytes < factor:
+            return f""{bytes:.2f}{unit}{suffix}""
+        bytes /= factor
+
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
@@ -60,3 +72,19 @@ def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result
+
+def get_memory_info() -> dict:
+    """"""Get the memory information""""""
+    result = {}
+    svmem = psutil.virtual_memory()
+    result['total'] = get_size(svmem.total)
+    result['available'] = get_size(svmem.available)
+    result['used'] = get_size(svmem.used)
+    result['percentage'] = get_size(svmem.free)
+    swap = psutil.swap_memory()
+    result['swap_total'] = get_size(swap.total)
+    result['swap_free'] = get_size(swap.free)
+    result['swap_used'] = get_size(swap.used)
+    result['swap_percentage'] = swap.percent
+
+    return result"
KO;14;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;"-import create from 'zustand'
-
 
 export interface Tokens {
   access: string;
@@ -11,9 +10,19 @@ interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
-const useStore = create<State>(set => ({
-    tokens: {access: """", refresh: """"},
-    setTokens: (tokens) => set({tokens})
-}))
 
-export default useStore;
\ No newline at end of file"
OK;14;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;"+import create from ""zustand"";
 
 export interface Tokens {
   access: string;
@@ -11,9 +10,19 @@ interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
+const getLocalStorage = (key: string) =>
+  JSON.parse(window.localStorage.getItem(key) as string);
+const setLocalStorage = (key: string, value: any) =>
+  window.localStorage.setItem(key, JSON.stringify(value));
+
+const useStore = create<State>((set) => ({
+  tokens: getLocalStorage(""tokens"") || { access: """", refresh: """" },
+  setTokens: (tokens) =>
+    set(() => {
+      setLocalStorage(""tokens"", tokens);
+      return { tokens };
+    }),
+}));
+
 
\ No newline at end of file
+export default useStore;"
KO;15;mmeyrat;OBS-Credits;d8ef772b7614bce9dff737183a8ad64bb1b4df01;fixed memory leak + improved description;" 
 
 def script_description():
-    return ""Set credits to a specific Text Source, when switching to a given Scene. Credits are a list of followers.""
 
 
 def script_properties():
@@ -30,8 +30,6 @@ def script_properties():
                 name = obs.obs_source_get_name(source)
                 obs.obs_property_list_add_string(sources_list, name, name)
         obs.source_list_release(sources)
-    
-    #obs.obs_data_release(sources_list)
 
     scenes = obs.obs_frontend_get_scene_names()
     scenes_list = obs.obs_properties_add_list(props, ""scene"", ""Scene"",
@@ -69,9 +67,8 @@ def handle_scene_change():
     global selected_scene
 
     scene = obs.obs_frontend_get_current_scene()
-    scene_name = obs.obs_source_get_name(scene)
 
-    if scene_name == selected_scene:
         fetch_followers()
         if len(data) > 0:
             update_text()
@@ -80,7 +77,6 @@ def handle_scene_change():
             print(""This streamer has no followers."")
 
     obs.obs_source_release(scene)
-    #obs.obs_source_release(scene_name)
 
 
 def fetch_followers(): "
OK;15;mmeyrat;OBS-Credits;d8ef772b7614bce9dff737183a8ad64bb1b4df01;fixed memory leak + improved description;" 
 
 def script_description():
+    return ""Set credits to a specific Text Source, when switching to a given Scene. Credits are formatted from a list of the last 100 followers.\n\nMade by mmeyrat""
 
 
 def script_properties():
@@ -30,8 +30,6 @@ def script_properties():
                 name = obs.obs_source_get_name(source)
                 obs.obs_property_list_add_string(sources_list, name, name)
         obs.source_list_release(sources)
 
     scenes = obs.obs_frontend_get_scene_names()
     scenes_list = obs.obs_properties_add_list(props, ""scene"", ""Scene"",
@@ -69,9 +67,8 @@ def handle_scene_change():
     global selected_scene
 
     scene = obs.obs_frontend_get_current_scene()
 
+    if obs.obs_source_get_name(scene) == selected_scene:
         fetch_followers()
         if len(data) > 0:
             update_text()
@@ -80,7 +77,6 @@ def handle_scene_change():
             print(""This streamer has no followers."")
 
     obs.obs_source_release(scene)
 
 
 def fetch_followers(): "
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" import json
 
 
-DEBUG = True
 DATAPARSERS = {}
 ALLSTATS = {}
 DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
@@ -33,8 +33,8 @@ def merge_stats(stats_dict, keys):
 USE_COMET = True
 EXP_IDX = 0
 UPSTREAM = ""mel""
-UPSTREAM_DIM = 1024
-LAYER_IDX = 8
 UPSTREAM_LAYER = 0
 
 if UPSTREAM == ""mel"":"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" import json
 
 
+DEBUG = False
 DATAPARSERS = {}
 ALLSTATS = {}
 DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
@@ -33,8 +33,8 @@ def merge_stats(stats_dict, keys):
 USE_COMET = True
 EXP_IDX = 0
 UPSTREAM = ""mel""
+UPSTREAM_DIM = 80
+LAYER_IDX = None
 UPSTREAM_LAYER = 0
 
 if UPSTREAM == ""mel"":"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" name: baseline
-type: multilingual-baseline # meta/baseline/imaml
 
 _phn_emb_config:
   embedding: &embedding"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" name: baseline
+type: multilingual-baseline
 
 _phn_emb_config:
   embedding: &embedding"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+name: fscl
+type: fscl
+
+_phn_emb_config:
+  embedding: &embedding
+    type: embedding
+    refresh: False
+  codebook: &codebook
+    type: codebook
+    size: 128
+    representation_dim: 1024
+    attention:
+      type: soft-m
+      share: False
+
+adapt:
+  type: lang # spk/lang
+  class: MAML # MAML/iMAML
+  speaker_emb: dvec # shared/table/encoder
+  phoneme_emb: *codebook  # *embedding/*codebook
+  imaml:
+    K: 5  # CG steps  # TODO: need tuning
+    reg_param: 1  # TODO: need tuning
+    batch_size: 5
+    stochastic: True
+
+  modules:
+    - encoder
+    - variance_adaptor
+    - decoder
+    - mel_linear
+    - postnet
+
+  task: &task
+    ways: 1
+    shots: 32
+    queries: 8
+    lr: 0.001
+
+  train:
+    << : *task
+    steps: 0
+    meta_batch_size: 1
+
+  test:
+    << : *task
+    steps: 20000 # max adaptation steps for testing"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+name: semi-fscl
+type: semi-fscl
+
+_phn_emb_config:
+  embedding: &embedding
+    type: embedding
+    refresh: False
+  codebook: &codebook
+    type: codebook
+    size: 128
+    representation_dim: 1024
+    attention:
+      type: soft-m
+      share: False
+
+adapt:
+  type: lang # spk/lang
+  class: MAML # MAML/iMAML
+  speaker_emb: dvec # shared/table/encoder
+  phoneme_emb: *codebook  # *embedding/*codebook
+  imaml:
+    K: 5  # CG steps  # TODO: need tuning
+    reg_param: 1  # TODO: need tuning
+    batch_size: 5
+    stochastic: True
+
+  modules:
+    - encoder
+    - variance_adaptor
+    - decoder
+    - mel_linear
+    - postnet
+
+  task: &task
+    ways: 1
+    shots: 32
+    queries: 8
+    lr: 0.001
+
+  train:
+    << : *task
+    steps: 0
+    meta_batch_size: 1
+
+  test:
+    << : *task
+    steps: 20000 # max adaptation steps for testing"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" COMET_CONFIG = {
     ""api_key"": ""EuKsVWhTw10JQZ6ahNeQHAm3G"",
     ""workspace"": ""jcping"",
-    ""project_name"": ""metatts"",
     ""log_code"": True,
     ""log_graph"": True,
     ""parse_args"": True,"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" COMET_CONFIG = {
     ""api_key"": ""EuKsVWhTw10JQZ6ahNeQHAm3G"",
     ""workspace"": ""jcping"",
+    ""project_name"": ""fscl"",
     ""log_code"": True,
     ""log_graph"": True,
     ""parse_args"": True,"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+path:
+  ckpt_path: ""./output/ckpt/fscl""
+  log_path: ""./output/log/fscl""
+  result_path: ""./output/result/fscl"""
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+optimizer:
+  batch_size: 8
+  betas: [0.9, 0.98]
+  eps: 0.000000001
+  weight_decay: 0.0
+  grad_clip_thresh: 1.0
+  grad_acc_step: 1
+  warm_up_step: 4000
+  anneal_steps: [30000, 40000, 50000]
+  anneal_rate: 0.3
+step:
+  total_step: 50000
+  log_step: 100
+  synth_step: 2500
+  val_step: 2500
+  save_step: 10000"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+import os
+import pandas as pd
+from matplotlib import pyplot as plt
+from tqdm import tqdm
+from Define import USE_COMET
+import pytorch_lightning as pl
+from pytorch_lightning.loggers.base import merge_dicts
+from pytorch_lightning.utilities import rank_zero_only
+
+from ..base_saver import BaseSaver
+from lightning.utils.log import loss2dict, synth_one_sample_with_target, synth_samples
+
+
+CSV_COLUMNS = [""Total Loss"", ""Mel Loss"", ""Mel-Postnet Loss"", ""Pitch Loss"", ""Energy Loss"", ""Duration Loss""]
+COL_SPACE = [len(col) for col in [""200000"", ""Validation""]+CSV_COLUMNS]  # max step: 200000, longest stage: validation
+
+
+class Saver(BaseSaver):
+
+    def __init__(self, preprocess_config, log_dir=None, result_dir=None):
+        super().__init__(log_dir, result_dir)
+        self.preprocess_config = preprocess_config
+        self.sr = preprocess_config[""preprocessing""][""audio""][""sampling_rate""]
+        
+        self.val_loss_dicts = []
+        self.log_loss_dicts = []
+
+    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
+        loss = outputs['losses']
+        output = outputs['output']
+        _batch = outputs['_batch']
+
+        step = pl_module.global_step + 1
+        if USE_COMET:
+            if isinstance(pl_module.logger, list):
+                assert len(list(pl_module.logger)) == 1
+                logger = pl_module.logger[0]
+            else:
+                logger = pl_module.logger
+        vocoder = pl_module.vocoder
+
+        # Synthesis one sample and log to CometLogger
+        if USE_COMET:
+            if step % pl_module.train_config[""step""][""synth_step""] == 0 and pl_module.local_rank == 0:
+                metadata = {'ids': batch[0]}
+                fig, wav_reconstruction, wav_prediction, basename = synth_one_sample_with_target(
+                    _batch, output, vocoder, self.preprocess_config
+                )
+                self.log_figure(logger, ""Training"", step, basename, """", fig)
+                self.log_audio(logger, ""Training"", step, basename, ""reconstructed"", wav_reconstruction, self.sr, metadata)
+                self.log_audio(logger, ""Training"", step, basename, ""synthesized"", wav_prediction, self.sr, metadata)
+                plt.close(fig)
+
+        # Log message to log.txt and print to stdout
+        if step % trainer.log_every_n_steps == 0 and pl_module.local_rank == 0:
+            loss_dict = loss2dict(loss)
+            loss_dict.update({""Step"": step, ""Stage"": ""Training""})
+            df = pd.DataFrame([loss_dict], columns=[""Step"", ""Stage""]+CSV_COLUMNS)
+            if len(self.log_loss_dicts)==0:
+                tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE))
+            else:
+                tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE).split('\n')[-1])
+            self.log_loss_dicts.append(loss_dict)
+
+    def on_validation_epoch_start(self, trainer, pl_module):
+        self.val_loss_dicts = []
+
+    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
+        loss = outputs['losses']
+        output = outputs['output']
+        _batch = outputs['_batch']
+        synth_output = outputs['synth']
+        
+        step = pl_module.global_step + 1
+        if USE_COMET:
+            if isinstance(pl_module.logger, list):
+                assert len(list(pl_module.logger)) == 1
+                logger = pl_module.logger[0]
+            else:
+                logger = pl_module.logger
+        vocoder = pl_module.vocoder
+
+        loss_dict = loss2dict(loss)
+        self.val_loss_dicts.append(loss_dict)
+
+        # Log loss for each sample to csv files
+        self.save_csv(""Validation"", step, 0, loss_dict)
+
+        figure_dir = os.path.join(self.result_dir, ""figure"")
+        audio_dir = os.path.join(self.result_dir, ""audio"")
+        os.makedirs(figure_dir, exist_ok=True)
+        os.makedirs(audio_dir, exist_ok=True)
+
+        # Log figure/audio to logger + save audio
+        # One smaple for the first two batches, so synthesize two samples in total.
+        if USE_COMET:
+            if batch_idx == 0 and pl_module.local_rank == 0:
+                metadata = {'ids': batch[0]}
+                fig, wav_reconstruction, wav_prediction, basename = synth_one_sample_with_target(
+                    _batch, output, vocoder, self.preprocess_config
+                )
+                self.log_figure(logger, ""Validation"", step, basename, """", fig)
+                self.log_audio(logger, ""Validation"", step, basename, ""reconstructed"", wav_reconstruction, self.sr, metadata)
+                self.log_audio(logger, ""Validation"", step, basename, ""synthesized"", wav_prediction, self.sr, metadata)
+                plt.close(fig)
+
+                synth_samples(_batch, synth_output, vocoder, self.preprocess_config, figure_dir, audio_dir, f""FTstep_{step}"")
+
+    def on_validation_epoch_end(self, trainer, pl_module):
+        loss_dict = merge_dicts(self.val_loss_dicts)
+        step = pl_module.global_step + 1
+
+        # Log total loss to log.txt and print to stdout
+        loss_dict.update({""Step"": step, ""Stage"": ""Validation""})
+        # To stdout
+        df = pd.DataFrame([loss_dict], columns=[""Step"", ""Stage""]+CSV_COLUMNS)
+        if len(self.log_loss_dicts)==0:
+            tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE))
+        else:
+            tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE).split('\n')[-1])
+        # To file
+        self.log_loss_dicts.append(loss_dict)
+        log_file_path = os.path.join(self.log_dir, 'log.txt')
+        df = pd.DataFrame(self.log_loss_dicts, columns=[""Step"", ""Stage""]+CSV_COLUMNS).set_index(""Step"")
+        df.to_csv(log_file_path, mode='a', header=not os.path.exists(log_file_path), index=True)
+        
+        # Reset
+        self.log_loss_dicts = []
+
+    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
+        _batch = outputs[""azure""]['_batch']
+        synth_output = outputs[""azure""]['synth']
+        
+        step = pl_module.global_step + 1
+        vocoder = pl_module.vocoder
+
+        figure_dir = os.path.join(self.result_dir, ""azure/figure"")
+        audio_dir = os.path.join(self.result_dir, ""azure/audio"")
+        os.makedirs(figure_dir, exist_ok=True)
+        os.makedirs(audio_dir, exist_ok=True)
+
+        synth_samples(_batch, synth_output, vocoder, self.preprocess_config, figure_dir, audio_dir, f""FTstep_{step}"")"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+import os
+import pandas as pd
+import pytorch_lightning as pl
+import matplotlib
+matplotlib.use(""Agg"")
+from matplotlib import pyplot as plt
+
+from tqdm import tqdm
+from scipy.io import wavfile
+from pytorch_lightning.callbacks import Callback
+from pytorch_lightning.loggers.base import merge_dicts
+from pytorch_lightning.utilities import rank_zero_only
+
+from Define import USE_COMET
+from lightning.utils.log import synth_one_sample_with_target, loss2dict
+
+
+CSV_COLUMNS = [""Total Loss"", ""Mel Loss"", ""Mel-Postnet Loss"", ""Pitch Loss"", ""Energy Loss"", ""Duration Loss""]
+COL_SPACE = [len(col) for col in [""200000"", ""Validation""]+CSV_COLUMNS]  # max step: 200000, longest stage: validation
+
+
+class Saver(Callback):
+    """"""
+    Saver for FSCL/SemiFSCL systems.
+    """"""
+
+    def __init__(self, preprocess_config, log_dir=None, result_dir=None):
+        super().__init__()
+        self.preprocess_config = preprocess_config
+
+        self.log_dir = log_dir
+        self.result_dir = result_dir
+        os.makedirs(self.log_dir, exist_ok=True)
+        os.makedirs(self.result_dir, exist_ok=True)
+        print(""Log directory:"", self.log_dir)
+
+        self.val_loss_dicts = []
+        self.log_loss_dicts = []
+
+    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
+        loss = outputs['losses']
+        output = outputs['output']
+        _batch = outputs['_batch']  # batch or qry_batch
+        step = pl_module.global_step+1
+        if USE_COMET:
+            if isinstance(pl_module.logger, list):
+                assert len(list(pl_module.logger)) == 1
+                logger = pl_module.logger[0]
+            else:
+                logger = pl_module.logger
+        vocoder = pl_module.vocoder
+
+        # Synthesis one sample and log to CometLogger
+        if USE_COMET:
+            if step % pl_module.train_config[""step""][""synth_step""] == 0 and pl_module.local_rank == 0:
+                metadata = {'sup_ids': batch[0][0][0][0]}
+                fig, wav_reconstruction, wav_prediction, basename = synth_one_sample_with_target(
+                    _batch, output, vocoder, self.preprocess_config
+                )
+                self.log_figure(logger, ""Training"", step, basename, """", fig)
+                self.log_audio(logger, ""Training"", step, basename, ""reconstructed"", wav_reconstruction, metadata)
+                self.log_audio(logger, ""Training"", step, basename, ""synthesized"", wav_prediction, metadata)
+                plt.close(fig)
+
+        # Log message to log.txt and print to stdout
+        if step % trainer.log_every_n_steps == 0 and pl_module.local_rank == 0:
+            loss_dict = loss2dict(loss)
+            loss_dict.update({""Step"": step, ""Stage"": ""Training""})
+            df = pd.DataFrame([loss_dict], columns=[""Step"", ""Stage""]+CSV_COLUMNS)
+            if len(self.log_loss_dicts)==0:
+                tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE))
+            else:
+                tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE).split('\n')[-1])
+            self.log_loss_dicts.append(loss_dict)
+
+
+    def on_validation_epoch_start(self, trainer, pl_module):
+        self.val_loss_dicts = []
+
+    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
+        if dataloader_idx == 1:  # unlabeled data, do nothing
+            return
+        loss = outputs['losses']
+        output = outputs['output']
+        _batch = outputs['_batch']  # batch or qry_batch
+        
+        step = pl_module.global_step+1
+        if USE_COMET:
+            if isinstance(pl_module.logger, list):
+                assert len(list(pl_module.logger)) == 1
+                logger = pl_module.logger[0]
+            else:
+                logger = pl_module.logger
+        vocoder = pl_module.vocoder
+
+        loss_dict = loss2dict(loss)
+        self.val_loss_dicts.append(loss_dict)
+
+        # Log loss for each sample to csv files
+        sup_ids = batch[0][0][0][0]
+        qry_ids = batch[0][1][0][0]
+        SQids = f""{'-'.join(sup_ids)}.{'-'.join(qry_ids)}""
+
+        if getattr(trainer.datamodule, ""sup_datamodule"", None) is None:
+            task_id = trainer.datamodule.val_SQids2Tid[SQids]
+        else:
+            task_id = trainer.datamodule.sup_datamodule.val_SQids2Tid[SQids]
+        self.log_csv(""Validation"", step, task_id, loss_dict)
+
+        # Log figure/audio to logger + save audio
+        if USE_COMET:
+            if batch_idx == 0 and pl_module.local_rank == 0:
+                metadata = {'sup_ids': sup_ids}
+                fig, wav_reconstruction, wav_prediction, basename = synth_one_sample_with_target(
+                    _batch, output, vocoder, self.preprocess_config
+                )
+                self.log_figure(logger, ""Validation"", step, basename, """", fig)
+                self.log_audio(logger, ""Validation"", step, basename, ""reconstructed"", wav_reconstruction, metadata)
+                self.log_audio(logger, ""Validation"", step, basename, ""synthesized"", wav_prediction, metadata)
+                plt.close(fig)
+
+    def on_validation_epoch_end(self, trainer, pl_module):
+        # if pl_module.global_step > 0:
+        if True:
+            loss_dict = merge_dicts(self.val_loss_dicts)
+            step = pl_module.global_step+1
+
+            # Log total loss to log.txt and print to stdout
+            loss_dict.update({""Step"": step, ""Stage"": ""Validation""})
+            # To stdout
+            df = pd.DataFrame([loss_dict], columns=[""Step"", ""Stage""]+CSV_COLUMNS)
+            if len(self.log_loss_dicts)==0:
+                tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE))
+            else:
+                tqdm.write(df.to_string(header=True, index=False, col_space=COL_SPACE).split('\n')[-1])
+            # To file
+            self.log_loss_dicts.append(loss_dict)
+            log_file_path = os.path.join(self.log_dir, 'log.txt')
+            df = pd.DataFrame(self.log_loss_dicts, columns=[""Step"", ""Stage""]+CSV_COLUMNS).set_index(""Step"")
+            df.to_csv(log_file_path, mode='a', header=not os.path.exists(log_file_path), index=True)
+            # Reset
+            self.log_loss_dicts = []
+
+    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
+        global_step = getattr(pl_module, 'test_global_step', pl_module.global_step)
+        adaptation_steps = pl_module.adaptation_steps           # log/save period
+        test_adaptation_steps = pl_module.test_adaptation_steps # total fine-tune steps
+        vocoder = pl_module.vocoder
+
+        sup_ids = batch[0][0][0][0]
+        qry_ids = batch[0][1][0][0]
+        SQids = f""{'-'.join(sup_ids)}.{'-'.join(qry_ids)}""
+        task_id = trainer.datamodule.test_SQids2Tid[SQids]
+
+        figure_dir = os.path.join(self.result_dir, ""figure"", ""Testing"", f""step_{global_step}"", task_id)
+        audio_dir = os.path.join(self.result_dir, ""audio"", ""Testing"", f""step_{global_step}"", task_id)
+        figure_fit_dir = os.path.join(self.result_dir, ""figure-fit"", ""Testing"", f""step_{global_step}"", task_id)
+        audio_fit_dir = os.path.join(self.result_dir, ""audio-fit"", ""Testing"", f""step_{global_step}"", task_id)
+        log_dir = os.path.join(self.result_dir, ""csv"", ""Testing"", f""step_{global_step}"")
+        os.makedirs(figure_dir, exist_ok=True)
+        os.makedirs(audio_dir, exist_ok=True)
+        os.makedirs(figure_fit_dir, exist_ok=True)
+        os.makedirs(audio_fit_dir, exist_ok=True)
+        os.makedirs(log_dir, exist_ok=True)
+        csv_file_path = os.path.join(log_dir, f""{task_id}.csv"")
+
+        loss_dicts = []
+        _batch, _batch_fit = outputs[""_batch""], outputs[""_batch_fit""]
+
+        for ft_step in range(0, test_adaptation_steps+1, adaptation_steps):
+            # if ft_step == 0:
+            #     predictions = outputs[f""step_{ft_step}""][""recon""][""output""]
+            #     recon_samples(
+            #         _batch, predictions, vocoder, self.preprocess_config,
+            #         figure_dir, audio_dir
+            #     )
+            #     predictions = outputs[f""step_{ft_step}""][""recon-fit""][""output""]
+            #     recon_samples(
+            #         _batch_fit, predictions, vocoder, self.preprocess_config,
+            #         figure_fit_dir, audio_fit_dir
+            #     )
+
+            if f""step_{ft_step}"" in outputs and ""recon"" in outputs[f""step_{ft_step}""]:
+                valid_error = outputs[f""step_{ft_step}""][""recon""][""losses""]
+                loss_dicts.append({""Step"": ft_step, **loss2dict(valid_error)})
+
+            # if ""synth"" in outputs[f""step_{ft_step}""]:
+            #     predictions = outputs[f""step_{ft_step}""][""synth""][""output""]
+            #     synth_samples(
+            #         _batch, predictions, vocoder, self.preprocess_config,
+            #         figure_dir, audio_dir, f""step_{global_step}-FTstep_{ft_step}""
+            #     )
+            # if ""synth-fit"" in outputs[f""step_{ft_step}""]:
+            #     predictions = outputs[f""step_{ft_step}""][""synth-fit""][""output""]
+            #     synth_samples(
+            #         _batch_fit, predictions, vocoder, self.preprocess_config,
+            #         figure_fit_dir, audio_fit_dir, f""step_{global_step}-FTstep_{ft_step}""
+            #     )
+
+        df = pd.DataFrame(loss_dicts, columns=[""Step""] + CSV_COLUMNS).set_index(""Step"")
+        df.to_csv(csv_file_path, mode='a', header=True, index=True)
+
+    def save_audio(self, stage, step, basename, tag, audio):
+        """"""
+        Parameters:
+            stage (str): {""Training"", ""Validation"", ""Testing""}.
+            step (int): Current step.
+            basename (str): Audio index (original filename).
+            tag (str): {""reconstructed"", ""synthesized""}.
+            audio (numpy): Audio waveform.
+        """"""
+        sample_rate = self.preprocess_config[""preprocessing""][""audio""][""sampling_rate""]
+        save_dir = os.path.join(self.log_dir, ""audio"", stage)
+        filename = f""step_{step}_{basename}_{tag}.wav""
+
+        os.makedirs(save_dir, exist_ok=True)
+        wavfile.write(os.path.join(save_dir, filename), sample_rate, audio)
+
+    # def log_text(self, stage, loss_dict, print_fn=print, max_steps=200000, header=False):
+    #     if header:
+    #         message = f""{'Step':^{len(str(max_steps))}}, {'Stage':^{len('Validation')}}, ""
+    #         pl_module.print(message + "", "".join(CSV_COLUMNS))
+    #     message = f""{loss_dict['Step']:{len(str(max_steps))}}, {loss_dict['Stage']:<{len('Validation')}}, ""
+    #     print_fn(message + "", "".join([f""{loss_dict[col]}:{len(col)}.4f"" for col in CSV_COLUMNS]))
+
+    def log_csv(self, stage, step, basename, loss_dict):
+        if stage in (""Training"", ""Validation""):
+            log_dir = os.path.join(self.log_dir, ""csv"", stage)
+        else:
+            log_dir = os.path.join(self.result_dir, ""csv"", stage)
+        os.makedirs(log_dir, exist_ok=True)
+        csv_file_path = os.path.join(log_dir, f""{basename}.csv"")
+
+        df = pd.DataFrame(loss_dict, columns=CSV_COLUMNS, index=[step])
+        df.to_csv(csv_file_path, mode='a', header=not os.path.exists(csv_file_path), index=True, index_label=""Step"")
+
+    def log_figure(self, logger, stage, step, basename, tag, figure):
+        """"""
+        Parameters:
+            logger (LightningLoggerBase): {pl.loggers.CometLogger, pl.loggers.TensorBoardLogger}.
+            stage (str): {""Training"", ""Validation"", ""Testing""}.
+            step (int): Current step.
+            basename (str): Audio index (original filename).
+            tag (str): {""reconstructed"", ""synthesized""}.
+            figure (matplotlib.pyplot.figure):
+        """"""
+        figure_name = f""{stage}/step_{step}_{basename}""
+        figure_name += f""_{tag}"" if tag != """" else """"
+
+        if isinstance(logger, pl.loggers.CometLogger):
+            logger.experiment.log_figure(
+                figure_name=figure_name,
+                figure=figure,
+                step=step,
+            )
+        elif isinstance(logger, pl.loggers.TensorBoardLogger):
+            logger.experiment.add_figure(
+                tag=figure_name,
+                figure=figure,
+                global_step=step,
+            )
+        else:
+            print(""Failed to log figure: not finding correct logger type"")
+
+    def log_audio(self, logger, stage, step, basename, tag, audio, metadata=None):
+        """"""
+        Parameters:
+            logger (LightningLoggerBase): {pl.loggers.CometLogger, pl.loggers.TensorBoardLogger}.
+            stage (str): {""Training"", ""Validation"", ""Testing""}.
+            step (int): Current step.
+            basename (str): Audio index (original filename).
+            tag (str): {""reconstructed"", ""synthesized""}.
+            audio (numpy): Audio waveform.
+        """"""
+        sample_rate = self.preprocess_config[""preprocessing""][""audio""][""sampling_rate""]
+        self.save_audio(stage, step, basename, tag, audio)
+
+        if isinstance(logger, pl.loggers.CometLogger):
+            if metadata is None:
+                metadata = {}
+            metadata.update({'stage': stage, 'type': tag, 'id': basename})
+            logger.experiment.log_audio(
+                audio_data=audio / max(abs(audio)),
+                sample_rate=sample_rate,
+                file_name=f""{basename}_{tag}.wav"",
+                step=step,
+                metadata=metadata,
+            )
+        elif isinstance(logger, pl.loggers.TensorBoardLogger):
+            logger.experiment.add_audio(
+                tag=f""{stage}/step_{step}_{basename}_{tag}"",
+                snd_tensor=audio / max(abs(audio)),
+                global_step=step,
+                sample_rate=sample_rate,
+            )
+        else:
+            print(""Failed to log audio: not finding correct logger type"")"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def _collate_fn(self, data, shots, queries, re_id=False):
         idx_arr = np.arange(data_size)
         if re_id:
             for idx in idx_arr:
-                data[idx][""text""] += self.re_id_increment[data[idx][""language""]]
         
         idx_arr = idx_arr.reshape((-1, batch_size))
 
@@ -58,7 +58,7 @@ def _collate_fn(self, data, shots, queries, re_id=False):
             qry_out.append(reprocess(data, qry_ids))
             # pad_qry = time.time() - st1
 
-            lang_id = data[idxs[0]][""language""]
             repr_info = {}
             repr_info[""lang_id""] = lang_id
             repr_info[""texts""] = [data[idx][""text""] for idx in idxs]"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def _collate_fn(self, data, shots, queries, re_id=False):
         idx_arr = np.arange(data_size)
         if re_id:
             for idx in idx_arr:
+                data[idx][""text""] += self.re_id_increment[data[idx][""lang_id""]]
         
         idx_arr = idx_arr.reshape((-1, batch_size))
 
@@ -58,7 +58,7 @@ def _collate_fn(self, data, shots, queries, re_id=False):
             qry_out.append(reprocess(data, qry_ids))
             # pad_qry = time.time() - st1
 
+            lang_id = data[idxs[0]][""lang_id""]
             repr_info = {}
             repr_info[""lang_id""] = lang_id
             repr_info[""texts""] = [data[idx][""text""] for idx in idxs]"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def reprocess(data, idxs):
 
     if not unsup:
         text_lens = np.array([text.shape[0] for text in texts])
     mel_lens = np.array([mel.shape[0] for mel in mels])
 
     speakers = np.array(speakers)
@@ -73,8 +75,8 @@ def reprocess(data, idxs):
             None,
             speaker_args,
             None,
-            None,
-            None,
             torch.from_numpy(mels).float(),
             torch.from_numpy(mel_lens),
             max(mel_lens),"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def reprocess(data, idxs):
 
     if not unsup:
         text_lens = np.array([text.shape[0] for text in texts])
+    else:  # Duration has same length with text, which is equal to the number of segments.
+        text_lens = np.array([len(duration) for duration in durations])
     mel_lens = np.array([mel.shape[0] for mel in mels])
 
     speakers = np.array(speakers)
@@ -73,8 +75,8 @@ def reprocess(data, idxs):
             None,
             speaker_args,
             None,
+            torch.from_numpy(text_lens),
+            max(text_lens),
             torch.from_numpy(mels).float(),
             torch.from_numpy(mel_lens),
             max(mel_lens),"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" 
 DATA_MODULE = {
     ""fscl"": language.FSCLDataModule,
-    ""fscl-tune"": language.FSCLDataModule,
     ""semi-fscl"": language.SemiFSCLDataModule,
     ""multilingual-baseline"": language.FastSpeech2DataModule,
     ""multilingual-baseline-tune"": language.FastSpeech2DataModule,"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" 
 DATA_MODULE = {
     ""fscl"": language.FSCLDataModule,
+    ""fscl-tune"": language.FastSpeech2DataModule,
     ""semi-fscl"": language.SemiFSCLDataModule,
     ""multilingual-baseline"": language.FastSpeech2DataModule,
     ""multilingual-baseline-tune"": language.FastSpeech2DataModule,"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def train_dataloader(self):
             self.train_task_dataset,
             batch_size=1,
             shuffle=True,
-            num_workers=4,
             collate_fn=lambda batch: batch,
         )
         return self.train_loader
@@ -190,7 +190,7 @@ def train_dataloader(self):
             batch_size=self.batch_size//torch.cuda.device_count(),
             shuffle=True,
             drop_last=True,
-            num_workers=4,
             collate_fn=self.collate.collate_fn(False),
         )
         return self.train_loader
@@ -215,9 +215,9 @@ class SemiFSCLDataModule(pl.LightningDataModule):
     """"""
     def __init__(self, data_configs, train_config, algorithm_config, log_dir, result_dir):
         super().__init__()
-        self.sup_datamodule = FSCLDataModule(data_configs[""sup""], 
                                                 train_config, algorithm_config, log_dir, result_dir)
-        self.unsup_datamodule = UnsupFSCLDataModule(data_configs[""unsup""], 
                                                 train_config, algorithm_config, log_dir, result_dir)
 
     def setup(self, stage=None):
@@ -226,12 +226,13 @@ def setup(self, stage=None):
 
     def train_dataloader(self):
         return {
-            ""sup"": self.sup_datamodule.train_loader(),
-            ""unsup"": self.unsup_datamodule.train_loader()
         }
 
     def val_dataloader(self):
-        return {
-            ""sup"": self.sup_datamodule.val_loader(),
-            ""unsup"": self.unsup_datamodule.val_loader()
-        }"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def train_dataloader(self):
             self.train_task_dataset,
             batch_size=1,
             shuffle=True,
+            num_workers=0,
             collate_fn=lambda batch: batch,
         )
         return self.train_loader
@@ -190,7 +190,7 @@ def train_dataloader(self):
             batch_size=self.batch_size//torch.cuda.device_count(),
             shuffle=True,
             drop_last=True,
+            num_workers=0,
             collate_fn=self.collate.collate_fn(False),
         )
         return self.train_loader
@@ -215,9 +215,9 @@ class SemiFSCLDataModule(pl.LightningDataModule):
     """"""
     def __init__(self, data_configs, train_config, algorithm_config, log_dir, result_dir):
         super().__init__()
+        self.sup_datamodule = FSCLDataModule(data_configs, 
                                                 train_config, algorithm_config, log_dir, result_dir)
+        self.unsup_datamodule = UnsupFSCLDataModule(data_configs, 
                                                 train_config, algorithm_config, log_dir, result_dir)
 
     def setup(self, stage=None):
@@ -226,12 +226,13 @@ def setup(self, stage=None):
 
     def train_dataloader(self):
         return {
+            ""sup"": self.sup_datamodule.train_dataloader(),
+            ""unsup"": self.unsup_datamodule.train_dataloader()
         }
 
     def val_dataloader(self):
+        # Validation loaders are sequentially combined.
+        return [
+            self.sup_datamodule.val_dataloader(),
+            self.unsup_datamodule.val_dataloader()
+        ]"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def train_dataloader(self):
             batch_size=self.batch_size//torch.cuda.device_count(),
             shuffle=True,
             drop_last=True,
-            num_workers=4,
             collate_fn=self.collate.collate_fn(False, re_id=self.re_id),  # CAUTION: tune does not need re_id
         )
         return self.train_loader"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def train_dataloader(self):
             batch_size=self.batch_size//torch.cuda.device_count(),
             shuffle=True,
             drop_last=True,
+            num_workers=2,
             collate_fn=self.collate.collate_fn(False, re_id=self.re_id),  # CAUTION: tune does not need re_id
         )
         return self.train_loader"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def __getitem__(self, idx):
         # For codebook module
         segment = self.data_parser.mfa_segment.read_from_query(query)
         if Define.UPSTREAM == ""mel"":
-            raw_feat = self.data_parser.mel.read_from_query(query)
             avg_frames = self.data_parser.mfa_duration.read_from_query(query)
         else:
             raw_feat = self.data_parser.wav_trim_16000.read_from_query(query)
@@ -177,7 +177,7 @@ def __getitem__(self, idx):
         # For codebook module
         segment = self.data_parser.unsup_segment.read_from_query(query)
         if Define.UPSTREAM == ""mel"":
-            raw_feat = self.data_parser.mel.read_from_query(query)
             avg_frames = self.data_parser.unsup_duration.read_from_query(query)
         else:
             raw_feat = self.data_parser.wav_trim_16000.read_from_query(query)"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def __getitem__(self, idx):
         # For codebook module
         segment = self.data_parser.mfa_segment.read_from_query(query)
         if Define.UPSTREAM == ""mel"":
+            raw_feat = mel
             avg_frames = self.data_parser.mfa_duration.read_from_query(query)
         else:
             raw_feat = self.data_parser.wav_trim_16000.read_from_query(query)
@@ -177,7 +177,7 @@ def __getitem__(self, idx):
         # For codebook module
         segment = self.data_parser.unsup_segment.read_from_query(query)
         if Define.UPSTREAM == ""mel"":
+            raw_feat = mel
             avg_frames = self.data_parser.unsup_duration.read_from_query(query)
         else:
             raw_feat = self.data_parser.wav_trim_16000.read_from_query(query)"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" import torch.nn.functional as F
 import pytorch_lightning as pl
 
 from transformer import Decoder, PostNet, Encoder2
 from .modules import VarianceAdaptor
 from .speaker_encoder import SpeakerEncoder
@@ -57,6 +58,9 @@ def forward(
             else None
         )
 
         output = self.encoder(texts, src_masks)
 
         if self.speaker_emb is not None:"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" import torch.nn.functional as F
 import pytorch_lightning as pl
 
+import Define
 from transformer import Decoder, PostNet, Encoder2
 from .modules import VarianceAdaptor
 from .speaker_encoder import SpeakerEncoder
@@ -57,6 +58,9 @@ def forward(
             else None
         )
 
+        if Define.DEBUG:
+            print(""FastSpeech2m input shape: "", texts.shape)
+            print(""FastSpeech2m mask shape: "", src_masks.shape)
         output = self.encoder(texts, src_masks)
 
         if self.speaker_emb is not None:"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def __init__(self, model_config, algorithm_config):
         self.emb_banks = nn.Parameter(torch.randn(self.codebook_size, self.d_word_vec))
 
         if Define.UPSTREAM != ""mel"" and Define.UPSTREAM is not None:
-            self.weight_raw = nn.Parameter(torch.zeros(1, 25, 1))
             
-            # normalize code debug
-            last_hidden = torch.ones(1, 25, 1) * float('-inf')
-            last_hidden[0][Define.LAYER_IDX][0] = 10.0
-            self.weight_raw = nn.Parameter(last_hidden)
-            self.weight_raw.requires_grad = False
 
         self.d_feat = self.codebook_config[""representation_dim""]
         self.q_linear = nn.Linear(self.d_feat, self.d_word_vec)
@@ -555,8 +556,11 @@ def __init__(self, model_config, algorithm_config):
 
     def get_new_embedding(self, ref, *args, **kwargs):
         """"""
-        ref: Tensor with size (vocab_size, 25, representation_dim).
         """"""
         try:
             assert ref.device == self.device
         except:
@@ -565,17 +569,15 @@ def get_new_embedding(self, ref, *args, **kwargs):
 
         if Define.UPSTREAM != ""mel"" and Define.UPSTREAM is not None:
             weighted_sum = torch.nn.functional.softmax(self.weight_raw, dim=1) * ref
-            ref = weighted_sum.sum(dim=1)
-        q = self.q_linear(ref).view(-1, self.num_heads, self.d_word_vec // self.num_heads)
-        q = q.transpose(0, 1).unsqueeze(0).contiguous()  # 1 x nH x vocab_size x dword // nH
         k = self.att_banks.view(-1, self.num_heads, self.d_word_vec // self.num_heads)
         k = k.transpose(0, 1).unsqueeze(0).contiguous()  # 1 x nH x codebook_size x dword // nH
         v = self.emb_banks.view(-1, self.num_heads, self.d_word_vec // self.num_heads)
         v = v.transpose(0, 1).unsqueeze(0).contiguous()
         weighted_embedding, attn = self.attention(q, k, v)
-        weighted_embedding = weighted_embedding.squeeze(0).transpose(0, 1).contiguous().view(-1, self.d_word_vec)
-        weighted_embedding[Constants.PAD].fill_(0)
-
         # print(torch.sum(self.att_banks), torch.sum(self.emb_banks))
         
         return weighted_embedding"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def __init__(self, model_config, algorithm_config):
         self.emb_banks = nn.Parameter(torch.randn(self.codebook_size, self.d_word_vec))
 
         if Define.UPSTREAM != ""mel"" and Define.UPSTREAM is not None:
+            self.weight_raw = nn.Parameter(torch.zeros(1, 1, 25, 1))
             
+            # specific layer
+            if Define.LAYER_IDX is not None:
+                last_hidden = torch.ones(1, 1, 25, 1) * float('-inf')
+                last_hidden[0][0][Define.LAYER_IDX][0] = 10.0
+                self.weight_raw = nn.Parameter(last_hidden)
+                self.weight_raw.requires_grad = False
 
         self.d_feat = self.codebook_config[""representation_dim""]
         self.q_linear = nn.Linear(self.d_feat, self.d_word_vec)
@@ -555,8 +556,11 @@ def __init__(self, model_config, algorithm_config):
 
     def get_new_embedding(self, ref, *args, **kwargs):
         """"""
+        ref: 
+            Sup: Tensor with size (B=1, vocab_size, 25, representation_dim).
+            Unsup: Tensor with size (B, L, 25, representation_dim).
         """"""
+        B = ref.shape[0]
         try:
             assert ref.device == self.device
         except:
@@ -565,17 +569,15 @@ def get_new_embedding(self, ref, *args, **kwargs):
 
         if Define.UPSTREAM != ""mel"" and Define.UPSTREAM is not None:
             weighted_sum = torch.nn.functional.softmax(self.weight_raw, dim=1) * ref
+            ref = weighted_sum.sum(dim=2)
+        q = self.q_linear(ref).view(B, -1, self.num_heads, self.d_word_vec // self.num_heads)
+        q = q.transpose(1, 2).contiguous()  # B x nH x vocab_size x dword // nH
         k = self.att_banks.view(-1, self.num_heads, self.d_word_vec // self.num_heads)
         k = k.transpose(0, 1).unsqueeze(0).contiguous()  # 1 x nH x codebook_size x dword // nH
         v = self.emb_banks.view(-1, self.num_heads, self.d_word_vec // self.num_heads)
         v = v.transpose(0, 1).unsqueeze(0).contiguous()
         weighted_embedding, attn = self.attention(q, k, v)
+        weighted_embedding = weighted_embedding.transpose(1, 2).contiguous().view(B, -1, self.d_word_vec)
         # print(torch.sum(self.att_banks), torch.sum(self.emb_banks))
         
         return weighted_embedding"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def extract(self, info, norm=False, batch_size=32, no_text=False):
             unsup_repr = []
             for d_list, repr in zip(avg_frames, representation_list):
                 pos = 0
-                for i, (t, d) in enumerate(zip(text, d_list)):
                     if d > 0 and not torch_exist_nan(repr[:, pos: pos + d, :]):
                         repr[:, i] = torch.mean(repr[:, pos: pos + d, :], axis=1)
                     else:
@@ -52,7 +52,9 @@ def extract(self, info, norm=False, batch_size=32, no_text=False):
                 repr = repr[:, :len(d_list)]
                 unsup_repr.append(repr.transpose(0, 1))
             unsup_repr = torch.nn.utils.rnn.pad_sequence(unsup_repr, batch_first=True)  # B, L, layer, dim
-            return unsup_repr.contiguous()
         else:
             lang_id = info[""lang_id""]
             n_symbols = len(LANG_ID2SYMBOLS[lang_id])
@@ -75,7 +77,10 @@ def extract(self, info, norm=False, batch_size=32, no_text=False):
                 else:
                     phn_repr[i] = torch.mean(torch.cat(table[i], axis=1), axis=1)
 
-            return phn_repr.unsqueeze(0).float()  # 1, n_symbols, layer, dim
 
 
 class HubertExtractor(S3PRLExtractor):
@@ -106,15 +111,17 @@ def extract(self, info, norm=False, no_text=False):
             unsup_repr = []
             for d_list, repr in zip(avg_frames, representation_list):
                 pos = 0
-                for i, (t, d) in enumerate(zip(text, d_list)):
                     if d > 0 and not torch_exist_nan(repr[pos: pos + d, :]):
                         repr[i] = torch.mean(repr[pos: pos + d, :], axis=0)
                     else:
                         repr[i] = torch.zeros(Define.UPSTREAM_DIM)
                     pos += d
-                repr = repr[:, :len(d_list)]
                 unsup_repr.append(repr)
             unsup_repr = torch.nn.utils.rnn.pad_sequence(unsup_repr, batch_first=True)  # B, L, 80
             return unsup_repr
         else:
             lang_id = info[""lang_id""]
@@ -139,7 +146,10 @@ def extract(self, info, norm=False, no_text=False):
                 else:
                     phn_repr[i] = torch.mean(torch.cat(table[i], axis=0), axis=0)
 
-            return phn_repr.unsqueeze(0).float()  # 1, n_symbols, 80
 
 
 if __name__ == ""__main__"":"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"def extract(self, info, norm=False, batch_size=32, no_text=False):
             unsup_repr = []
             for d_list, repr in zip(avg_frames, representation_list):
                 pos = 0
+                for i, d in enumerate(d_list):
                     if d > 0 and not torch_exist_nan(repr[:, pos: pos + d, :]):
                         repr[:, i] = torch.mean(repr[:, pos: pos + d, :], axis=1)
                     else:
@@ -52,7 +52,9 @@ def extract(self, info, norm=False, batch_size=32, no_text=False):
                 repr = repr[:, :len(d_list)]
                 unsup_repr.append(repr.transpose(0, 1))
             unsup_repr = torch.nn.utils.rnn.pad_sequence(unsup_repr, batch_first=True)  # B, L, layer, dim
+            if Define.DEBUG:
+                print(unsup_repr.shape)
+            return unsup_repr
         else:
             lang_id = info[""lang_id""]
             n_symbols = len(LANG_ID2SYMBOLS[lang_id])
@@ -75,7 +77,10 @@ def extract(self, info, norm=False, batch_size=32, no_text=False):
                 else:
                     phn_repr[i] = torch.mean(torch.cat(table[i], axis=1), axis=1)
 
+            phn_repr = phn_repr.unsqueeze(0).float()  # 1, n_symbols, layer, dim
+            if Define.DEBUG:
+                print(phn_repr.shape)
+            return phn_repr
 
 
 class HubertExtractor(S3PRLExtractor):
@@ -106,15 +111,17 @@ def extract(self, info, norm=False, no_text=False):
             unsup_repr = []
             for d_list, repr in zip(avg_frames, representation_list):
                 pos = 0
+                for i, d in enumerate(d_list):
                     if d > 0 and not torch_exist_nan(repr[pos: pos + d, :]):
                         repr[i] = torch.mean(repr[pos: pos + d, :], axis=0)
                     else:
                         repr[i] = torch.zeros(Define.UPSTREAM_DIM)
                     pos += d
+                repr = repr[:len(d_list)]
                 unsup_repr.append(repr)
             unsup_repr = torch.nn.utils.rnn.pad_sequence(unsup_repr, batch_first=True)  # B, L, 80
+            if Define.DEBUG:
+                print(unsup_repr.shape)
             return unsup_repr
         else:
             lang_id = info[""lang_id""]
@@ -139,7 +146,10 @@ def extract(self, info, norm=False, no_text=False):
                 else:
                     phn_repr[i] = torch.mean(torch.cat(table[i], axis=0), axis=0)
 
+            phn_repr = phn_repr.unsqueeze(0).float()  # 1, n_symbols, 80
+            if Define.DEBUG:
+                print(phn_repr.shape)
+            return phn_repr
 
 
 if __name__ == ""__main__"":"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" from lightning.utils.tool import LightningMelGAN
 from lightning.model.phoneme_embedding import PhonemeEmbedding
 from lightning.model import FastSpeech2Loss, FastSpeech2
-from lightning.callbacks.baseline_saver import Saver
 
 
 class BaselineSystem(System):"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" from lightning.utils.tool import LightningMelGAN
 from lightning.model.phoneme_embedding import PhonemeEmbedding
 from lightning.model import FastSpeech2Loss, FastSpeech2
+from lightning.callbacks.language.baseline_saver import Saver
 
 
 class BaselineSystem(System):"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" import torch.nn as nn
 import torch.nn.functional as F
 import matplotlib.pyplot as plt
@@ -7,16 +8,14 @@
 from lightning.utils.tool import LightningMelGAN
 from lightning.model.phoneme_embedding import PhonemeEmbedding
 from lightning.model import FastSpeech2Loss, FastSpeech2
-from lightning.model.reference_extractor import HubertExtractor, XLSR53Extractor, Wav2Vec2Extractor, MelExtractor
-from lightning.callbacks.baseline_saver import Saver
 from Objects.visualization import CodebookAnalyzer
 import Define
 
 
 class SemiTransEmbSystem(AdaptorSystem):
-    """"""
-    Semi-supervised version of TransEmb system.
-    """"""
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
@@ -53,7 +52,6 @@ def build_optimized_model(self):
 
     def build_saver(self):
         saver = Saver(self.preprocess_config, self.log_dir, self.result_dir)
-        saver.set_meta_saver()
         return saver
     
     def init_codebook_type(self):        
@@ -66,47 +64,105 @@ def init_codebook_type(self):
             raise NotImplementedError
         self.adaptation_steps = self.algorithm_config[""adapt""][""train""][""steps""]
 
-    def build_embedding_table(self, batch):
-        _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
-            ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
 
-        embedding = self.embedding_model.get_new_embedding(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         return embedding
 
-    def common_step(self, batch, batch_idx, train=True):
-        emb_table = self.build_embedding_table(batch)
-        _, qry_batch, _, _ = batch[0]
         qry_batch = qry_batch[0]
         emb_texts = F.embedding(qry_batch[3], emb_table, padding_idx=0)
-        output = self.model(qry_batch[2], emb_texts, *(qry_batch[4:]))
-        loss = self.loss_func(batch, output)
-        return loss, output
 
     def training_step(self, batch, batch_idx):
-        train_loss, predictions = self.common_step(batch, batch_idx, train=True)
-        qry_batch = batch[0][1][0]
 
         # Log metrics to CometLogger
         loss_dict = {f""Train/{k}"": v for k, v in loss2dict(train_loss).items()}
         self.log_dict(loss_dict, sync_dist=True)
         return {'loss': train_loss[0], 'losses': train_loss, 'output': predictions, '_batch': qry_batch}
 
-    def validation_step(self, batch, batch_idx):
-        self.log_matching(batch, batch_idx)
-        val_loss, predictions = self.common_step(batch, batch_idx)
-        qry_batch = batch[0][1][0]
-
-        # Log metrics to CometLogger
-        loss_dict = {f""Val/{k}"": v for k, v in loss2dict(val_loss).items()}
-        self.log_dict(loss_dict, sync_dist=True)
-        return {'losses': val_loss, 'output': predictions, '_batch': qry_batch}
     
     def visualize_matching(self, batch, batch_idx):
         if self.codebook_type != ""table-sep"":
             _, _, ref_phn_feats, lang_id = batch[0]
             with torch.no_grad():
                 ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
 
             matching = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
             self.codebook_analyzer.visualize_matching(batch_idx, matching)
@@ -117,11 +173,14 @@ def log_matching(self, batch, batch_idx, stage=""val""):
         _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
             ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
         matchings = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         for matching in matchings:
             fig = self.codebook_analyzer.plot_matching(matching, quantized=False)
             figure_name = f""{stage}/step_{step}_{batch_idx:03d}_{matching['title']}""
-            self.logger[0].experiment.log_figure(
                 figure_name=figure_name,
                 figure=fig,
                 step=step,"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import matplotlib.pyplot as plt
@@ -7,16 +8,14 @@
 from lightning.utils.tool import LightningMelGAN
 from lightning.model.phoneme_embedding import PhonemeEmbedding
 from lightning.model import FastSpeech2Loss, FastSpeech2
+from lightning.callbacks.language.fscl_saver import Saver
 from Objects.visualization import CodebookAnalyzer
+from lightning.model.reference_extractor import HubertExtractor, XLSR53Extractor, Wav2Vec2Extractor, MelExtractor
+from transformer import Constants
 import Define
 
 
 class SemiTransEmbSystem(AdaptorSystem):
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
@@ -53,7 +52,6 @@ def build_optimized_model(self):
 
     def build_saver(self):
         saver = Saver(self.preprocess_config, self.log_dir, self.result_dir)
         return saver
     
     def init_codebook_type(self):        
@@ -66,47 +64,105 @@ def init_codebook_type(self):
             raise NotImplementedError
         self.adaptation_steps = self.algorithm_config[""adapt""][""train""][""steps""]
 
+    def build_embedding_table(self, repr_info):
         with torch.no_grad():
+            ref_phn_feats = self.reference_extractor.extract(repr_info, norm=False)
+        embedding = self.embedding_model.get_new_embedding(self.codebook_type, ref_phn_feats=ref_phn_feats)
+        embedding = embedding.squeeze(0)
+        embedding[Constants.PAD].fill_(0)
 
+        if Define.DEBUG:
+            print(""Embedding shape "", embedding.shape)
         return embedding
 
+    def get_unsup_representation(self, repr_info):
+        with torch.no_grad():
+            unsup_repr = self.reference_extractor.extract(repr_info, norm=False, no_text=True)
+        unsup_repr = self.embedding_model.get_new_embedding(self.codebook_type, ref_phn_feats=unsup_repr)
+
+        if Define.DEBUG:
+            print(""Unsup Representation shape "", unsup_repr.shape)
+        return unsup_repr
+
+    def s_common_step(self, s_batch, batch_idx, train=True):
+        # supervised loss
+        _, qry_batch, s_repr_info, lang_id = s_batch[0]
+        emb_table = self.build_embedding_table(s_repr_info)
         qry_batch = qry_batch[0]
         emb_texts = F.embedding(qry_batch[3], emb_table, padding_idx=0)
+        s_output = self.model(qry_batch[2], emb_texts, *(qry_batch[4:]))
+        s_loss = self.loss_func(qry_batch, s_output)
+
+        return s_loss, s_output
+
+    def u_common_step(self, u_batch, batch_idx, train=True):
+        # unsupervised loss
+        u_batch_data, u_repr_info = u_batch
+        unsup_repr = self.get_unsup_representation(u_repr_info)
+        u_output = self.model(u_batch_data[2], unsup_repr, *(u_batch_data[4:]))
+        u_loss = self.loss_func(u_batch_data, u_output)
+
+        return u_loss
+
+    def check_s_batch(self, s_batch):
+        assert len(s_batch) == 1, ""meta_batch_per_gpu""
+        assert len(s_batch[0]) == 2 or len(s_batch[0]) == 4, ""sup + qry (+ ref_phn_feats + lang_id)""
+        assert len(s_batch[0][0]) == 1, ""n_batch == 1""
+        assert len(s_batch[0][0][0]) == 12, ""data with 12 elements""
 
+    def check_u_batch(self, u_batch):
+        pass
+
+    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):
+        self.check_s_batch(batch[""sup""])
+        self.check_u_batch(batch[""unsup""])
+    
+    def on_validation_batch_start(self, batch, batch_idx, dataloader_idx):
+        if dataloader_idx == 0:
+            self.check_s_batch(batch)
+        elif dataloader_idx == 1:
+            self.check_u_batch(batch)
+        else:
+            raise NotImplementedError
+    
     def training_step(self, batch, batch_idx):
+        s_train_loss, predictions = self.s_common_step(batch[""sup""], batch_idx, train=True)
+        u_train_loss = self.u_common_step(batch[""unsup""], batch_idx, train=True)
+        train_loss = []
+        for i in range(len(s_train_loss)):
+            train_loss.append(0.5 * s_train_loss[i] + 0.5 * u_train_loss[i])
+
+        qry_batch = batch[""sup""][0][1][0]
 
         # Log metrics to CometLogger
         loss_dict = {f""Train/{k}"": v for k, v in loss2dict(train_loss).items()}
         self.log_dict(loss_dict, sync_dist=True)
         return {'loss': train_loss[0], 'losses': train_loss, 'output': predictions, '_batch': qry_batch}
 
+    def validation_step(self, batch, batch_idx, dataloader_idx):
+        if dataloader_idx == 0:
+            self.log_matching(batch, batch_idx)
+            val_loss, predictions = self.s_common_step(batch, batch_idx)
+            qry_batch = batch[0][1][0]
+
+            # Log metrics to CometLogger
+            loss_dict = {f""Val/{k}"": v for k, v in loss2dict(val_loss).items()}
+            self.log_dict(loss_dict, sync_dist=True)
+            return {'losses': val_loss, 'output': predictions, '_batch': qry_batch}
+        elif dataloader_idx == 1:
+            val_loss = self.u_common_step(batch, batch_idx)
+            loss_dict = {f""Val/{k}"": v for k, v in loss2dict(val_loss).items()}
+            self.log_dict(loss_dict, sync_dist=True)
+        else:
+            raise NotImplementedError
     
     def visualize_matching(self, batch, batch_idx):
         if self.codebook_type != ""table-sep"":
             _, _, ref_phn_feats, lang_id = batch[0]
             with torch.no_grad():
                 ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
+                ref_phn_feats = ref_phn_feats.squeeze(0)
+                ref_phn_feats[Constants.PAD].fill_(0)
 
             matching = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
             self.codebook_analyzer.visualize_matching(batch_idx, matching)
@@ -117,11 +173,14 @@ def log_matching(self, batch, batch_idx, stage=""val""):
         _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
             ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
+            ref_phn_feats = ref_phn_feats.squeeze(0)
+            ref_phn_feats[Constants.PAD].fill_(0)
+        
         matchings = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         for matching in matchings:
             fig = self.codebook_analyzer.plot_matching(matching, quantized=False)
             figure_name = f""{stage}/step_{step}_{batch_idx:03d}_{matching['title']}""
+            self.logger.experiment.log_figure(
                 figure_name=figure_name,
                 figure=fig,
                 step=step,"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;" import torch.nn as nn
 import torch.nn.functional as F
 import matplotlib.pyplot as plt
@@ -7,10 +8,11 @@
 from lightning.utils.tool import LightningMelGAN
 from lightning.model.phoneme_embedding import PhonemeEmbedding
 from lightning.model import FastSpeech2Loss, FastSpeech2
-from lightning.callbacks.baseline_saver import Saver
 from Objects.visualization import CodebookAnalyzer
 from lightning.model.reference_extractor import HubertExtractor, XLSR53Extractor, Wav2Vec2Extractor, MelExtractor
 import Define
 
 
 class TransEmbSystem(AdaptorSystem):
@@ -50,7 +52,6 @@ def build_optimized_model(self):
 
     def build_saver(self):
         saver = Saver(self.preprocess_config, self.log_dir, self.result_dir)
-        saver.set_meta_saver()
         return saver
     
     def init_codebook_type(self):        
@@ -66,7 +67,9 @@ def init_codebook_type(self):
     def build_embedding_table(self, batch):
         _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
-            ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
 
         embedding = self.embedding_model.get_new_embedding(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         return embedding
@@ -77,7 +80,7 @@ def common_step(self, batch, batch_idx, train=True):
         qry_batch = qry_batch[0]
         emb_texts = F.embedding(qry_batch[3], emb_table, padding_idx=0)
         output = self.model(qry_batch[2], emb_texts, *(qry_batch[4:]))
-        loss = self.loss_func(batch, output)
         return loss, output
 
     def training_step(self, batch, batch_idx):
@@ -104,6 +107,8 @@ def visualize_matching(self, batch, batch_idx):
             _, _, ref_phn_feats, lang_id = batch[0]
             with torch.no_grad():
                 ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
 
             matching = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
             self.codebook_analyzer.visualize_matching(batch_idx, matching)
@@ -114,11 +119,14 @@ def log_matching(self, batch, batch_idx, stage=""val""):
         _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
             ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
         matchings = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         for matching in matchings:
             fig = self.codebook_analyzer.plot_matching(matching, quantized=False)
             figure_name = f""{stage}/step_{step}_{batch_idx:03d}_{matching['title']}""
-            self.logger[0].experiment.log_figure(
                 figure_name=figure_name,
                 figure=fig,
                 step=step,"
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"+import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import matplotlib.pyplot as plt
@@ -7,10 +8,11 @@
 from lightning.utils.tool import LightningMelGAN
 from lightning.model.phoneme_embedding import PhonemeEmbedding
 from lightning.model import FastSpeech2Loss, FastSpeech2
+from lightning.callbacks.language.fscl_saver import Saver
 from Objects.visualization import CodebookAnalyzer
 from lightning.model.reference_extractor import HubertExtractor, XLSR53Extractor, Wav2Vec2Extractor, MelExtractor
 import Define
+from transformer import Constants
 
 
 class TransEmbSystem(AdaptorSystem):
@@ -50,7 +52,6 @@ def build_optimized_model(self):
 
     def build_saver(self):
         saver = Saver(self.preprocess_config, self.log_dir, self.result_dir)
         return saver
     
     def init_codebook_type(self):        
@@ -66,7 +67,9 @@ def init_codebook_type(self):
     def build_embedding_table(self, batch):
         _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
+            ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=False)
+            ref_phn_feats = ref_phn_feats.squeeze(0)
+            ref_phn_feats[Constants.PAD].fill_(0)
 
         embedding = self.embedding_model.get_new_embedding(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         return embedding
@@ -77,7 +80,7 @@ def common_step(self, batch, batch_idx, train=True):
         qry_batch = qry_batch[0]
         emb_texts = F.embedding(qry_batch[3], emb_table, padding_idx=0)
         output = self.model(qry_batch[2], emb_texts, *(qry_batch[4:]))
+        loss = self.loss_func(qry_batch, output)
         return loss, output
 
     def training_step(self, batch, batch_idx):
@@ -104,6 +107,8 @@ def visualize_matching(self, batch, batch_idx):
             _, _, ref_phn_feats, lang_id = batch[0]
             with torch.no_grad():
                 ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
+                ref_phn_feats = ref_phn_feats.squeeze(0)
+                ref_phn_feats[Constants.PAD].fill_(0)
 
             matching = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
             self.codebook_analyzer.visualize_matching(batch_idx, matching)
@@ -114,11 +119,14 @@ def log_matching(self, batch, batch_idx, stage=""val""):
         _, _, ref_phn_feats, lang_id = batch[0]
         with torch.no_grad():
             ref_phn_feats = self.reference_extractor.extract(ref_phn_feats, norm=True)
+            ref_phn_feats = ref_phn_feats.squeeze(0)
+            ref_phn_feats[Constants.PAD].fill_(0)
+        
         matchings = self.embedding_model.get_matching(self.codebook_type, ref_phn_feats=ref_phn_feats, lang_id=lang_id)
         for matching in matchings:
             fig = self.codebook_analyzer.plot_matching(matching, quantized=False)
             figure_name = f""{stage}/step_{step}_{batch_idx:03d}_{matching['title']}""
+            self.logger.experiment.log_figure(
                 figure_name=figure_name,
                 figure=fig,
                 step=step,"
KO;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"     import logging
     # configure logging at the root level of lightning
     logging.getLogger(""pytorch_lightning"").setLevel(logging.ERROR)
 
 
 TRAINER_CONFIG = {
@@ -70,7 +72,6 @@ def main(args, configs):
     Define.ALLSTATS[""global""] = Define.merge_stats(Define.ALLSTATS, keys)
     if Define.DEBUG:
         print(""Initialize data parsers and build normalize stats, done."")
-        print(Define.ALLSTATS)
     #==========================================================
 
     for p in train_config[""path""].values():
@@ -163,6 +164,8 @@ def main(args, configs):
             )
 
         # Train
         if Define.USE_COMET:
             trainer = pl.Trainer(logger=loggers, **TRAINER_CONFIG, **trainer_training_config)
             pl.seed_everything(43, True)
@@ -262,8 +265,8 @@ def main(args, configs):
         default=0,
     )
     parser.add_argument(
-        ""-le"", ""--layer_exp"", type=int, help=""1-24"",
-        default=24,
     )
     parser.add_argument(
         ""-ue"", ""--upstream_exp"", type=str, help=""upstream options"","
OK;15;hhhaaahhhaa;Few-Shot-Cross-Lingual-TTS;e730ffaff7903a64e0ac59388652d0ec80d54824;semi-fscl done, need to fix ddp memory issue;"     import logging
     # configure logging at the root level of lightning
     logging.getLogger(""pytorch_lightning"").setLevel(logging.ERROR)
+if Define.DEBUG:
+    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
 
 
 TRAINER_CONFIG = {
@@ -70,7 +72,6 @@ def main(args, configs):
     Define.ALLSTATS[""global""] = Define.merge_stats(Define.ALLSTATS, keys)
     if Define.DEBUG:
         print(""Initialize data parsers and build normalize stats, done."")
     #==========================================================
 
     for p in train_config[""path""].values():
@@ -163,6 +164,8 @@ def main(args, configs):
             )
 
         # Train
+        if Define.DEBUG:
+            print(""Start Training!"")
         if Define.USE_COMET:
             trainer = pl.Trainer(logger=loggers, **TRAINER_CONFIG, **trainer_training_config)
             pl.seed_everything(43, True)
@@ -262,8 +265,8 @@ def main(args, configs):
         default=0,
     )
     parser.add_argument(
+        ""-le"", ""--layer_exp"", help=""1-24"",
+        default=None,
     )
     parser.add_argument(
         ""-ue"", ""--upstream_exp"", type=str, help=""upstream options"","
KO;16;JdavisBro;Chicorobot;1180ade27b0dd268fde91e9315cc044bb85d52f3;"Changes list in description

crop to transparency
animation fps
sprite command arg descriptions
use frame
frames command
save images when creating (instead of keeping them all in memory)";" import discord
 from discord import app_commands
 from PIL import Image, ImageChops#, ImageDraw, ImageFont
-#from moviepy.editor import ImageSequenceClip
-#import numpy
 
 with Path(""TOKEN.txt"").open(""r"") as f:
     TOKEN = f.readline().rstrip()
@@ -105,6 +104,18 @@ async def on_ready():
     
     print(""LOGGED IN!"")
 
 def clamp(value, min, max):
     return min if value <= min else max if value >= max else value
 
@@ -302,69 +313,122 @@ async def hair_autocomplete(interaction: discord.Interaction, current:str):
 
 
 @tree.command(guild=TEST_GUILD, description=""Show a sprite."")
-async def sprite(interaction: discord.Interaction, sprite: str, animated: bool=False):
     if animated and not imagemagick:
         await interaction.response.send_message(content=""Animated sprites are unavailable, making non animated version."")
         animated = False
-    else:
-        await interaction.response.defer()
     if sprite.lower() == ""random"":
         sprite = random.choice([i.name for i in Path(""sprites/"").iterdir()])
-    ims = []
     spriteDir = Path(f""sprites/{sprite}/"")
     layers = sorted([i for i in spriteDir.iterdir()], key=lambda item: item.stem)
     if sprite.endswith(""_A""):
         layers += [i/""1"" for i in Path(""sprites/"").glob(""_"".join(sprite.split(""_"")[:-1])+""_*"") if i != spriteDir]
     i = 0
-    for filePath in sorted(layers[0].iterdir(), key=lambda path: path.stem):
         im = Image.open(filePath).convert(""RGBA"")
         for layer in layers[1:]:
             name = filePath.name
             if (layer / name).exists():
                 im2 = Image.open(layer / name).convert(""RGBA"")
                 im.alpha_composite(im2)
-        ims.append(im)
-        if not animated:
-            break
     if animated:
-        name = tempfile.mkdtemp()
-        name = Path(name)
-        print(f""Making temp: {str(name)}"")
-        msg = await interaction.followup.send(content=""Saving PNGs."")
-        for i, im in enumerate(ims):
-            im.save(name / f""{i:02}.png"")
-        await msg.edit(content=""Converting to gif."")
         process = await asyncio.create_subprocess_shell(
-            f""{imagemagick} -delay 10 -loop 0 -dispose Background {name / '*.png'} {name / 'out.gif'}"",
             stdout=asyncio.subprocess.PIPE,
             stderr=asyncio.subprocess.PIPE
         )
         await process.communicate()
-        if process.returncode != 0:
             await msg.edit(content=""<:Pizza_Depressaroli:967482279670718474> Something went wrong (gif conversion error)."")
             print(f""GIF CONVERSION ERROR: {process.stderr}"")
             try:
-                shutil.rmtree(name)
             except PermissionError:
                 print(""Failed to delete file."")
             else:
-                print(f""Deleted temp: {str(name)}"")
             return
-        file = discord.File(name / ""out.gif"", f""{sprite}.gif"")
-        await msg.edit(content=f""{sprite}:"", attachments=[file])
-        del file
         try:
-            shutil.rmtree(name)
         except PermissionError:
             print(""Failed to delete file."")
         else:
-            print(f""Deleted temp: {str(name)}"")
     else:
         imbyte = BytesIO()
-        ims[0].save(imbyte, ""PNG"")
         imbyte.seek(0)
         file = discord.File(imbyte, f""{sprite}..png"")
-        await interaction.followup.send(content=f""{sprite}:"", file=file)
 
 # @say.autocomplete(""font"")
 # async def font_autocomplete(interaction: discord.Interaction, current: str):
@@ -373,23 +437,39 @@ async def sprite(interaction: discord.Interaction, sprite: str, animated: bool=F
 #         for i in [""g"",""gg""] if current.lower() in i 
 #     ]
 
 @sprite.autocomplete(""sprite"")
 async def sprite_autocomplete(interaction: discord.Interaction, current: str):
     lst = sorted([
         i.name
         for i in Path(""sprites/"").iterdir()
     ] + [""Random""])
     return [app_commands.Choice(name=i, value=i) for i in lst if current.lower() in i.lower()][:25]
 
-@dog.error
-@random_dog.error
-@sprite.error
-async def dog_error(interaction: discord.Interaction, error):
-    if not interaction.response.is_done():
-        await interaction.response.send_message(""<:Pizza_Depressaroli:967482279670718474> Something went wrong."")
-    else:
-        await interaction.followup.send(""<:Pizza_Depressaroli:967482279670718474> Something went wrong."")
-    raise getattr(error,""original"",error)
 
 @tree.command(guild=TEST_GUILD, description=""Get a palette from the game!"")
 async def palette(interaction: discord.Interaction, palette: str=""Random""):
@@ -425,4 +505,15 @@ async def palette_autocomplete(interaction: discord.Interaction, current: str):
 async def hair(interaction: discord.Interaction):
     await interaction.response.send_message(content=""https://cdn.discordapp.com/attachments/947900270992556033/967877113099219004/unknown.png"", ephemeral=True)
 
 client.run(TOKEN)"
OK;16;JdavisBro;Chicorobot;1180ade27b0dd268fde91e9315cc044bb85d52f3;"Changes list in description

crop to transparency
animation fps
sprite command arg descriptions
use frame
frames command
save images when creating (instead of keeping them all in memory)";" import discord
 from discord import app_commands
 from PIL import Image, ImageChops#, ImageDraw, ImageFont
+import numpy
 
 with Path(""TOKEN.txt"").open(""r"") as f:
     TOKEN = f.readline().rstrip()
@@ -105,6 +104,18 @@ async def on_ready():
     
     print(""LOGGED IN!"")
 
+@tree.error
+async def command_error(interaction: discord.Interaction, error):
+    error = getattr(error, ""original"", error)
+    if isinstance(error, app_commands.CheckFailure):
+        await interaction.response.send_message(""<:Pizza_Angry:967482622194372650> You're not allowed to do that."", ephemeral=True)
+        return
+    if not interaction.response.is_done():
+        await interaction.response.send_message(""<:Pizza_Depressaroli:967482279670718474> Something went wrong."")
+    else:
+        await interaction.followup.send(""<:Pizza_Depressaroli:967482279670718474> Something went wrong."")
+    raise error
+
 def clamp(value, min, max):
     return min if value <= min else max if value >= max else value
 
@@ -302,69 +313,122 @@ async def hair_autocomplete(interaction: discord.Interaction, current:str):
 
 
 @tree.command(guild=TEST_GUILD, description=""Show a sprite."")
+@app_commands.describe(sprite=""The sprite to show"", animated=""If True, sends an animated gif"", animation_fps=""If animated. Sets the FPS of the animation"", crop_transparency=""Removes any blank area around the image"", use_frame=""If not animated, choose a frame of the sprite to send (starts at 1)"")
+async def sprite(interaction: discord.Interaction, sprite: str, animated: bool=False, animation_fps: int=10, crop_transparency: bool=True, use_frame: str=None):
+    sprite = to_titlecase(sprite.replace("" "",""_""))
+
+    try:
+        animation_fps = round(animation_fps)
+        if animation_fps < 1:
+            animation_fps = 1
+    except ValueError:
+        await interaction.response.send_message(""FPS incorrect."")
+        return
+
     if animated and not imagemagick:
         await interaction.response.send_message(content=""Animated sprites are unavailable, making non animated version."")
         animated = False
+    
+    await interaction.response.defer()
+    
     if sprite.lower() == ""random"":
         sprite = random.choice([i.name for i in Path(""sprites/"").iterdir()])
+    
+    ims = None
     spriteDir = Path(f""sprites/{sprite}/"")
+    
     layers = sorted([i for i in spriteDir.iterdir()], key=lambda item: item.stem)
+    
     if sprite.endswith(""_A""):
         layers += [i/""1"" for i in Path(""sprites/"").glob(""_"".join(sprite.split(""_"")[:-1])+""_*"") if i != spriteDir]
+    
     i = 0
+    if animated:
+        temp = tempfile.mkdtemp()
+        temp = Path(temp)
+        print(f""Making temp: {str(temp)}"")
+    
+    msg = await interaction.followup.send(content=""Making Image"" + (""s and saving to PNG (1/2)"" if animated else """"))
+
+    crop = None
+
+    zero_use_frame = None
+    if use_frame:
+        try:
+            zero_use_frame = f""{int(use_frame)-1:02}""
+        except ValueError:
+            pass
+        files = list(layers[0].glob(f""{zero_use_frame}.png""))
+        if not files:
+            await msg.edit(""Invalid frame. Use `/frames` to check available frames!"")
+            return
+    else:
+        files = sorted(layers[0].iterdir(), key=lambda path: path.stem)
+
+    for filePath in files:
         im = Image.open(filePath).convert(""RGBA"")
         for layer in layers[1:]:
             name = filePath.name
             if (layer / name).exists():
                 im2 = Image.open(layer / name).convert(""RGBA"")
                 im.alpha_composite(im2)
+        if crop_transparency:
+            imnp = numpy.array(im)
+            imnp = numpy.where(imnp[:, :, 3] > 0)
+            try:
+                if not crop:
+                    crop  = [imnp[1].min(), imnp[0].min(), imnp[1].max(), imnp[0].max()]
+                else:
+                    crop[0] = min(crop[0], imnp[1].min())
+                    crop[1] = min(crop[1], imnp[0].min())
+                    crop[2] = max(crop[2], imnp[1].max())
+                    crop[3] = max(crop[3], imnp[0].max())
+            except ValueError:
+                pass # Blank Image
+        if animated:
+            im.save(temp / f""{i:02}.png"")
+            i += 1
+        else:
+            if crop_transparency and crop:
+                ims = im.crop(box=crop)
+            else:
+                ims = im
+
     if animated:
+        await msg.edit(content=""Converting PNGs to GIF (2/2)"")
+        addcrop = f""-crop {crop[2]-crop[0]}x{crop[3]-crop[1]}+{crop[0]}+{crop[1]} +repage "" if crop_transparency else """"
+        print(crop, addcrop)
         process = await asyncio.create_subprocess_shell(
+            f""{imagemagick} -delay 1x{animation_fps} -loop 0 -dispose Background {addcrop}{temp / '*.png'} {temp / 'out.gif'}"",
             stdout=asyncio.subprocess.PIPE,
             stderr=asyncio.subprocess.PIPE
         )
         await process.communicate()
+        if process.returncode != 0: # Error
             await msg.edit(content=""<:Pizza_Depressaroli:967482279670718474> Something went wrong (gif conversion error)."")
             print(f""GIF CONVERSION ERROR: {process.stderr}"")
             try:
+                shutil.rmtree(temp)
             except PermissionError:
                 print(""Failed to delete file."")
             else:
+                print(f""Deleted temp: {str(temp)}"")
             return
+        file = discord.File(temp / ""out.gif"", f""{sprite}.gif"")
+        await msg.edit(content=f""{sprite} at {animation_fps} fps:"", attachments=[file])
+        del file # Release out.gif
         try:
+            shutil.rmtree(temp)
         except PermissionError:
             print(""Failed to delete file."")
         else:
+            print(f""Deleted temp: {str(temp)}"")
     else:
         imbyte = BytesIO()
+        ims.save(imbyte, ""PNG"")
         imbyte.seek(0)
         file = discord.File(imbyte, f""{sprite}..png"")
+        await msg.edit(content=f""{sprite}{(' frame ' + use_frame) if use_frame else ''}:"", attachments=[file])
 
 # @say.autocomplete(""font"")
 # async def font_autocomplete(interaction: discord.Interaction, current: str):
@@ -373,23 +437,39 @@ async def sprite(interaction: discord.Interaction, sprite: str, animated: bool=F
 #         for i in [""g"",""gg""] if current.lower() in i 
 #     ]
 
+@tree.command(guild=TEST_GUILD, description=""Lists frames for a specific sprite"")
+async def frames(interaction: discord.Interaction, sprite: str):
+    if not (Path(f""sprites/"") / sprite).exists():
+        await interaction.response.send_message(content=""Incorrect Sprite"")
+        return
+    spriteDir = Path(f""sprites/"") / sprite / ""1""    
+    layers = sorted([i for i in spriteDir.iterdir()], key=lambda item: item.stem)
+    files = sorted(spriteDir.iterdir(), key=lambda path: path.stem)
+    numbers = []
+    strings = []
+    for file in files:
+        try:
+            numbers.append(int(file.stem))
+        except ValueError:
+            strings.append(file.stem)
+    out = """"
+    if numbers:
+        out += f""Frames {min(numbers)+1} to {max(numbers)+1}\n""
+    if strings:
+        out += f""Frames: `{'`, `'.join(strings)}`""
+    if not out:
+        out = ""None""
+    await interaction.response.send_message(content=out)
+
 @sprite.autocomplete(""sprite"")
+@frames.autocomplete(""sprite"")
 async def sprite_autocomplete(interaction: discord.Interaction, current: str):
     lst = sorted([
         i.name
         for i in Path(""sprites/"").iterdir()
     ] + [""Random""])
     return [app_commands.Choice(name=i, value=i) for i in lst if current.lower() in i.lower()][:25]
 
 
 @tree.command(guild=TEST_GUILD, description=""Get a palette from the game!"")
 async def palette(interaction: discord.Interaction, palette: str=""Random""):
@@ -425,4 +505,15 @@ async def palette_autocomplete(interaction: discord.Interaction, current: str):
 async def hair(interaction: discord.Interaction):
     await interaction.response.send_message(content=""https://cdn.discordapp.com/attachments/947900270992556033/967877113099219004/unknown.png"", ephemeral=True)
 
+def is_me():
+    def predicate(interaction: discord.Interaction) -> bool:
+        return interaction.user.id == 105725338541101056
+    return app_commands.check(predicate)
+
+@tree.command(guild=TEST_GUILD, description=""Death."")
+@is_me()
+async def die(interaction: discord.Interaction):
+    await interaction.response.send_message(content=""I hath been slayn."")
+    await client.close()
+
 client.run(TOKEN)"
KO;16;jaredliw;juejin-toolbox;7ff0fd3abade52ba1fd6d8578ecd0156138b8096;Encode/decode history records to save memory;" 
 
 class Direction(Enum):
-    LEFT = ""L""
-    RIGHT = ""R""
-    UP = ""U""
-    DOWN = ""D""
 
 
 class NumberPuzzle:
@@ -149,7 +149,6 @@ def __move_piece(self, from_x: int, from_y: int, to_x: int, to_y: int) -> None:
         self.__full_history.append((""move"", from_x, from_y, to_x, to_y))
 
     # noinspection PyTypeHints
-    # Concatenate two numbers, just like strings
     def __concat_numbers(self, from_x: int, to_x: int, y: int) -> Tuple[Tuple[int, int], Tuple[int, Literal[0.7], int]]:
         num1, num2 = self[from_x, y], self[to_x, y]
         # Result is evaluating from left to right, swap the numbers if to_x < from_x
@@ -166,6 +165,7 @@ def __concat_numbers(self, from_x: int, to_x: int, y: int) -> Tuple[Tuple[int, i
         return (to_x, y), (num2, 0.7, num1) if from_x > to_x else (num1, 0.7, num2)
 
     # noinspection PyTypeHints
     def __eval_numbers(self, symbol_x: int, y: int) \
             -> Tuple[Tuple[int, int], Tuple[int, Literal[0.3, 0.4, 0.5, 0.6], int]]:
         # Make sure that the expression is evaluated from left to right
@@ -188,6 +188,7 @@ def __eval_numbers(self, symbol_x: int, y: int) \
         self.__full_history.append((""eval"", val1, symbol, val2, symbol_x, y))
         return (symbol_x, y), (val1, symbol, val2)
 
     def __find_destination_and_move(self, x: int, y: int, direction: Direction) -> Tuple[int, int]:
         is_increasing = direction in (Direction.RIGHT, Direction.DOWN)
         is_moving_horizontally = direction in (Direction.LEFT, Direction.RIGHT)
@@ -217,6 +218,17 @@ def __find_destination_and_move(self, x: int, y: int, direction: Direction) -> T
             self.__move_piece(x, y, x, loc)
             return x, loc
 
     # noinspection PyTypeHints
     def move(self, x: int, y: int, direction: Direction) \
             -> Tuple[Tuple[int, int], Tuple[int, Literal[0.3, 0.4, 0.5, 0.6, 0.7], int] | None]:
@@ -268,7 +280,7 @@ def move(self, x: int, y: int, direction: Direction) \
         if original_x == x and original_y == y:
             self.__full_history.pop()
         else:
-            self.history.append((original_x, original_y, direction))
         return (x, y), operands
 
     def is_solved(self) -> bool:
@@ -286,15 +298,15 @@ def reset(self) -> None:
         """"""
         self.undo(len(self.__full_history))
 
-    def restore_state_to(self, history: List[Tuple[int, int, Direction]]) -> None:
         longest_common_prefix_length = 0
-        for longest_common_prefix_length, (this, that) in enumerate(zip_longest(self.history, history)):
             if this != that:
                 break
 
-        self.undo(len(self.history) - longest_common_prefix_length)
-        for args in history[longest_common_prefix_length:]:
-            self.move(*args)
 
     def undo(self, move_count: int = 1) -> None:
         """"""Return the state of the puzzle to `move_count` number of moves before."
OK;16;jaredliw;juejin-toolbox;7ff0fd3abade52ba1fd6d8578ecd0156138b8096;Encode/decode history records to save memory;" 
 
 class Direction(Enum):
+    LEFT = 0
+    RIGHT = 1
+    UP = 2
+    DOWN = 3
 
 
 class NumberPuzzle:
@@ -149,7 +149,6 @@ def __move_piece(self, from_x: int, from_y: int, to_x: int, to_y: int) -> None:
         self.__full_history.append((""move"", from_x, from_y, to_x, to_y))
 
     # noinspection PyTypeHints
     def __concat_numbers(self, from_x: int, to_x: int, y: int) -> Tuple[Tuple[int, int], Tuple[int, Literal[0.7], int]]:
         num1, num2 = self[from_x, y], self[to_x, y]
         # Result is evaluating from left to right, swap the numbers if to_x < from_x
@@ -166,6 +165,7 @@ def __concat_numbers(self, from_x: int, to_x: int, y: int) -> Tuple[Tuple[int, i
         return (to_x, y), (num2, 0.7, num1) if from_x > to_x else (num1, 0.7, num2)
 
     # noinspection PyTypeHints
+    # Concatenate two numbers, just like strings
     def __eval_numbers(self, symbol_x: int, y: int) \
             -> Tuple[Tuple[int, int], Tuple[int, Literal[0.3, 0.4, 0.5, 0.6], int]]:
         # Make sure that the expression is evaluated from left to right
@@ -188,6 +188,7 @@ def __eval_numbers(self, symbol_x: int, y: int) \
         self.__full_history.append((""eval"", val1, symbol, val2, symbol_x, y))
         return (symbol_x, y), (val1, symbol, val2)
 
+    # noinspection PyTypeHints
     def __find_destination_and_move(self, x: int, y: int, direction: Direction) -> Tuple[int, int]:
         is_increasing = direction in (Direction.RIGHT, Direction.DOWN)
         is_moving_horizontally = direction in (Direction.LEFT, Direction.RIGHT)
@@ -217,6 +218,17 @@ def __find_destination_and_move(self, x: int, y: int, direction: Direction) -> T
             self.__move_piece(x, y, x, loc)
             return x, loc
 
+    def encode_history_record(self, x: int, y: int, direction: Direction) -> int:
+        return x << (self.WIDTH.bit_length() + 2) | y << 2 | direction.value
+
+    def decode_history_record(self, value: int) -> Tuple[int, int, Direction]:
+        direction = Direction(value & 0b11)
+
+        value >>= 2
+        y = value & ((1 << self.WIDTH.bit_length()) - 1)
+        x = value >> self.WIDTH.bit_length()
+        return x, y, direction
+
     # noinspection PyTypeHints
     def move(self, x: int, y: int, direction: Direction) \
             -> Tuple[Tuple[int, int], Tuple[int, Literal[0.3, 0.4, 0.5, 0.6, 0.7], int] | None]:
@@ -268,7 +280,7 @@ def move(self, x: int, y: int, direction: Direction) \
         if original_x == x and original_y == y:
             self.__full_history.pop()
         else:
+            self.history.append(self.encode_history_record(original_x, original_y, direction))
         return (x, y), operands
 
     def is_solved(self) -> bool:
@@ -286,15 +298,15 @@ def reset(self) -> None:
         """"""
         self.undo(len(self.__full_history))
 
+    def restore_state_to(self, history: List[int], *, offset: int = 0) -> None:
         longest_common_prefix_length = 0
+        for longest_common_prefix_length, (this, that) in enumerate(zip_longest(self.history[offset:], history)):
             if this != that:
                 break
 
+        self.undo(len(self.history[offset:]) - longest_common_prefix_length)
+        for item in history[longest_common_prefix_length:]:
+            self.move(*self.decode_history_record(item))
 
     def undo(self, move_count: int = 1) -> None:
         """"""Return the state of the puzzle to `move_count` number of moves before."
KO;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def val_dataloader(self):
@@ -45,6 +46,7 @@ def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def test_dataloader(self):
@@ -53,6 +55,7 @@ def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def __collate_fn(self, batch):"
OK;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def val_dataloader(self):
@@ -45,6 +46,7 @@ def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def test_dataloader(self):
@@ -53,6 +55,7 @@ def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def __collate_fn(self, batch):"
KO;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
OK;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
+
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
KO;17;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
-        return self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
-            single_layer_size = (json_data['layer_width'], json_data['layer_height'])
-            asset_image_path = str(self.data_folder / artboard['layerassets'])
-            asset_image_rgb = Image.open(asset_image_path).convert('RGB')
-            asset_image_tensor = self.img_transform(asset_image_rgb)
-            images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
-            data.append((images, names, bboxes, colors, classes, labels))
         return data
 
 "
OK;17;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
+from typing import Any, Dict, List
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
+        self.artboard_detail: List[Dict[str, Any]] = []
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
+        artboard = self.index_json[idx]
+        json_data = self.artboard_detail[idx]
+        single_layer_size = (json_data['layer_width'], json_data['layer_height'])
+        asset_image_path = str(self.data_folder / artboard['layerassets'])
+        asset_image_rgb = Image.open(asset_image_path).convert('RGB')
+        asset_image_tensor = self.img_transform(asset_image_rgb)
+        images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
+        return images, *self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
+            self.artboard_detail.append(json_data)
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
+            data.append((names, bboxes, colors, classes, labels))
         return data
 
 "
KO;18;delivey;momar;feae0690a07f22d5b79a65f73ade2bb943b3a7e7;"get_movies doesn't return discarded; saves some memory";"def __init__(self, directory, data_filename):
         self.movies = []
         self.discarded = []
         self.first_time = True
 
         datafile = open(data_filename)
         self.data = json.load(datafile)
@@ -77,7 +78,7 @@ def get_movies(self):
         self.get_movie_data()
         self.save_data()
         if self.first_time: self.first_time = False
-        return self.data, self.discarded
 
     def get_imdb_data(self, id, name):
         try:
@@ -136,9 +137,9 @@ def get_imdb_id(self, name):
 
     def show_movies(self):
         if self.first_time:
-            movies, discarded = self.get_movies()
         else:
-            movies, discarded = self.data, self.discarded
         sorted_movies = dict(sorted(movies.items(), key=lambda item: item[1]['data'][""rating""], reverse=True))
 
         idx = 0
@@ -170,6 +171,7 @@ def showDiscarded(self):
 
     def showGenre(self, command):
         genre = command.split("" "")[1]
 
     def doCommand(self, com):
         comdict = {"
OK;18;delivey;momar;feae0690a07f22d5b79a65f73ade2bb943b3a7e7;"get_movies doesn't return discarded; saves some memory";"def __init__(self, directory, data_filename):
         self.movies = []
         self.discarded = []
         self.first_time = True
+        self.genre = ""all""
 
         datafile = open(data_filename)
         self.data = json.load(datafile)
@@ -77,7 +78,7 @@ def get_movies(self):
         self.get_movie_data()
         self.save_data()
         if self.first_time: self.first_time = False
+        return self.data
 
     def get_imdb_data(self, id, name):
         try:
@@ -136,9 +137,9 @@ def get_imdb_id(self, name):
 
     def show_movies(self):
         if self.first_time:
+            movies = self.get_movies()
         else:
+            movies = self.data
         sorted_movies = dict(sorted(movies.items(), key=lambda item: item[1]['data'][""rating""], reverse=True))
 
         idx = 0
@@ -170,6 +171,7 @@ def showDiscarded(self):
 
     def showGenre(self, command):
         genre = command.split("" "")[1]
+        manager.genre = genre
 
     def doCommand(self, com):
         comdict = {"
KO;18;flatironinstitute;inferelator-velocity;9c71ef58915ce852c19e6148ee91f44d38f61884;Add memory for time flag;"def test_ksearch_regression(self):
             mse
         )
 
     def test_knn_select_stack_regression(self):
 
         opt_pc, opt_k, local_ks = knn_noise2self(
@@ -94,3 +104,28 @@ def test_knn_select_stack_regression(self):
 
         self.assertEqual(opt_pc, 3)
         self.assertEqual(opt_k, 4)
\ No newline at end of file"
OK;18;flatironinstitute;inferelator-velocity;9c71ef58915ce852c19e6148ee91f44d38f61884;Add memory for time flag;"def test_ksearch_regression(self):
             mse
         )
 
+    def test_ksearch_regression_sparse(self):
+
+        mse = _search_k(sps.csr_matrix(EXPR), DIST, np.arange(1, 7), X_compare=EXPR)
+        self.assertEqual(np.argmin(mse), 4)
+
+        npt.assert_almost_equal(
+            np.array([234.314, 166.83420601, 149.88290938, 143.72348837, 138.18590639, 139.83859323]),
+            mse
+        )
+
     def test_knn_select_stack_regression(self):
 
         opt_pc, opt_k, local_ks = knn_noise2self(
@@ -94,3 +104,28 @@ def test_knn_select_stack_regression(self):
 
         self.assertEqual(opt_pc, 3)
         self.assertEqual(opt_k, 4)
+
+    def test_knn_select_stack_regression_sparse(self):
+
+        opt_pc, opt_k, local_ks = knn_noise2self(
+            sps.csr_matrix(EXPR),
+            np.arange(1, 11),
+            np.array([3, 5, 7]),
+            verbose=True
+        )
+
+        self.assertEqual(opt_pc, 3)
+        self.assertEqual(opt_k, 4)
+
+    def test_knn_select_stack_regression_sparse_but_flagged(self):
+
+        opt_pc, opt_k, local_ks = knn_noise2self(
+            sps.csr_matrix(EXPR),
+            np.arange(1, 11),
+            np.array([3, 5, 7]),
+            verbose=True,
+            use_sparse=False
+        )
+
+        self.assertEqual(opt_pc, 3)
+        self.assertEqual(opt_k, 4)
\ No newline at end of file"
KO;18;flatironinstitute;inferelator-velocity;9c71ef58915ce852c19e6148ee91f44d38f61884;Add memory for time flag;"def knn_noise2self(
     neighbors=None,
     npcs=None,
     verbose=False,
-    metric='euclidean'
 ):
     """"""
     Select an optimal set of graph parameters based on noise2self
@@ -49,7 +51,10 @@ def knn_noise2self(
         defaults to False
     :type verbose: bool, optional
     :param metric: Distance metric to use, defaults to 'euclidean'
-    :type metric: str
     :return: Global optimal # of PCs,
         global optimal k,
         local optimal k for each observation
@@ -70,6 +75,8 @@ def knn_noise2self(
 
     mses = np.zeros((len(npcs), len(neighbors)))
 
     # Search for the smallest MSE for each n_pcs / k combination
     for i, pc in tqdm.tqdm(enumerate(npcs)):
         tqdm.tqdm.write(f""Searching graphs from {pc} PCs"")
@@ -87,9 +94,10 @@ def knn_noise2self(
         data_obj.obsp['distances'] = data_obj.obsp['distances'].astype(np.float32)
 
         mses[i, :] = _search_k(
-            data_obj.X,
             data_obj.obsp['distances'],
-            neighbors
         )
 
     op_pc = np.argmin(np.min(mses, axis=1))
@@ -113,26 +121,36 @@ def knn_noise2self(
     set_diag(data_obj.obsp['distances'], 0)
     data_obj.obsp['distances'] = data_obj.obsp['distances'].astype(np.float32)
 
     # Search for the optimal number of k for each obs
     # For the global optimal n_pc
     local_k = np.argmin(
         _search_k(
-            data_obj.X,
             data_obj.obsp['distances'],
-            np.arange(np.max(neighbors)),
-            by_row=True
         ),
         axis=0
     )
 
-    return npcs[op_pc], neighbors[op_k], neighbors[local_k]
 
 
 def _search_k(
     X,
     graph,
     k,
-    by_row=False
 ):
     """"""
     Find optimal number of neighbors for a given graph
@@ -154,6 +172,8 @@ def _search_k(
     n, _ = X.shape
     n_k = len(k)
 
     mses = np.zeros(n_k) if not by_row else np.zeros((n_k, n))
 
     for i in tqdm.trange(n_k):
@@ -172,8 +192,8 @@ def _search_k(
 
         # Calculate mean squared error
         mses[i] = mean_squared_error(
-            X,
-            dot(k_graph, X),
             by_row=by_row
         )
 "
OK;18;flatironinstitute;inferelator-velocity;9c71ef58915ce852c19e6148ee91f44d38f61884;Add memory for time flag;"def knn_noise2self(
     neighbors=None,
     npcs=None,
     verbose=False,
+    metric='euclidean',
+    return_errors=False,
+    use_sparse=True
 ):
     """"""
     Select an optimal set of graph parameters based on noise2self
@@ -49,7 +51,10 @@ def knn_noise2self(
         defaults to False
     :type verbose: bool, optional
     :param metric: Distance metric to use, defaults to 'euclidean'
+    :type metric: str, optional
+    :param return_errors: Return the mean square errors for global
+        neighbor/nPC search, defaults to False
+    :type return_errors: bool, optional
     :return: Global optimal # of PCs,
         global optimal k,
         local optimal k for each observation
@@ -70,6 +75,8 @@ def knn_noise2self(
 
     mses = np.zeros((len(npcs), len(neighbors)))
 
+    expr_data = data_obj.X if use_sparse or not sps.issparse(data_obj.X) else data_obj.X.A
+
     # Search for the smallest MSE for each n_pcs / k combination
     for i, pc in tqdm.tqdm(enumerate(npcs)):
         tqdm.tqdm.write(f""Searching graphs from {pc} PCs"")
@@ -87,9 +94,10 @@ def knn_noise2self(
         data_obj.obsp['distances'] = data_obj.obsp['distances'].astype(np.float32)
 
         mses[i, :] = _search_k(
+            expr_data,
             data_obj.obsp['distances'],
+            neighbors,
+            X_compare=expr_data
         )
 
     op_pc = np.argmin(np.min(mses, axis=1))
@@ -113,26 +121,36 @@ def knn_noise2self(
     set_diag(data_obj.obsp['distances'], 0)
     data_obj.obsp['distances'] = data_obj.obsp['distances'].astype(np.float32)
 
+    local_neighbors = np.arange(np.max(neighbors))
+
     # Search for the optimal number of k for each obs
     # For the global optimal n_pc
     local_k = np.argmin(
         _search_k(
+            expr_data,
             data_obj.obsp['distances'],
+            local_neighbors,
+            by_row=True,
+            X_compare=expr_data
         ),
         axis=0
     )
 
+    optimals = npcs[op_pc], neighbors[op_k], local_neighbors[local_k]
+
+    if return_errors:
+        return optimals, mses
+
+    else:
+        return optimals
 
 
 def _search_k(
     X,
     graph,
     k,
+    by_row=False,
+    X_compare=None
 ):
     """"""
     Find optimal number of neighbors for a given graph
@@ -154,6 +172,8 @@ def _search_k(
     n, _ = X.shape
     n_k = len(k)
 
+    X_compare = X_compare if X_compare is not None else X
+
     mses = np.zeros(n_k) if not by_row else np.zeros((n_k, n))
 
     for i in tqdm.trange(n_k):
@@ -172,8 +192,8 @@ def _search_k(
 
         # Calculate mean squared error
         mses[i] = mean_squared_error(
+            X_compare,
+            dot(k_graph, X, dense=True),
             by_row=by_row
         )
 "
KO;18;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
-        return self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
-            single_layer_size = (json_data['layer_width'], json_data['layer_height'])
-            asset_image_path = str(self.data_folder / artboard['layerassets'])
-            asset_image_rgb = Image.open(asset_image_path).convert('RGB')
-            asset_image_tensor = self.img_transform(asset_image_rgb)
-            images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
-            data.append((images, names, bboxes, colors, classes, labels))
         return data
 
 "
OK;18;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
+from typing import Any, Dict, List
 
 import torch
 import torchvision.transforms as T
@@ -35,24 +36,28 @@ def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
+        self.artboard_detail: List[Dict[str, Any]] = []
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
+        artboard = self.index_json[idx]
+        json_data = self.artboard_detail[idx]
+        single_layer_size = (json_data['layer_width'], json_data['layer_height'])
+        asset_image_path = str(self.data_folder / artboard['layerassets'])
+        asset_image_rgb = Image.open(asset_image_path).convert('RGB')
+        asset_image_tensor = self.img_transform(asset_image_rgb)
+        images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
+        return images, *self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
+            self.artboard_detail.append(json_data)
             names = []
             bboxes = []
             colors = []
@@ -78,7 +83,7 @@ def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
+            data.append((names, bboxes, colors, classes, labels))
         return data
 
 "
KO;19;Barchid;dvs_ssl;31546c820be8d9644e2a5aa1cef41cb22d6279fc;try snn + in memory;"def main(args):
         timesteps,
         data_dir='data',
         barlow_transf=args['transforms'],
-        in_memory=False,
-        num_workers=3
     )
-    
     if 'ssl_loss' in args:
         ssl = args['ssl_loss']
     else:
         ssl = ssl_loss
 
     module = SSLModule(
         n_classes=datamodule.num_classes,
         learning_rate=learning_rate,
         epochs=epochs,
         ssl_loss=ssl,
-        timesteps=timesteps
     )
 
     name = f""{dataset}_{ssl}""
@@ -83,13 +90,16 @@ def main(args):
     # # exp - barlow
     # trans = ['flip', 'background_activity', 'reverse', 'flip_polarity']
     # main({'transforms': trans, 'ssl_loss': 'barlow_twins'})
-    
-    # exp - vicreg
     trans = ['flip', 'background_activity', 'reverse', 'flip_polarity']
-    main({'transforms': trans, 'ssl_loss': 'vicreg'})
-    
     exit()
-    
     # exp 2 (+crop)
     trans = ['flip', 'background_activity', 'reverse', 'flip_polarity', 'crop']
     main({'transforms': trans})"
OK;19;Barchid;dvs_ssl;31546c820be8d9644e2a5aa1cef41cb22d6279fc;try snn + in memory;"def main(args):
         timesteps,
         data_dir='data',
         barlow_transf=args['transforms'],
+        in_memory=True,
+        num_workers=0
     )
+
     if 'ssl_loss' in args:
         ssl = args['ssl_loss']
     else:
         ssl = ssl_loss
 
+    if 'mode' in args:
+        mode = args['mode']
+    else:
+        mode = 'cnn'
+
     module = SSLModule(
         n_classes=datamodule.num_classes,
         learning_rate=learning_rate,
         epochs=epochs,
         ssl_loss=ssl,
+        timesteps=timesteps,
+        enc1=mode,
+        enc2=mode
     )
 
     name = f""{dataset}_{ssl}""
@@ -83,13 +90,16 @@ def main(args):
     # # exp - barlow
     # trans = ['flip', 'background_activity', 'reverse', 'flip_polarity']
     # main({'transforms': trans, 'ssl_loss': 'barlow_twins'})
+
+    # # exp - vicreg
+    # trans = ['flip', 'background_activity', 'reverse', 'flip_polarity']
+    # main({'transforms': trans, 'ssl_loss': 'vicreg'})
+
+    # exp - try snn
     trans = ['flip', 'background_activity', 'reverse', 'flip_polarity']
+    main({'transforms': trans, 'ssl_loss': 'barlow_twins', 'mode': 'snn'})
     exit()
+
     # exp 2 (+crop)
     trans = ['flip', 'background_activity', 'reverse', 'flip_polarity', 'crop']
     main({'transforms': trans})"
KO;19;Barchid;dvs_ssl;31546c820be8d9644e2a5aa1cef41cb22d6279fc;try snn + in memory;"def setup(self, stage: Optional[str] = None) -> None:
             self.val_set = DvsMemory(self.val_set, transform=self.val_transform)
 
     def train_dataloader(self):
-        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=8, shuffle=True)
 
     def val_dataloader(self):
-        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=2, shuffle=False) #self.num_workers
 
     def test_dataloader(self):
         return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)"
OK;19;Barchid;dvs_ssl;31546c820be8d9644e2a5aa1cef41cb22d6279fc;try snn + in memory;"def setup(self, stage: Optional[str] = None) -> None:
             self.val_set = DvsMemory(self.val_set, transform=self.val_transform)
 
     def train_dataloader(self):
+        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=8 if not self.in_memory else 0, shuffle=True)
 
     def val_dataloader(self):
+        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=2 if not self.in_memory else 0, shuffle=False) #self.num_workers
 
     def test_dataloader(self):
         return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)"
KO;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
+
+.idea/
\ No newline at end of file"
KO;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;19;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"+import pygame
+from pygame.locals import (
+    K_UP,
+    K_DOWN,
+    K_LEFT,
+    K_RIGHT,
+    K_ESCAPE,
+    KEYDOWN,
+    QUIT,
+)
+mat = [
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0],
+  [0,0,0,0,0]
+]
+def placematrix(i,j):
+  #insx = round(x//100)
+  #insy = round(y//100)
+  mat[i][j] = 1
+
+def print_matrix(matrix):
+  for i in matrix:
+    print(*i)
+  print()
+pygame.init()
+
+def isStop(i,j):
+    return i >= 5 or mat[i + 1][j] == 1
+def contgame(i,j):
+    placematrix(i,j)
+    print_matrix(mat)
+    #shuld summon new block
+
+screen = pygame.display.set_mode([600, 600])
+i = 0
+j = 2
+running = True
+x = 200
+y = 100
+z = 0
+while running:
+    for event in pygame.event.get():
+        if event.type == KEYDOWN:
+            if event.key == K_ESCAPE:
+                running = False
+            if event.key == K_LEFT:
+                if j >= 0:
+                    #x = x - 100
+                    j = j - 1
+            if event.key == K_RIGHT:
+                if j <= 4:
+                    #x = x + 100
+                    j = j + 1
+    screen.fill((0, 0, 0))
+
+    pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
+    if i < 9:
+        if z == 500:
+            if isStop(i,j):
+                contgame(i,j)
+                running = False
+            #y = y + 100
+            i = i + 1
+            z = 0
+        z = z + 1
+
+    pygame.display.flip()
+
+pygame.quit()
\ No newline at end of file"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+import requests
+import socket
+
+
+class BingTranslate:
+    def __init__(self):
+        self.connect = False
+        self.checkConnect()
+
+    def warning(self):
+        print(""De su dung bing search , chay cau lenh: 'node app.js' de bat server"")
+
+    def checkConnect(self):
+        a_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        location = (""127.0.0.1"", 8989)
+        try:
+            a_socket.connect(location)
+            a_socket.close()
+            self.connect = True
+        except:
+            self.connect = False
+            self.warning()
+
+    def translate(self, text):
+        if not self.connect:
+            self.checkConnect()
+            return """"
+        try:
+            url = ""http://127.0.0.1:8989/translate?text="" + text
+            r = requests.get(url)
+            if r.status_code == 200:
+                return r.text
+            else:
+                return text
+        except Exception as e:
+            print(e)
+            return text"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+const express = require(""express"");
+const { translate } = require(""bing-translate-api"");
+
+const app = express();
+const port = 8989;
+
+async function dich(text) {
+  try {
+    const res = await translate(text, null, ""vi"", true);
+    return res.translation;
+  } catch (err) {
+    console.error(err);
+  }
+}
+
+app.get(""/translate"", function (req, res) {
+  var text = req.query.text;
+  console.log(text);
+  if (text != null || text != """") {
+    dich(text).then((result) => {
+      res.send(result);
+      console.log(""Nghia: "" + result + ""\n"");
+    });
+  } else {
+    res.send("""");
+  }
+});
+
+app.listen(port, function () {
+  console.log(""Your app running on port "" + port);
+});"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# editorconfig.org
+root = true
+
+[*]
+indent_style = space
+indent_size = 2
+end_of_line = lf
+charset = utf-8
+trim_trailing_whitespace = true
+insert_final_newline = true
+
+[*.md]
+trim_trailing_whitespace = false"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# Created by .ignore support plugin (hsz.mobi)
+### Node template
+# Logs
+logs
+*.log
+npm-debug.log*
+yarn-debug.log*
+yarn-error.log*
+
+# Runtime data
+pids
+*.pid
+*.seed
+*.pid.lock
+
+# Directory for instrumented libs generated by jscoverage/JSCover
+lib-cov
+
+# Coverage directory used by tools like istanbul
+coverage
+
+# nyc test coverage
+.nyc_output
+
+# Grunt intermediate storage (http://gruntjs.com/creating-plugins#storing-task-files)
+.grunt
+
+# Bower dependency directory (https://bower.io/)
+bower_components
+
+# node-waf configuration
+.lock-wscript
+
+# Compiled binary addons (https://nodejs.org/api/addons.html)
+build/Release
+
+# Dependency directories
+node_modules/
+jspm_packages/
+
+# TypeScript v1 declaration files
+typings/
+
+# Optional npm cache directory
+.npm
+
+# Optional eslint cache
+.eslintcache
+
+# Optional REPL history
+.node_repl_history
+
+# Output of 'npm pack'
+*.tgz
+
+# Yarn Integrity file
+.yarn-integrity
+
+# dotenv environment variables file
+.env
+
+# parcel-bundler cache (https://parceljs.org/)
+.cache
+
+# vuepress build output
+.vuepress/dist
+
+# Serverless directories
+.serverless
+
+# IDE
+.idea
+
+# Service worker
+sw.*
+"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+MIT License
+
+Copyright (c) 2021-2022 Zhongxiang.Wang
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the ""Software""), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# bing-translate-api
+[![NPM version](https://img.shields.io/npm/v/bing-translate-api.svg?style=flat)](https://www.npmjs.org/package/bing-translate-api)
+[![Auto Test](https://github.com/plainheart/bing-translate-api/actions/workflows/autotest.yml/badge.svg)](https://github.com/plainheart/bing-translate-api/actions/workflows/autotest.yml)
+[![NPM Downloads](https://img.shields.io/npm/dm/bing-translate-api.svg)](https://npmcharts.com/compare/bing-translate-api?minimal=true)
+[![License](https://img.shields.io/npm/l/bing-translate-api.svg)](https://github.com/plainheart/bing-translate-api/blob/master/LICENSE)
+
+A **simple** and **free** API for [Bing Translator](https://bing.com/translator) for Node.js.
+
+## Install 
+
+```
+npm install bing-translate-api
+```
+
+## Usage
+
+From auto-detected language to English:
+
+```js
+const { translate } = require('bing-translate-api');
+
+translate('你好', null, 'en', true).then(res => {
+  console.log(res.translation);
+}).catch(err => {
+  console.error(err);
+});
+```
+
+Translation result
+
+```js
+{
+  ""text"": ""你好"",
+  ""userLang"": ""auto-detect"",
+  ""translation"": ""Hello"",
+  // `correctedText` is returned only when `correct` is set as `true`
+  // supported since v1.1.0
+  ""correctedText"": """",
+  ""language"": {
+    ""to"": ""en"",
+    ""from"": ""zh-Hans"",
+    // supported since v1.1.0
+    ""score"": 1
+  }
+}
+```
+
+## API
+
+### translate(text, [from], [to], [correct], [raw], [userAgent], [proxyAgents])
+
+#### _text_
+
+Type: `string`
+
+The text to be translated, can't be blank. The **maximum** text length is **1000**.
+
+##### _from_
+Type: `string` Default: `auto-detect`
+
+The language code of source text.
+**MUST** be `auto-detect` or one of the codes/names (not case sensitive) contained in [lang.json](https://github.com/plainheart/bing-translate-api/blob/master/src/lang.json)
+
+##### _to_
+Type: `string` Default: `en`
+
+The language in which the text should be translated.
+**MUST** be one of the codes/names (not case sensitive) contained in [lang.json](https://github.com/plainheart/bing-translate-api/blob/master/src/lang.json).
+
+##### _correct_
+Type: `boolean` Default: `false` Since: `v1.1.0`
+
+Whether to correct the input text.
+
+Note that:
+1) There is currently a **limit** of **50 characters** for correction service.
+2) **Only** [the languages in the list](https://github.com/plainheart/bing-translate-api/blob/master/src/lang.js#L99-L120) are supported to be corrected.
+
+##### _raw_
+Type: `boolean` Default: `false`
+
+Whether the translation result contains raw response from Bing API.
+
+##### _userAgent_
+Type: `string`
+
+The header value of `user-agent` used in API requests. 
+
+Default:
+```
+Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.88 Safari/537.36
+```
+
+##### _proxyAgents_
+Type: [`Got['Agents']`](https://github.com/sindresorhus/got/blob/main/source/core/options.ts#L50-L54) Default: `undefined` Since: `v2.4.0`
+
+Set [agents](https://github.com/sindresorhus/got/blob/main/documentation/tips.md#proxying) of [`got`](https://github.com/sindresorhus/got) for proxy.
+
+## License
+
+MIT &copy; 2021-2022 [plainheart](https://github.com/plainheart).
+
+## Thanks
+
+Great thanks to [Bing Translator](https://bing.com/translator) for providing so excellent translation service."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""name"": ""bing-translate-api"",
+  ""version"": ""2.6.0"",
+  ""lockfileVersion"": 2,
+  ""requires"": true,
+  ""packages"": {
+    """": {
+      ""name"": ""bing-translate-api"",
+      ""version"": ""2.6.0"",
+      ""license"": ""MIT"",
+      ""dependencies"": {
+        ""got"": ""^11.8.3""
+      },
+      ""devDependencies"": {
+        ""cheerio"": ""^1.0.0-rc.10""
+      }
+    },
+    ""node_modules/@sindresorhus/is"": {
+      ""version"": ""4.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/@sindresorhus/is/-/is-4.0.1.tgz"",
+      ""integrity"": ""sha512-Qm9hBEBu18wt1PO2flE7LPb30BHMQt1eQgbV76YntdNk73XZGpn3izvGTYxbGgzXKgbCjiia0uxTd3aTNQrY/g=="",
+      ""engines"": {
+        ""node"": "">=10""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sindresorhus/is?sponsor=1""
+      }
+    },
+    ""node_modules/@szmarczak/http-timer"": {
+      ""version"": ""4.0.5"",
+      ""resolved"": ""https://registry.npmjs.org/@szmarczak/http-timer/-/http-timer-4.0.5.tgz"",
+      ""integrity"": ""sha512-PyRA9sm1Yayuj5OIoJ1hGt2YISX45w9WcFbh6ddT0Z/0yaFxOtGLInr4jUfU1EAFVs0Yfyfev4RNwBlUaHdlDQ=="",
+      ""dependencies"": {
+        ""defer-to-connect"": ""^2.0.0""
+      },
+      ""engines"": {
+        ""node"": "">=10""
+      }
+    },
+    ""node_modules/@types/cacheable-request"": {
+      ""version"": ""6.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/@types/cacheable-request/-/cacheable-request-6.0.1.tgz"",
+      ""integrity"": ""sha512-ykFq2zmBGOCbpIXtoVbz4SKY5QriWPh3AjyU4G74RYbtt5yOc5OfaY75ftjg7mikMOla1CTGpX3lLbuJh8DTrQ=="",
+      ""dependencies"": {
+        ""@types/http-cache-semantics"": ""*"",
+        ""@types/keyv"": ""*"",
+        ""@types/node"": ""*"",
+        ""@types/responselike"": ""*""
+      }
+    },
+    ""node_modules/@types/http-cache-semantics"": {
+      ""version"": ""4.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/@types/http-cache-semantics/-/http-cache-semantics-4.0.0.tgz"",
+      ""integrity"": ""sha512-c3Xy026kOF7QOTn00hbIllV1dLR9hG9NkSrLQgCVs8NF6sBU+VGWjD3wLPhmh1TYAc7ugCFsvHYMN4VcBN1U1A==""
+    },
+    ""node_modules/@types/keyv"": {
+      ""version"": ""3.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/@types/keyv/-/keyv-3.1.1.tgz"",
+      ""integrity"": ""sha512-MPtoySlAZQ37VoLaPcTHCu1RWJ4llDkULYZIzOYxlhxBqYPB0RsRlmMU0R6tahtFe27mIdkHV+551ZWV4PLmVw=="",
+      ""dependencies"": {
+        ""@types/node"": ""*""
+      }
+    },
+    ""node_modules/@types/node"": {
+      ""version"": ""15.12.5"",
+      ""resolved"": ""https://registry.npmjs.org/@types/node/-/node-15.12.5.tgz"",
+      ""integrity"": ""sha512-se3yX7UHv5Bscf8f1ERKvQOD6sTyycH3hdaoozvaLxgUiY5lIGEeH37AD0G0Qi9kPqihPn0HOfd2yaIEN9VwEg==""
+    },
+    ""node_modules/@types/responselike"": {
+      ""version"": ""1.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/@types/responselike/-/responselike-1.0.0.tgz"",
+      ""integrity"": ""sha512-85Y2BjiufFzaMIlvJDvTTB8Fxl2xfLo4HgmHzVBz08w4wDePCTjYw66PdrolO0kzli3yam/YCgRufyo1DdQVTA=="",
+      ""dependencies"": {
+        ""@types/node"": ""*""
+      }
+    },
+    ""node_modules/boolbase"": {
+      ""version"": ""1.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/boolbase/-/boolbase-1.0.0.tgz"",
+      ""integrity"": ""sha1-aN/1++YMUes3cl6p4+0xDcwed24="",
+      ""dev"": true
+    },
+    ""node_modules/cacheable-lookup"": {
+      ""version"": ""5.0.4"",
+      ""resolved"": ""https://registry.npmjs.org/cacheable-lookup/-/cacheable-lookup-5.0.4.tgz"",
+      ""integrity"": ""sha512-2/kNscPhpcxrOigMZzbiWF7dz8ilhb/nIHU3EyZiXWXpeq/au8qJ8VhdftMkty3n7Gj6HIGalQG8oiBNB3AJgA=="",
+      ""engines"": {
+        ""node"": "">=10.6.0""
+      }
+    },
+    ""node_modules/cacheable-request"": {
+      ""version"": ""7.0.2"",
+      ""resolved"": ""https://registry.npmjs.org/cacheable-request/-/cacheable-request-7.0.2.tgz"",
+      ""integrity"": ""sha512-pouW8/FmiPQbuGpkXQ9BAPv/Mo5xDGANgSNXzTzJ8DrKGuXOssM4wIQRjfanNRh3Yu5cfYPvcorqbhg2KIJtew=="",
+      ""dependencies"": {
+        ""clone-response"": ""^1.0.2"",
+        ""get-stream"": ""^5.1.0"",
+        ""http-cache-semantics"": ""^4.0.0"",
+        ""keyv"": ""^4.0.0"",
+        ""lowercase-keys"": ""^2.0.0"",
+        ""normalize-url"": ""^6.0.1"",
+        ""responselike"": ""^2.0.0""
+      },
+      ""engines"": {
+        ""node"": "">=8""
+      }
+    },
+    ""node_modules/cheerio"": {
+      ""version"": ""1.0.0-rc.10"",
+      ""resolved"": ""https://registry.npmjs.org/cheerio/-/cheerio-1.0.0-rc.10.tgz"",
+      ""integrity"": ""sha512-g0J0q/O6mW8z5zxQ3A8E8J1hUgp4SMOvEoW/x84OwyHKe/Zccz83PVT4y5Crcr530FV6NgmKI1qvGTKVl9XXVw=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""cheerio-select"": ""^1.5.0"",
+        ""dom-serializer"": ""^1.3.2"",
+        ""domhandler"": ""^4.2.0"",
+        ""htmlparser2"": ""^6.1.0"",
+        ""parse5"": ""^6.0.1"",
+        ""parse5-htmlparser2-tree-adapter"": ""^6.0.1"",
+        ""tslib"": ""^2.2.0""
+      },
+      ""engines"": {
+        ""node"": "">= 6""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/cheeriojs/cheerio?sponsor=1""
+      }
+    },
+    ""node_modules/cheerio-select"": {
+      ""version"": ""1.5.0"",
+      ""resolved"": ""https://registry.npmjs.org/cheerio-select/-/cheerio-select-1.5.0.tgz"",
+      ""integrity"": ""sha512-qocaHPv5ypefh6YNxvnbABM07KMxExbtbfuJoIie3iZXX1ERwYmJcIiRrr9H05ucQP1k28dav8rpdDgjQd8drg=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""css-select"": ""^4.1.3"",
+        ""css-what"": ""^5.0.1"",
+        ""domelementtype"": ""^2.2.0"",
+        ""domhandler"": ""^4.2.0"",
+        ""domutils"": ""^2.7.0""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/fb55""
+      }
+    },
+    ""node_modules/clone-response"": {
+      ""version"": ""1.0.2"",
+      ""resolved"": ""https://registry.npmjs.org/clone-response/-/clone-response-1.0.2.tgz"",
+      ""integrity"": ""sha1-0dyXOSAxTfZ/vrlCI7TuNQI56Ws="",
+      ""dependencies"": {
+        ""mimic-response"": ""^1.0.0""
+      }
+    },
+    ""node_modules/css-select"": {
+      ""version"": ""4.2.1"",
+      ""resolved"": ""https://registry.npmjs.org/css-select/-/css-select-4.2.1.tgz"",
+      ""integrity"": ""sha512-/aUslKhzkTNCQUB2qTX84lVmfia9NyjP3WpDGtj/WxhwBzWBYUV3DgUpurHTme8UTPcPlAD1DJ+b0nN/t50zDQ=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""boolbase"": ""^1.0.0"",
+        ""css-what"": ""^5.1.0"",
+        ""domhandler"": ""^4.3.0"",
+        ""domutils"": ""^2.8.0"",
+        ""nth-check"": ""^2.0.1""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/fb55""
+      }
+    },
+    ""node_modules/css-what"": {
+      ""version"": ""5.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/css-what/-/css-what-5.1.0.tgz"",
+      ""integrity"": ""sha512-arSMRWIIFY0hV8pIxZMEfmMI47Wj3R/aWpZDDxWYCPEiOMv6tfOrnpDtgxBYPEQD4V0Y/958+1TdC3iWTFcUPw=="",
+      ""dev"": true,
+      ""engines"": {
+        ""node"": "">= 6""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/fb55""
+      }
+    },
+    ""node_modules/decompress-response"": {
+      ""version"": ""6.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/decompress-response/-/decompress-response-6.0.0.tgz"",
+      ""integrity"": ""sha512-aW35yZM6Bb/4oJlZncMH2LCoZtJXTRxES17vE3hoRiowU2kWHaJKFkSBDnDR+cm9J+9QhXmREyIfv0pji9ejCQ=="",
+      ""dependencies"": {
+        ""mimic-response"": ""^3.1.0""
+      },
+      ""engines"": {
+        ""node"": "">=10""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/sindresorhus""
+      }
+    },
+    ""node_modules/decompress-response/node_modules/mimic-response"": {
+      ""version"": ""3.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/mimic-response/-/mimic-response-3.1.0.tgz"",
+      ""integrity"": ""sha512-z0yWI+4FDrrweS8Zmt4Ej5HdJmky15+L2e6Wgn3+iK5fWzb6T3fhNFq2+MeTRb064c6Wr4N/wv0DzQTjNzHNGQ=="",
+      ""engines"": {
+        ""node"": "">=10""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/sindresorhus""
+      }
+    },
+    ""node_modules/defer-to-connect"": {
+      ""version"": ""2.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/defer-to-connect/-/defer-to-connect-2.0.1.tgz"",
+      ""integrity"": ""sha512-4tvttepXG1VaYGrRibk5EwJd1t4udunSOVMdLSAL6mId1ix438oPwPZMALY41FCijukO1L0twNcGsdzS7dHgDg=="",
+      ""engines"": {
+        ""node"": "">=10""
+      }
+    },
+    ""node_modules/dom-serializer"": {
+      ""version"": ""1.3.2"",
+      ""resolved"": ""https://registry.npmjs.org/dom-serializer/-/dom-serializer-1.3.2.tgz"",
+      ""integrity"": ""sha512-5c54Bk5Dw4qAxNOI1pFEizPSjVsx5+bpJKmL2kPn8JhBUq2q09tTCa3mjijun2NfK78NMouDYNMBkOrPZiS+ig=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""domelementtype"": ""^2.0.1"",
+        ""domhandler"": ""^4.2.0"",
+        ""entities"": ""^2.0.0""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/cheeriojs/dom-serializer?sponsor=1""
+      }
+    },
+    ""node_modules/domelementtype"": {
+      ""version"": ""2.2.0"",
+      ""resolved"": ""https://registry.npmjs.org/domelementtype/-/domelementtype-2.2.0.tgz"",
+      ""integrity"": ""sha512-DtBMo82pv1dFtUmHyr48beiuq792Sxohr+8Hm9zoxklYPfa6n0Z3Byjj2IV7bmr2IyqClnqEQhfgHJJ5QF0R5A=="",
+      ""dev"": true,
+      ""funding"": [
+        {
+          ""type"": ""github"",
+          ""url"": ""https://github.com/sponsors/fb55""
+        }
+      ]
+    },
+    ""node_modules/domhandler"": {
+      ""version"": ""4.3.0"",
+      ""resolved"": ""https://registry.npmjs.org/domhandler/-/domhandler-4.3.0.tgz"",
+      ""integrity"": ""sha512-fC0aXNQXqKSFTr2wDNZDhsEYjCiYsDWl3D01kwt25hm1YIPyDGHvvi3rw+PLqHAl/m71MaiF7d5zvBr0p5UB2g=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""domelementtype"": ""^2.2.0""
+      },
+      ""engines"": {
+        ""node"": "">= 4""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/fb55/domhandler?sponsor=1""
+      }
+    },
+    ""node_modules/domutils"": {
+      ""version"": ""2.8.0"",
+      ""resolved"": ""https://registry.npmjs.org/domutils/-/domutils-2.8.0.tgz"",
+      ""integrity"": ""sha512-w96Cjofp72M5IIhpjgobBimYEfoPjx1Vx0BSX9P30WBdZW2WIKU0T1Bd0kz2eNZ9ikjKgHbEyKx8BB6H1L3h3A=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""dom-serializer"": ""^1.0.1"",
+        ""domelementtype"": ""^2.2.0"",
+        ""domhandler"": ""^4.2.0""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/fb55/domutils?sponsor=1""
+      }
+    },
+    ""node_modules/end-of-stream"": {
+      ""version"": ""1.4.4"",
+      ""resolved"": ""https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.4.4.tgz"",
+      ""integrity"": ""sha512-+uw1inIHVPQoaVuHzRyXd21icM+cnt4CzD5rW+NC1wjOUSTOs+Te7FOv7AhN7vS9x/oIyhLP5PR1H+phQAHu5Q=="",
+      ""dependencies"": {
+        ""once"": ""^1.4.0""
+      }
+    },
+    ""node_modules/entities"": {
+      ""version"": ""2.2.0"",
+      ""resolved"": ""https://registry.npmjs.org/entities/-/entities-2.2.0.tgz"",
+      ""integrity"": ""sha512-p92if5Nz619I0w+akJrLZH0MX0Pb5DX39XOwQTtXSdQQOaYH03S1uIQp4mhOZtAXrxq4ViO67YTiLBo2638o9A=="",
+      ""dev"": true,
+      ""funding"": {
+        ""url"": ""https://github.com/fb55/entities?sponsor=1""
+      }
+    },
+    ""node_modules/get-stream"": {
+      ""version"": ""5.2.0"",
+      ""resolved"": ""https://registry.npmjs.org/get-stream/-/get-stream-5.2.0.tgz"",
+      ""integrity"": ""sha512-nBF+F1rAZVCu/p7rjzgA+Yb4lfYXrpl7a6VmJrU8wF9I1CKvP/QwPNZHnOlwbTkY6dvtFIzFMSyQXbLoTQPRpA=="",
+      ""dependencies"": {
+        ""pump"": ""^3.0.0""
+      },
+      ""engines"": {
+        ""node"": "">=8""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/sindresorhus""
+      }
+    },
+    ""node_modules/got"": {
+      ""version"": ""11.8.3"",
+      ""resolved"": ""https://registry.npmjs.org/got/-/got-11.8.3.tgz"",
+      ""integrity"": ""sha512-7gtQ5KiPh1RtGS9/Jbv1ofDpBFuq42gyfEib+ejaRBJuj/3tQFeR5+gw57e4ipaU8c/rCjvX6fkQz2lyDlGAOg=="",
+      ""dependencies"": {
+        ""@sindresorhus/is"": ""^4.0.0"",
+        ""@szmarczak/http-timer"": ""^4.0.5"",
+        ""@types/cacheable-request"": ""^6.0.1"",
+        ""@types/responselike"": ""^1.0.0"",
+        ""cacheable-lookup"": ""^5.0.3"",
+        ""cacheable-request"": ""^7.0.2"",
+        ""decompress-response"": ""^6.0.0"",
+        ""http2-wrapper"": ""^1.0.0-beta.5.2"",
+        ""lowercase-keys"": ""^2.0.0"",
+        ""p-cancelable"": ""^2.0.0"",
+        ""responselike"": ""^2.0.0""
+      },
+      ""engines"": {
+        ""node"": "">=10.19.0""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sindresorhus/got?sponsor=1""
+      }
+    },
+    ""node_modules/htmlparser2"": {
+      ""version"": ""6.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/htmlparser2/-/htmlparser2-6.1.0.tgz"",
+      ""integrity"": ""sha512-gyyPk6rgonLFEDGoeRgQNaEUvdJ4ktTmmUh/h2t7s+M8oPpIPxgNACWa+6ESR57kXstwqPiCut0V8NRpcwgU7A=="",
+      ""dev"": true,
+      ""funding"": [
+        ""https://github.com/fb55/htmlparser2?sponsor=1"",
+        {
+          ""type"": ""github"",
+          ""url"": ""https://github.com/sponsors/fb55""
+        }
+      ],
+      ""dependencies"": {
+        ""domelementtype"": ""^2.0.1"",
+        ""domhandler"": ""^4.0.0"",
+        ""domutils"": ""^2.5.2"",
+        ""entities"": ""^2.0.0""
+      }
+    },
+    ""node_modules/http-cache-semantics"": {
+      ""version"": ""4.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/http-cache-semantics/-/http-cache-semantics-4.1.0.tgz"",
+      ""integrity"": ""sha512-carPklcUh7ROWRK7Cv27RPtdhYhUsela/ue5/jKzjegVvXDqM2ILE9Q2BGn9JZJh1g87cp56su/FgQSzcWS8cQ==""
+    },
+    ""node_modules/http2-wrapper"": {
+      ""version"": ""1.0.3"",
+      ""resolved"": ""https://registry.npmjs.org/http2-wrapper/-/http2-wrapper-1.0.3.tgz"",
+      ""integrity"": ""sha512-V+23sDMr12Wnz7iTcDeJr3O6AIxlnvT/bmaAAAP/Xda35C90p9599p0F1eHR/N1KILWSoWVAiOMFjBBXaXSMxg=="",
+      ""dependencies"": {
+        ""quick-lru"": ""^5.1.1"",
+        ""resolve-alpn"": ""^1.0.0""
+      },
+      ""engines"": {
+        ""node"": "">=10.19.0""
+      }
+    },
+    ""node_modules/json-buffer"": {
+      ""version"": ""3.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.1.tgz"",
+      ""integrity"": ""sha512-4bV5BfR2mqfQTJm+V5tPPdf+ZpuhiIvTuAB5g8kcrXOZpTT/QwwVRWBywX1ozr6lEuPdbHxwaJlm9G6mI2sfSQ==""
+    },
+    ""node_modules/keyv"": {
+      ""version"": ""4.0.3"",
+      ""resolved"": ""https://registry.npmjs.org/keyv/-/keyv-4.0.3.tgz"",
+      ""integrity"": ""sha512-zdGa2TOpSZPq5mU6iowDARnMBZgtCqJ11dJROFi6tg6kTn4nuUdU09lFyLFSaHrWqpIJ+EBq4E8/Dc0Vx5vLdA=="",
+      ""dependencies"": {
+        ""json-buffer"": ""3.0.1""
+      }
+    },
+    ""node_modules/lowercase-keys"": {
+      ""version"": ""2.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/lowercase-keys/-/lowercase-keys-2.0.0.tgz"",
+      ""integrity"": ""sha512-tqNXrS78oMOE73NMxK4EMLQsQowWf8jKooH9g7xPavRT706R6bkQJ6DY2Te7QukaZsulxa30wQ7bk0pm4XiHmA=="",
+      ""engines"": {
+        ""node"": "">=8""
+      }
+    },
+    ""node_modules/mimic-response"": {
+      ""version"": ""1.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/mimic-response/-/mimic-response-1.0.1.tgz"",
+      ""integrity"": ""sha512-j5EctnkH7amfV/q5Hgmoal1g2QHFJRraOtmx0JpIqkxhBhI/lJSl1nMpQ45hVarwNETOoWEimndZ4QK0RHxuxQ=="",
+      ""engines"": {
+        ""node"": "">=4""
+      }
+    },
+    ""node_modules/normalize-url"": {
+      ""version"": ""6.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/normalize-url/-/normalize-url-6.1.0.tgz"",
+      ""integrity"": ""sha512-DlL+XwOy3NxAQ8xuC0okPgK46iuVNAK01YN7RueYBqqFeGsBjV9XmCAzAdgt+667bCl5kPh9EqKKDwnaPG1I7A=="",
+      ""engines"": {
+        ""node"": "">=10""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/sindresorhus""
+      }
+    },
+    ""node_modules/nth-check"": {
+      ""version"": ""2.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/nth-check/-/nth-check-2.0.1.tgz"",
+      ""integrity"": ""sha512-it1vE95zF6dTT9lBsYbxvqh0Soy4SPowchj0UBGj/V6cTPnXXtQOPUbhZ6CmGzAD/rW22LQK6E96pcdJXk4A4w=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""boolbase"": ""^1.0.0""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/fb55/nth-check?sponsor=1""
+      }
+    },
+    ""node_modules/once"": {
+      ""version"": ""1.4.0"",
+      ""resolved"": ""https://registry.npmjs.org/once/-/once-1.4.0.tgz"",
+      ""integrity"": ""sha1-WDsap3WWHUsROsF9nFC6753Xa9E="",
+      ""dependencies"": {
+        ""wrappy"": ""1""
+      }
+    },
+    ""node_modules/p-cancelable"": {
+      ""version"": ""2.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/p-cancelable/-/p-cancelable-2.1.1.tgz"",
+      ""integrity"": ""sha512-BZOr3nRQHOntUjTrH8+Lh54smKHoHyur8We1V8DSMVrl5A2malOOwuJRnKRDjSnkoeBh4at6BwEnb5I7Jl31wg=="",
+      ""engines"": {
+        ""node"": "">=8""
+      }
+    },
+    ""node_modules/parse5"": {
+      ""version"": ""6.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/parse5/-/parse5-6.0.1.tgz"",
+      ""integrity"": ""sha512-Ofn/CTFzRGTTxwpNEs9PP93gXShHcTq255nzRYSKe8AkVpZY7e1fpmTfOyoIvjP5HG7Z2ZM7VS9PPhQGW2pOpw=="",
+      ""dev"": true
+    },
+    ""node_modules/parse5-htmlparser2-tree-adapter"": {
+      ""version"": ""6.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/parse5-htmlparser2-tree-adapter/-/parse5-htmlparser2-tree-adapter-6.0.1.tgz"",
+      ""integrity"": ""sha512-qPuWvbLgvDGilKc5BoicRovlT4MtYT6JfJyBOMDsKoiT+GiuP5qyrPCnR9HcPECIJJmZh5jRndyNThnhhb/vlA=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""parse5"": ""^6.0.1""
+      }
+    },
+    ""node_modules/pump"": {
+      ""version"": ""3.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/pump/-/pump-3.0.0.tgz"",
+      ""integrity"": ""sha512-LwZy+p3SFs1Pytd/jYct4wpv49HiYCqd9Rlc5ZVdk0V+8Yzv6jR5Blk3TRmPL1ft69TxP0IMZGJ+WPFU2BFhww=="",
+      ""dependencies"": {
+        ""end-of-stream"": ""^1.1.0"",
+        ""once"": ""^1.3.1""
+      }
+    },
+    ""node_modules/quick-lru"": {
+      ""version"": ""5.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/quick-lru/-/quick-lru-5.1.1.tgz"",
+      ""integrity"": ""sha512-WuyALRjWPDGtt/wzJiadO5AXY+8hZ80hVpe6MyivgraREW751X3SbhRvG3eLKOYN+8VEvqLcf3wdnt44Z4S4SA=="",
+      ""engines"": {
+        ""node"": "">=10""
+      },
+      ""funding"": {
+        ""url"": ""https://github.com/sponsors/sindresorhus""
+      }
+    },
+    ""node_modules/resolve-alpn"": {
+      ""version"": ""1.1.2"",
+      ""resolved"": ""https://registry.npmjs.org/resolve-alpn/-/resolve-alpn-1.1.2.tgz"",
+      ""integrity"": ""sha512-8OyfzhAtA32LVUsJSke3auIyINcwdh5l3cvYKdKO0nvsYSKuiLfTM5i78PJswFPT8y6cPW+L1v6/hE95chcpDA==""
+    },
+    ""node_modules/responselike"": {
+      ""version"": ""2.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/responselike/-/responselike-2.0.0.tgz"",
+      ""integrity"": ""sha512-xH48u3FTB9VsZw7R+vvgaKeLKzT6jOogbQhEe/jewwnZgzPcnyWui2Av6JpoYZF/91uueC+lqhWqeURw5/qhCw=="",
+      ""dependencies"": {
+        ""lowercase-keys"": ""^2.0.0""
+      }
+    },
+    ""node_modules/tslib"": {
+      ""version"": ""2.3.1"",
+      ""resolved"": ""https://registry.npmjs.org/tslib/-/tslib-2.3.1.tgz"",
+      ""integrity"": ""sha512-77EbyPPpMz+FRFRuAFlWMtmgUWGe9UOG2Z25NqCwiIjRhOf5iKGuzSe5P2w1laq+FkRy4p+PCuVkJSGkzTEKVw=="",
+      ""dev"": true
+    },
+    ""node_modules/wrappy"": {
+      ""version"": ""1.0.2"",
+      ""resolved"": ""https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz"",
+      ""integrity"": ""sha1-tSQ9jz7BqjXxNkYFvA0QNuMKtp8=""
+    }
+  },
+  ""dependencies"": {
+    ""@sindresorhus/is"": {
+      ""version"": ""4.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/@sindresorhus/is/-/is-4.0.1.tgz"",
+      ""integrity"": ""sha512-Qm9hBEBu18wt1PO2flE7LPb30BHMQt1eQgbV76YntdNk73XZGpn3izvGTYxbGgzXKgbCjiia0uxTd3aTNQrY/g==""
+    },
+    ""@szmarczak/http-timer"": {
+      ""version"": ""4.0.5"",
+      ""resolved"": ""https://registry.npmjs.org/@szmarczak/http-timer/-/http-timer-4.0.5.tgz"",
+      ""integrity"": ""sha512-PyRA9sm1Yayuj5OIoJ1hGt2YISX45w9WcFbh6ddT0Z/0yaFxOtGLInr4jUfU1EAFVs0Yfyfev4RNwBlUaHdlDQ=="",
+      ""requires"": {
+        ""defer-to-connect"": ""^2.0.0""
+      }
+    },
+    ""@types/cacheable-request"": {
+      ""version"": ""6.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/@types/cacheable-request/-/cacheable-request-6.0.1.tgz"",
+      ""integrity"": ""sha512-ykFq2zmBGOCbpIXtoVbz4SKY5QriWPh3AjyU4G74RYbtt5yOc5OfaY75ftjg7mikMOla1CTGpX3lLbuJh8DTrQ=="",
+      ""requires"": {
+        ""@types/http-cache-semantics"": ""*"",
+        ""@types/keyv"": ""*"",
+        ""@types/node"": ""*"",
+        ""@types/responselike"": ""*""
+      }
+    },
+    ""@types/http-cache-semantics"": {
+      ""version"": ""4.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/@types/http-cache-semantics/-/http-cache-semantics-4.0.0.tgz"",
+      ""integrity"": ""sha512-c3Xy026kOF7QOTn00hbIllV1dLR9hG9NkSrLQgCVs8NF6sBU+VGWjD3wLPhmh1TYAc7ugCFsvHYMN4VcBN1U1A==""
+    },
+    ""@types/keyv"": {
+      ""version"": ""3.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/@types/keyv/-/keyv-3.1.1.tgz"",
+      ""integrity"": ""sha512-MPtoySlAZQ37VoLaPcTHCu1RWJ4llDkULYZIzOYxlhxBqYPB0RsRlmMU0R6tahtFe27mIdkHV+551ZWV4PLmVw=="",
+      ""requires"": {
+        ""@types/node"": ""*""
+      }
+    },
+    ""@types/node"": {
+      ""version"": ""15.12.5"",
+      ""resolved"": ""https://registry.npmjs.org/@types/node/-/node-15.12.5.tgz"",
+      ""integrity"": ""sha512-se3yX7UHv5Bscf8f1ERKvQOD6sTyycH3hdaoozvaLxgUiY5lIGEeH37AD0G0Qi9kPqihPn0HOfd2yaIEN9VwEg==""
+    },
+    ""@types/responselike"": {
+      ""version"": ""1.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/@types/responselike/-/responselike-1.0.0.tgz"",
+      ""integrity"": ""sha512-85Y2BjiufFzaMIlvJDvTTB8Fxl2xfLo4HgmHzVBz08w4wDePCTjYw66PdrolO0kzli3yam/YCgRufyo1DdQVTA=="",
+      ""requires"": {
+        ""@types/node"": ""*""
+      }
+    },
+    ""boolbase"": {
+      ""version"": ""1.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/boolbase/-/boolbase-1.0.0.tgz"",
+      ""integrity"": ""sha1-aN/1++YMUes3cl6p4+0xDcwed24="",
+      ""dev"": true
+    },
+    ""cacheable-lookup"": {
+      ""version"": ""5.0.4"",
+      ""resolved"": ""https://registry.npmjs.org/cacheable-lookup/-/cacheable-lookup-5.0.4.tgz"",
+      ""integrity"": ""sha512-2/kNscPhpcxrOigMZzbiWF7dz8ilhb/nIHU3EyZiXWXpeq/au8qJ8VhdftMkty3n7Gj6HIGalQG8oiBNB3AJgA==""
+    },
+    ""cacheable-request"": {
+      ""version"": ""7.0.2"",
+      ""resolved"": ""https://registry.npmjs.org/cacheable-request/-/cacheable-request-7.0.2.tgz"",
+      ""integrity"": ""sha512-pouW8/FmiPQbuGpkXQ9BAPv/Mo5xDGANgSNXzTzJ8DrKGuXOssM4wIQRjfanNRh3Yu5cfYPvcorqbhg2KIJtew=="",
+      ""requires"": {
+        ""clone-response"": ""^1.0.2"",
+        ""get-stream"": ""^5.1.0"",
+        ""http-cache-semantics"": ""^4.0.0"",
+        ""keyv"": ""^4.0.0"",
+        ""lowercase-keys"": ""^2.0.0"",
+        ""normalize-url"": ""^6.0.1"",
+        ""responselike"": ""^2.0.0""
+      }
+    },
+    ""cheerio"": {
+      ""version"": ""1.0.0-rc.10"",
+      ""resolved"": ""https://registry.npmjs.org/cheerio/-/cheerio-1.0.0-rc.10.tgz"",
+      ""integrity"": ""sha512-g0J0q/O6mW8z5zxQ3A8E8J1hUgp4SMOvEoW/x84OwyHKe/Zccz83PVT4y5Crcr530FV6NgmKI1qvGTKVl9XXVw=="",
+      ""dev"": true,
+      ""requires"": {
+        ""cheerio-select"": ""^1.5.0"",
+        ""dom-serializer"": ""^1.3.2"",
+        ""domhandler"": ""^4.2.0"",
+        ""htmlparser2"": ""^6.1.0"",
+        ""parse5"": ""^6.0.1"",
+        ""parse5-htmlparser2-tree-adapter"": ""^6.0.1"",
+        ""tslib"": ""^2.2.0""
+      }
+    },
+    ""cheerio-select"": {
+      ""version"": ""1.5.0"",
+      ""resolved"": ""https://registry.npmjs.org/cheerio-select/-/cheerio-select-1.5.0.tgz"",
+      ""integrity"": ""sha512-qocaHPv5ypefh6YNxvnbABM07KMxExbtbfuJoIie3iZXX1ERwYmJcIiRrr9H05ucQP1k28dav8rpdDgjQd8drg=="",
+      ""dev"": true,
+      ""requires"": {
+        ""css-select"": ""^4.1.3"",
+        ""css-what"": ""^5.0.1"",
+        ""domelementtype"": ""^2.2.0"",
+        ""domhandler"": ""^4.2.0"",
+        ""domutils"": ""^2.7.0""
+      }
+    },
+    ""clone-response"": {
+      ""version"": ""1.0.2"",
+      ""resolved"": ""https://registry.npmjs.org/clone-response/-/clone-response-1.0.2.tgz"",
+      ""integrity"": ""sha1-0dyXOSAxTfZ/vrlCI7TuNQI56Ws="",
+      ""requires"": {
+        ""mimic-response"": ""^1.0.0""
+      }
+    },
+    ""css-select"": {
+      ""version"": ""4.2.1"",
+      ""resolved"": ""https://registry.npmjs.org/css-select/-/css-select-4.2.1.tgz"",
+      ""integrity"": ""sha512-/aUslKhzkTNCQUB2qTX84lVmfia9NyjP3WpDGtj/WxhwBzWBYUV3DgUpurHTme8UTPcPlAD1DJ+b0nN/t50zDQ=="",
+      ""dev"": true,
+      ""requires"": {
+        ""boolbase"": ""^1.0.0"",
+        ""css-what"": ""^5.1.0"",
+        ""domhandler"": ""^4.3.0"",
+        ""domutils"": ""^2.8.0"",
+        ""nth-check"": ""^2.0.1""
+      }
+    },
+    ""css-what"": {
+      ""version"": ""5.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/css-what/-/css-what-5.1.0.tgz"",
+      ""integrity"": ""sha512-arSMRWIIFY0hV8pIxZMEfmMI47Wj3R/aWpZDDxWYCPEiOMv6tfOrnpDtgxBYPEQD4V0Y/958+1TdC3iWTFcUPw=="",
+      ""dev"": true
+    },
+    ""decompress-response"": {
+      ""version"": ""6.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/decompress-response/-/decompress-response-6.0.0.tgz"",
+      ""integrity"": ""sha512-aW35yZM6Bb/4oJlZncMH2LCoZtJXTRxES17vE3hoRiowU2kWHaJKFkSBDnDR+cm9J+9QhXmREyIfv0pji9ejCQ=="",
+      ""requires"": {
+        ""mimic-response"": ""^3.1.0""
+      },
+      ""dependencies"": {
+        ""mimic-response"": {
+          ""version"": ""3.1.0"",
+          ""resolved"": ""https://registry.npmjs.org/mimic-response/-/mimic-response-3.1.0.tgz"",
+          ""integrity"": ""sha512-z0yWI+4FDrrweS8Zmt4Ej5HdJmky15+L2e6Wgn3+iK5fWzb6T3fhNFq2+MeTRb064c6Wr4N/wv0DzQTjNzHNGQ==""
+        }
+      }
+    },
+    ""defer-to-connect"": {
+      ""version"": ""2.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/defer-to-connect/-/defer-to-connect-2.0.1.tgz"",
+      ""integrity"": ""sha512-4tvttepXG1VaYGrRibk5EwJd1t4udunSOVMdLSAL6mId1ix438oPwPZMALY41FCijukO1L0twNcGsdzS7dHgDg==""
+    },
+    ""dom-serializer"": {
+      ""version"": ""1.3.2"",
+      ""resolved"": ""https://registry.npmjs.org/dom-serializer/-/dom-serializer-1.3.2.tgz"",
+      ""integrity"": ""sha512-5c54Bk5Dw4qAxNOI1pFEizPSjVsx5+bpJKmL2kPn8JhBUq2q09tTCa3mjijun2NfK78NMouDYNMBkOrPZiS+ig=="",
+      ""dev"": true,
+      ""requires"": {
+        ""domelementtype"": ""^2.0.1"",
+        ""domhandler"": ""^4.2.0"",
+        ""entities"": ""^2.0.0""
+      }
+    },
+    ""domelementtype"": {
+      ""version"": ""2.2.0"",
+      ""resolved"": ""https://registry.npmjs.org/domelementtype/-/domelementtype-2.2.0.tgz"",
+      ""integrity"": ""sha512-DtBMo82pv1dFtUmHyr48beiuq792Sxohr+8Hm9zoxklYPfa6n0Z3Byjj2IV7bmr2IyqClnqEQhfgHJJ5QF0R5A=="",
+      ""dev"": true
+    },
+    ""domhandler"": {
+      ""version"": ""4.3.0"",
+      ""resolved"": ""https://registry.npmjs.org/domhandler/-/domhandler-4.3.0.tgz"",
+      ""integrity"": ""sha512-fC0aXNQXqKSFTr2wDNZDhsEYjCiYsDWl3D01kwt25hm1YIPyDGHvvi3rw+PLqHAl/m71MaiF7d5zvBr0p5UB2g=="",
+      ""dev"": true,
+      ""requires"": {
+        ""domelementtype"": ""^2.2.0""
+      }
+    },
+    ""domutils"": {
+      ""version"": ""2.8.0"",
+      ""resolved"": ""https://registry.npmjs.org/domutils/-/domutils-2.8.0.tgz"",
+      ""integrity"": ""sha512-w96Cjofp72M5IIhpjgobBimYEfoPjx1Vx0BSX9P30WBdZW2WIKU0T1Bd0kz2eNZ9ikjKgHbEyKx8BB6H1L3h3A=="",
+      ""dev"": true,
+      ""requires"": {
+        ""dom-serializer"": ""^1.0.1"",
+        ""domelementtype"": ""^2.2.0"",
+        ""domhandler"": ""^4.2.0""
+      }
+    },
+    ""end-of-stream"": {
+      ""version"": ""1.4.4"",
+      ""resolved"": ""https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.4.4.tgz"",
+      ""integrity"": ""sha512-+uw1inIHVPQoaVuHzRyXd21icM+cnt4CzD5rW+NC1wjOUSTOs+Te7FOv7AhN7vS9x/oIyhLP5PR1H+phQAHu5Q=="",
+      ""requires"": {
+        ""once"": ""^1.4.0""
+      }
+    },
+    ""entities"": {
+      ""version"": ""2.2.0"",
+      ""resolved"": ""https://registry.npmjs.org/entities/-/entities-2.2.0.tgz"",
+      ""integrity"": ""sha512-p92if5Nz619I0w+akJrLZH0MX0Pb5DX39XOwQTtXSdQQOaYH03S1uIQp4mhOZtAXrxq4ViO67YTiLBo2638o9A=="",
+      ""dev"": true
+    },
+    ""get-stream"": {
+      ""version"": ""5.2.0"",
+      ""resolved"": ""https://registry.npmjs.org/get-stream/-/get-stream-5.2.0.tgz"",
+      ""integrity"": ""sha512-nBF+F1rAZVCu/p7rjzgA+Yb4lfYXrpl7a6VmJrU8wF9I1CKvP/QwPNZHnOlwbTkY6dvtFIzFMSyQXbLoTQPRpA=="",
+      ""requires"": {
+        ""pump"": ""^3.0.0""
+      }
+    },
+    ""got"": {
+      ""version"": ""11.8.3"",
+      ""resolved"": ""https://registry.npmjs.org/got/-/got-11.8.3.tgz"",
+      ""integrity"": ""sha512-7gtQ5KiPh1RtGS9/Jbv1ofDpBFuq42gyfEib+ejaRBJuj/3tQFeR5+gw57e4ipaU8c/rCjvX6fkQz2lyDlGAOg=="",
+      ""requires"": {
+        ""@sindresorhus/is"": ""^4.0.0"",
+        ""@szmarczak/http-timer"": ""^4.0.5"",
+        ""@types/cacheable-request"": ""^6.0.1"",
+        ""@types/responselike"": ""^1.0.0"",
+        ""cacheable-lookup"": ""^5.0.3"",
+        ""cacheable-request"": ""^7.0.2"",
+        ""decompress-response"": ""^6.0.0"",
+        ""http2-wrapper"": ""^1.0.0-beta.5.2"",
+        ""lowercase-keys"": ""^2.0.0"",
+        ""p-cancelable"": ""^2.0.0"",
+        ""responselike"": ""^2.0.0""
+      }
+    },
+    ""htmlparser2"": {
+      ""version"": ""6.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/htmlparser2/-/htmlparser2-6.1.0.tgz"",
+      ""integrity"": ""sha512-gyyPk6rgonLFEDGoeRgQNaEUvdJ4ktTmmUh/h2t7s+M8oPpIPxgNACWa+6ESR57kXstwqPiCut0V8NRpcwgU7A=="",
+      ""dev"": true,
+      ""requires"": {
+        ""domelementtype"": ""^2.0.1"",
+        ""domhandler"": ""^4.0.0"",
+        ""domutils"": ""^2.5.2"",
+        ""entities"": ""^2.0.0""
+      }
+    },
+    ""http-cache-semantics"": {
+      ""version"": ""4.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/http-cache-semantics/-/http-cache-semantics-4.1.0.tgz"",
+      ""integrity"": ""sha512-carPklcUh7ROWRK7Cv27RPtdhYhUsela/ue5/jKzjegVvXDqM2ILE9Q2BGn9JZJh1g87cp56su/FgQSzcWS8cQ==""
+    },
+    ""http2-wrapper"": {
+      ""version"": ""1.0.3"",
+      ""resolved"": ""https://registry.npmjs.org/http2-wrapper/-/http2-wrapper-1.0.3.tgz"",
+      ""integrity"": ""sha512-V+23sDMr12Wnz7iTcDeJr3O6AIxlnvT/bmaAAAP/Xda35C90p9599p0F1eHR/N1KILWSoWVAiOMFjBBXaXSMxg=="",
+      ""requires"": {
+        ""quick-lru"": ""^5.1.1"",
+        ""resolve-alpn"": ""^1.0.0""
+      }
+    },
+    ""json-buffer"": {
+      ""version"": ""3.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.1.tgz"",
+      ""integrity"": ""sha512-4bV5BfR2mqfQTJm+V5tPPdf+ZpuhiIvTuAB5g8kcrXOZpTT/QwwVRWBywX1ozr6lEuPdbHxwaJlm9G6mI2sfSQ==""
+    },
+    ""keyv"": {
+      ""version"": ""4.0.3"",
+      ""resolved"": ""https://registry.npmjs.org/keyv/-/keyv-4.0.3.tgz"",
+      ""integrity"": ""sha512-zdGa2TOpSZPq5mU6iowDARnMBZgtCqJ11dJROFi6tg6kTn4nuUdU09lFyLFSaHrWqpIJ+EBq4E8/Dc0Vx5vLdA=="",
+      ""requires"": {
+        ""json-buffer"": ""3.0.1""
+      }
+    },
+    ""lowercase-keys"": {
+      ""version"": ""2.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/lowercase-keys/-/lowercase-keys-2.0.0.tgz"",
+      ""integrity"": ""sha512-tqNXrS78oMOE73NMxK4EMLQsQowWf8jKooH9g7xPavRT706R6bkQJ6DY2Te7QukaZsulxa30wQ7bk0pm4XiHmA==""
+    },
+    ""mimic-response"": {
+      ""version"": ""1.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/mimic-response/-/mimic-response-1.0.1.tgz"",
+      ""integrity"": ""sha512-j5EctnkH7amfV/q5Hgmoal1g2QHFJRraOtmx0JpIqkxhBhI/lJSl1nMpQ45hVarwNETOoWEimndZ4QK0RHxuxQ==""
+    },
+    ""normalize-url"": {
+      ""version"": ""6.1.0"",
+      ""resolved"": ""https://registry.npmjs.org/normalize-url/-/normalize-url-6.1.0.tgz"",
+      ""integrity"": ""sha512-DlL+XwOy3NxAQ8xuC0okPgK46iuVNAK01YN7RueYBqqFeGsBjV9XmCAzAdgt+667bCl5kPh9EqKKDwnaPG1I7A==""
+    },
+    ""nth-check"": {
+      ""version"": ""2.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/nth-check/-/nth-check-2.0.1.tgz"",
+      ""integrity"": ""sha512-it1vE95zF6dTT9lBsYbxvqh0Soy4SPowchj0UBGj/V6cTPnXXtQOPUbhZ6CmGzAD/rW22LQK6E96pcdJXk4A4w=="",
+      ""dev"": true,
+      ""requires"": {
+        ""boolbase"": ""^1.0.0""
+      }
+    },
+    ""once"": {
+      ""version"": ""1.4.0"",
+      ""resolved"": ""https://registry.npmjs.org/once/-/once-1.4.0.tgz"",
+      ""integrity"": ""sha1-WDsap3WWHUsROsF9nFC6753Xa9E="",
+      ""requires"": {
+        ""wrappy"": ""1""
+      }
+    },
+    ""p-cancelable"": {
+      ""version"": ""2.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/p-cancelable/-/p-cancelable-2.1.1.tgz"",
+      ""integrity"": ""sha512-BZOr3nRQHOntUjTrH8+Lh54smKHoHyur8We1V8DSMVrl5A2malOOwuJRnKRDjSnkoeBh4at6BwEnb5I7Jl31wg==""
+    },
+    ""parse5"": {
+      ""version"": ""6.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/parse5/-/parse5-6.0.1.tgz"",
+      ""integrity"": ""sha512-Ofn/CTFzRGTTxwpNEs9PP93gXShHcTq255nzRYSKe8AkVpZY7e1fpmTfOyoIvjP5HG7Z2ZM7VS9PPhQGW2pOpw=="",
+      ""dev"": true
+    },
+    ""parse5-htmlparser2-tree-adapter"": {
+      ""version"": ""6.0.1"",
+      ""resolved"": ""https://registry.npmjs.org/parse5-htmlparser2-tree-adapter/-/parse5-htmlparser2-tree-adapter-6.0.1.tgz"",
+      ""integrity"": ""sha512-qPuWvbLgvDGilKc5BoicRovlT4MtYT6JfJyBOMDsKoiT+GiuP5qyrPCnR9HcPECIJJmZh5jRndyNThnhhb/vlA=="",
+      ""dev"": true,
+      ""requires"": {
+        ""parse5"": ""^6.0.1""
+      }
+    },
+    ""pump"": {
+      ""version"": ""3.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/pump/-/pump-3.0.0.tgz"",
+      ""integrity"": ""sha512-LwZy+p3SFs1Pytd/jYct4wpv49HiYCqd9Rlc5ZVdk0V+8Yzv6jR5Blk3TRmPL1ft69TxP0IMZGJ+WPFU2BFhww=="",
+      ""requires"": {
+        ""end-of-stream"": ""^1.1.0"",
+        ""once"": ""^1.3.1""
+      }
+    },
+    ""quick-lru"": {
+      ""version"": ""5.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/quick-lru/-/quick-lru-5.1.1.tgz"",
+      ""integrity"": ""sha512-WuyALRjWPDGtt/wzJiadO5AXY+8hZ80hVpe6MyivgraREW751X3SbhRvG3eLKOYN+8VEvqLcf3wdnt44Z4S4SA==""
+    },
+    ""resolve-alpn"": {
+      ""version"": ""1.1.2"",
+      ""resolved"": ""https://registry.npmjs.org/resolve-alpn/-/resolve-alpn-1.1.2.tgz"",
+      ""integrity"": ""sha512-8OyfzhAtA32LVUsJSke3auIyINcwdh5l3cvYKdKO0nvsYSKuiLfTM5i78PJswFPT8y6cPW+L1v6/hE95chcpDA==""
+    },
+    ""responselike"": {
+      ""version"": ""2.0.0"",
+      ""resolved"": ""https://registry.npmjs.org/responselike/-/responselike-2.0.0.tgz"",
+      ""integrity"": ""sha512-xH48u3FTB9VsZw7R+vvgaKeLKzT6jOogbQhEe/jewwnZgzPcnyWui2Av6JpoYZF/91uueC+lqhWqeURw5/qhCw=="",
+      ""requires"": {
+        ""lowercase-keys"": ""^2.0.0""
+      }
+    },
+    ""tslib"": {
+      ""version"": ""2.3.1"",
+      ""resolved"": ""https://registry.npmjs.org/tslib/-/tslib-2.3.1.tgz"",
+      ""integrity"": ""sha512-77EbyPPpMz+FRFRuAFlWMtmgUWGe9UOG2Z25NqCwiIjRhOf5iKGuzSe5P2w1laq+FkRy4p+PCuVkJSGkzTEKVw=="",
+      ""dev"": true
+    },
+    ""wrappy"": {
+      ""version"": ""1.0.2"",
+      ""resolved"": ""https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz"",
+      ""integrity"": ""sha1-tSQ9jz7BqjXxNkYFvA0QNuMKtp8=""
+    }
+  }
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""name"": ""bing-translate-api"",
+  ""version"": ""2.6.0"",
+  ""description"": ""A simple and free API for Bing Translator for Node.js"",
+  ""main"": ""src/index.js"",
+  ""scripts"": {
+    ""test"": ""node test/index.js && node test/lang.js"",
+    ""gen:langmap"": ""node scripts/generate-lang-map.js""
+  },
+  ""keywords"": [
+    ""bing"",
+    ""translator"",
+    ""api"",
+    ""javascript"",
+    ""free"",
+    ""node"",
+    ""translate""
+  ],
+  ""files"": [
+    ""src"",
+    ""test""
+  ],
+  ""author"": ""plainheart"",
+  ""license"": ""MIT"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/plainheart/bing-translate-api.git""
+  },
+  ""bugs"": {
+    ""url"": ""https://github.com/plainheart/bing-translate-api/issues""
+  },
+  ""homepage"": ""https://github.com/plainheart/bing-translate-api#readme"",
+  ""dependencies"": {
+    ""got"": ""^11.8.3""
+  },
+  ""devDependencies"": {
+    ""cheerio"": ""^1.0.0-rc.10""
+  }
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+const got = require('got')
+const cheerio = require('cheerio')
+const fs = require('node:fs')
+const path = require('node:path')
+
+;(async () => {
+  const { body } = await got('https://bing.com/translator', {
+    headers: {
+      'Accept-Language': 'en-US,en'
+    }
+  })
+  const $ = cheerio.load(body)
+  const options = $('#t_tgtAllLang').children('option')
+  const langMap = {}
+  for (let i = 0, len = options.length, option; i < len; i++) {
+    option = $(options[i])
+    langMap[option.attr('value')] = option.text().trim()
+  }
+  console.log('Generated language map', langMap)
+  fs.writeFileSync(
+    path.resolve(__dirname, '../src/lang.json'),
+    JSON.stringify(langMap, null, 2),
+    { charset: 'utf-8' }
+  )
+})()"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+const got = require('got')
+
+const lang = require('./lang')
+
+const TRANSLATE_API_ROOT = 'https://{tld}bing.com'
+const TRANSLATE_WEBSITE = TRANSLATE_API_ROOT + '/translator'
+const TRANSLATE_API = TRANSLATE_API_ROOT + '/ttranslatev3'
+const TRANSLATE_SPELL_CHECK_API = TRANSLATE_API_ROOT + '/tspellcheckv3'
+
+const USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.88 Safari/537.36'
+
+// PENDING: fetch from `params_RichTranslate`?
+const MAX_TEXT_LEN = 1000
+// PENDING
+const MAX_CORRECT_TEXT_LEN = 50
+
+let globalConfig
+let globalConfigPromise
+
+function replaceTld(url, tld) {
+  return url.replace('{tld}', tld ? tld + '.' : '')
+}
+
+/**
+ * refetch global config if token is expired
+ * @return {boolean} whether token is expired or not
+ */
+function isTokenExpired() {
+  if (!globalConfig) {
+    return true
+  }
+  const { tokenTs, tokenExpiryInterval } = globalConfig
+  return Date.now() - tokenTs > tokenExpiryInterval
+}
+
+/**
+ * fetch global config including `IG`, `IID`, `token`, `key`, `tokenTs`, `tokenExpiryInterval` and `cookie`
+ * @param {string} userAgent
+ * @param {import('got').Agents} proxyAgents
+ */
+async function fetchGlobalConfig(userAgent, proxyAgents) {
+  let tld
+  let IG
+  let IID
+  let token
+  let key
+  let tokenExpiryInterval
+  let isVertical
+  let frontDoorBotClassification
+  let isSignedInOrCorporateUser
+  let cookie
+  try {
+    const { body, headers, request: { redirects } } = await got(replaceTld(TRANSLATE_WEBSITE, tld), {
+      headers: {
+        'user-agent': userAgent || USER_AGENT
+      },
+      agent: proxyAgents
+    })
+
+    tld = redirects[0].match(/^https?:\/\/(\w+)\.bing\.com/)[1]
+
+    // PENDING: optional?
+    cookie = headers['set-cookie'].map(c => c.split(';')[0]).join('; ')
+
+    IG = body.match(/IG:""([^""]+)""/)[1]
+    IID = body.match(/data-iid=""([^""]+)""/)[1]
+
+    // required
+    ;[key, token, tokenExpiryInterval, isVertical, frontDoorBotClassification, isSignedInOrCorporateUser] = JSON.parse(
+      body.match(/params_RichTranslateHelper\s?=\s?([^\]]+\])/)[1]
+    )
+  } catch (e) {
+    console.error('failed to fetch global config', e)
+    throw e
+  }
+  return globalConfig = {
+    tld,
+    IG,
+    IID,
+    key,
+    token,
+    tokenTs: key,
+    tokenExpiryInterval,
+    isVertical,
+    frontDoorBotClassification,
+    isSignedInOrCorporateUser,
+    cookie,
+    // PENDING: reset count if count value is large?
+    count: 0
+  }
+}
+
+function makeRequestURL(isSpellCheck) {
+  const { IG, IID, tld, isVertical } = globalConfig
+  return replaceTld(isSpellCheck ? TRANSLATE_SPELL_CHECK_API : TRANSLATE_API, tld)
+    + '?isVertical=' + +isVertical
+    + (IG && IG.length ? '&IG=' + IG : '')
+    + (IID && IID.length ? '&IID=' + IID + '.' + (globalConfig.count++) : '')
+}
+
+function makeRequestBody(isSpellCheck, text, fromLang, toLang) {
+  const { token, key } = globalConfig
+  const body = {
+    fromLang,
+    text,
+    token,
+    key
+  }
+  if (!isSpellCheck && toLang) {
+    body.to = toLang
+  }
+  return body
+}
+
+/**
+ * To translate
+ *
+ * @param {string} text content to be translated
+ * @param {string} from <optional> source language code. `auto-detect` by default.
+ * @param {string} to <optional> target language code. `en` by default.
+ * @param {boolean} correct <optional> whether to correct the input text. `false` by default.
+ * @param {boolean} raw <optional> the result contains raw response if `true`
+ * @param {string} userAgent <optional> the expected user agent header
+ * @param {import('got').Agents} proxyAgents <optional> set agents of `got` for proxy
+ */
+async function translate(text, from, to, correct, raw, userAgent, proxyAgents) {
+  if (!text || !(text = text.trim())) {
+    return
+  }
+
+  if (text.length > MAX_TEXT_LEN) {
+    throw new Error(`The supported maximum length of text is ${MAX_TEXT_LEN}. Please shorten the text.`)
+  }
+
+  if (!globalConfigPromise) {
+    globalConfigPromise = fetchGlobalConfig(userAgent, proxyAgents)
+  }
+
+  await globalConfigPromise
+
+  if (isTokenExpired()) {
+    globalConfigPromise = fetchGlobalConfig(userAgent, proxyAgents)
+
+    await globalConfigPromise
+  }
+
+  from = from || 'auto-detect'
+  to = to || 'en'
+
+  const fromSupported = lang.isSupported(from)
+  const toSupported = lang.isSupported(to)
+
+  if (!fromSupported || !toSupported) {
+    throw new Error(`The language '${!fromSupported ? from : !toSupported ? to : ''}' is not supported!`)
+  }
+
+  from = lang.getLangCode(from)
+  to = lang.getLangCode(to)
+
+  const requestURL = makeRequestURL(false)
+  const requestBody = makeRequestBody(false, text, from, to === 'auto-detect' ? 'en' : to)
+
+  const requestHeaders = {
+    'user-agent': userAgent || USER_AGENT,
+    referer: replaceTld(TRANSLATE_WEBSITE, globalConfig.tld),
+    cookie: globalConfig.cookie
+  }
+
+  const { body } = await got.post(requestURL, {
+    headers: requestHeaders,
+    // got will set CONTENT_TYPE as `application/x-www-form-urlencoded`
+    form: requestBody,
+    responseType: 'json',
+    agent: proxyAgents
+  })
+
+  if (body.ShowCaptcha) {
+    throw new Error(`
+      Sorry that bing translator seems to be asking for the captcha,
+      Please take care not to request too frequently.
+      The response code is ${body.StatusCode}.
+    `)
+  }
+
+  if (body.StatusCode === 401) {
+    throw new Error(`
+      Max count of translation exceeded. Please try it again later.
+      The response code is 401.
+    `)
+  }
+
+  if (body.statusCode) {
+    throw new Error(`Something went wrong! The response is ${JSON.stringify(body)}.`)
+  }
+
+  const translation = body[0].translations[0]
+  const detectedLang = body[0].detectedLanguage
+
+  const res = {
+    text,
+    userLang: from,
+    translation: translation.text,
+    language: {
+      from: detectedLang.language,
+      to: translation.to,
+      score: detectedLang.score
+    }
+  }
+
+  if (correct) {
+    const correctLang = detectedLang.language
+    const matcher = text.match(/""/g)
+    const len = text.length + (matcher && matcher.length || 0)
+    // currently, there is a limit of 50 characters for correction service
+    // and only parts of languages are supported
+    // otherwise, it will return status code 400
+    if (len <= MAX_CORRECT_TEXT_LEN && lang.canCorrect(correctLang)) {
+      const requestURL = makeRequestURL(true)
+      const requestBody = makeRequestBody(true, text, correctLang)
+
+      const { body } = await got.post(requestURL, {
+        headers: requestHeaders,
+        form: requestBody,
+        responseType: 'json',
+        agent: proxyAgents
+      })
+
+      res.correctedText = body && body.correctedText
+    }
+    else {
+      console.warn(`The detected language '${correctLang}' is not supported to be corrected or the length of text is more than ${MAX_CORRECT_TEXT_LEN}.`)
+    }
+  }
+
+  if (raw) {
+    res.raw = body
+  }
+
+  return res
+}
+
+module.exports = {
+  translate,
+  lang
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * Generated from https://bing.com/translator
+ */
+const LANGS = {
+  'auto-detect': 'Auto-detect',
+  ...require('./lang.json')
+}
+
+const LANGS_CAN_CORRECT = [
+  'da',
+  'en',
+  'nl',
+  'fi',
+  'fr',
+  'fr-CA',
+  'de',
+  'it',
+  'ja',
+  'ko',
+  'no',
+  'pl',
+  'pt',
+  'pt-PT',
+  'ru',
+  'es',
+  'sv',
+  'tr',
+  'zh-Hant',
+  'zh-Hans'
+]
+
+function getLangCode(lang) {
+  if (!lang || typeof lang !== 'string') {
+    return
+  }
+
+  if (LANGS[lang]) {
+    return lang
+  }
+
+  lang = lang.toLowerCase()
+
+  const supportedLangCodes = Object.keys(LANGS)
+
+  for (let i = 0, len = supportedLangCodes.length, code; i < len; i++) {
+    code = supportedLangCodes[i]
+    if (code.toLowerCase() === lang || LANGS[code].toLowerCase() === lang) {
+      return code
+    }
+  }
+}
+
+function isSupported(lang) {
+  return !!getLangCode(lang)
+}
+
+function canCorrect(lang) {
+  const langCode = getLangCode(lang)
+  return langCode && LANGS_CAN_CORRECT.indexOf(langCode) > -1
+}
+
+module.exports = {
+  LANGS,
+  getLangCode,
+  isSupported,
+  canCorrect
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;\ No newline at end of file
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""af"": ""Afrikaans"",
+  ""sq"": ""Albanian"",
+  ""am"": ""Amharic"",
+  ""ar"": ""Arabic"",
+  ""hy"": ""Armenian"",
+  ""as"": ""Assamese"",
+  ""az"": ""Azerbaijani"",
+  ""bn"": ""Bangla"",
+  ""ba"": ""Bashkir"",
+  ""eu"": ""Basque"",
+  ""bs"": ""Bosnian"",
+  ""bg"": ""Bulgarian"",
+  ""yue"": ""Cantonese (Traditional)"",
+  ""ca"": ""Catalan"",
+  ""lzh"": ""Chinese (Literary)"",
+  ""zh-Hans"": ""Chinese Simplified"",
+  ""zh-Hant"": ""Chinese Traditional"",
+  ""hr"": ""Croatian"",
+  ""cs"": ""Czech"",
+  ""da"": ""Danish"",
+  ""prs"": ""Dari"",
+  ""dv"": ""Divehi"",
+  ""nl"": ""Dutch"",
+  ""en"": ""English"",
+  ""et"": ""Estonian"",
+  ""fo"": ""Faroese"",
+  ""fj"": ""Fijian"",
+  ""fil"": ""Filipino"",
+  ""fi"": ""Finnish"",
+  ""fr"": ""French"",
+  ""fr-CA"": ""French (Canada)"",
+  ""gl"": ""Galician"",
+  ""ka"": ""Georgian"",
+  ""de"": ""German"",
+  ""el"": ""Greek"",
+  ""gu"": ""Gujarati"",
+  ""ht"": ""Haitian Creole"",
+  ""he"": ""Hebrew"",
+  ""hi"": ""Hindi"",
+  ""mww"": ""Hmong Daw"",
+  ""hu"": ""Hungarian"",
+  ""is"": ""Icelandic"",
+  ""id"": ""Indonesian"",
+  ""ikt"": ""Inuinnaqtun"",
+  ""iu"": ""Inuktitut"",
+  ""iu-Latn"": ""Inuktitut (Latin)"",
+  ""ga"": ""Irish"",
+  ""it"": ""Italian"",
+  ""ja"": ""Japanese"",
+  ""kn"": ""Kannada"",
+  ""kk"": ""Kazakh"",
+  ""km"": ""Khmer"",
+  ""tlh-Latn"": ""Klingon (Latin)"",
+  ""ko"": ""Korean"",
+  ""ku"": ""Kurdish (Central)"",
+  ""kmr"": ""Kurdish (Northern)"",
+  ""ky"": ""Kyrgyz"",
+  ""lo"": ""Lao"",
+  ""lv"": ""Latvian"",
+  ""lt"": ""Lithuanian"",
+  ""mk"": ""Macedonian"",
+  ""mg"": ""Malagasy"",
+  ""ms"": ""Malay"",
+  ""ml"": ""Malayalam"",
+  ""mt"": ""Maltese"",
+  ""mi"": ""Māori"",
+  ""mr"": ""Marathi"",
+  ""mn-Cyrl"": ""Mongolian (Cyrillic)"",
+  ""mn-Mong"": ""Mongolian (Traditional)"",
+  ""my"": ""Myanmar (Burmese)"",
+  ""ne"": ""Nepali"",
+  ""nb"": ""Norwegian"",
+  ""or"": ""Odia"",
+  ""ps"": ""Pashto"",
+  ""fa"": ""Persian"",
+  ""pl"": ""Polish"",
+  ""pt"": ""Portuguese (Brazil)"",
+  ""pt-PT"": ""Portuguese (Portugal)"",
+  ""pa"": ""Punjabi"",
+  ""otq"": ""Querétaro Otomi"",
+  ""ro"": ""Romanian"",
+  ""ru"": ""Russian"",
+  ""sm"": ""Samoan"",
+  ""sr-Cyrl"": ""Serbian (Cyrillic)"",
+  ""sr-Latn"": ""Serbian (Latin)"",
+  ""sk"": ""Slovak"",
+  ""sl"": ""Slovenian"",
+  ""so"": ""Somali"",
+  ""es"": ""Spanish"",
+  ""sw"": ""Swahili"",
+  ""sv"": ""Swedish"",
+  ""ty"": ""Tahitian"",
+  ""ta"": ""Tamil"",
+  ""tt"": ""Tatar"",
+  ""te"": ""Telugu"",
+  ""th"": ""Thai"",
+  ""bo"": ""Tibetan"",
+  ""ti"": ""Tigrinya"",
+  ""to"": ""Tongan"",
+  ""tr"": ""Turkish"",
+  ""tk"": ""Turkmen"",
+  ""uk"": ""Ukrainian"",
+  ""hsb"": ""Upper Sorbian"",
+  ""ur"": ""Urdu"",
+  ""ug"": ""Uyghur"",
+  ""uz"": ""Uzbek (Latin)"",
+  ""vi"": ""Vietnamese"",
+  ""cy"": ""Welsh"",
+  ""yua"": ""Yucatec Maya"",
+  ""zu"": ""Zulu""
+}
\ No newline at end of file"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+const { translate } = require('../src/index')
+
+function printRes(res) {
+  console.log(res.text, '----->', res.translation, 'fromLang', res.language.from)
+  console.log()
+}
+
+function printCorrectRes(res) {
+  console.log(res.text, '----->', res.correctedText)
+  console.log()
+}
+
+function onErr(e, notExit) {
+  console.error(e)
+  notExit || process.exit(1)
+}
+
+// default: auto-detect(zh-Hans) -> en
+translate('你好')
+.then(printRes)
+.catch(onErr)
+
+// auto-detect(English) to zh-Hans
+translate('Hello', null, 'zh-Hans')
+.then(printRes)
+.catch(onErr)
+
+// auto-detect(English) to Georgian
+translate('Hello', null, 'ka')
+.then(printRes)
+.catch(onErr)
+
+// Literary Chinese(lzh) to Simplified Chinese(zh-Hans)
+translate('邹忌修八尺有余，而形貌昳丽。朝服衣冠，窥镜，谓其妻曰：“我孰与城北徐公美？”其妻曰：“君美甚，徐公何能及君也？”', 'lzh', 'zh-Hans')
+.then(printRes)
+.catch(onErr)
+
+// auto-detect(Korean) to zh-Hant
+translate('안녕하십니까', null, 'zh-Hant')
+.then(printRes)
+.catch(onErr)
+
+// correct `gradent`` to `gradient`
+translate('gradent', null, 'en', true)
+.then(printCorrectRes)
+.catch(onErr)
+
+// correct short text to `this text is very long`
+translate('this text is very lang', null, 'en', true)
+.then(printCorrectRes)
+.catch(onErr)
+
+// correct short text -> return `undefined` for the language is not supported
+translate('Bore da', null, 'en', true)
+.then(printCorrectRes)
+.catch(onErr)
+
+// correct long text -> return `undefined` for exceeding max length
+translate('this text is very long this text is very long this text is very long this text is very long this text is very long this text is very long ', null, 'en', true)
+.then(printCorrectRes)
+.catch(onErr)
+
+// max text len -> return `undefined` for exceeding max length
+translate((() => {
+  let text = ''
+  while (text.length < 1001) {
+    text += ~~(Math.random() * 10) + ''
+  }
+  return text
+})(), null, 'en')
+.then(printRes)
+.catch(e => onErr(e, true))"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+const { lang } = require('../src/index')
+// const lang = require('../src/lang')
+
+console.log('en supported:', lang.isSupported('en'))
+console.log('en1 supported:', lang.isSupported('en1'))
+console.log('Korean language code:', lang.getLangCode('Korean'))"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;" import fitz  # this is pymupdf
 from google_trans_new.google_trans_new import google_translator
 import re
 import numpy as np
 import time
 import random
 import os
 import glob
-translator = google_translator()
 dct = {}
 dem = 0
 nameDict = 'dict.npy'
 # Load
-# np.save(nameDict, dct)   #Neu bao loi thi chay cau lenh nay truoc
 dct = np.load(nameDict, allow_pickle='TRUE').item()
 if os.path.isdir('pdf'):
     print('Exists folder pdf')
@@ -28,24 +32,65 @@
     result = set(result)
     print(""Da doc"", len(result), ""tu"")
     for word in result:
         word = word.lower().strip()
         if word in dct:
             print(word, ""da co trong tu dien"")
             continue
         try:
-            trans = translator.translate(
                 word, lang_src='en', lang_tgt='vi')
-            trans = trans.lower().strip()
-            dem += 1
         except Exception as e:
-            trans = word
-            print(""ERROR: "", e)
             continue
-        if trans == word:
             continue
         print(""NEW:"", word, "":"", trans)
-        dct[word] = trans
         if dem % 10 == 0:
             np.save(nameDict, dct)
             print(""SAVE IN DICT"")
-            time.sleep(random.randint(10, 15))"
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;" import fitz  # this is pymupdf
 from google_trans_new.google_trans_new import google_translator
+from translatepython.translate import Translator
+from Bing_Translate import BingTranslate
 import re
 import numpy as np
 import time
 import random
 import os
 import glob
+googleTranslate = google_translator()
+myMemoryTranslate = Translator(to_lang=""vi"")
+bingTranslate = BingTranslate()
 dct = {}
 dem = 0
 nameDict = 'dict.npy'
 # Load
+# np.save(nameDict, dct)  # Neu bao loi thi chay cau lenh nay truoc
 dct = np.load(nameDict, allow_pickle='TRUE').item()
 if os.path.isdir('pdf'):
     print('Exists folder pdf')
@@ -28,24 +32,65 @@
     result = set(result)
     print(""Da doc"", len(result), ""tu"")
     for word in result:
+        check = False
+        add = 0
         word = word.lower().strip()
         if word in dct:
             print(word, ""da co trong tu dien"")
             continue
         try:
+            resultGoogle = googleTranslate.translate(
                 word, lang_src='en', lang_tgt='vi')
+            resultGoogle = resultGoogle.lower().strip()
+            check = True
         except Exception as e:
+            resultGoogle = word
+            print(""Google translate error: "", e)
             continue
+        if resultGoogle != word:
+            add += 1
+
+        try:
+            resultBing = bingTranslate.translate(word)
+            resultBing = resultBing.lower().strip()
+            check = True
+        except Exception as e:
+            resultBing = word
+            print(""Bing translate error: "", e)
+            continue
+        if resultBing != word:
+            add += 1
+
+        try:
+            resultMyMemory = myMemoryTranslate.translate(word)
+            resultMyMemory = resultMyMemory.lower().strip()
+            check = True
+        except Exception as e:
+            resultMyMemory = word
+            print(""MyMemory translate error: "", e)
+            continue
+        if resultMyMemory != word:
+            add += 1
+        if check == False or add < 2:
             continue
+        trans = ""Google: ""+resultGoogle+""\tBing: "" + \
+            resultBing+""\tMymemory: ""+resultMyMemory
         print(""NEW:"", word, "":"", trans)
+
+        value = ""<div>""
+        if resultGoogle != """":
+            value += ""<b>- Google :</b><br />&emsp;+ {0}<br/>"".format(
+                resultGoogle.lower().strip().capitalize())
+        if resultBing != """":
+            value += ""<b>- Bing :</b><br />&emsp;+ {0}<br/>"".format(
+                resultBing.lower().strip().capitalize())
+        if resultMyMemory != """":
+            value += ""<b>- MyMemory :</b><br />&emsp;+ {0}<br/>"".format(
+                resultMyMemory.lower().strip().capitalize())
+        value += ""</div>""
+        dct[word] = value
+        dem += 1
         if dem % 10 == 0:
             np.save(nameDict, dct)
             print(""SAVE IN DICT"")
+            time.sleep(random.randint(5, 10))"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+#!/bin/sh
+basedir=$(dirname ""$(echo ""$0"" | sed -e 's,\\,/,g')"")
+
+case `uname` in
+    *CYGWIN*|*MINGW*|*MSYS*) basedir=`cygpath -w ""$basedir""`;;
+esac
+
+if [ -x ""$basedir/node"" ]; then
+  ""$basedir/node""  ""$basedir/../mime/cli.js"" ""$@""
+  ret=$?
+else 
+  node  ""$basedir/../mime/cli.js"" ""$@""
+  ret=$?
+fi
+exit $ret"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+@ECHO off
+SETLOCAL
+CALL :find_dp0
+
+IF EXIST ""%dp0%\node.exe"" (
+  SET ""_prog=%dp0%\node.exe""
+) ELSE (
+  SET ""_prog=node""
+  SET PATHEXT=%PATHEXT:;.JS;=;%
+)
+
+""%_prog%""  ""%dp0%\..\mime\cli.js"" %*
+ENDLOCAL
+EXIT /b %errorlevel%
+:find_dp0
+SET dp0=%~dp0
+EXIT /b"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+#!/usr/bin/env pwsh
+$basedir=Split-Path $MyInvocation.MyCommand.Definition -Parent
+
+$exe=""""
+if ($PSVersionTable.PSVersion -lt ""6.0"" -or $IsWindows) {
+  # Fix case when both the Windows and Linux builds of Node
+  # are installed in the same directory
+  $exe="".exe""
+}
+$ret=0
+if (Test-Path ""$basedir/node$exe"") {
+  & ""$basedir/node$exe""  ""$basedir/../mime/cli.js"" $args
+  $ret=$LASTEXITCODE
+} else {
+  & ""node$exe""  ""$basedir/../mime/cli.js"" $args
+  $ret=$LASTEXITCODE
+}
+exit $ret"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/// <reference types=""node"" />
+/// <reference lib=""es2018"" />
+/// <reference lib=""dom"" />
+import { Class, Falsy, TypedArray, ObservableLike, Primitive } from './types';
+declare const objectTypeNames: readonly [""Function"", ""Generator"", ""AsyncGenerator"", ""GeneratorFunction"", ""AsyncGeneratorFunction"", ""AsyncFunction"", ""Observable"", ""Array"", ""Buffer"", ""Blob"", ""Object"", ""RegExp"", ""Date"", ""Error"", ""Map"", ""Set"", ""WeakMap"", ""WeakSet"", ""ArrayBuffer"", ""SharedArrayBuffer"", ""DataView"", ""Promise"", ""URL"", ""FormData"", ""URLSearchParams"", ""HTMLElement"", ...(""Int8Array"" | ""Uint8Array"" | ""Uint8ClampedArray"" | ""Int16Array"" | ""Uint16Array"" | ""Int32Array"" | ""Uint32Array"" | ""Float32Array"" | ""Float64Array"" | ""BigInt64Array"" | ""BigUint64Array"")[]];
+declare type ObjectTypeName = typeof objectTypeNames[number];
+declare const primitiveTypeNames: readonly [""null"", ""undefined"", ""string"", ""number"", ""bigint"", ""boolean"", ""symbol""];
+declare type PrimitiveTypeName = typeof primitiveTypeNames[number];
+export declare type TypeName = ObjectTypeName | PrimitiveTypeName;
+declare function is(value: unknown): TypeName;
+declare namespace is {
+    var undefined: (value: unknown) => value is undefined;
+    var string: (value: unknown) => value is string;
+    var number: (value: unknown) => value is number;
+    var bigint: (value: unknown) => value is bigint;
+    var function_: (value: unknown) => value is Function;
+    var null_: (value: unknown) => value is null;
+    var class_: (value: unknown) => value is Class<unknown, any[]>;
+    var boolean: (value: unknown) => value is boolean;
+    var symbol: (value: unknown) => value is symbol;
+    var numericString: (value: unknown) => value is string;
+    var array: <T = unknown>(value: unknown, assertion?: ((value: T) => value is T) | undefined) => value is T[];
+    var buffer: (value: unknown) => value is Buffer;
+    var blob: (value: unknown) => value is Blob;
+    var nullOrUndefined: (value: unknown) => value is null | undefined;
+    var object: (value: unknown) => value is object;
+    var iterable: <T = unknown>(value: unknown) => value is Iterable<T>;
+    var asyncIterable: <T = unknown>(value: unknown) => value is AsyncIterable<T>;
+    var generator: (value: unknown) => value is Generator<unknown, any, unknown>;
+    var asyncGenerator: (value: unknown) => value is AsyncGenerator<unknown, any, unknown>;
+    var nativePromise: <T = unknown>(value: unknown) => value is Promise<T>;
+    var promise: <T = unknown>(value: unknown) => value is Promise<T>;
+    var generatorFunction: (value: unknown) => value is GeneratorFunction;
+    var asyncGeneratorFunction: (value: unknown) => value is (...args: any[]) => Promise<unknown>;
+    var asyncFunction: <T = unknown>(value: unknown) => value is (...args: any[]) => Promise<T>;
+    var boundFunction: (value: unknown) => value is Function;
+    var regExp: (value: unknown) => value is RegExp;
+    var date: (value: unknown) => value is Date;
+    var error: (value: unknown) => value is Error;
+    var map: <Key = unknown, Value = unknown>(value: unknown) => value is Map<Key, Value>;
+    var set: <T = unknown>(value: unknown) => value is Set<T>;
+    var weakMap: <Key extends object = object, Value = unknown>(value: unknown) => value is WeakMap<Key, Value>;
+    var weakSet: (value: unknown) => value is WeakSet<object>;
+    var int8Array: (value: unknown) => value is Int8Array;
+    var uint8Array: (value: unknown) => value is Uint8Array;
+    var uint8ClampedArray: (value: unknown) => value is Uint8ClampedArray;
+    var int16Array: (value: unknown) => value is Int16Array;
+    var uint16Array: (value: unknown) => value is Uint16Array;
+    var int32Array: (value: unknown) => value is Int32Array;
+    var uint32Array: (value: unknown) => value is Uint32Array;
+    var float32Array: (value: unknown) => value is Float32Array;
+    var float64Array: (value: unknown) => value is Float64Array;
+    var bigInt64Array: (value: unknown) => value is BigInt64Array;
+    var bigUint64Array: (value: unknown) => value is BigUint64Array;
+    var arrayBuffer: (value: unknown) => value is ArrayBuffer;
+    var sharedArrayBuffer: (value: unknown) => value is SharedArrayBuffer;
+    var dataView: (value: unknown) => value is DataView;
+    var enumCase: <T = unknown>(value: unknown, targetEnum: T) => boolean;
+    var directInstanceOf: <T>(instance: unknown, class_: Class<T, any[]>) => instance is T;
+    var urlInstance: (value: unknown) => value is URL;
+    var urlString: (value: unknown) => value is string;
+    var truthy: <T>(value: false | """" | 0 | 0n | T | null | undefined) => value is T;
+    var falsy: <T>(value: false | """" | 0 | 0n | T | null | undefined) => value is Falsy;
+    var nan: (value: unknown) => boolean;
+    var primitive: (value: unknown) => value is Primitive;
+    var integer: (value: unknown) => value is number;
+    var safeInteger: (value: unknown) => value is number;
+    var plainObject: <Value = unknown>(value: unknown) => value is Record<string | number | symbol, Value>;
+    var typedArray: (value: unknown) => value is TypedArray;
+    var arrayLike: <T = unknown>(value: unknown) => value is ArrayLike<T>;
+    var inRange: (value: number, range: number | number[]) => value is number;
+    var domElement: (value: unknown) => value is HTMLElement;
+    var observable: (value: unknown) => value is ObservableLike;
+    var nodeStream: (value: unknown) => value is NodeStream;
+    var infinite: (value: unknown) => value is number;
+    var evenInteger: (value: number) => value is number;
+    var oddInteger: (value: number) => value is number;
+    var emptyArray: (value: unknown) => value is never[];
+    var nonEmptyArray: (value: unknown) => value is unknown[];
+    var emptyString: (value: unknown) => value is """";
+    var emptyStringOrWhitespace: (value: unknown) => value is string;
+    var nonEmptyString: (value: unknown) => value is string;
+    var nonEmptyStringAndNotWhitespace: (value: unknown) => value is string;
+    var emptyObject: <Key extends string | number | symbol = string>(value: unknown) => value is Record<Key, never>;
+    var nonEmptyObject: <Key extends string | number | symbol = string, Value = unknown>(value: unknown) => value is Record<Key, Value>;
+    var emptySet: (value: unknown) => value is Set<never>;
+    var nonEmptySet: <T = unknown>(value: unknown) => value is Set<T>;
+    var emptyMap: (value: unknown) => value is Map<never, never>;
+    var nonEmptyMap: <Key = unknown, Value = unknown>(value: unknown) => value is Map<Key, Value>;
+    var propertyKey: (value: unknown) => value is string | number | symbol;
+    var formData: (value: unknown) => value is FormData;
+    var urlSearchParams: (value: unknown) => value is URLSearchParams;
+    var any: (predicate: Predicate | Predicate[], ...values: unknown[]) => boolean;
+    var all: (predicate: Predicate, ...values: unknown[]) => boolean;
+}
+export interface ArrayLike<T> {
+    readonly [index: number]: T;
+    readonly length: number;
+}
+export interface NodeStream extends NodeJS.EventEmitter {
+    pipe<T extends NodeJS.WritableStream>(destination: T, options?: {
+        end?: boolean;
+    }): T;
+}
+export declare type Predicate = (value: unknown) => boolean;
+export declare const enum AssertionTypeDescription {
+    class_ = ""Class"",
+    numericString = ""string with a number"",
+    nullOrUndefined = ""null or undefined"",
+    iterable = ""Iterable"",
+    asyncIterable = ""AsyncIterable"",
+    nativePromise = ""native Promise"",
+    urlString = ""string with a URL"",
+    truthy = ""truthy"",
+    falsy = ""falsy"",
+    nan = ""NaN"",
+    primitive = ""primitive"",
+    integer = ""integer"",
+    safeInteger = ""integer"",
+    plainObject = ""plain object"",
+    arrayLike = ""array-like"",
+    typedArray = ""TypedArray"",
+    domElement = ""HTMLElement"",
+    nodeStream = ""Node.js Stream"",
+    infinite = ""infinite number"",
+    emptyArray = ""empty array"",
+    nonEmptyArray = ""non-empty array"",
+    emptyString = ""empty string"",
+    emptyStringOrWhitespace = ""empty string or whitespace"",
+    nonEmptyString = ""non-empty string"",
+    nonEmptyStringAndNotWhitespace = ""non-empty string and not whitespace"",
+    emptyObject = ""empty object"",
+    nonEmptyObject = ""non-empty object"",
+    emptySet = ""empty set"",
+    nonEmptySet = ""non-empty set"",
+    emptyMap = ""empty map"",
+    nonEmptyMap = ""non-empty map"",
+    evenInteger = ""even integer"",
+    oddInteger = ""odd integer"",
+    directInstanceOf = ""T"",
+    inRange = ""in range"",
+    any = ""predicate returns truthy for any value"",
+    all = ""predicate returns truthy for all values""
+}
+interface Assert {
+    undefined: (value: unknown) => asserts value is undefined;
+    string: (value: unknown) => asserts value is string;
+    number: (value: unknown) => asserts value is number;
+    bigint: (value: unknown) => asserts value is bigint;
+    function_: (value: unknown) => asserts value is Function;
+    null_: (value: unknown) => asserts value is null;
+    class_: (value: unknown) => asserts value is Class;
+    boolean: (value: unknown) => asserts value is boolean;
+    symbol: (value: unknown) => asserts value is symbol;
+    numericString: (value: unknown) => asserts value is string;
+    array: <T = unknown>(value: unknown, assertion?: (element: unknown) => asserts element is T) => asserts value is T[];
+    buffer: (value: unknown) => asserts value is Buffer;
+    blob: (value: unknown) => asserts value is Blob;
+    nullOrUndefined: (value: unknown) => asserts value is null | undefined;
+    object: <Key extends keyof any = string, Value = unknown>(value: unknown) => asserts value is Record<Key, Value>;
+    iterable: <T = unknown>(value: unknown) => asserts value is Iterable<T>;
+    asyncIterable: <T = unknown>(value: unknown) => asserts value is AsyncIterable<T>;
+    generator: (value: unknown) => asserts value is Generator;
+    asyncGenerator: (value: unknown) => asserts value is AsyncGenerator;
+    nativePromise: <T = unknown>(value: unknown) => asserts value is Promise<T>;
+    promise: <T = unknown>(value: unknown) => asserts value is Promise<T>;
+    generatorFunction: (value: unknown) => asserts value is GeneratorFunction;
+    asyncGeneratorFunction: (value: unknown) => asserts value is AsyncGeneratorFunction;
+    asyncFunction: (value: unknown) => asserts value is Function;
+    boundFunction: (value: unknown) => asserts value is Function;
+    regExp: (value: unknown) => asserts value is RegExp;
+    date: (value: unknown) => asserts value is Date;
+    error: (value: unknown) => asserts value is Error;
+    map: <Key = unknown, Value = unknown>(value: unknown) => asserts value is Map<Key, Value>;
+    set: <T = unknown>(value: unknown) => asserts value is Set<T>;
+    weakMap: <Key extends object = object, Value = unknown>(value: unknown) => asserts value is WeakMap<Key, Value>;
+    weakSet: <T extends object = object>(value: unknown) => asserts value is WeakSet<T>;
+    int8Array: (value: unknown) => asserts value is Int8Array;
+    uint8Array: (value: unknown) => asserts value is Uint8Array;
+    uint8ClampedArray: (value: unknown) => asserts value is Uint8ClampedArray;
+    int16Array: (value: unknown) => asserts value is Int16Array;
+    uint16Array: (value: unknown) => asserts value is Uint16Array;
+    int32Array: (value: unknown) => asserts value is Int32Array;
+    uint32Array: (value: unknown) => asserts value is Uint32Array;
+    float32Array: (value: unknown) => asserts value is Float32Array;
+    float64Array: (value: unknown) => asserts value is Float64Array;
+    bigInt64Array: (value: unknown) => asserts value is BigInt64Array;
+    bigUint64Array: (value: unknown) => asserts value is BigUint64Array;
+    arrayBuffer: (value: unknown) => asserts value is ArrayBuffer;
+    sharedArrayBuffer: (value: unknown) => asserts value is SharedArrayBuffer;
+    dataView: (value: unknown) => asserts value is DataView;
+    enumCase: <T = unknown>(value: unknown, targetEnum: T) => asserts value is T[keyof T];
+    urlInstance: (value: unknown) => asserts value is URL;
+    urlString: (value: unknown) => asserts value is string;
+    truthy: (value: unknown) => asserts value is unknown;
+    falsy: (value: unknown) => asserts value is unknown;
+    nan: (value: unknown) => asserts value is unknown;
+    primitive: (value: unknown) => asserts value is Primitive;
+    integer: (value: unknown) => asserts value is number;
+    safeInteger: (value: unknown) => asserts value is number;
+    plainObject: <Value = unknown>(value: unknown) => asserts value is Record<PropertyKey, Value>;
+    typedArray: (value: unknown) => asserts value is TypedArray;
+    arrayLike: <T = unknown>(value: unknown) => asserts value is ArrayLike<T>;
+    domElement: (value: unknown) => asserts value is HTMLElement;
+    observable: (value: unknown) => asserts value is ObservableLike;
+    nodeStream: (value: unknown) => asserts value is NodeStream;
+    infinite: (value: unknown) => asserts value is number;
+    emptyArray: (value: unknown) => asserts value is never[];
+    nonEmptyArray: (value: unknown) => asserts value is unknown[];
+    emptyString: (value: unknown) => asserts value is '';
+    emptyStringOrWhitespace: (value: unknown) => asserts value is string;
+    nonEmptyString: (value: unknown) => asserts value is string;
+    nonEmptyStringAndNotWhitespace: (value: unknown) => asserts value is string;
+    emptyObject: <Key extends keyof any = string>(value: unknown) => asserts value is Record<Key, never>;
+    nonEmptyObject: <Key extends keyof any = string, Value = unknown>(value: unknown) => asserts value is Record<Key, Value>;
+    emptySet: (value: unknown) => asserts value is Set<never>;
+    nonEmptySet: <T = unknown>(value: unknown) => asserts value is Set<T>;
+    emptyMap: (value: unknown) => asserts value is Map<never, never>;
+    nonEmptyMap: <Key = unknown, Value = unknown>(value: unknown) => asserts value is Map<Key, Value>;
+    propertyKey: (value: unknown) => asserts value is PropertyKey;
+    formData: (value: unknown) => asserts value is FormData;
+    urlSearchParams: (value: unknown) => asserts value is URLSearchParams;
+    evenInteger: (value: number) => asserts value is number;
+    oddInteger: (value: number) => asserts value is number;
+    directInstanceOf: <T>(instance: unknown, class_: Class<T>) => asserts instance is T;
+    inRange: (value: number, range: number | number[]) => asserts value is number;
+    any: (predicate: Predicate | Predicate[], ...values: unknown[]) => void | never;
+    all: (predicate: Predicate, ...values: unknown[]) => void | never;
+}
+export declare const assert: Assert;
+export default is;
+export { Class, TypedArray, ObservableLike, Primitive } from './types';"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+""use strict"";
+/// <reference lib=""es2018""/>
+/// <reference lib=""dom""/>
+/// <reference types=""node""/>
+Object.defineProperty(exports, ""__esModule"", { value: true });
+const typedArrayTypeNames = [
+    'Int8Array',
+    'Uint8Array',
+    'Uint8ClampedArray',
+    'Int16Array',
+    'Uint16Array',
+    'Int32Array',
+    'Uint32Array',
+    'Float32Array',
+    'Float64Array',
+    'BigInt64Array',
+    'BigUint64Array'
+];
+function isTypedArrayName(name) {
+    return typedArrayTypeNames.includes(name);
+}
+const objectTypeNames = [
+    'Function',
+    'Generator',
+    'AsyncGenerator',
+    'GeneratorFunction',
+    'AsyncGeneratorFunction',
+    'AsyncFunction',
+    'Observable',
+    'Array',
+    'Buffer',
+    'Blob',
+    'Object',
+    'RegExp',
+    'Date',
+    'Error',
+    'Map',
+    'Set',
+    'WeakMap',
+    'WeakSet',
+    'ArrayBuffer',
+    'SharedArrayBuffer',
+    'DataView',
+    'Promise',
+    'URL',
+    'FormData',
+    'URLSearchParams',
+    'HTMLElement',
+    ...typedArrayTypeNames
+];
+function isObjectTypeName(name) {
+    return objectTypeNames.includes(name);
+}
+const primitiveTypeNames = [
+    'null',
+    'undefined',
+    'string',
+    'number',
+    'bigint',
+    'boolean',
+    'symbol'
+];
+function isPrimitiveTypeName(name) {
+    return primitiveTypeNames.includes(name);
+}
+// eslint-disable-next-line @typescript-eslint/ban-types
+function isOfType(type) {
+    return (value) => typeof value === type;
+}
+const { toString } = Object.prototype;
+const getObjectType = (value) => {
+    const objectTypeName = toString.call(value).slice(8, -1);
+    if (/HTML\w+Element/.test(objectTypeName) && is.domElement(value)) {
+        return 'HTMLElement';
+    }
+    if (isObjectTypeName(objectTypeName)) {
+        return objectTypeName;
+    }
+    return undefined;
+};
+const isObjectOfType = (type) => (value) => getObjectType(value) === type;
+function is(value) {
+    if (value === null) {
+        return 'null';
+    }
+    switch (typeof value) {
+        case 'undefined':
+            return 'undefined';
+        case 'string':
+            return 'string';
+        case 'number':
+            return 'number';
+        case 'boolean':
+            return 'boolean';
+        case 'function':
+            return 'Function';
+        case 'bigint':
+            return 'bigint';
+        case 'symbol':
+            return 'symbol';
+        default:
+    }
+    if (is.observable(value)) {
+        return 'Observable';
+    }
+    if (is.array(value)) {
+        return 'Array';
+    }
+    if (is.buffer(value)) {
+        return 'Buffer';
+    }
+    const tagType = getObjectType(value);
+    if (tagType) {
+        return tagType;
+    }
+    if (value instanceof String || value instanceof Boolean || value instanceof Number) {
+        throw new TypeError('Please don\'t use object wrappers for primitive types');
+    }
+    return 'Object';
+}
+is.undefined = isOfType('undefined');
+is.string = isOfType('string');
+const isNumberType = isOfType('number');
+is.number = (value) => isNumberType(value) && !is.nan(value);
+is.bigint = isOfType('bigint');
+// eslint-disable-next-line @typescript-eslint/ban-types
+is.function_ = isOfType('function');
+is.null_ = (value) => value === null;
+is.class_ = (value) => is.function_(value) && value.toString().startsWith('class ');
+is.boolean = (value) => value === true || value === false;
+is.symbol = isOfType('symbol');
+is.numericString = (value) => is.string(value) && !is.emptyStringOrWhitespace(value) && !Number.isNaN(Number(value));
+is.array = (value, assertion) => {
+    if (!Array.isArray(value)) {
+        return false;
+    }
+    if (!is.function_(assertion)) {
+        return true;
+    }
+    return value.every(assertion);
+};
+is.buffer = (value) => { var _a, _b, _c, _d; return (_d = (_c = (_b = (_a = value) === null || _a === void 0 ? void 0 : _a.constructor) === null || _b === void 0 ? void 0 : _b.isBuffer) === null || _c === void 0 ? void 0 : _c.call(_b, value)) !== null && _d !== void 0 ? _d : false; };
+is.blob = (value) => isObjectOfType('Blob')(value);
+is.nullOrUndefined = (value) => is.null_(value) || is.undefined(value);
+is.object = (value) => !is.null_(value) && (typeof value === 'object' || is.function_(value));
+is.iterable = (value) => { var _a; return is.function_((_a = value) === null || _a === void 0 ? void 0 : _a[Symbol.iterator]); };
+is.asyncIterable = (value) => { var _a; return is.function_((_a = value) === null || _a === void 0 ? void 0 : _a[Symbol.asyncIterator]); };
+is.generator = (value) => { var _a, _b; return is.iterable(value) && is.function_((_a = value) === null || _a === void 0 ? void 0 : _a.next) && is.function_((_b = value) === null || _b === void 0 ? void 0 : _b.throw); };
+is.asyncGenerator = (value) => is.asyncIterable(value) && is.function_(value.next) && is.function_(value.throw);
+is.nativePromise = (value) => isObjectOfType('Promise')(value);
+const hasPromiseAPI = (value) => {
+    var _a, _b;
+    return is.function_((_a = value) === null || _a === void 0 ? void 0 : _a.then) &&
+        is.function_((_b = value) === null || _b === void 0 ? void 0 : _b.catch);
+};
+is.promise = (value) => is.nativePromise(value) || hasPromiseAPI(value);
+is.generatorFunction = isObjectOfType('GeneratorFunction');
+is.asyncGeneratorFunction = (value) => getObjectType(value) === 'AsyncGeneratorFunction';
+is.asyncFunction = (value) => getObjectType(value) === 'AsyncFunction';
+// eslint-disable-next-line no-prototype-builtins, @typescript-eslint/ban-types
+is.boundFunction = (value) => is.function_(value) && !value.hasOwnProperty('prototype');
+is.regExp = isObjectOfType('RegExp');
+is.date = isObjectOfType('Date');
+is.error = isObjectOfType('Error');
+is.map = (value) => isObjectOfType('Map')(value);
+is.set = (value) => isObjectOfType('Set')(value);
+is.weakMap = (value) => isObjectOfType('WeakMap')(value);
+is.weakSet = (value) => isObjectOfType('WeakSet')(value);
+is.int8Array = isObjectOfType('Int8Array');
+is.uint8Array = isObjectOfType('Uint8Array');
+is.uint8ClampedArray = isObjectOfType('Uint8ClampedArray');
+is.int16Array = isObjectOfType('Int16Array');
+is.uint16Array = isObjectOfType('Uint16Array');
+is.int32Array = isObjectOfType('Int32Array');
+is.uint32Array = isObjectOfType('Uint32Array');
+is.float32Array = isObjectOfType('Float32Array');
+is.float64Array = isObjectOfType('Float64Array');
+is.bigInt64Array = isObjectOfType('BigInt64Array');
+is.bigUint64Array = isObjectOfType('BigUint64Array');
+is.arrayBuffer = isObjectOfType('ArrayBuffer');
+is.sharedArrayBuffer = isObjectOfType('SharedArrayBuffer');
+is.dataView = isObjectOfType('DataView');
+is.enumCase = (value, targetEnum) => Object.values(targetEnum).includes(value);
+is.directInstanceOf = (instance, class_) => Object.getPrototypeOf(instance) === class_.prototype;
+is.urlInstance = (value) => isObjectOfType('URL')(value);
+is.urlString = (value) => {
+    if (!is.string(value)) {
+        return false;
+    }
+    try {
+        new URL(value); // eslint-disable-line no-new
+        return true;
+    }
+    catch (_a) {
+        return false;
+    }
+};
+// Example: `is.truthy = (value: unknown): value is (not false | not 0 | not '' | not undefined | not null) => Boolean(value);`
+is.truthy = (value) => Boolean(value);
+// Example: `is.falsy = (value: unknown): value is (not true | 0 | '' | undefined | null) => Boolean(value);`
+is.falsy = (value) => !value;
+is.nan = (value) => Number.isNaN(value);
+is.primitive = (value) => is.null_(value) || isPrimitiveTypeName(typeof value);
+is.integer = (value) => Number.isInteger(value);
+is.safeInteger = (value) => Number.isSafeInteger(value);
+is.plainObject = (value) => {
+    // From: https://github.com/sindresorhus/is-plain-obj/blob/main/index.js
+    if (toString.call(value) !== '[object Object]') {
+        return false;
+    }
+    const prototype = Object.getPrototypeOf(value);
+    return prototype === null || prototype === Object.getPrototypeOf({});
+};
+is.typedArray = (value) => isTypedArrayName(getObjectType(value));
+const isValidLength = (value) => is.safeInteger(value) && value >= 0;
+is.arrayLike = (value) => !is.nullOrUndefined(value) && !is.function_(value) && isValidLength(value.length);
+is.inRange = (value, range) => {
+    if (is.number(range)) {
+        return value >= Math.min(0, range) && value <= Math.max(range, 0);
+    }
+    if (is.array(range) && range.length === 2) {
+        return value >= Math.min(...range) && value <= Math.max(...range);
+    }
+    throw new TypeError(`Invalid range: ${JSON.stringify(range)}`);
+};
+const NODE_TYPE_ELEMENT = 1;
+const DOM_PROPERTIES_TO_CHECK = [
+    'innerHTML',
+    'ownerDocument',
+    'style',
+    'attributes',
+    'nodeValue'
+];
+is.domElement = (value) => {
+    return is.object(value) &&
+        value.nodeType === NODE_TYPE_ELEMENT &&
+        is.string(value.nodeName) &&
+        !is.plainObject(value) &&
+        DOM_PROPERTIES_TO_CHECK.every(property => property in value);
+};
+is.observable = (value) => {
+    var _a, _b, _c, _d;
+    if (!value) {
+        return false;
+    }
+    // eslint-disable-next-line no-use-extend-native/no-use-extend-native
+    if (value === ((_b = (_a = value)[Symbol.observable]) === null || _b === void 0 ? void 0 : _b.call(_a))) {
+        return true;
+    }
+    if (value === ((_d = (_c = value)['@@observable']) === null || _d === void 0 ? void 0 : _d.call(_c))) {
+        return true;
+    }
+    return false;
+};
+is.nodeStream = (value) => is.object(value) && is.function_(value.pipe) && !is.observable(value);
+is.infinite = (value) => value === Infinity || value === -Infinity;
+const isAbsoluteMod2 = (remainder) => (value) => is.integer(value) && Math.abs(value % 2) === remainder;
+is.evenInteger = isAbsoluteMod2(0);
+is.oddInteger = isAbsoluteMod2(1);
+is.emptyArray = (value) => is.array(value) && value.length === 0;
+is.nonEmptyArray = (value) => is.array(value) && value.length > 0;
+is.emptyString = (value) => is.string(value) && value.length === 0;
+const isWhiteSpaceString = (value) => is.string(value) && !/\S/.test(value);
+is.emptyStringOrWhitespace = (value) => is.emptyString(value) || isWhiteSpaceString(value);
+// TODO: Use `not ''` when the `not` operator is available.
+is.nonEmptyString = (value) => is.string(value) && value.length > 0;
+// TODO: Use `not ''` when the `not` operator is available.
+is.nonEmptyStringAndNotWhitespace = (value) => is.string(value) && !is.emptyStringOrWhitespace(value);
+is.emptyObject = (value) => is.object(value) && !is.map(value) && !is.set(value) && Object.keys(value).length === 0;
+// TODO: Use `not` operator here to remove `Map` and `Set` from type guard:
+// - https://github.com/Microsoft/TypeScript/pull/29317
+is.nonEmptyObject = (value) => is.object(value) && !is.map(value) && !is.set(value) && Object.keys(value).length > 0;
+is.emptySet = (value) => is.set(value) && value.size === 0;
+is.nonEmptySet = (value) => is.set(value) && value.size > 0;
+is.emptyMap = (value) => is.map(value) && value.size === 0;
+is.nonEmptyMap = (value) => is.map(value) && value.size > 0;
+// `PropertyKey` is any value that can be used as an object key (string, number, or symbol)
+is.propertyKey = (value) => is.any([is.string, is.number, is.symbol], value);
+is.formData = (value) => isObjectOfType('FormData')(value);
+is.urlSearchParams = (value) => isObjectOfType('URLSearchParams')(value);
+const predicateOnArray = (method, predicate, values) => {
+    if (!is.function_(predicate)) {
+        throw new TypeError(`Invalid predicate: ${JSON.stringify(predicate)}`);
+    }
+    if (values.length === 0) {
+        throw new TypeError('Invalid number of values');
+    }
+    return method.call(values, predicate);
+};
+is.any = (predicate, ...values) => {
+    const predicates = is.array(predicate) ? predicate : [predicate];
+    return predicates.some(singlePredicate => predicateOnArray(Array.prototype.some, singlePredicate, values));
+};
+is.all = (predicate, ...values) => predicateOnArray(Array.prototype.every, predicate, values);
+const assertType = (condition, description, value, options = {}) => {
+    if (!condition) {
+        const { multipleValues } = options;
+        const valuesMessage = multipleValues ?
+            `received values of types ${[
+                ...new Set(value.map(singleValue => `\`${is(singleValue)}\``))
+            ].join(', ')}` :
+            `received value of type \`${is(value)}\``;
+        throw new TypeError(`Expected value which is \`${description}\`, ${valuesMessage}.`);
+    }
+};
+exports.assert = {
+    // Unknowns.
+    undefined: (value) => assertType(is.undefined(value), 'undefined', value),
+    string: (value) => assertType(is.string(value), 'string', value),
+    number: (value) => assertType(is.number(value), 'number', value),
+    bigint: (value) => assertType(is.bigint(value), 'bigint', value),
+    // eslint-disable-next-line @typescript-eslint/ban-types
+    function_: (value) => assertType(is.function_(value), 'Function', value),
+    null_: (value) => assertType(is.null_(value), 'null', value),
+    class_: (value) => assertType(is.class_(value), ""Class"" /* class_ */, value),
+    boolean: (value) => assertType(is.boolean(value), 'boolean', value),
+    symbol: (value) => assertType(is.symbol(value), 'symbol', value),
+    numericString: (value) => assertType(is.numericString(value), ""string with a number"" /* numericString */, value),
+    array: (value, assertion) => {
+        const assert = assertType;
+        assert(is.array(value), 'Array', value);
+        if (assertion) {
+            value.forEach(assertion);
+        }
+    },
+    buffer: (value) => assertType(is.buffer(value), 'Buffer', value),
+    blob: (value) => assertType(is.blob(value), 'Blob', value),
+    nullOrUndefined: (value) => assertType(is.nullOrUndefined(value), ""null or undefined"" /* nullOrUndefined */, value),
+    object: (value) => assertType(is.object(value), 'Object', value),
+    iterable: (value) => assertType(is.iterable(value), ""Iterable"" /* iterable */, value),
+    asyncIterable: (value) => assertType(is.asyncIterable(value), ""AsyncIterable"" /* asyncIterable */, value),
+    generator: (value) => assertType(is.generator(value), 'Generator', value),
+    asyncGenerator: (value) => assertType(is.asyncGenerator(value), 'AsyncGenerator', value),
+    nativePromise: (value) => assertType(is.nativePromise(value), ""native Promise"" /* nativePromise */, value),
+    promise: (value) => assertType(is.promise(value), 'Promise', value),
+    generatorFunction: (value) => assertType(is.generatorFunction(value), 'GeneratorFunction', value),
+    asyncGeneratorFunction: (value) => assertType(is.asyncGeneratorFunction(value), 'AsyncGeneratorFunction', value),
+    // eslint-disable-next-line @typescript-eslint/ban-types
+    asyncFunction: (value) => assertType(is.asyncFunction(value), 'AsyncFunction', value),
+    // eslint-disable-next-line @typescript-eslint/ban-types
+    boundFunction: (value) => assertType(is.boundFunction(value), 'Function', value),
+    regExp: (value) => assertType(is.regExp(value), 'RegExp', value),
+    date: (value) => assertType(is.date(value), 'Date', value),
+    error: (value) => assertType(is.error(value), 'Error', value),
+    map: (value) => assertType(is.map(value), 'Map', value),
+    set: (value) => assertType(is.set(value), 'Set', value),
+    weakMap: (value) => assertType(is.weakMap(value), 'WeakMap', value),
+    weakSet: (value) => assertType(is.weakSet(value), 'WeakSet', value),
+    int8Array: (value) => assertType(is.int8Array(value), 'Int8Array', value),
+    uint8Array: (value) => assertType(is.uint8Array(value), 'Uint8Array', value),
+    uint8ClampedArray: (value) => assertType(is.uint8ClampedArray(value), 'Uint8ClampedArray', value),
+    int16Array: (value) => assertType(is.int16Array(value), 'Int16Array', value),
+    uint16Array: (value) => assertType(is.uint16Array(value), 'Uint16Array', value),
+    int32Array: (value) => assertType(is.int32Array(value), 'Int32Array', value),
+    uint32Array: (value) => assertType(is.uint32Array(value), 'Uint32Array', value),
+    float32Array: (value) => assertType(is.float32Array(value), 'Float32Array', value),
+    float64Array: (value) => assertType(is.float64Array(value), 'Float64Array', value),
+    bigInt64Array: (value) => assertType(is.bigInt64Array(value), 'BigInt64Array', value),
+    bigUint64Array: (value) => assertType(is.bigUint64Array(value), 'BigUint64Array', value),
+    arrayBuffer: (value) => assertType(is.arrayBuffer(value), 'ArrayBuffer', value),
+    sharedArrayBuffer: (value) => assertType(is.sharedArrayBuffer(value), 'SharedArrayBuffer', value),
+    dataView: (value) => assertType(is.dataView(value), 'DataView', value),
+    enumCase: (value, targetEnum) => assertType(is.enumCase(value, targetEnum), 'EnumCase', value),
+    urlInstance: (value) => assertType(is.urlInstance(value), 'URL', value),
+    urlString: (value) => assertType(is.urlString(value), ""string with a URL"" /* urlString */, value),
+    truthy: (value) => assertType(is.truthy(value), ""truthy"" /* truthy */, value),
+    falsy: (value) => assertType(is.falsy(value), ""falsy"" /* falsy */, value),
+    nan: (value) => assertType(is.nan(value), ""NaN"" /* nan */, value),
+    primitive: (value) => assertType(is.primitive(value), ""primitive"" /* primitive */, value),
+    integer: (value) => assertType(is.integer(value), ""integer"" /* integer */, value),
+    safeInteger: (value) => assertType(is.safeInteger(value), ""integer"" /* safeInteger */, value),
+    plainObject: (value) => assertType(is.plainObject(value), ""plain object"" /* plainObject */, value),
+    typedArray: (value) => assertType(is.typedArray(value), ""TypedArray"" /* typedArray */, value),
+    arrayLike: (value) => assertType(is.arrayLike(value), ""array-like"" /* arrayLike */, value),
+    domElement: (value) => assertType(is.domElement(value), ""HTMLElement"" /* domElement */, value),
+    observable: (value) => assertType(is.observable(value), 'Observable', value),
+    nodeStream: (value) => assertType(is.nodeStream(value), ""Node.js Stream"" /* nodeStream */, value),
+    infinite: (value) => assertType(is.infinite(value), ""infinite number"" /* infinite */, value),
+    emptyArray: (value) => assertType(is.emptyArray(value), ""empty array"" /* emptyArray */, value),
+    nonEmptyArray: (value) => assertType(is.nonEmptyArray(value), ""non-empty array"" /* nonEmptyArray */, value),
+    emptyString: (value) => assertType(is.emptyString(value), ""empty string"" /* emptyString */, value),
+    emptyStringOrWhitespace: (value) => assertType(is.emptyStringOrWhitespace(value), ""empty string or whitespace"" /* emptyStringOrWhitespace */, value),
+    nonEmptyString: (value) => assertType(is.nonEmptyString(value), ""non-empty string"" /* nonEmptyString */, value),
+    nonEmptyStringAndNotWhitespace: (value) => assertType(is.nonEmptyStringAndNotWhitespace(value), ""non-empty string and not whitespace"" /* nonEmptyStringAndNotWhitespace */, value),
+    emptyObject: (value) => assertType(is.emptyObject(value), ""empty object"" /* emptyObject */, value),
+    nonEmptyObject: (value) => assertType(is.nonEmptyObject(value), ""non-empty object"" /* nonEmptyObject */, value),
+    emptySet: (value) => assertType(is.emptySet(value), ""empty set"" /* emptySet */, value),
+    nonEmptySet: (value) => assertType(is.nonEmptySet(value), ""non-empty set"" /* nonEmptySet */, value),
+    emptyMap: (value) => assertType(is.emptyMap(value), ""empty map"" /* emptyMap */, value),
+    nonEmptyMap: (value) => assertType(is.nonEmptyMap(value), ""non-empty map"" /* nonEmptyMap */, value),
+    propertyKey: (value) => assertType(is.propertyKey(value), 'PropertyKey', value),
+    formData: (value) => assertType(is.formData(value), 'FormData', value),
+    urlSearchParams: (value) => assertType(is.urlSearchParams(value), 'URLSearchParams', value),
+    // Numbers.
+    evenInteger: (value) => assertType(is.evenInteger(value), ""even integer"" /* evenInteger */, value),
+    oddInteger: (value) => assertType(is.oddInteger(value), ""odd integer"" /* oddInteger */, value),
+    // Two arguments.
+    directInstanceOf: (instance, class_) => assertType(is.directInstanceOf(instance, class_), ""T"" /* directInstanceOf */, instance),
+    inRange: (value, range) => assertType(is.inRange(value, range), ""in range"" /* inRange */, value),
+    // Variadic functions.
+    any: (predicate, ...values) => {
+        return assertType(is.any(predicate, ...values), ""predicate returns truthy for any value"" /* any */, values, { multipleValues: true });
+    },
+    all: (predicate, ...values) => assertType(is.all(predicate, ...values), ""predicate returns truthy for all values"" /* all */, values, { multipleValues: true })
+};
+// Some few keywords are reserved, but we'll populate them for Node.js users
+// See https://github.com/Microsoft/TypeScript/issues/2536
+Object.defineProperties(is, {
+    class: {
+        value: is.class_
+    },
+    function: {
+        value: is.function_
+    },
+    null: {
+        value: is.null_
+    }
+});
+Object.defineProperties(exports.assert, {
+    class: {
+        value: exports.assert.class_
+    },
+    function: {
+        value: exports.assert.function_
+    },
+    null: {
+        value: exports.assert.null_
+    }
+});
+exports.default = is;
+// For CommonJS default export support
+module.exports = is;
+module.exports.default = is;
+module.exports.assert = exports.assert;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+Matches any [primitive value](https://developer.mozilla.org/en-US/docs/Glossary/Primitive).
+*/
+export declare type Primitive = null | undefined | string | number | boolean | symbol | bigint;
+/**
+Matches a [`class` constructor](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes).
+*/
+export declare type Class<T = unknown, Arguments extends any[] = any[]> = new (...arguments_: Arguments) => T;
+/**
+Matches any [typed array](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray), like `Uint8Array` or `Float64Array`.
+*/
+export declare type TypedArray = Int8Array | Uint8Array | Uint8ClampedArray | Int16Array | Uint16Array | Int32Array | Uint32Array | Float32Array | Float64Array | BigInt64Array | BigUint64Array;
+declare global {
+    interface SymbolConstructor {
+        readonly observable: symbol;
+    }
+}
+/**
+Matches a value that is like an [Observable](https://github.com/tc39/proposal-observable).
+*/
+export interface ObservableLike {
+    subscribe(observer: (value: unknown) => void): void;
+    [Symbol.observable](): ObservableLike;
+}
+export declare type Falsy = false | 0 | 0n | '' | null | undefined;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+""use strict"";
+// Extracted from https://github.com/sindresorhus/type-fest/blob/78019f42ea888b0cdceb41a4a78163868de57555/index.d.ts
+Object.defineProperty(exports, ""__esModule"", { value: true });"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+MIT License
+
+Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (https://sindresorhus.com)
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@sindresorhus/is@^4.0.0"",
+  ""_id"": ""@sindresorhus/is@4.6.0"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-t09vSN3MdfsyCHoFcTRCH/iUtG7OJ0CsjzB8cjAmKc/va/kIgeDI/TxsigdncE/4be734m0cvIYwNaV4i2XqAw=="",
+  ""_location"": ""/@sindresorhus/is"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@sindresorhus/is@^4.0.0"",
+    ""name"": ""@sindresorhus/is"",
+    ""escapedName"": ""@sindresorhus%2fis"",
+    ""scope"": ""@sindresorhus"",
+    ""rawSpec"": ""^4.0.0"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""^4.0.0""
+  },
+  ""_requiredBy"": [
+    ""/got""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@sindresorhus/is/-/is-4.6.0.tgz"",
+  ""_shasum"": ""3c7c9c46e678feefe7a2e5bb609d3dbd665ffb3f"",
+  ""_spec"": ""@sindresorhus/is@^4.0.0"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\got"",
+  ""author"": {
+    ""name"": ""Sindre Sorhus"",
+    ""email"": ""sindresorhus@gmail.com"",
+    ""url"": ""https://sindresorhus.com""
+  },
+  ""ava"": {
+    ""extensions"": [
+      ""ts""
+    ],
+    ""require"": [
+      ""ts-node/register""
+    ]
+  },
+  ""bugs"": {
+    ""url"": ""https://github.com/sindresorhus/is/issues""
+  },
+  ""bundleDependencies"": false,
+  ""deprecated"": false,
+  ""description"": ""Type check values"",
+  ""devDependencies"": {
+    ""@sindresorhus/tsconfig"": ""^0.7.0"",
+    ""@types/jsdom"": ""^16.1.0"",
+    ""@types/node"": ""^14.0.13"",
+    ""@types/zen-observable"": ""^0.8.0"",
+    ""@typescript-eslint/eslint-plugin"": ""^2.20.0"",
+    ""@typescript-eslint/parser"": ""^2.20.0"",
+    ""ava"": ""^3.3.0"",
+    ""del-cli"": ""^2.0.0"",
+    ""eslint-config-xo-typescript"": ""^0.26.0"",
+    ""jsdom"": ""^16.0.1"",
+    ""rxjs"": ""^6.4.0"",
+    ""tempy"": ""^0.4.0"",
+    ""ts-node"": ""^8.3.0"",
+    ""typescript"": ""~3.8.2"",
+    ""xo"": ""^0.26.1"",
+    ""zen-observable"": ""^0.8.8""
+  },
+  ""engines"": {
+    ""node"": "">=10""
+  },
+  ""files"": [
+    ""dist""
+  ],
+  ""funding"": ""https://github.com/sindresorhus/is?sponsor=1"",
+  ""homepage"": ""https://github.com/sindresorhus/is#readme"",
+  ""keywords"": [
+    ""type"",
+    ""types"",
+    ""is"",
+    ""check"",
+    ""checking"",
+    ""validate"",
+    ""validation"",
+    ""utility"",
+    ""util"",
+    ""typeof"",
+    ""instanceof"",
+    ""object"",
+    ""assert"",
+    ""assertion"",
+    ""test"",
+    ""kind"",
+    ""primitive"",
+    ""verify"",
+    ""compare"",
+    ""typescript"",
+    ""typeguards"",
+    ""types""
+  ],
+  ""license"": ""MIT"",
+  ""main"": ""dist/index.js"",
+  ""name"": ""@sindresorhus/is"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/sindresorhus/is.git""
+  },
+  ""scripts"": {
+    ""build"": ""del dist && tsc"",
+    ""prepare"": ""npm run build"",
+    ""test"": ""xo && ava""
+  },
+  ""sideEffects"": false,
+  ""types"": ""dist/index.d.ts"",
+  ""version"": ""4.6.0"",
+  ""xo"": {
+    ""extends"": ""xo-typescript"",
+    ""extensions"": [
+      ""ts""
+    ],
+    ""parserOptions"": {
+      ""project"": ""./tsconfig.xo.json""
+    },
+    ""globals"": [
+      ""BigInt"",
+      ""BigInt64Array"",
+      ""BigUint64Array""
+    ],
+    ""rules"": {
+      ""@typescript-eslint/promise-function-async"": ""off"",
+      ""@typescript-eslint/no-empty-function"": ""off"",
+      ""@typescript-eslint/explicit-function-return-type"": ""off""
+    }
+  }
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# is
+
+> Type check values
+
+For example, `is.string('🦄') //=> true`
+
+<img src=""header.gif"" width=""182"" align=""right"">
+
+## Highlights
+
+- Written in TypeScript
+- [Extensive use of type guards](#type-guards)
+- [Supports type assertions](#type-assertions)
+- [Aware of generic type parameters](#generic-type-parameters) (use with caution)
+- Actively maintained
+- ![Millions of downloads per week](https://img.shields.io/npm/dw/@sindresorhus/is)
+
+## Install
+
+```sh
+npm install @sindresorhus/is
+```
+
+## Usage
+
+```js
+const is = require('@sindresorhus/is');
+
+is('🦄');
+//=> 'string'
+
+is(new Map());
+//=> 'Map'
+
+is.number(6);
+//=> true
+```
+
+[Assertions](#type-assertions) perform the same type checks, but throw an error if the type does not match.
+
+```js
+const {assert} = require('@sindresorhus/is');
+
+assert.string(2);
+//=> Error: Expected value which is `string`, received value of type `number`.
+```
+
+And with TypeScript:
+
+```ts
+import {assert} from '@sindresorhus/is';
+
+assert.string(foo);
+// `foo` is now typed as a `string`.
+```
+
+## API
+
+### is(value)
+
+Returns the type of `value`.
+
+Primitives are lowercase and object types are camelcase.
+
+Example:
+
+- `'undefined'`
+- `'null'`
+- `'string'`
+- `'symbol'`
+- `'Array'`
+- `'Function'`
+- `'Object'`
+
+Note: It will throw an error if you try to feed it object-wrapped primitives, as that's a bad practice. For example `new String('foo')`.
+
+### is.{method}
+
+All the below methods accept a value and returns a boolean for whether the value is of the desired type.
+
+#### Primitives
+
+##### .undefined(value)
+##### .null(value)
+
+**Note:** TypeScript users must use `.null_()` because of a TypeScript naming limitation.
+
+##### .string(value)
+##### .number(value)
+
+Note: `is.number(NaN)` returns `false`. This intentionally deviates from `typeof` behavior to increase user-friendliness of `is` type checks.
+
+##### .boolean(value)
+##### .symbol(value)
+##### .bigint(value)
+
+#### Built-in types
+
+##### .array(value, assertion?)
+
+Returns true if `value` is an array and all of its items match the assertion (if provided).
+
+```js
+is.array(value); // Validate `value` is an array.
+is.array(value, is.number); // Validate `value` is an array and all of its items are numbers.
+```
+
+##### .function(value)
+
+**Note:** TypeScript users must use `.function_()` because of a TypeScript naming limitation.
+
+##### .buffer(value)
+##### .blob(value)
+##### .object(value)
+
+Keep in mind that [functions are objects too](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions).
+
+##### .numericString(value)
+
+Returns `true` for a string that represents a number satisfying `is.number`, for example, `'42'` and `'-8.3'`.
+
+Note: `'NaN'` returns `false`, but `'Infinity'` and `'-Infinity'` return `true`.
+
+##### .regExp(value)
+##### .date(value)
+##### .error(value)
+##### .nativePromise(value)
+##### .promise(value)
+
+Returns `true` for any object with a `.then()` and `.catch()` method. Prefer this one over `.nativePromise()` as you usually want to allow userland promise implementations too.
+
+##### .generator(value)
+
+Returns `true` for any object that implements its own `.next()` and `.throw()` methods and has a function definition for `Symbol.iterator`.
+
+##### .generatorFunction(value)
+
+##### .asyncFunction(value)
+
+Returns `true` for any `async` function that can be called with the `await` operator.
+
+```js
+is.asyncFunction(async () => {});
+//=> true
+
+is.asyncFunction(() => {});
+//=> false
+```
+
+##### .asyncGenerator(value)
+
+```js
+is.asyncGenerator(
+	(async function * () {
+		yield 4;
+	})()
+);
+//=> true
+
+is.asyncGenerator(
+	(function * () {
+		yield 4;
+	})()
+);
+//=> false
+```
+
+##### .asyncGeneratorFunction(value)
+
+```js
+is.asyncGeneratorFunction(async function * () {
+	yield 4;
+});
+//=> true
+
+is.asyncGeneratorFunction(function * () {
+	yield 4;
+});
+//=> false
+```
+
+##### .boundFunction(value)
+
+Returns `true` for any `bound` function.
+
+```js
+is.boundFunction(() => {});
+//=> true
+
+is.boundFunction(function () {}.bind(null));
+//=> true
+
+is.boundFunction(function () {});
+//=> false
+```
+
+##### .map(value)
+##### .set(value)
+##### .weakMap(value)
+##### .weakSet(value)
+
+#### Typed arrays
+
+##### .int8Array(value)
+##### .uint8Array(value)
+##### .uint8ClampedArray(value)
+##### .int16Array(value)
+##### .uint16Array(value)
+##### .int32Array(value)
+##### .uint32Array(value)
+##### .float32Array(value)
+##### .float64Array(value)
+##### .bigInt64Array(value)
+##### .bigUint64Array(value)
+
+#### Structured data
+
+##### .arrayBuffer(value)
+##### .sharedArrayBuffer(value)
+##### .dataView(value)
+
+##### .enumCase(value, enum)
+
+TypeScript-only. Returns `true` if `value` is a member of `enum`.
+
+```ts
+enum Direction {
+	Ascending = 'ascending',
+	Descending = 'descending'
+}
+
+is.enumCase('ascending', Direction);
+//=> true
+
+is.enumCase('other', Direction);
+//=> false
+```
+
+#### Emptiness
+
+##### .emptyString(value)
+
+Returns `true` if the value is a `string` and the `.length` is 0.
+
+##### .emptyStringOrWhitespace(value)
+
+Returns `true` if `is.emptyString(value)` or if it's a `string` that is all whitespace.
+
+##### .nonEmptyString(value)
+
+Returns `true` if the value is a `string` and the `.length` is more than 0.
+
+##### .nonEmptyStringAndNotWhitespace(value)
+
+Returns `true` if the value is a `string` that is not empty and not whitespace.
+
+```js
+const values = ['property1', '', null, 'property2', '    ', undefined];
+
+values.filter(is.nonEmptyStringAndNotWhitespace);
+//=> ['property1', 'property2']
+```
+
+##### .emptyArray(value)
+
+Returns `true` if the value is an `Array` and the `.length` is 0.
+
+##### .nonEmptyArray(value)
+
+Returns `true` if the value is an `Array` and the `.length` is more than 0.
+
+##### .emptyObject(value)
+
+Returns `true` if the value is an `Object` and `Object.keys(value).length` is 0.
+
+Please note that `Object.keys` returns only own enumerable properties. Hence something like this can happen:
+
+```js
+const object1 = {};
+
+Object.defineProperty(object1, 'property1', {
+	value: 42,
+	writable: true,
+	enumerable: false,
+	configurable: true
+});
+
+is.emptyObject(object1);
+//=> true
+```
+
+##### .nonEmptyObject(value)
+
+Returns `true` if the value is an `Object` and `Object.keys(value).length` is more than 0.
+
+##### .emptySet(value)
+
+Returns `true` if the value is a `Set` and the `.size` is 0.
+
+##### .nonEmptySet(Value)
+
+Returns `true` if the value is a `Set` and the `.size` is more than 0.
+
+##### .emptyMap(value)
+
+Returns `true` if the value is a `Map` and the `.size` is 0.
+
+##### .nonEmptyMap(value)
+
+Returns `true` if the value is a `Map` and the `.size` is more than 0.
+
+#### Miscellaneous
+
+##### .directInstanceOf(value, class)
+
+Returns `true` if `value` is a direct instance of `class`.
+
+```js
+is.directInstanceOf(new Error(), Error);
+//=> true
+
+class UnicornError extends Error {}
+
+is.directInstanceOf(new UnicornError(), Error);
+//=> false
+```
+
+##### .urlInstance(value)
+
+Returns `true` if `value` is an instance of the [`URL` class](https://developer.mozilla.org/en-US/docs/Web/API/URL).
+
+```js
+const url = new URL('https://example.com');
+
+is.urlInstance(url);
+//=> true
+```
+
+##### .urlString(value)
+
+Returns `true` if `value` is a URL string.
+
+Note: this only does basic checking using the [`URL` class](https://developer.mozilla.org/en-US/docs/Web/API/URL) constructor.
+
+```js
+const url = 'https://example.com';
+
+is.urlString(url);
+//=> true
+
+is.urlString(new URL(url));
+//=> false
+```
+
+##### .truthy(value)
+
+Returns `true` for all values that evaluate to true in a boolean context:
+
+```js
+is.truthy('🦄');
+//=> true
+
+is.truthy(undefined);
+//=> false
+```
+
+##### .falsy(value)
+
+Returns `true` if `value` is one of: `false`, `0`, `''`, `null`, `undefined`, `NaN`.
+
+##### .nan(value)
+##### .nullOrUndefined(value)
+##### .primitive(value)
+
+JavaScript primitives are as follows: `null`, `undefined`, `string`, `number`, `boolean`, `symbol`.
+
+##### .integer(value)
+
+##### .safeInteger(value)
+
+Returns `true` if `value` is a [safe integer](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/isSafeInteger).
+
+##### .plainObject(value)
+
+An object is plain if it's created by either `{}`, `new Object()`, or `Object.create(null)`.
+
+##### .iterable(value)
+##### .asyncIterable(value)
+##### .class(value)
+
+Returns `true` for instances created by a class.
+
+**Note:** TypeScript users must use `.class_()` because of a TypeScript naming limitation.
+
+##### .typedArray(value)
+
+##### .arrayLike(value)
+
+A `value` is array-like if it is not a function and has a `value.length` that is a safe integer greater than or equal to 0.
+
+```js
+is.arrayLike(document.forms);
+//=> true
+
+function foo() {
+	is.arrayLike(arguments);
+	//=> true
+}
+foo();
+```
+
+##### .inRange(value, range)
+
+Check if `value` (number) is in the given `range`. The range is an array of two values, lower bound and upper bound, in no specific order.
+
+```js
+is.inRange(3, [0, 5]);
+is.inRange(3, [5, 0]);
+is.inRange(0, [-2, 2]);
+```
+
+##### .inRange(value, upperBound)
+
+Check if `value` (number) is in the range of `0` to `upperBound`.
+
+```js
+is.inRange(3, 10);
+```
+
+##### .domElement(value)
+
+Returns `true` if `value` is a DOM Element.
+
+##### .nodeStream(value)
+
+Returns `true` if `value` is a Node.js [stream](https://nodejs.org/api/stream.html).
+
+```js
+const fs = require('fs');
+
+is.nodeStream(fs.createReadStream('unicorn.png'));
+//=> true
+```
+
+##### .observable(value)
+
+Returns `true` if `value` is an `Observable`.
+
+```js
+const {Observable} = require('rxjs');
+
+is.observable(new Observable());
+//=> true
+```
+
+##### .infinite(value)
+
+Check if `value` is `Infinity` or `-Infinity`.
+
+##### .evenInteger(value)
+
+Returns `true` if `value` is an even integer.
+
+##### .oddInteger(value)
+
+Returns `true` if `value` is an odd integer.
+
+##### .propertyKey(value)
+
+Returns `true` if `value` can be used as an object property key (either `string`, `number`, or `symbol`).
+
+##### .formData(value)
+
+Returns `true` if `value` is an instance of the [`FormData` class](https://developer.mozilla.org/en-US/docs/Web/API/FormData).
+
+```js
+const data = new FormData();
+
+is.formData(data);
+//=> true
+```
+
+##### .urlSearchParams(value)
+
+Returns `true` if `value` is an instance of the [`URLSearchParams` class](https://developer.mozilla.org/en-US/docs/Web/API/URLSearchParams).
+
+```js
+const searchParams = new URLSearchParams();
+
+is.urlSearchParams(searchParams);
+//=> true
+```
+
+##### .any(predicate | predicate[], ...values)
+
+Using a single `predicate` argument, returns `true` if **any** of the input `values` returns true in the `predicate`:
+
+```js
+is.any(is.string, {}, true, '🦄');
+//=> true
+
+is.any(is.boolean, 'unicorns', [], new Map());
+//=> false
+```
+
+Using an array of `predicate[]`, returns `true` if **any** of the input `values` returns true for **any** of the `predicates` provided in an array:
+
+```js
+is.any([is.string, is.number], {}, true, '🦄');
+//=> true
+
+is.any([is.boolean, is.number], 'unicorns', [], new Map());
+//=> false
+```
+
+##### .all(predicate, ...values)
+
+Returns `true` if **all** of the input `values` returns true in the `predicate`:
+
+```js
+is.all(is.object, {}, new Map(), new Set());
+//=> true
+
+is.all(is.string, '🦄', [], 'unicorns');
+//=> false
+```
+
+## Type guards
+
+When using `is` together with TypeScript, [type guards](http://www.typescriptlang.org/docs/handbook/advanced-types.html#type-guards-and-differentiating-types) are being used extensively to infer the correct type inside if-else statements.
+
+```ts
+import is from '@sindresorhus/is';
+
+const padLeft = (value: string, padding: string | number) => {
+	if (is.number(padding)) {
+		// `padding` is typed as `number`
+		return Array(padding + 1).join(' ') + value;
+	}
+
+	if (is.string(padding)) {
+		// `padding` is typed as `string`
+		return padding + value;
+	}
+
+	throw new TypeError(`Expected 'padding' to be of type 'string' or 'number', got '${is(padding)}'.`);
+}
+
+padLeft('🦄', 3);
+//=> '   🦄'
+
+padLeft('🦄', '🌈');
+//=> '🌈🦄'
+```
+
+## Type assertions
+
+The type guards are also available as [type assertions](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-7.html#assertion-functions), which throw an error for unexpected types. It is a convenient one-line version of the often repetitive ""if-not-expected-type-throw"" pattern.
+
+```ts
+import {assert} from '@sindresorhus/is';
+
+const handleMovieRatingApiResponse = (response: unknown) => {
+	assert.plainObject(response);
+	// `response` is now typed as a plain `object` with `unknown` properties.
+
+	assert.number(response.rating);
+	// `response.rating` is now typed as a `number`.
+
+	assert.string(response.title);
+	// `response.title` is now typed as a `string`.
+
+	return `${response.title} (${response.rating * 10})`;
+};
+
+handleMovieRatingApiResponse({rating: 0.87, title: 'The Matrix'});
+//=> 'The Matrix (8.7)'
+
+// This throws an error.
+handleMovieRatingApiResponse({rating: '🦄'});
+```
+
+## Generic type parameters
+
+The type guards and type assertions are aware of [generic type parameters](https://www.typescriptlang.org/docs/handbook/generics.html), such as `Promise<T>` and `Map<Key, Value>`. The default is `unknown` for most cases, since `is` cannot check them at runtime. If the generic type is known at compile-time, either implicitly (inferred) or explicitly (provided), `is` propagates the type so it can be used later.
+
+Use generic type parameters with caution. They are only checked by the TypeScript compiler, and not checked by `is` at runtime. This can lead to unexpected behavior, where the generic type is _assumed_ at compile-time, but actually is something completely different at runtime. It is best to use `unknown` (default) and type-check the value of the generic type parameter at runtime with `is` or `assert`.
+
+```ts
+import {assert} from '@sindresorhus/is';
+
+async function badNumberAssumption(input: unknown) {
+	// Bad assumption about the generic type parameter fools the compile-time type system.
+	assert.promise<number>(input);
+	// `input` is a `Promise` but only assumed to be `Promise<number>`.
+
+	const resolved = await input;
+	// `resolved` is typed as `number` but was not actually checked at runtime.
+
+	// Multiplication will return NaN if the input promise did not actually contain a number.
+	return 2 * resolved;
+}
+
+async function goodNumberAssertion(input: unknown) {
+	assert.promise(input);
+	// `input` is typed as `Promise<unknown>`
+
+	const resolved = await input;
+	// `resolved` is typed as `unknown`
+
+	assert.number(resolved);
+	// `resolved` is typed as `number`
+
+	// Uses runtime checks so only numbers will reach the multiplication.
+	return 2 * resolved;
+}
+
+badNumberAssumption(Promise.resolve('An unexpected string'));
+//=> NaN
+
+// This correctly throws an error because of the unexpected string value.
+goodNumberAssertion(Promise.resolve('An unexpected string'));
+```
+
+## FAQ
+
+### Why yet another type checking module?
+
+There are hundreds of type checking modules on npm, unfortunately, I couldn't find any that fit my needs:
+
+- Includes both type methods and ability to get the type
+- Types of primitives returned as lowercase and object types as camelcase
+- Covers all built-ins
+- Unsurprising behavior
+- Well-maintained
+- Comprehensive test suite
+
+For the ones I found, pick 3 of these.
+
+The most common mistakes I noticed in these modules was using `instanceof` for type checking, forgetting that functions are objects, and omitting `symbol` as a primitive.
+
+### Why not just use `instanceof` instead of this package?
+
+`instanceof` does not work correctly for all types and it does not work across [realms](https://stackoverflow.com/a/49832343/64949). Examples of realms are iframes, windows, web workers, and the `vm` module in Node.js.
+
+## For enterprise
+
+Available as part of the Tidelift Subscription.
+
+The maintainers of @sindresorhus/is and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. [Learn more.](https://tidelift.com/subscription/pkg/npm-sindresorhus-is?utm_source=npm-sindresorhus-is&utm_medium=referral&utm_campaign=enterprise&utm_term=repo)
+
+## Related
+
+- [ow](https://github.com/sindresorhus/ow) - Function argument validation for humans
+- [is-stream](https://github.com/sindresorhus/is-stream) - Check if something is a Node.js stream
+- [is-observable](https://github.com/sindresorhus/is-observable) - Check if a value is an Observable
+- [file-type](https://github.com/sindresorhus/file-type) - Detect the file type of a Buffer/Uint8Array
+- [is-ip](https://github.com/sindresorhus/is-ip) - Check if a string is an IP address
+- [is-array-sorted](https://github.com/sindresorhus/is-array-sorted) - Check if an Array is sorted
+- [is-error-constructor](https://github.com/sindresorhus/is-error-constructor) - Check if a value is an error constructor
+- [is-empty-iterable](https://github.com/sindresorhus/is-empty-iterable) - Check if an Iterable is empty
+- [is-blob](https://github.com/sindresorhus/is-blob) - Check if a value is a Blob - File-like object of immutable, raw data
+- [has-emoji](https://github.com/sindresorhus/has-emoji) - Check whether a string has any emoji
+
+## Maintainers
+
+- [Sindre Sorhus](https://github.com/sindresorhus)
+- [Giora Guttsait](https://github.com/gioragutt)
+- [Brandon Smith](https://github.com/brandon93s)"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+MIT License
+
+Copyright (c) 2018 Szymon Marczak
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the ""Software""), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# http-timer
+> Timings for HTTP requests
+
+[![Build Status](https://travis-ci.org/szmarczak/http-timer.svg?branch=master)](https://travis-ci.org/szmarczak/http-timer)
+[![Coverage Status](https://coveralls.io/repos/github/szmarczak/http-timer/badge.svg?branch=master)](https://coveralls.io/github/szmarczak/http-timer?branch=master)
+[![install size](https://packagephobia.now.sh/badge?p=@szmarczak/http-timer)](https://packagephobia.now.sh/result?p=@szmarczak/http-timer)
+
+Inspired by the [`request` package](https://github.com/request/request).
+
+## Installation
+
+NPM:
+
+> `npm install @szmarczak/http-timer`
+
+Yarn:
+
+> `yarn add @szmarczak/http-timer`
+
+## Usage
+**Note:**
+> - The measured events resemble Node.js events, not the kernel ones.
+> - Sending a chunk greater than [`highWaterMark`](https://nodejs.org/api/stream.html#stream_new_stream_writable_options) will result in invalid `upload` and `response` timings. You can avoid this by splitting the payload into smaller chunks.
+
+```js
+const https = require('https');
+const timer = require('@szmarczak/http-timer');
+
+const request = https.get('https://httpbin.org/anything');
+timer(request);
+
+request.once('response', response => {
+	response.resume();
+	response.once('end', () => {
+		console.log(response.timings); // You can use `request.timings` as well
+	});
+});
+
+// {
+//   start: 1572712180361,
+//   socket: 1572712180362,
+//   lookup: 1572712180415,
+//   connect: 1572712180571,
+//   upload: 1572712180884,
+//   response: 1572712181037,
+//   end: 1572712181039,
+//   error: undefined,
+//   abort: undefined,
+//   phases: {
+//     wait: 1,
+//     dns: 53,
+//     tcp: 156,
+//     request: 313,
+//     firstByte: 153,
+//     download: 2,
+//     total: 678
+//   }
+// }
+```
+
+## API
+
+### timer(request)
+
+Returns: `Object`
+
+**Note**: The time is a `number` representing the milliseconds elapsed since the UNIX epoch.
+
+- `start` - Time when the request started.
+- `socket` - Time when a socket was assigned to the request.
+- `lookup` - Time when the DNS lookup finished.
+- `connect` - Time when the socket successfully connected.
+- `secureConnect` - Time when the socket securely connected.
+- `upload` - Time when the request finished uploading.
+- `response` - Time when the request fired `response` event.
+- `end` - Time when the response fired `end` event.
+- `error` - Time when the request fired `error` event.
+- `abort` - Time when the request fired `abort` event.
+- `phases`
+	- `wait` - `timings.socket - timings.start`
+	- `dns` - `timings.lookup - timings.socket`
+	- `tcp` - `timings.connect - timings.lookup`
+	- `tls` - `timings.secureConnect - timings.connect`
+	- `request` - `timings.upload - (timings.secureConnect || timings.connect)`
+	- `firstByte` - `timings.response - timings.upload`
+	- `download` - `timings.end - timings.response`
+	- `total` - `(timings.end || timings.error || timings.abort) - timings.start`
+
+If something has not been measured yet, it will be `undefined`.
+
+## License
+
+MIT"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/// <reference types=""node"" />
+import { ClientRequest, IncomingMessage } from 'http';
+export interface Timings {
+    start: number;
+    socket?: number;
+    lookup?: number;
+    connect?: number;
+    secureConnect?: number;
+    upload?: number;
+    response?: number;
+    end?: number;
+    error?: number;
+    abort?: number;
+    phases: {
+        wait?: number;
+        dns?: number;
+        tcp?: number;
+        tls?: number;
+        request?: number;
+        firstByte?: number;
+        download?: number;
+        total?: number;
+    };
+}
+export interface ClientRequestWithTimings extends ClientRequest {
+    timings?: Timings;
+}
+export interface IncomingMessageWithTimings extends IncomingMessage {
+    timings?: Timings;
+}
+declare const timer: (request: ClientRequestWithTimings) => Timings;
+export default timer;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+""use strict"";
+Object.defineProperty(exports, ""__esModule"", { value: true });
+const defer_to_connect_1 = require(""defer-to-connect"");
+const util_1 = require(""util"");
+const nodejsMajorVersion = Number(process.versions.node.split('.')[0]);
+const timer = (request) => {
+    if (request.timings) {
+        return request.timings;
+    }
+    const timings = {
+        start: Date.now(),
+        socket: undefined,
+        lookup: undefined,
+        connect: undefined,
+        secureConnect: undefined,
+        upload: undefined,
+        response: undefined,
+        end: undefined,
+        error: undefined,
+        abort: undefined,
+        phases: {
+            wait: undefined,
+            dns: undefined,
+            tcp: undefined,
+            tls: undefined,
+            request: undefined,
+            firstByte: undefined,
+            download: undefined,
+            total: undefined
+        }
+    };
+    request.timings = timings;
+    const handleError = (origin) => {
+        const emit = origin.emit.bind(origin);
+        origin.emit = (event, ...args) => {
+            // Catches the `error` event
+            if (event === 'error') {
+                timings.error = Date.now();
+                timings.phases.total = timings.error - timings.start;
+                origin.emit = emit;
+            }
+            // Saves the original behavior
+            return emit(event, ...args);
+        };
+    };
+    handleError(request);
+    const onAbort = () => {
+        timings.abort = Date.now();
+        // Let the `end` response event be responsible for setting the total phase,
+        // unless the Node.js major version is >= 13.
+        if (!timings.response || nodejsMajorVersion >= 13) {
+            timings.phases.total = Date.now() - timings.start;
+        }
+    };
+    request.prependOnceListener('abort', onAbort);
+    const onSocket = (socket) => {
+        timings.socket = Date.now();
+        timings.phases.wait = timings.socket - timings.start;
+        if (util_1.types.isProxy(socket)) {
+            return;
+        }
+        const lookupListener = () => {
+            timings.lookup = Date.now();
+            timings.phases.dns = timings.lookup - timings.socket;
+        };
+        socket.prependOnceListener('lookup', lookupListener);
+        defer_to_connect_1.default(socket, {
+            connect: () => {
+                timings.connect = Date.now();
+                if (timings.lookup === undefined) {
+                    socket.removeListener('lookup', lookupListener);
+                    timings.lookup = timings.connect;
+                    timings.phases.dns = timings.lookup - timings.socket;
+                }
+                timings.phases.tcp = timings.connect - timings.lookup;
+                // This callback is called before flushing any data,
+                // so we don't need to set `timings.phases.request` here.
+            },
+            secureConnect: () => {
+                timings.secureConnect = Date.now();
+                timings.phases.tls = timings.secureConnect - timings.connect;
+            }
+        });
+    };
+    if (request.socket) {
+        onSocket(request.socket);
+    }
+    else {
+        request.prependOnceListener('socket', onSocket);
+    }
+    const onUpload = () => {
+        var _a;
+        timings.upload = Date.now();
+        timings.phases.request = timings.upload - ((_a = timings.secureConnect) !== null && _a !== void 0 ? _a : timings.connect);
+    };
+    const writableFinished = () => {
+        if (typeof request.writableFinished === 'boolean') {
+            return request.writableFinished;
+        }
+        // Node.js doesn't have `request.writableFinished` property
+        return request.finished && request.outputSize === 0 && (!request.socket || request.socket.writableLength === 0);
+    };
+    if (writableFinished()) {
+        onUpload();
+    }
+    else {
+        request.prependOnceListener('finish', onUpload);
+    }
+    request.prependOnceListener('response', (response) => {
+        timings.response = Date.now();
+        timings.phases.firstByte = timings.response - timings.upload;
+        response.timings = timings;
+        handleError(response);
+        response.prependOnceListener('end', () => {
+            timings.end = Date.now();
+            timings.phases.download = timings.end - timings.response;
+            timings.phases.total = timings.end - timings.start;
+        });
+        response.prependOnceListener('aborted', onAbort);
+    });
+    return timings;
+};
+exports.default = timer;
+// For CommonJS default export support
+module.exports = timer;
+module.exports.default = timer;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@szmarczak/http-timer@^4.0.5"",
+  ""_id"": ""@szmarczak/http-timer@4.0.6"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-4BAffykYOgO+5nzBWYwE3W90sBgLJoUPRWWcL8wlyiM8IB8ipJz3UMJ9KXQd1RKQXpKp8Tutn80HZtWsu2u76w=="",
+  ""_location"": ""/@szmarczak/http-timer"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@szmarczak/http-timer@^4.0.5"",
+    ""name"": ""@szmarczak/http-timer"",
+    ""escapedName"": ""@szmarczak%2fhttp-timer"",
+    ""scope"": ""@szmarczak"",
+    ""rawSpec"": ""^4.0.5"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""^4.0.5""
+  },
+  ""_requiredBy"": [
+    ""/got""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@szmarczak/http-timer/-/http-timer-4.0.6.tgz"",
+  ""_shasum"": ""b4a914bb62e7c272d4e5989fe4440f812ab1d807"",
+  ""_spec"": ""@szmarczak/http-timer@^4.0.5"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\got"",
+  ""author"": {
+    ""name"": ""Szymon Marczak""
+  },
+  ""ava"": {
+    ""typescript"": {
+      ""compile"": false,
+      ""rewritePaths"": {
+        ""tests/"": ""dist/tests/""
+      }
+    }
+  },
+  ""bugs"": {
+    ""url"": ""https://github.com/szmarczak/http-timer/issues""
+  },
+  ""bundleDependencies"": false,
+  ""dependencies"": {
+    ""defer-to-connect"": ""^2.0.0""
+  },
+  ""deprecated"": false,
+  ""description"": ""Timings for HTTP requests"",
+  ""devDependencies"": {
+    ""@ava/typescript"": ""^2.0.0"",
+    ""@sindresorhus/tsconfig"": ""^1.0.2"",
+    ""@types/node"": ""^16.3.1"",
+    ""ava"": ""^3.15.0"",
+    ""coveralls"": ""^3.1.1"",
+    ""del-cli"": ""^3.0.1"",
+    ""http2-wrapper"": ""^2.0.7"",
+    ""nyc"": ""^15.1.0"",
+    ""p-event"": ""^4.2.0"",
+    ""typescript"": ""^4.3.5"",
+    ""xo"": ""^0.39.1""
+  },
+  ""engines"": {
+    ""node"": "">=10""
+  },
+  ""files"": [
+    ""dist/source""
+  ],
+  ""homepage"": ""https://github.com/szmarczak/http-timer#readme"",
+  ""keywords"": [
+    ""http"",
+    ""https"",
+    ""timer"",
+    ""timings""
+  ],
+  ""license"": ""MIT"",
+  ""main"": ""dist/source"",
+  ""name"": ""@szmarczak/http-timer"",
+  ""nyc"": {
+    ""extension"": [
+      "".ts""
+    ],
+    ""exclude"": [
+      ""**/tests/**""
+    ]
+  },
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/szmarczak/http-timer.git""
+  },
+  ""scripts"": {
+    ""build"": ""del-cli dist && tsc"",
+    ""coveralls"": ""nyc report --reporter=text-lcov | coveralls"",
+    ""prepare"": ""npm run build"",
+    ""test"": ""xo && tsc --noEmit && nyc ava""
+  },
+  ""types"": ""dist/source"",
+  ""version"": ""4.0.6"",
+  ""xo"": {
+    ""rules"": {
+      ""@typescript-eslint/no-non-null-assertion"": ""off""
+    }
+  }
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+    MIT License
+
+    Copyright (c) Microsoft Corporation.
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy
+    of this software and associated documentation files (the ""Software""), to deal
+    in the Software without restriction, including without limitation the rights
+    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+    copies of the Software, and to permit persons to whom the Software is
+    furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all
+    copies or substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+    SOFTWARE"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# Installation
+> `npm install --save @types/cacheable-request`
+
+# Summary
+This package contains type definitions for cacheable-request (https://github.com/lukechilds/cacheable-request#readme).
+
+# Details
+Files were exported from https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/cacheable-request.
+
+### Additional Details
+ * Last updated: Tue, 06 Jul 2021 18:05:48 GMT
+ * Dependencies: [@types/keyv](https://npmjs.com/package/@types/keyv), [@types/http-cache-semantics](https://npmjs.com/package/@types/http-cache-semantics), [@types/responselike](https://npmjs.com/package/@types/responselike), [@types/node](https://npmjs.com/package/@types/node)
+ * Global values: none
+
+# Credits
+These definitions were written by [BendingBender](https://github.com/BendingBender), and [Paul Melnikow](https://github.com/paulmelnikow)."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+// Type definitions for cacheable-request 6.0
+// Project: https://github.com/lukechilds/cacheable-request#readme
+// Definitions by: BendingBender <https://github.com/BendingBender>
+//                 Paul Melnikow <https://github.com/paulmelnikow>
+// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped
+// TypeScript Version: 2.3
+
+/// <reference types=""node"" />
+
+import { request, RequestOptions, ClientRequest, ServerResponse } from 'http';
+import { URL } from 'url';
+import { EventEmitter } from 'events';
+import { Store } from 'keyv';
+import { Options as CacheSemanticsOptions } from 'http-cache-semantics';
+import ResponseLike = require('responselike');
+
+export = CacheableRequest;
+
+declare const CacheableRequest: CacheableRequest;
+
+type RequestFn = typeof request;
+
+interface CacheableRequest {
+    new (requestFn: RequestFn, storageAdapter?: string | CacheableRequest.StorageAdapter): (
+        opts: string | URL | (RequestOptions & CacheSemanticsOptions),
+        cb?: (response: ServerResponse | ResponseLike) => void
+    ) => CacheableRequest.Emitter;
+
+    RequestError: typeof RequestErrorCls;
+    CacheError: typeof CacheErrorCls;
+}
+
+declare namespace CacheableRequest {
+    type StorageAdapter = Store<any>;
+
+    interface Options {
+        /**
+         * If the cache should be used. Setting this to `false` will completely bypass the cache for the current request.
+         * @default true
+         */
+        cache?: boolean | undefined;
+
+        /**
+         * If set to `true` once a cached resource has expired it is deleted and will have to be re-requested.
+         *
+         * If set to `false`, after a cached resource's TTL expires it is kept in the cache and will be revalidated
+         * on the next request with `If-None-Match`/`If-Modified-Since` headers.
+         * @default false
+         */
+        strictTtl?: boolean | undefined;
+
+        /**
+         * Limits TTL. The `number` represents milliseconds.
+         * @default undefined
+         */
+        maxTtl?: number | undefined;
+
+        /**
+         * When set to `true`, if the DB connection fails we will automatically fallback to a network request.
+         * DB errors will still be emitted to notify you of the problem even though the request callback may succeed.
+         * @default false
+         */
+        automaticFailover?: boolean | undefined;
+
+        /**
+         * Forces refreshing the cache. If the response could be retrieved from the cache, it will perform a
+         * new request and override the cache instead.
+         * @default false
+         */
+        forceRefresh?: boolean | undefined;
+    }
+
+    interface Emitter extends EventEmitter {
+        addListener(event: 'request', listener: (request: ClientRequest) => void): this;
+        addListener(
+            event: 'response',
+            listener: (response: ServerResponse | ResponseLike) => void
+        ): this;
+        addListener(event: 'error', listener: (error: RequestError | CacheError) => void): this;
+        on(event: 'request', listener: (request: ClientRequest) => void): this;
+        on(event: 'response', listener: (response: ServerResponse | ResponseLike) => void): this;
+        on(event: 'error', listener: (error: RequestError | CacheError) => void): this;
+        once(event: 'request', listener: (request: ClientRequest) => void): this;
+        once(event: 'response', listener: (response: ServerResponse | ResponseLike) => void): this;
+        once(event: 'error', listener: (error: RequestError | CacheError) => void): this;
+        prependListener(event: 'request', listener: (request: ClientRequest) => void): this;
+        prependListener(
+            event: 'response',
+            listener: (response: ServerResponse | ResponseLike) => void
+        ): this;
+        prependListener(event: 'error', listener: (error: RequestError | CacheError) => void): this;
+        prependOnceListener(event: 'request', listener: (request: ClientRequest) => void): this;
+        prependOnceListener(
+            event: 'response',
+            listener: (response: ServerResponse | ResponseLike) => void
+        ): this;
+        prependOnceListener(
+            event: 'error',
+            listener: (error: RequestError | CacheError) => void
+        ): this;
+        removeListener(event: 'request', listener: (request: ClientRequest) => void): this;
+        removeListener(
+            event: 'response',
+            listener: (response: ServerResponse | ResponseLike) => void
+        ): this;
+        removeListener(event: 'error', listener: (error: RequestError | CacheError) => void): this;
+        off(event: 'request', listener: (request: ClientRequest) => void): this;
+        off(event: 'response', listener: (response: ServerResponse | ResponseLike) => void): this;
+        off(event: 'error', listener: (error: RequestError | CacheError) => void): this;
+        removeAllListeners(event?: 'request' | 'response' | 'error'): this;
+        listeners(event: 'request'): Array<(request: ClientRequest) => void>;
+        listeners(event: 'response'): Array<(response: ServerResponse | ResponseLike) => void>;
+        listeners(event: 'error'): Array<(error: RequestError | CacheError) => void>;
+        rawListeners(event: 'request'): Array<(request: ClientRequest) => void>;
+        rawListeners(event: 'response'): Array<(response: ServerResponse | ResponseLike) => void>;
+        rawListeners(event: 'error'): Array<(error: RequestError | CacheError) => void>;
+        emit(event: 'request', request: ClientRequest): boolean;
+        emit(event: 'response', response: ServerResponse | ResponseLike): boolean;
+        emit(event: 'error', error: RequestError | CacheError): boolean;
+        eventNames(): Array<'request' | 'response' | 'error'>;
+        listenerCount(type: 'request' | 'response' | 'error'): number;
+    }
+
+    type RequestError = RequestErrorCls;
+    type CacheError = CacheErrorCls;
+}
+
+declare class RequestErrorCls extends Error {
+    readonly name: 'RequestError';
+
+    constructor(error: Error);
+}
+declare class CacheErrorCls extends Error {
+    readonly name: 'CacheError';
+
+    constructor(error: Error);
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@types/cacheable-request@^6.0.1"",
+  ""_id"": ""@types/cacheable-request@6.0.2"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-B3xVo+dlKM6nnKTcmm5ZtY/OL8bOAOd2Olee9M1zft65ox50OzjEHW91sDiU9j6cvW8Ejg1/Qkf4xd2kugApUA=="",
+  ""_location"": ""/@types/cacheable-request"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@types/cacheable-request@^6.0.1"",
+    ""name"": ""@types/cacheable-request"",
+    ""escapedName"": ""@types%2fcacheable-request"",
+    ""scope"": ""@types"",
+    ""rawSpec"": ""^6.0.1"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""^6.0.1""
+  },
+  ""_requiredBy"": [
+    ""/got""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@types/cacheable-request/-/cacheable-request-6.0.2.tgz"",
+  ""_shasum"": ""c324da0197de0a98a2312156536ae262429ff6b9"",
+  ""_spec"": ""@types/cacheable-request@^6.0.1"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\got"",
+  ""bugs"": {
+    ""url"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/issues""
+  },
+  ""bundleDependencies"": false,
+  ""contributors"": [
+    {
+      ""name"": ""BendingBender"",
+      ""url"": ""https://github.com/BendingBender""
+    },
+    {
+      ""name"": ""Paul Melnikow"",
+      ""url"": ""https://github.com/paulmelnikow""
+    }
+  ],
+  ""dependencies"": {
+    ""@types/http-cache-semantics"": ""*"",
+    ""@types/keyv"": ""*"",
+    ""@types/node"": ""*"",
+    ""@types/responselike"": ""*""
+  },
+  ""deprecated"": false,
+  ""description"": ""TypeScript definitions for cacheable-request"",
+  ""homepage"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/cacheable-request"",
+  ""license"": ""MIT"",
+  ""main"": """",
+  ""name"": ""@types/cacheable-request"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/DefinitelyTyped/DefinitelyTyped.git"",
+    ""directory"": ""types/cacheable-request""
+  },
+  ""scripts"": {},
+  ""typeScriptVersion"": ""3.6"",
+  ""types"": ""index.d.ts"",
+  ""typesPublisherContentHash"": ""5bc07db78df7c21a4d6250dbb806ad088df376f7ed46c63b60bb0e08488dcdc4"",
+  ""version"": ""6.0.2""
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+    MIT License
+
+    Copyright (c) Microsoft Corporation.
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy
+    of this software and associated documentation files (the ""Software""), to deal
+    in the Software without restriction, including without limitation the rights
+    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+    copies of the Software, and to permit persons to whom the Software is
+    furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all
+    copies or substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+    SOFTWARE"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# Installation
+> `npm install --save @types/http-cache-semantics`
+
+# Summary
+This package contains type definitions for http-cache-semantics (https://github.com/kornelski/http-cache-semantics#readme).
+
+# Details
+Files were exported from https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/http-cache-semantics.
+
+### Additional Details
+ * Last updated: Tue, 06 Jul 2021 21:33:36 GMT
+ * Dependencies: none
+ * Global values: none
+
+# Credits
+These definitions were written by [BendingBender](https://github.com/BendingBender)."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+// Type definitions for http-cache-semantics 4.0
+// Project: https://github.com/kornelski/http-cache-semantics#readme
+// Definitions by: BendingBender <https://github.com/BendingBender>
+// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped
+
+export = CachePolicy;
+
+declare class CachePolicy {
+    constructor(req: CachePolicy.Request, res: CachePolicy.Response, options?: CachePolicy.Options);
+
+    /**
+     * Returns `true` if the response can be stored in a cache.
+     * If it's `false` then you MUST NOT store either the request or the response.
+     */
+    storable(): boolean;
+
+    /**
+     * This is the most important method. Use this method to check whether the cached response is still fresh
+     * in the context of the new request.
+     *
+     * If it returns `true`, then the given `request` matches the original response this cache policy has been
+     * created with, and the response can be reused without contacting the server. Note that the old response
+     * can't be returned without being updated, see `responseHeaders()`.
+     *
+     * If it returns `false`, then the response may not be matching at all (e.g. it's for a different URL or method),
+     * or may require to be refreshed first (see `revalidationHeaders()`).
+     */
+    satisfiesWithoutRevalidation(newRequest: CachePolicy.Request): boolean;
+
+    /**
+     * Returns updated, filtered set of response headers to return to clients receiving the cached response.
+     * This function is necessary, because proxies MUST always remove hop-by-hop headers (such as `TE` and `Connection`)
+     * and update response's `Age` to avoid doubling cache time.
+     *
+     * @example
+     * cachedResponse.headers = cachePolicy.responseHeaders(cachedResponse);
+     */
+    responseHeaders(): CachePolicy.Headers;
+
+    /**
+     * Returns approximate time in milliseconds until the response becomes stale (i.e. not fresh).
+     *
+     * After that time (when `timeToLive() <= 0`) the response might not be usable without revalidation. However,
+     * there are exceptions, e.g. a client can explicitly allow stale responses, so always check with
+     * `satisfiesWithoutRevalidation()`.
+     */
+    timeToLive(): number;
+
+    /**
+     * Chances are you'll want to store the `CachePolicy` object along with the cached response.
+     * `obj = policy.toObject()` gives a plain JSON-serializable object.
+     */
+    toObject(): CachePolicy.CachePolicyObject;
+
+    /**
+     * `policy = CachePolicy.fromObject(obj)` creates an instance from object created by `toObject()`.
+     */
+    static fromObject(obj: CachePolicy.CachePolicyObject): CachePolicy;
+
+    /**
+     * Returns updated, filtered set of request headers to send to the origin server to check if the cached
+     * response can be reused. These headers allow the origin server to return status 304 indicating the
+     * response is still fresh. All headers unrelated to caching are passed through as-is.
+     *
+     * Use this method when updating cache from the origin server.
+     *
+     * @example
+     * updateRequest.headers = cachePolicy.revalidationHeaders(updateRequest);
+     */
+    revalidationHeaders(newRequest: CachePolicy.Request): CachePolicy.Headers;
+
+    /**
+     * Use this method to update the cache after receiving a new response from the origin server.
+     */
+    revalidatedPolicy(
+        revalidationRequest: CachePolicy.Request,
+        revalidationResponse: CachePolicy.Response
+    ): CachePolicy.RevalidationPolicy;
+}
+
+declare namespace CachePolicy {
+    interface Request {
+        url?: string | undefined;
+        method?: string | undefined;
+        headers: Headers;
+    }
+
+    interface Response {
+        status?: number | undefined;
+        headers: Headers;
+    }
+
+    interface Options {
+        /**
+         * If `true`, then the response is evaluated from a perspective of a shared cache (i.e. `private` is not
+         * cacheable and `s-maxage` is respected). If `false`, then the response is evaluated from a perspective
+         * of a single-user cache (i.e. `private` is cacheable and `s-maxage` is ignored).
+         * `true` is recommended for HTTP clients.
+         * @default true
+         */
+        shared?: boolean | undefined;
+        /**
+         * A fraction of response's age that is used as a fallback cache duration. The default is 0.1 (10%),
+         * e.g. if a file hasn't been modified for 100 days, it'll be cached for 100*0.1 = 10 days.
+         * @default 0.1
+         */
+        cacheHeuristic?: number | undefined;
+        /**
+         * A number of milliseconds to assume as the default time to cache responses with `Cache-Control: immutable`.
+         * Note that [per RFC](https://httpwg.org/specs/rfc8246.html#the-immutable-cache-control-extension)
+         * these can become stale, so `max-age` still overrides the default.
+         * @default 24*3600*1000 (24h)
+         */
+        immutableMinTimeToLive?: number | undefined;
+        /**
+         * If `true`, common anti-cache directives will be completely ignored if the non-standard `pre-check`
+         * and `post-check` directives are present. These two useless directives are most commonly found
+         * in bad StackOverflow answers and PHP's ""session limiter"" defaults.
+         * @default false
+         */
+        ignoreCargoCult?: boolean | undefined;
+        /**
+         * If `false`, then server's `Date` header won't be used as the base for `max-age`. This is against the RFC,
+         * but it's useful if you want to cache responses with very short `max-age`, but your local clock
+         * is not exactly in sync with the server's.
+         * @default true
+         */
+        trustServerDate?: boolean | undefined;
+    }
+
+    interface CachePolicyObject {
+        v: number;
+        t: number;
+        sh: boolean;
+        ch: number;
+        imm: number;
+        st: number;
+        resh: Headers;
+        rescc: { [key: string]: string };
+        m: string;
+        u?: string | undefined;
+        h?: string | undefined;
+        a: boolean;
+        reqh: Headers | null;
+        reqcc: { [key: string]: string };
+    }
+
+    interface Headers {
+        [header: string]: string | string[] | undefined;
+    }
+
+    interface RevalidationPolicy {
+        /**
+         * A new `CachePolicy` with HTTP headers updated from `revalidationResponse`. You can always replace
+         * the old cached `CachePolicy` with the new one.
+         */
+        policy: CachePolicy;
+        /**
+         * Boolean indicating whether the response body has changed.
+         *
+         * - If `false`, then a valid 304 Not Modified response has been received, and you can reuse the old
+         * cached response body.
+         * - If `true`, you should use new response's body (if present), or make another request to the origin
+         * server without any conditional headers (i.e. don't use `revalidationHeaders()` this time) to get
+         * the new resource.
+         */
+        modified: boolean;
+        matches: boolean;
+    }
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@types/http-cache-semantics@*"",
+  ""_id"": ""@types/http-cache-semantics@4.0.1"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-SZs7ekbP8CN0txVG2xVRH6EgKmEm31BOxA07vkFaETzZz1xh+cbt8BcI0slpymvwhx5dlFnQG2rTlPVQn+iRPQ=="",
+  ""_location"": ""/@types/http-cache-semantics"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@types/http-cache-semantics@*"",
+    ""name"": ""@types/http-cache-semantics"",
+    ""escapedName"": ""@types%2fhttp-cache-semantics"",
+    ""scope"": ""@types"",
+    ""rawSpec"": ""*"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""*""
+  },
+  ""_requiredBy"": [
+    ""/@types/cacheable-request""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@types/http-cache-semantics/-/http-cache-semantics-4.0.1.tgz"",
+  ""_shasum"": ""0ea7b61496902b95890dc4c3a116b60cb8dae812"",
+  ""_spec"": ""@types/http-cache-semantics@*"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\@types\\cacheable-request"",
+  ""bugs"": {
+    ""url"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/issues""
+  },
+  ""bundleDependencies"": false,
+  ""contributors"": [
+    {
+      ""name"": ""BendingBender"",
+      ""url"": ""https://github.com/BendingBender""
+    }
+  ],
+  ""dependencies"": {},
+  ""deprecated"": false,
+  ""description"": ""TypeScript definitions for http-cache-semantics"",
+  ""homepage"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/http-cache-semantics"",
+  ""license"": ""MIT"",
+  ""main"": """",
+  ""name"": ""@types/http-cache-semantics"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/DefinitelyTyped/DefinitelyTyped.git"",
+    ""directory"": ""types/http-cache-semantics""
+  },
+  ""scripts"": {},
+  ""typeScriptVersion"": ""3.6"",
+  ""types"": ""index.d.ts"",
+  ""typesPublisherContentHash"": ""9ecb3137d8c0ede7c06f5d90c7d4759e560a26effb8846bc51a99b63f03dd2d1"",
+  ""version"": ""4.0.1""
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+    MIT License
+
+    Copyright (c) Microsoft Corporation. All rights reserved.
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy
+    of this software and associated documentation files (the ""Software""), to deal
+    in the Software without restriction, including without limitation the rights
+    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+    copies of the Software, and to permit persons to whom the Software is
+    furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all
+    copies or substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+    SOFTWARE"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# Installation
+> `npm install --save @types/json-buffer`
+
+# Summary
+This package contains type definitions for json-buffer (https://github.com/dominictarr/json-buffer).
+
+# Details
+Files were exported from https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/json-buffer
+
+Additional Details
+ * Last updated: Fri, 26 Jul 2019 18:14:43 GMT
+ * Dependencies: none
+ * Global values: none
+
+# Credits
+These definitions were written by Paul Hawxby <https://github.com/phawxby>."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+// Type definitions for json-buffer 3.0
+// Project: https://github.com/dominictarr/json-buffer
+// Definitions by: Paul Hawxby <https://github.com/phawxby>
+// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped
+
+/**
+ * Converts supplied object to a string and safely handles buffers by converting them to a base64 string.
+ *
+ * @param o - Object to convert.
+ * @returns - JSON string.
+ */
+export function stringify(o: any): string;
+/**
+ * Converts JSON string back to an object. Converts base64 encoded buffers back to buffers
+ *
+ * @param s - String to convert back to object.
+ * @returns - Object.
+ */
+export function parse(s: string): any;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@types/json-buffer@~3.0.0"",
+  ""_id"": ""@types/json-buffer@3.0.0"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-3YP80IxxFJB4b5tYC2SUPwkg0XQLiu0nWvhRgEatgjf+29IcWO9X1k8xRv5DGssJ/lCrjYTjQPcobJr2yWIVuQ=="",
+  ""_location"": ""/@types/json-buffer"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@types/json-buffer@~3.0.0"",
+    ""name"": ""@types/json-buffer"",
+    ""escapedName"": ""@types%2fjson-buffer"",
+    ""scope"": ""@types"",
+    ""rawSpec"": ""~3.0.0"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""~3.0.0""
+  },
+  ""_requiredBy"": [
+    ""/compress-brotli""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@types/json-buffer/-/json-buffer-3.0.0.tgz"",
+  ""_shasum"": ""85c1ff0f0948fc159810d4b5be35bf8c20875f64"",
+  ""_spec"": ""@types/json-buffer@~3.0.0"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\compress-brotli"",
+  ""bugs"": {
+    ""url"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/issues""
+  },
+  ""bundleDependencies"": false,
+  ""contributors"": [
+    {
+      ""name"": ""Paul Hawxby"",
+      ""url"": ""https://github.com/phawxby""
+    }
+  ],
+  ""dependencies"": {},
+  ""deprecated"": false,
+  ""description"": ""TypeScript definitions for json-buffer"",
+  ""homepage"": ""https://github.com/DefinitelyTyped/DefinitelyTyped#readme"",
+  ""license"": ""MIT"",
+  ""main"": """",
+  ""name"": ""@types/json-buffer"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/DefinitelyTyped/DefinitelyTyped.git"",
+    ""directory"": ""types/json-buffer""
+  },
+  ""scripts"": {},
+  ""typeScriptVersion"": ""2.0"",
+  ""types"": ""index"",
+  ""typesPublisherContentHash"": ""3e653db758e5ea8bb43afa948bb570021ae1b6d1fc4169f3fc96ec2a25faff3a"",
+  ""version"": ""3.0.0""
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+    MIT License
+
+    Copyright (c) Microsoft Corporation.
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy
+    of this software and associated documentation files (the ""Software""), to deal
+    in the Software without restriction, including without limitation the rights
+    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+    copies of the Software, and to permit persons to whom the Software is
+    furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all
+    copies or substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+    SOFTWARE"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# Installation
+> `npm install --save @types/keyv`
+
+# Summary
+This package contains type definitions for keyv (https://github.com/lukechilds/keyv).
+
+# Details
+Files were exported from https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/keyv.
+
+### Additional Details
+ * Last updated: Thu, 17 Mar 2022 05:31:42 GMT
+ * Dependencies: [@types/node](https://npmjs.com/package/@types/node)
+ * Global values: none
+
+# Credits
+These definitions were written by [AryloYeung](https://github.com/Arylo), and [BendingBender](https://github.com/BendingBender)."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+// Type definitions for keyv 3.1
+// Project: https://github.com/lukechilds/keyv
+// Definitions by: AryloYeung <https://github.com/Arylo>
+//                 BendingBender <https://github.com/BendingBender>
+// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped
+// TypeScript Version: 2.8
+
+/// <reference types=""node"" />
+import { EventEmitter } from 'events';
+
+type WithRequiredProperties<T, K extends keyof T> = T & Required<Pick<T, K>>;
+
+declare class Keyv<TValue = any, TOpts extends { [key: string]: any } = {}> extends EventEmitter {
+    /**
+     * `this.opts` is an object containing at least the properties listed
+     * below. However, `Keyv.Options` allows arbitrary properties as well.
+     * These properties can be specified as the second type parameter to `Keyv`.
+     */
+    opts: WithRequiredProperties<
+        Keyv.Options<TValue>,
+        'deserialize' | 'namespace' | 'serialize' | 'store' | 'uri'
+    > &
+        TOpts;
+
+    /**
+     * @param opts The options object is also passed through to the storage adapter. Check your storage adapter docs for any extra options.
+     */
+    constructor(opts?: Keyv.Options<TValue> & TOpts);
+    /**
+     * @param uri The connection string URI.
+     *
+     * Merged into the options object as options.uri.
+     * @param opts The options object is also passed through to the storage adapter. Check your storage adapter docs for any extra options.
+     */
+    constructor(uri?: string, opts?: Keyv.Options<TValue> & TOpts);
+
+    /** Returns the value. */
+    get<TRaw extends boolean = false>(key: string, options?: { raw?: TRaw }):
+      Promise<(TRaw extends false
+        ? TValue
+        : Keyv.DeserializedData<TValue>)  | undefined>;
+    /**
+     * Set a value.
+     *
+     * By default keys are persistent. You can set an expiry TTL in milliseconds.
+     */
+    set(key: string, value: TValue, ttl?: number): Promise<true>;
+    /**
+     * Deletes an entry.
+     *
+     * Returns `true` if the key existed, `false` if not.
+     */
+    delete(key: string): Promise<boolean>;
+    /** Delete all entries in the current namespace. */
+    clear(): Promise<void>;
+}
+
+declare namespace Keyv {
+    interface Options<TValue> {
+        /** Namespace for the current instance. */
+        namespace?: string | undefined;
+        /** A custom serialization function. */
+        serialize?: ((data: DeserializedData<TValue>) => string) | undefined;
+        /** A custom deserialization function. */
+        deserialize?: ((data: string) => DeserializedData<TValue> | undefined) | undefined;
+        /** The connection string URI. */
+        uri?: string | undefined;
+        /** The storage adapter instance to be used by Keyv. */
+        store?: Store<TValue> | undefined;
+        /** Default TTL. Can be overridden by specififying a TTL on `.set()`. */
+        ttl?: number | undefined;
+        /** Specify an adapter to use. e.g `'redis'` or `'mongodb'`. */
+        adapter?: 'redis' | 'mongodb' | 'mongo' | 'sqlite' | 'postgresql' | 'postgres' | 'mysql' | undefined;
+
+        [key: string]: any;
+    }
+
+    interface DeserializedData<TValue> {
+        value: TValue; expires: number | null;
+    }
+
+    interface Store<TValue> {
+        get(key: string): TValue | Promise<TValue | undefined> | undefined;
+        set(key: string, value: TValue, ttl?: number): any;
+        delete(key: string): boolean | Promise<boolean>;
+        clear(): void | Promise<void>;
+    }
+}
+
+export = Keyv;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@types/keyv@*"",
+  ""_id"": ""@types/keyv@3.1.4"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-BQ5aZNSCpj7D6K2ksrRCTmKRLEpnPvWDiLPfoGyhZ++8YtiK9d/3DBKPJgry359X/P1PfruyYwvnvwFjuEiEIg=="",
+  ""_location"": ""/@types/keyv"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@types/keyv@*"",
+    ""name"": ""@types/keyv"",
+    ""escapedName"": ""@types%2fkeyv"",
+    ""scope"": ""@types"",
+    ""rawSpec"": ""*"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""*""
+  },
+  ""_requiredBy"": [
+    ""/@types/cacheable-request""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@types/keyv/-/keyv-3.1.4.tgz"",
+  ""_shasum"": ""3ccdb1c6751b0c7e52300bcdacd5bcbf8faa75b6"",
+  ""_spec"": ""@types/keyv@*"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\@types\\cacheable-request"",
+  ""bugs"": {
+    ""url"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/issues""
+  },
+  ""bundleDependencies"": false,
+  ""contributors"": [
+    {
+      ""name"": ""AryloYeung"",
+      ""url"": ""https://github.com/Arylo""
+    },
+    {
+      ""name"": ""BendingBender"",
+      ""url"": ""https://github.com/BendingBender""
+    }
+  ],
+  ""dependencies"": {
+    ""@types/node"": ""*""
+  },
+  ""deprecated"": false,
+  ""description"": ""TypeScript definitions for keyv"",
+  ""homepage"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/keyv"",
+  ""license"": ""MIT"",
+  ""main"": """",
+  ""name"": ""@types/keyv"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/DefinitelyTyped/DefinitelyTyped.git"",
+    ""directory"": ""types/keyv""
+  },
+  ""scripts"": {},
+  ""typeScriptVersion"": ""3.9"",
+  ""types"": ""index.d.ts"",
+  ""typesPublisherContentHash"": ""e83393e0860475d12e960cede22532e18e129cf659f31f2a0298a88cb5d02d36"",
+  ""version"": ""3.1.4""
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+    MIT License
+
+    Copyright (c) Microsoft Corporation.
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy
+    of this software and associated documentation files (the ""Software""), to deal
+    in the Software without restriction, including without limitation the rights
+    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+    copies of the Software, and to permit persons to whom the Software is
+    furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all
+    copies or substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+    SOFTWARE"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+# Installation
+> `npm install --save @types/node`
+
+# Summary
+This package contains type definitions for Node.js (https://nodejs.org/).
+
+# Details
+Files were exported from https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/node.
+
+### Additional Details
+ * Last updated: Sun, 01 May 2022 20:31:38 GMT
+ * Dependencies: none
+ * Global values: `AbortController`, `AbortSignal`, `__dirname`, `__filename`, `console`, `exports`, `gc`, `global`, `module`, `process`, `require`, `structuredClone`
+
+# Credits
+These definitions were written by [Microsoft TypeScript](https://github.com/Microsoft), [DefinitelyTyped](https://github.com/DefinitelyTyped), [Alberto Schiabel](https://github.com/jkomyno), [Alvis HT Tang](https://github.com/alvis), [Andrew Makarov](https://github.com/r3nya), [Benjamin Toueg](https://github.com/btoueg), [Chigozirim C.](https://github.com/smac89), [David Junger](https://github.com/touffy), [Deividas Bakanas](https://github.com/DeividasBakanas), [Eugene Y. Q. Shen](https://github.com/eyqs), [Hannes Magnusson](https://github.com/Hannes-Magnusson-CK), [Huw](https://github.com/hoo29), [Kelvin Jin](https://github.com/kjin), [Klaus Meinhardt](https://github.com/ajafff), [Lishude](https://github.com/islishude), [Mariusz Wiktorczyk](https://github.com/mwiktorczyk), [Mohsen Azimi](https://github.com/mohsen1), [Nicolas Even](https://github.com/n-e), [Nikita Galkin](https://github.com/galkin), [Parambir Singh](https://github.com/parambirs), [Sebastian Silbermann](https://github.com/eps1lon), [Simon Schick](https://github.com/SimonSchick), [Thomas den Hollander](https://github.com/ThomasdenH), [Wilco Bakker](https://github.com/WilcoBakker), [wwwy3y3](https://github.com/wwwy3y3), [Samuel Ainsworth](https://github.com/samuela), [Kyle Uehlein](https://github.com/kuehlein), [Thanik Bhongbhibhat](https://github.com/bhongy), [Marcin Kopacz](https://github.com/chyzwar), [Trivikram Kamat](https://github.com/trivikr), [Junxiao Shi](https://github.com/yoursunny), [Ilia Baryshnikov](https://github.com/qwelias), [ExE Boss](https://github.com/ExE-Boss), [Piotr Błażejewicz](https://github.com/peterblazejewicz), [Anna Henningsen](https://github.com/addaleax), [Victor Perin](https://github.com/victorperin), [Yongsheng Zhang](https://github.com/ZYSzys), [NodeJS Contributors](https://github.com/NodeJS), [Linus Unnebäck](https://github.com/LinusU), and [wafuwafu13](https://github.com/wafuwafu13)."
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `assert` module provides a set of assertion functions for verifying
+ * invariants.
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/assert.js)
+ */
+declare module 'assert' {
+    /**
+     * An alias of {@link ok}.
+     * @since v0.5.9
+     * @param value The input that is checked for being truthy.
+     */
+    function assert(value: unknown, message?: string | Error): asserts value;
+    namespace assert {
+        /**
+         * Indicates the failure of an assertion. All errors thrown by the `assert` module
+         * will be instances of the `AssertionError` class.
+         */
+        class AssertionError extends Error {
+            actual: unknown;
+            expected: unknown;
+            operator: string;
+            generatedMessage: boolean;
+            code: 'ERR_ASSERTION';
+            constructor(options?: {
+                /** If provided, the error message is set to this value. */
+                message?: string | undefined;
+                /** The `actual` property on the error instance. */
+                actual?: unknown | undefined;
+                /** The `expected` property on the error instance. */
+                expected?: unknown | undefined;
+                /** The `operator` property on the error instance. */
+                operator?: string | undefined;
+                /** If provided, the generated stack trace omits frames before this function. */
+                // tslint:disable-next-line:ban-types
+                stackStartFn?: Function | undefined;
+            });
+        }
+        /**
+         * This feature is currently experimental and behavior might still change.
+         * @since v14.2.0, v12.19.0
+         * @experimental
+         */
+        class CallTracker {
+            /**
+             * The wrapper function is expected to be called exactly `exact` times. If the
+             * function has not been called exactly `exact` times when `tracker.verify()` is called, then `tracker.verify()` will throw an
+             * error.
+             *
+             * ```js
+             * import assert from 'assert';
+             *
+             * // Creates call tracker.
+             * const tracker = new assert.CallTracker();
+             *
+             * function func() {}
+             *
+             * // Returns a function that wraps func() that must be called exact times
+             * // before tracker.verify().
+             * const callsfunc = tracker.calls(func);
+             * ```
+             * @since v14.2.0, v12.19.0
+             * @param [fn='A no-op function']
+             * @param [exact=1]
+             * @return that wraps `fn`.
+             */
+            calls(exact?: number): () => void;
+            calls<Func extends (...args: any[]) => any>(fn?: Func, exact?: number): Func;
+            /**
+             * The arrays contains information about the expected and actual number of calls of
+             * the functions that have not been called the expected number of times.
+             *
+             * ```js
+             * import assert from 'assert';
+             *
+             * // Creates call tracker.
+             * const tracker = new assert.CallTracker();
+             *
+             * function func() {}
+             *
+             * function foo() {}
+             *
+             * // Returns a function that wraps func() that must be called exact times
+             * // before tracker.verify().
+             * const callsfunc = tracker.calls(func, 2);
+             *
+             * // Returns an array containing information on callsfunc()
+             * tracker.report();
+             * // [
+             * //  {
+             * //    message: 'Expected the func function to be executed 2 time(s) but was
+             * //    executed 0 time(s).',
+             * //    actual: 0,
+             * //    expected: 2,
+             * //    operator: 'func',
+             * //    stack: stack trace
+             * //  }
+             * // ]
+             * ```
+             * @since v14.2.0, v12.19.0
+             * @return of objects containing information about the wrapper functions returned by `calls`.
+             */
+            report(): CallTrackerReportInformation[];
+            /**
+             * Iterates through the list of functions passed to `tracker.calls()` and will throw an error for functions that
+             * have not been called the expected number of times.
+             *
+             * ```js
+             * import assert from 'assert';
+             *
+             * // Creates call tracker.
+             * const tracker = new assert.CallTracker();
+             *
+             * function func() {}
+             *
+             * // Returns a function that wraps func() that must be called exact times
+             * // before tracker.verify().
+             * const callsfunc = tracker.calls(func, 2);
+             *
+             * callsfunc();
+             *
+             * // Will throw an error since callsfunc() was only called once.
+             * tracker.verify();
+             * ```
+             * @since v14.2.0, v12.19.0
+             */
+            verify(): void;
+        }
+        interface CallTrackerReportInformation {
+            message: string;
+            /** The actual number of times the function was called. */
+            actual: number;
+            /** The number of times the function was expected to be called. */
+            expected: number;
+            /** The name of the function that is wrapped. */
+            operator: string;
+            /** A stack trace of the function. */
+            stack: object;
+        }
+        type AssertPredicate = RegExp | (new () => object) | ((thrown: unknown) => boolean) | object | Error;
+        /**
+         * Throws an `AssertionError` with the provided error message or a default
+         * error message. If the `message` parameter is an instance of an `Error` then
+         * it will be thrown instead of the `AssertionError`.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.fail();
+         * // AssertionError [ERR_ASSERTION]: Failed
+         *
+         * assert.fail('boom');
+         * // AssertionError [ERR_ASSERTION]: boom
+         *
+         * assert.fail(new TypeError('need array'));
+         * // TypeError: need array
+         * ```
+         *
+         * Using `assert.fail()` with more than two arguments is possible but deprecated.
+         * See below for further details.
+         * @since v0.1.21
+         * @param [message='Failed']
+         */
+        function fail(message?: string | Error): never;
+        /** @deprecated since v10.0.0 - use fail([message]) or other assert functions instead. */
+        function fail(
+            actual: unknown,
+            expected: unknown,
+            message?: string | Error,
+            operator?: string,
+            // tslint:disable-next-line:ban-types
+            stackStartFn?: Function
+        ): never;
+        /**
+         * Tests if `value` is truthy. It is equivalent to`assert.equal(!!value, true, message)`.
+         *
+         * If `value` is not truthy, an `AssertionError` is thrown with a `message`property set equal to the value of the `message` parameter. If the `message`parameter is `undefined`, a default
+         * error message is assigned. If the `message`parameter is an instance of an `Error` then it will be thrown instead of the`AssertionError`.
+         * If no arguments are passed in at all `message` will be set to the string:`` 'No value argument passed to `assert.ok()`' ``.
+         *
+         * Be aware that in the `repl` the error message will be different to the one
+         * thrown in a file! See below for further details.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.ok(true);
+         * // OK
+         * assert.ok(1);
+         * // OK
+         *
+         * assert.ok();
+         * // AssertionError: No value argument passed to `assert.ok()`
+         *
+         * assert.ok(false, 'it\'s false');
+         * // AssertionError: it's false
+         *
+         * // In the repl:
+         * assert.ok(typeof 123 === 'string');
+         * // AssertionError: false == true
+         *
+         * // In a file (e.g. test.js):
+         * assert.ok(typeof 123 === 'string');
+         * // AssertionError: The expression evaluated to a falsy value:
+         * //
+         * //   assert.ok(typeof 123 === 'string')
+         *
+         * assert.ok(false);
+         * // AssertionError: The expression evaluated to a falsy value:
+         * //
+         * //   assert.ok(false)
+         *
+         * assert.ok(0);
+         * // AssertionError: The expression evaluated to a falsy value:
+         * //
+         * //   assert.ok(0)
+         * ```
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * // Using `assert()` works the same:
+         * assert(0);
+         * // AssertionError: The expression evaluated to a falsy value:
+         * //
+         * //   assert(0)
+         * ```
+         * @since v0.1.21
+         */
+        function ok(value: unknown, message?: string | Error): asserts value;
+        /**
+         * **Strict assertion mode**
+         *
+         * An alias of {@link strictEqual}.
+         *
+         * **Legacy assertion mode**
+         *
+         * > Stability: 3 - Legacy: Use {@link strictEqual} instead.
+         *
+         * Tests shallow, coercive equality between the `actual` and `expected` parameters
+         * using the [Abstract Equality Comparison](https://tc39.github.io/ecma262/#sec-abstract-equality-comparison) ( `==` ). `NaN` is special handled
+         * and treated as being identical in case both sides are `NaN`.
+         *
+         * ```js
+         * import assert from 'assert';
+         *
+         * assert.equal(1, 1);
+         * // OK, 1 == 1
+         * assert.equal(1, '1');
+         * // OK, 1 == '1'
+         * assert.equal(NaN, NaN);
+         * // OK
+         *
+         * assert.equal(1, 2);
+         * // AssertionError: 1 == 2
+         * assert.equal({ a: { b: 1 } }, { a: { b: 1 } });
+         * // AssertionError: { a: { b: 1 } } == { a: { b: 1 } }
+         * ```
+         *
+         * If the values are not equal, an `AssertionError` is thrown with a `message`property set equal to the value of the `message` parameter. If the `message`parameter is undefined, a default
+         * error message is assigned. If the `message`parameter is an instance of an `Error` then it will be thrown instead of the`AssertionError`.
+         * @since v0.1.21
+         */
+        function equal(actual: unknown, expected: unknown, message?: string | Error): void;
+        /**
+         * **Strict assertion mode**
+         *
+         * An alias of {@link notStrictEqual}.
+         *
+         * **Legacy assertion mode**
+         *
+         * > Stability: 3 - Legacy: Use {@link notStrictEqual} instead.
+         *
+         * Tests shallow, coercive inequality with the [Abstract Equality Comparison](https://tc39.github.io/ecma262/#sec-abstract-equality-comparison)(`!=` ). `NaN` is special handled and treated as
+         * being identical in case both
+         * sides are `NaN`.
+         *
+         * ```js
+         * import assert from 'assert';
+         *
+         * assert.notEqual(1, 2);
+         * // OK
+         *
+         * assert.notEqual(1, 1);
+         * // AssertionError: 1 != 1
+         *
+         * assert.notEqual(1, '1');
+         * // AssertionError: 1 != '1'
+         * ```
+         *
+         * If the values are equal, an `AssertionError` is thrown with a `message`property set equal to the value of the `message` parameter. If the `message`parameter is undefined, a default error
+         * message is assigned. If the `message`parameter is an instance of an `Error` then it will be thrown instead of the`AssertionError`.
+         * @since v0.1.21
+         */
+        function notEqual(actual: unknown, expected: unknown, message?: string | Error): void;
+        /**
+         * **Strict assertion mode**
+         *
+         * An alias of {@link deepStrictEqual}.
+         *
+         * **Legacy assertion mode**
+         *
+         * > Stability: 3 - Legacy: Use {@link deepStrictEqual} instead.
+         *
+         * Tests for deep equality between the `actual` and `expected` parameters. Consider
+         * using {@link deepStrictEqual} instead. {@link deepEqual} can have
+         * surprising results.
+         *
+         * _Deep equality_ means that the enumerable ""own"" properties of child objects
+         * are also recursively evaluated by the following rules.
+         * @since v0.1.21
+         */
+        function deepEqual(actual: unknown, expected: unknown, message?: string | Error): void;
+        /**
+         * **Strict assertion mode**
+         *
+         * An alias of {@link notDeepStrictEqual}.
+         *
+         * **Legacy assertion mode**
+         *
+         * > Stability: 3 - Legacy: Use {@link notDeepStrictEqual} instead.
+         *
+         * Tests for any deep inequality. Opposite of {@link deepEqual}.
+         *
+         * ```js
+         * import assert from 'assert';
+         *
+         * const obj1 = {
+         *   a: {
+         *     b: 1
+         *   }
+         * };
+         * const obj2 = {
+         *   a: {
+         *     b: 2
+         *   }
+         * };
+         * const obj3 = {
+         *   a: {
+         *     b: 1
+         *   }
+         * };
+         * const obj4 = Object.create(obj1);
+         *
+         * assert.notDeepEqual(obj1, obj1);
+         * // AssertionError: { a: { b: 1 } } notDeepEqual { a: { b: 1 } }
+         *
+         * assert.notDeepEqual(obj1, obj2);
+         * // OK
+         *
+         * assert.notDeepEqual(obj1, obj3);
+         * // AssertionError: { a: { b: 1 } } notDeepEqual { a: { b: 1 } }
+         *
+         * assert.notDeepEqual(obj1, obj4);
+         * // OK
+         * ```
+         *
+         * If the values are deeply equal, an `AssertionError` is thrown with a`message` property set equal to the value of the `message` parameter. If the`message` parameter is undefined, a default
+         * error message is assigned. If the`message` parameter is an instance of an `Error` then it will be thrown
+         * instead of the `AssertionError`.
+         * @since v0.1.21
+         */
+        function notDeepEqual(actual: unknown, expected: unknown, message?: string | Error): void;
+        /**
+         * Tests strict equality between the `actual` and `expected` parameters as
+         * determined by the [SameValue Comparison](https://tc39.github.io/ecma262/#sec-samevalue).
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.strictEqual(1, 2);
+         * // AssertionError [ERR_ASSERTION]: Expected inputs to be strictly equal:
+         * //
+         * // 1 !== 2
+         *
+         * assert.strictEqual(1, 1);
+         * // OK
+         *
+         * assert.strictEqual('Hello foobar', 'Hello World!');
+         * // AssertionError [ERR_ASSERTION]: Expected inputs to be strictly equal:
+         * // + actual - expected
+         * //
+         * // + 'Hello foobar'
+         * // - 'Hello World!'
+         * //          ^
+         *
+         * const apples = 1;
+         * const oranges = 2;
+         * assert.strictEqual(apples, oranges, `apples ${apples} !== oranges ${oranges}`);
+         * // AssertionError [ERR_ASSERTION]: apples 1 !== oranges 2
+         *
+         * assert.strictEqual(1, '1', new TypeError('Inputs are not identical'));
+         * // TypeError: Inputs are not identical
+         * ```
+         *
+         * If the values are not strictly equal, an `AssertionError` is thrown with a`message` property set equal to the value of the `message` parameter. If the`message` parameter is undefined, a
+         * default error message is assigned. If the`message` parameter is an instance of an `Error` then it will be thrown
+         * instead of the `AssertionError`.
+         * @since v0.1.21
+         */
+        function strictEqual<T>(actual: unknown, expected: T, message?: string | Error): asserts actual is T;
+        /**
+         * Tests strict inequality between the `actual` and `expected` parameters as
+         * determined by the [SameValue Comparison](https://tc39.github.io/ecma262/#sec-samevalue).
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.notStrictEqual(1, 2);
+         * // OK
+         *
+         * assert.notStrictEqual(1, 1);
+         * // AssertionError [ERR_ASSERTION]: Expected ""actual"" to be strictly unequal to:
+         * //
+         * // 1
+         *
+         * assert.notStrictEqual(1, '1');
+         * // OK
+         * ```
+         *
+         * If the values are strictly equal, an `AssertionError` is thrown with a`message` property set equal to the value of the `message` parameter. If the`message` parameter is undefined, a
+         * default error message is assigned. If the`message` parameter is an instance of an `Error` then it will be thrown
+         * instead of the `AssertionError`.
+         * @since v0.1.21
+         */
+        function notStrictEqual(actual: unknown, expected: unknown, message?: string | Error): void;
+        /**
+         * Tests for deep equality between the `actual` and `expected` parameters.
+         * ""Deep"" equality means that the enumerable ""own"" properties of child objects
+         * are recursively evaluated also by the following rules.
+         * @since v1.2.0
+         */
+        function deepStrictEqual<T>(actual: unknown, expected: T, message?: string | Error): asserts actual is T;
+        /**
+         * Tests for deep strict inequality. Opposite of {@link deepStrictEqual}.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.notDeepStrictEqual({ a: 1 }, { a: '1' });
+         * // OK
+         * ```
+         *
+         * If the values are deeply and strictly equal, an `AssertionError` is thrown
+         * with a `message` property set equal to the value of the `message` parameter. If
+         * the `message` parameter is undefined, a default error message is assigned. If
+         * the `message` parameter is an instance of an `Error` then it will be thrown
+         * instead of the `AssertionError`.
+         * @since v1.2.0
+         */
+        function notDeepStrictEqual(actual: unknown, expected: unknown, message?: string | Error): void;
+        /**
+         * Expects the function `fn` to throw an error.
+         *
+         * If specified, `error` can be a [`Class`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes),
+         * [`RegExp`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions), a validation function,
+         * a validation object where each property will be tested for strict deep equality,
+         * or an instance of error where each property will be tested for strict deep
+         * equality including the non-enumerable `message` and `name` properties. When
+         * using an object, it is also possible to use a regular expression, when
+         * validating against a string property. See below for examples.
+         *
+         * If specified, `message` will be appended to the message provided by the`AssertionError` if the `fn` call fails to throw or in case the error validation
+         * fails.
+         *
+         * Custom validation object/error instance:
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * const err = new TypeError('Wrong value');
+         * err.code = 404;
+         * err.foo = 'bar';
+         * err.info = {
+         *   nested: true,
+         *   baz: 'text'
+         * };
+         * err.reg = /abc/i;
+         *
+         * assert.throws(
+         *   () => {
+         *     throw err;
+         *   },
+         *   {
+         *     name: 'TypeError',
+         *     message: 'Wrong value',
+         *     info: {
+         *       nested: true,
+         *       baz: 'text'
+         *     }
+         *     // Only properties on the validation object will be tested for.
+         *     // Using nested objects requires all properties to be present. Otherwise
+         *     // the validation is going to fail.
+         *   }
+         * );
+         *
+         * // Using regular expressions to validate error properties:
+         * throws(
+         *   () => {
+         *     throw err;
+         *   },
+         *   {
+         *     // The `name` and `message` properties are strings and using regular
+         *     // expressions on those will match against the string. If they fail, an
+         *     // error is thrown.
+         *     name: /^TypeError$/,
+         *     message: /Wrong/,
+         *     foo: 'bar',
+         *     info: {
+         *       nested: true,
+         *       // It is not possible to use regular expressions for nested properties!
+         *       baz: 'text'
+         *     },
+         *     // The `reg` property contains a regular expression and only if the
+         *     // validation object contains an identical regular expression, it is going
+         *     // to pass.
+         *     reg: /abc/i
+         *   }
+         * );
+         *
+         * // Fails due to the different `message` and `name` properties:
+         * throws(
+         *   () => {
+         *     const otherErr = new Error('Not found');
+         *     // Copy all enumerable properties from `err` to `otherErr`.
+         *     for (const [key, value] of Object.entries(err)) {
+         *       otherErr[key] = value;
+         *     }
+         *     throw otherErr;
+         *   },
+         *   // The error's `message` and `name` properties will also be checked when using
+         *   // an error as validation object.
+         *   err
+         * );
+         * ```
+         *
+         * Validate instanceof using constructor:
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.throws(
+         *   () => {
+         *     throw new Error('Wrong value');
+         *   },
+         *   Error
+         * );
+         * ```
+         *
+         * Validate error message using [`RegExp`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions):
+         *
+         * Using a regular expression runs `.toString` on the error object, and will
+         * therefore also include the error name.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.throws(
+         *   () => {
+         *     throw new Error('Wrong value');
+         *   },
+         *   /^Error: Wrong value$/
+         * );
+         * ```
+         *
+         * Custom error validation:
+         *
+         * The function must return `true` to indicate all internal validations passed.
+         * It will otherwise fail with an `AssertionError`.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.throws(
+         *   () => {
+         *     throw new Error('Wrong value');
+         *   },
+         *   (err) => {
+         *     assert(err instanceof Error);
+         *     assert(/value/.test(err));
+         *     // Avoid returning anything from validation functions besides `true`.
+         *     // Otherwise, it's not clear what part of the validation failed. Instead,
+         *     // throw an error about the specific validation that failed (as done in this
+         *     // example) and add as much helpful debugging information to that error as
+         *     // possible.
+         *     return true;
+         *   },
+         *   'unexpected error'
+         * );
+         * ```
+         *
+         * `error` cannot be a string. If a string is provided as the second
+         * argument, then `error` is assumed to be omitted and the string will be used for`message` instead. This can lead to easy-to-miss mistakes. Using the same
+         * message as the thrown error message is going to result in an`ERR_AMBIGUOUS_ARGUMENT` error. Please read the example below carefully if using
+         * a string as the second argument gets considered:
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * function throwingFirst() {
+         *   throw new Error('First');
+         * }
+         *
+         * function throwingSecond() {
+         *   throw new Error('Second');
+         * }
+         *
+         * function notThrowing() {}
+         *
+         * // The second argument is a string and the input function threw an Error.
+         * // The first case will not throw as it does not match for the error message
+         * // thrown by the input function!
+         * assert.throws(throwingFirst, 'Second');
+         * // In the next example the message has no benefit over the message from the
+         * // error and since it is not clear if the user intended to actually match
+         * // against the error message, Node.js throws an `ERR_AMBIGUOUS_ARGUMENT` error.
+         * assert.throws(throwingSecond, 'Second');
+         * // TypeError [ERR_AMBIGUOUS_ARGUMENT]
+         *
+         * // The string is only used (as message) in case the function does not throw:
+         * assert.throws(notThrowing, 'Second');
+         * // AssertionError [ERR_ASSERTION]: Missing expected exception: Second
+         *
+         * // If it was intended to match for the error message do this instead:
+         * // It does not throw because the error messages match.
+         * assert.throws(throwingSecond, /Second$/);
+         *
+         * // If the error message does not match, an AssertionError is thrown.
+         * assert.throws(throwingFirst, /Second$/);
+         * // AssertionError [ERR_ASSERTION]
+         * ```
+         *
+         * Due to the confusing error-prone notation, avoid a string as the second
+         * argument.
+         * @since v0.1.21
+         */
+        function throws(block: () => unknown, message?: string | Error): void;
+        function throws(block: () => unknown, error: AssertPredicate, message?: string | Error): void;
+        /**
+         * Asserts that the function `fn` does not throw an error.
+         *
+         * Using `assert.doesNotThrow()` is actually not useful because there
+         * is no benefit in catching an error and then rethrowing it. Instead, consider
+         * adding a comment next to the specific code path that should not throw and keep
+         * error messages as expressive as possible.
+         *
+         * When `assert.doesNotThrow()` is called, it will immediately call the `fn`function.
+         *
+         * If an error is thrown and it is the same type as that specified by the `error`parameter, then an `AssertionError` is thrown. If the error is of a
+         * different type, or if the `error` parameter is undefined, the error is
+         * propagated back to the caller.
+         *
+         * If specified, `error` can be a [`Class`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes),
+         * [`RegExp`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions) or a validation
+         * function. See {@link throws} for more details.
+         *
+         * The following, for instance, will throw the `TypeError` because there is no
+         * matching error type in the assertion:
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.doesNotThrow(
+         *   () => {
+         *     throw new TypeError('Wrong value');
+         *   },
+         *   SyntaxError
+         * );
+         * ```
+         *
+         * However, the following will result in an `AssertionError` with the message
+         * 'Got unwanted exception...':
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.doesNotThrow(
+         *   () => {
+         *     throw new TypeError('Wrong value');
+         *   },
+         *   TypeError
+         * );
+         * ```
+         *
+         * If an `AssertionError` is thrown and a value is provided for the `message`parameter, the value of `message` will be appended to the `AssertionError` message:
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.doesNotThrow(
+         *   () => {
+         *     throw new TypeError('Wrong value');
+         *   },
+         *   /Wrong value/,
+         *   'Whoops'
+         * );
+         * // Throws: AssertionError: Got unwanted exception: Whoops
+         * ```
+         * @since v0.1.21
+         */
+        function doesNotThrow(block: () => unknown, message?: string | Error): void;
+        function doesNotThrow(block: () => unknown, error: AssertPredicate, message?: string | Error): void;
+        /**
+         * Throws `value` if `value` is not `undefined` or `null`. This is useful when
+         * testing the `error` argument in callbacks. The stack trace contains all frames
+         * from the error passed to `ifError()` including the potential new frames for`ifError()` itself.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.ifError(null);
+         * // OK
+         * assert.ifError(0);
+         * // AssertionError [ERR_ASSERTION]: ifError got unwanted exception: 0
+         * assert.ifError('error');
+         * // AssertionError [ERR_ASSERTION]: ifError got unwanted exception: 'error'
+         * assert.ifError(new Error());
+         * // AssertionError [ERR_ASSERTION]: ifError got unwanted exception: Error
+         *
+         * // Create some random error frames.
+         * let err;
+         * (function errorFrame() {
+         *   err = new Error('test error');
+         * })();
+         *
+         * (function ifErrorFrame() {
+         *   assert.ifError(err);
+         * })();
+         * // AssertionError [ERR_ASSERTION]: ifError got unwanted exception: test error
+         * //     at ifErrorFrame
+         * //     at errorFrame
+         * ```
+         * @since v0.1.97
+         */
+        function ifError(value: unknown): asserts value is null | undefined;
+        /**
+         * Awaits the `asyncFn` promise or, if `asyncFn` is a function, immediately
+         * calls the function and awaits the returned promise to complete. It will then
+         * check that the promise is rejected.
+         *
+         * If `asyncFn` is a function and it throws an error synchronously,`assert.rejects()` will return a rejected `Promise` with that error. If the
+         * function does not return a promise, `assert.rejects()` will return a rejected`Promise` with an `ERR_INVALID_RETURN_VALUE` error. In both cases the error
+         * handler is skipped.
+         *
+         * Besides the async nature to await the completion behaves identically to {@link throws}.
+         *
+         * If specified, `error` can be a [`Class`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes),
+         * [`RegExp`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions), a validation function,
+         * an object where each property will be tested for, or an instance of error where
+         * each property will be tested for including the non-enumerable `message` and`name` properties.
+         *
+         * If specified, `message` will be the message provided by the `AssertionError` if the `asyncFn` fails to reject.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * await assert.rejects(
+         *   async () => {
+         *     throw new TypeError('Wrong value');
+         *   },
+         *   {
+         *     name: 'TypeError',
+         *     message: 'Wrong value'
+         *   }
+         * );
+         * ```
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * await assert.rejects(
+         *   async () => {
+         *     throw new TypeError('Wrong value');
+         *   },
+         *   (err) => {
+         *     assert.strictEqual(err.name, 'TypeError');
+         *     assert.strictEqual(err.message, 'Wrong value');
+         *     return true;
+         *   }
+         * );
+         * ```
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.rejects(
+         *   Promise.reject(new Error('Wrong value')),
+         *   Error
+         * ).then(() => {
+         *   // ...
+         * });
+         * ```
+         *
+         * `error` cannot be a string. If a string is provided as the second
+         * argument, then `error` is assumed to be omitted and the string will be used for`message` instead. This can lead to easy-to-miss mistakes. Please read the
+         * example in {@link throws} carefully if using a string as the second
+         * argument gets considered.
+         * @since v10.0.0
+         */
+        function rejects(block: (() => Promise<unknown>) | Promise<unknown>, message?: string | Error): Promise<void>;
+        function rejects(block: (() => Promise<unknown>) | Promise<unknown>, error: AssertPredicate, message?: string | Error): Promise<void>;
+        /**
+         * Awaits the `asyncFn` promise or, if `asyncFn` is a function, immediately
+         * calls the function and awaits the returned promise to complete. It will then
+         * check that the promise is not rejected.
+         *
+         * If `asyncFn` is a function and it throws an error synchronously,`assert.doesNotReject()` will return a rejected `Promise` with that error. If
+         * the function does not return a promise, `assert.doesNotReject()` will return a
+         * rejected `Promise` with an `ERR_INVALID_RETURN_VALUE` error. In both cases
+         * the error handler is skipped.
+         *
+         * Using `assert.doesNotReject()` is actually not useful because there is little
+         * benefit in catching a rejection and then rejecting it again. Instead, consider
+         * adding a comment next to the specific code path that should not reject and keep
+         * error messages as expressive as possible.
+         *
+         * If specified, `error` can be a [`Class`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes),
+         * [`RegExp`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions) or a validation
+         * function. See {@link throws} for more details.
+         *
+         * Besides the async nature to await the completion behaves identically to {@link doesNotThrow}.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * await assert.doesNotReject(
+         *   async () => {
+         *     throw new TypeError('Wrong value');
+         *   },
+         *   SyntaxError
+         * );
+         * ```
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.doesNotReject(Promise.reject(new TypeError('Wrong value')))
+         *   .then(() => {
+         *     // ...
+         *   });
+         * ```
+         * @since v10.0.0
+         */
+        function doesNotReject(block: (() => Promise<unknown>) | Promise<unknown>, message?: string | Error): Promise<void>;
+        function doesNotReject(block: (() => Promise<unknown>) | Promise<unknown>, error: AssertPredicate, message?: string | Error): Promise<void>;
+        /**
+         * Expects the `string` input to match the regular expression.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.match('I will fail', /pass/);
+         * // AssertionError [ERR_ASSERTION]: The input did not match the regular ...
+         *
+         * assert.match(123, /pass/);
+         * // AssertionError [ERR_ASSERTION]: The ""string"" argument must be of type string.
+         *
+         * assert.match('I will pass', /pass/);
+         * // OK
+         * ```
+         *
+         * If the values do not match, or if the `string` argument is of another type than`string`, an `AssertionError` is thrown with a `message` property set equal
+         * to the value of the `message` parameter. If the `message` parameter is
+         * undefined, a default error message is assigned. If the `message` parameter is an
+         * instance of an `Error` then it will be thrown instead of the `AssertionError`.
+         * @since v13.6.0, v12.16.0
+         */
+        function match(value: string, regExp: RegExp, message?: string | Error): void;
+        /**
+         * Expects the `string` input not to match the regular expression.
+         *
+         * ```js
+         * import assert from 'assert/strict';
+         *
+         * assert.doesNotMatch('I will fail', /fail/);
+         * // AssertionError [ERR_ASSERTION]: The input was expected to not match the ...
+         *
+         * assert.doesNotMatch(123, /pass/);
+         * // AssertionError [ERR_ASSERTION]: The ""string"" argument must be of type string.
+         *
+         * assert.doesNotMatch('I will pass', /different/);
+         * // OK
+         * ```
+         *
+         * If the values do match, or if the `string` argument is of another type than`string`, an `AssertionError` is thrown with a `message` property set equal
+         * to the value of the `message` parameter. If the `message` parameter is
+         * undefined, a default error message is assigned. If the `message` parameter is an
+         * instance of an `Error` then it will be thrown instead of the `AssertionError`.
+         * @since v13.6.0, v12.16.0
+         */
+        function doesNotMatch(value: string, regExp: RegExp, message?: string | Error): void;
+        const strict: Omit<typeof assert, 'equal' | 'notEqual' | 'deepEqual' | 'notDeepEqual' | 'ok' | 'strictEqual' | 'deepStrictEqual' | 'ifError' | 'strict'> & {
+            (value: unknown, message?: string | Error): asserts value;
+            equal: typeof strictEqual;
+            notEqual: typeof notStrictEqual;
+            deepEqual: typeof deepStrictEqual;
+            notDeepEqual: typeof notDeepStrictEqual;
+            // Mapped types and assertion functions are incompatible?
+            // TS2775: Assertions require every name in the call target
+            // to be declared with an explicit type annotation.
+            ok: typeof ok;
+            strictEqual: typeof strictEqual;
+            deepStrictEqual: typeof deepStrictEqual;
+            ifError: typeof ifError;
+            strict: typeof strict;
+        };
+    }
+    export = assert;
+}
+declare module 'node:assert' {
+    import assert = require('assert');
+    export = assert;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+declare module 'assert/strict' {
+    import { strict } from 'node:assert';
+    export = strict;
+}
+declare module 'node:assert/strict' {
+    import { strict } from 'node:assert';
+    export = strict;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `async_hooks` module provides an API to track asynchronous resources. It
+ * can be accessed using:
+ *
+ * ```js
+ * import async_hooks from 'async_hooks';
+ * ```
+ * @experimental
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/async_hooks.js)
+ */
+declare module 'async_hooks' {
+    /**
+     * ```js
+     * import { executionAsyncId } from 'async_hooks';
+     *
+     * console.log(executionAsyncId());  // 1 - bootstrap
+     * fs.open(path, 'r', (err, fd) => {
+     *   console.log(executionAsyncId());  // 6 - open()
+     * });
+     * ```
+     *
+     * The ID returned from `executionAsyncId()` is related to execution timing, not
+     * causality (which is covered by `triggerAsyncId()`):
+     *
+     * ```js
+     * const server = net.createServer((conn) => {
+     *   // Returns the ID of the server, not of the new connection, because the
+     *   // callback runs in the execution scope of the server's MakeCallback().
+     *   async_hooks.executionAsyncId();
+     *
+     * }).listen(port, () => {
+     *   // Returns the ID of a TickObject (process.nextTick()) because all
+     *   // callbacks passed to .listen() are wrapped in a nextTick().
+     *   async_hooks.executionAsyncId();
+     * });
+     * ```
+     *
+     * Promise contexts may not get precise `executionAsyncIds` by default.
+     * See the section on `promise execution tracking`.
+     * @since v8.1.0
+     * @return The `asyncId` of the current execution context. Useful to track when something calls.
+     */
+    function executionAsyncId(): number;
+    /**
+     * Resource objects returned by `executionAsyncResource()` are most often internal
+     * Node.js handle objects with undocumented APIs. Using any functions or properties
+     * on the object is likely to crash your application and should be avoided.
+     *
+     * Using `executionAsyncResource()` in the top-level execution context will
+     * return an empty object as there is no handle or request object to use,
+     * but having an object representing the top-level can be helpful.
+     *
+     * ```js
+     * import { open } from 'fs';
+     * import { executionAsyncId, executionAsyncResource } from 'async_hooks';
+     *
+     * console.log(executionAsyncId(), executionAsyncResource());  // 1 {}
+     * open(new URL(import.meta.url), 'r', (err, fd) => {
+     *   console.log(executionAsyncId(), executionAsyncResource());  // 7 FSReqWrap
+     * });
+     * ```
+     *
+     * This can be used to implement continuation local storage without the
+     * use of a tracking `Map` to store the metadata:
+     *
+     * ```js
+     * import { createServer } from 'http';
+     * import {
+     *   executionAsyncId,
+     *   executionAsyncResource,
+     *   createHook
+     * } from 'async_hooks';
+     * const sym = Symbol('state'); // Private symbol to avoid pollution
+     *
+     * createHook({
+     *   init(asyncId, type, triggerAsyncId, resource) {
+     *     const cr = executionAsyncResource();
+     *     if (cr) {
+     *       resource[sym] = cr[sym];
+     *     }
+     *   }
+     * }).enable();
+     *
+     * const server = createServer((req, res) => {
+     *   executionAsyncResource()[sym] = { state: req.url };
+     *   setTimeout(function() {
+     *     res.end(JSON.stringify(executionAsyncResource()[sym]));
+     *   }, 100);
+     * }).listen(3000);
+     * ```
+     * @since v13.9.0, v12.17.0
+     * @return The resource representing the current execution. Useful to store data within the resource.
+     */
+    function executionAsyncResource(): object;
+    /**
+     * ```js
+     * const server = net.createServer((conn) => {
+     *   // The resource that caused (or triggered) this callback to be called
+     *   // was that of the new connection. Thus the return value of triggerAsyncId()
+     *   // is the asyncId of ""conn"".
+     *   async_hooks.triggerAsyncId();
+     *
+     * }).listen(port, () => {
+     *   // Even though all callbacks passed to .listen() are wrapped in a nextTick()
+     *   // the callback itself exists because the call to the server's .listen()
+     *   // was made. So the return value would be the ID of the server.
+     *   async_hooks.triggerAsyncId();
+     * });
+     * ```
+     *
+     * Promise contexts may not get valid `triggerAsyncId`s by default. See
+     * the section on `promise execution tracking`.
+     * @return The ID of the resource responsible for calling the callback that is currently being executed.
+     */
+    function triggerAsyncId(): number;
+    interface HookCallbacks {
+        /**
+         * Called when a class is constructed that has the possibility to emit an asynchronous event.
+         * @param asyncId a unique ID for the async resource
+         * @param type the type of the async resource
+         * @param triggerAsyncId the unique ID of the async resource in whose execution context this async resource was created
+         * @param resource reference to the resource representing the async operation, needs to be released during destroy
+         */
+        init?(asyncId: number, type: string, triggerAsyncId: number, resource: object): void;
+        /**
+         * When an asynchronous operation is initiated or completes a callback is called to notify the user.
+         * The before callback is called just before said callback is executed.
+         * @param asyncId the unique identifier assigned to the resource about to execute the callback.
+         */
+        before?(asyncId: number): void;
+        /**
+         * Called immediately after the callback specified in before is completed.
+         * @param asyncId the unique identifier assigned to the resource which has executed the callback.
+         */
+        after?(asyncId: number): void;
+        /**
+         * Called when a promise has resolve() called. This may not be in the same execution id
+         * as the promise itself.
+         * @param asyncId the unique id for the promise that was resolve()d.
+         */
+        promiseResolve?(asyncId: number): void;
+        /**
+         * Called after the resource corresponding to asyncId is destroyed
+         * @param asyncId a unique ID for the async resource
+         */
+        destroy?(asyncId: number): void;
+    }
+    interface AsyncHook {
+        /**
+         * Enable the callbacks for a given AsyncHook instance. If no callbacks are provided enabling is a noop.
+         */
+        enable(): this;
+        /**
+         * Disable the callbacks for a given AsyncHook instance from the global pool of AsyncHook callbacks to be executed. Once a hook has been disabled it will not be called again until enabled.
+         */
+        disable(): this;
+    }
+    /**
+     * Registers functions to be called for different lifetime events of each async
+     * operation.
+     *
+     * The callbacks `init()`/`before()`/`after()`/`destroy()` are called for the
+     * respective asynchronous event during a resource's lifetime.
+     *
+     * All callbacks are optional. For example, if only resource cleanup needs to
+     * be tracked, then only the `destroy` callback needs to be passed. The
+     * specifics of all functions that can be passed to `callbacks` is in the `Hook Callbacks` section.
+     *
+     * ```js
+     * import { createHook } from 'async_hooks';
+     *
+     * const asyncHook = createHook({
+     *   init(asyncId, type, triggerAsyncId, resource) { },
+     *   destroy(asyncId) { }
+     * });
+     * ```
+     *
+     * The callbacks will be inherited via the prototype chain:
+     *
+     * ```js
+     * class MyAsyncCallbacks {
+     *   init(asyncId, type, triggerAsyncId, resource) { }
+     *   destroy(asyncId) {}
+     * }
+     *
+     * class MyAddedCallbacks extends MyAsyncCallbacks {
+     *   before(asyncId) { }
+     *   after(asyncId) { }
+     * }
+     *
+     * const asyncHook = async_hooks.createHook(new MyAddedCallbacks());
+     * ```
+     *
+     * Because promises are asynchronous resources whose lifecycle is tracked
+     * via the async hooks mechanism, the `init()`, `before()`, `after()`, and`destroy()` callbacks _must not_ be async functions that return promises.
+     * @since v8.1.0
+     * @param callbacks The `Hook Callbacks` to register
+     * @return Instance used for disabling and enabling hooks
+     */
+    function createHook(callbacks: HookCallbacks): AsyncHook;
+    interface AsyncResourceOptions {
+        /**
+         * The ID of the execution context that created this async event.
+         * @default executionAsyncId()
+         */
+        triggerAsyncId?: number | undefined;
+        /**
+         * Disables automatic `emitDestroy` when the object is garbage collected.
+         * This usually does not need to be set (even if `emitDestroy` is called
+         * manually), unless the resource's `asyncId` is retrieved and the
+         * sensitive API's `emitDestroy` is called with it.
+         * @default false
+         */
+        requireManualDestroy?: boolean | undefined;
+    }
+    /**
+     * The class `AsyncResource` is designed to be extended by the embedder's async
+     * resources. Using this, users can easily trigger the lifetime events of their
+     * own resources.
+     *
+     * The `init` hook will trigger when an `AsyncResource` is instantiated.
+     *
+     * The following is an overview of the `AsyncResource` API.
+     *
+     * ```js
+     * import { AsyncResource, executionAsyncId } from 'async_hooks';
+     *
+     * // AsyncResource() is meant to be extended. Instantiating a
+     * // new AsyncResource() also triggers init. If triggerAsyncId is omitted then
+     * // async_hook.executionAsyncId() is used.
+     * const asyncResource = new AsyncResource(
+     *   type, { triggerAsyncId: executionAsyncId(), requireManualDestroy: false }
+     * );
+     *
+     * // Run a function in the execution context of the resource. This will
+     * // * establish the context of the resource
+     * // * trigger the AsyncHooks before callbacks
+     * // * call the provided function `fn` with the supplied arguments
+     * // * trigger the AsyncHooks after callbacks
+     * // * restore the original execution context
+     * asyncResource.runInAsyncScope(fn, thisArg, ...args);
+     *
+     * // Call AsyncHooks destroy callbacks.
+     * asyncResource.emitDestroy();
+     *
+     * // Return the unique ID assigned to the AsyncResource instance.
+     * asyncResource.asyncId();
+     *
+     * // Return the trigger ID for the AsyncResource instance.
+     * asyncResource.triggerAsyncId();
+     * ```
+     */
+    class AsyncResource {
+        /**
+         * AsyncResource() is meant to be extended. Instantiating a
+         * new AsyncResource() also triggers init. If triggerAsyncId is omitted then
+         * async_hook.executionAsyncId() is used.
+         * @param type The type of async event.
+         * @param triggerAsyncId The ID of the execution context that created
+         *   this async event (default: `executionAsyncId()`), or an
+         *   AsyncResourceOptions object (since v9.3.0)
+         */
+        constructor(type: string, triggerAsyncId?: number | AsyncResourceOptions);
+        /**
+         * Binds the given function to the current execution context.
+         *
+         * The returned function will have an `asyncResource` property referencing
+         * the `AsyncResource` to which the function is bound.
+         * @since v14.8.0, v12.19.0
+         * @param fn The function to bind to the current execution context.
+         * @param type An optional name to associate with the underlying `AsyncResource`.
+         */
+        static bind<Func extends (this: ThisArg, ...args: any[]) => any, ThisArg>(
+            fn: Func,
+            type?: string,
+            thisArg?: ThisArg
+        ): Func & {
+            asyncResource: AsyncResource;
+        };
+        /**
+         * Binds the given function to execute to this `AsyncResource`'s scope.
+         *
+         * The returned function will have an `asyncResource` property referencing
+         * the `AsyncResource` to which the function is bound.
+         * @since v14.8.0, v12.19.0
+         * @param fn The function to bind to the current `AsyncResource`.
+         */
+        bind<Func extends (...args: any[]) => any>(
+            fn: Func
+        ): Func & {
+            asyncResource: AsyncResource;
+        };
+        /**
+         * Call the provided function with the provided arguments in the execution context
+         * of the async resource. This will establish the context, trigger the AsyncHooks
+         * before callbacks, call the function, trigger the AsyncHooks after callbacks, and
+         * then restore the original execution context.
+         * @since v9.6.0
+         * @param fn The function to call in the execution context of this async resource.
+         * @param thisArg The receiver to be used for the function call.
+         * @param args Optional arguments to pass to the function.
+         */
+        runInAsyncScope<This, Result>(fn: (this: This, ...args: any[]) => Result, thisArg?: This, ...args: any[]): Result;
+        /**
+         * Call all `destroy` hooks. This should only ever be called once. An error will
+         * be thrown if it is called more than once. This **must** be manually called. If
+         * the resource is left to be collected by the GC then the `destroy` hooks will
+         * never be called.
+         * @return A reference to `asyncResource`.
+         */
+        emitDestroy(): this;
+        /**
+         * @return The unique `asyncId` assigned to the resource.
+         */
+        asyncId(): number;
+        /**
+         *
+         * @return The same `triggerAsyncId` that is passed to the `AsyncResource` constructor.
+         */
+        triggerAsyncId(): number;
+    }
+    /**
+     * This class creates stores that stay coherent through asynchronous operations.
+     *
+     * While you can create your own implementation on top of the `async_hooks` module,`AsyncLocalStorage` should be preferred as it is a performant and memory safe
+     * implementation that involves significant optimizations that are non-obvious to
+     * implement.
+     *
+     * The following example uses `AsyncLocalStorage` to build a simple logger
+     * that assigns IDs to incoming HTTP requests and includes them in messages
+     * logged within each request.
+     *
+     * ```js
+     * import http from 'http';
+     * import { AsyncLocalStorage } from 'async_hooks';
+     *
+     * const asyncLocalStorage = new AsyncLocalStorage();
+     *
+     * function logWithId(msg) {
+     *   const id = asyncLocalStorage.getStore();
+     *   console.log(`${id !== undefined ? id : '-'}:`, msg);
+     * }
+     *
+     * let idSeq = 0;
+     * http.createServer((req, res) => {
+     *   asyncLocalStorage.run(idSeq++, () => {
+     *     logWithId('start');
+     *     // Imagine any chain of async operations here
+     *     setImmediate(() => {
+     *       logWithId('finish');
+     *       res.end();
+     *     });
+     *   });
+     * }).listen(8080);
+     *
+     * http.get('http://localhost:8080');
+     * http.get('http://localhost:8080');
+     * // Prints:
+     * //   0: start
+     * //   1: start
+     * //   0: finish
+     * //   1: finish
+     * ```
+     *
+     * Each instance of `AsyncLocalStorage` maintains an independent storage context.
+     * Multiple instances can safely exist simultaneously without risk of interfering
+     * with each other data.
+     * @since v13.10.0, v12.17.0
+     */
+    class AsyncLocalStorage<T> {
+        /**
+         * Disables the instance of `AsyncLocalStorage`. All subsequent calls
+         * to `asyncLocalStorage.getStore()` will return `undefined` until`asyncLocalStorage.run()` or `asyncLocalStorage.enterWith()` is called again.
+         *
+         * When calling `asyncLocalStorage.disable()`, all current contexts linked to the
+         * instance will be exited.
+         *
+         * Calling `asyncLocalStorage.disable()` is required before the`asyncLocalStorage` can be garbage collected. This does not apply to stores
+         * provided by the `asyncLocalStorage`, as those objects are garbage collected
+         * along with the corresponding async resources.
+         *
+         * Use this method when the `asyncLocalStorage` is not in use anymore
+         * in the current process.
+         * @since v13.10.0, v12.17.0
+         * @experimental
+         */
+        disable(): void;
+        /**
+         * Returns the current store.
+         * If called outside of an asynchronous context initialized by
+         * calling `asyncLocalStorage.run()` or `asyncLocalStorage.enterWith()`, it
+         * returns `undefined`.
+         * @since v13.10.0, v12.17.0
+         */
+        getStore(): T | undefined;
+        /**
+         * Runs a function synchronously within a context and returns its
+         * return value. The store is not accessible outside of the callback function.
+         * The store is accessible to any asynchronous operations created within the
+         * callback.
+         *
+         * The optional `args` are passed to the callback function.
+         *
+         * If the callback function throws an error, the error is thrown by `run()` too.
+         * The stacktrace is not impacted by this call and the context is exited.
+         *
+         * Example:
+         *
+         * ```js
+         * const store = { id: 2 };
+         * try {
+         *   asyncLocalStorage.run(store, () => {
+         *     asyncLocalStorage.getStore(); // Returns the store object
+         *     setTimeout(() => {
+         *       asyncLocalStorage.getStore(); // Returns the store object
+         *     }, 200);
+         *     throw new Error();
+         *   });
+         * } catch (e) {
+         *   asyncLocalStorage.getStore(); // Returns undefined
+         *   // The error will be caught here
+         * }
+         * ```
+         * @since v13.10.0, v12.17.0
+         */
+        run<R, TArgs extends any[]>(store: T, callback: (...args: TArgs) => R, ...args: TArgs): R;
+        /**
+         * Runs a function synchronously outside of a context and returns its
+         * return value. The store is not accessible within the callback function or
+         * the asynchronous operations created within the callback. Any `getStore()`call done within the callback function will always return `undefined`.
+         *
+         * The optional `args` are passed to the callback function.
+         *
+         * If the callback function throws an error, the error is thrown by `exit()` too.
+         * The stacktrace is not impacted by this call and the context is re-entered.
+         *
+         * Example:
+         *
+         * ```js
+         * // Within a call to run
+         * try {
+         *   asyncLocalStorage.getStore(); // Returns the store object or value
+         *   asyncLocalStorage.exit(() => {
+         *     asyncLocalStorage.getStore(); // Returns undefined
+         *     throw new Error();
+         *   });
+         * } catch (e) {
+         *   asyncLocalStorage.getStore(); // Returns the same object or value
+         *   // The error will be caught here
+         * }
+         * ```
+         * @since v13.10.0, v12.17.0
+         * @experimental
+         */
+        exit<R, TArgs extends any[]>(callback: (...args: TArgs) => R, ...args: TArgs): R;
+        /**
+         * Transitions into the context for the remainder of the current
+         * synchronous execution and then persists the store through any following
+         * asynchronous calls.
+         *
+         * Example:
+         *
+         * ```js
+         * const store = { id: 1 };
+         * // Replaces previous store with the given store object
+         * asyncLocalStorage.enterWith(store);
+         * asyncLocalStorage.getStore(); // Returns the store object
+         * someAsyncOperation(() => {
+         *   asyncLocalStorage.getStore(); // Returns the same object
+         * });
+         * ```
+         *
+         * This transition will continue for the _entire_ synchronous execution.
+         * This means that if, for example, the context is entered within an event
+         * handler subsequent event handlers will also run within that context unless
+         * specifically bound to another context with an `AsyncResource`. That is why`run()` should be preferred over `enterWith()` unless there are strong reasons
+         * to use the latter method.
+         *
+         * ```js
+         * const store = { id: 1 };
+         *
+         * emitter.on('my-event', () => {
+         *   asyncLocalStorage.enterWith(store);
+         * });
+         * emitter.on('my-event', () => {
+         *   asyncLocalStorage.getStore(); // Returns the same object
+         * });
+         *
+         * asyncLocalStorage.getStore(); // Returns undefined
+         * emitter.emit('my-event');
+         * asyncLocalStorage.getStore(); // Returns the same object
+         * ```
+         * @since v13.11.0, v12.17.0
+         * @experimental
+         */
+        enterWith(store: T): void;
+    }
+}
+declare module 'node:async_hooks' {
+    export * from 'async_hooks';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `child_process` module provides the ability to spawn subprocesses in
+ * a manner that is similar, but not identical, to [`popen(3)`](http://man7.org/linux/man-pages/man3/popen.3.html). This capability
+ * is primarily provided by the {@link spawn} function:
+ *
+ * ```js
+ * const { spawn } = require('child_process');
+ * const ls = spawn('ls', ['-lh', '/usr']);
+ *
+ * ls.stdout.on('data', (data) => {
+ *   console.log(`stdout: ${data}`);
+ * });
+ *
+ * ls.stderr.on('data', (data) => {
+ *   console.error(`stderr: ${data}`);
+ * });
+ *
+ * ls.on('close', (code) => {
+ *   console.log(`child process exited with code ${code}`);
+ * });
+ * ```
+ *
+ * By default, pipes for `stdin`, `stdout`, and `stderr` are established between
+ * the parent Node.js process and the spawned subprocess. These pipes have
+ * limited (and platform-specific) capacity. If the subprocess writes to
+ * stdout in excess of that limit without the output being captured, the
+ * subprocess blocks waiting for the pipe buffer to accept more data. This is
+ * identical to the behavior of pipes in the shell. Use the `{ stdio: 'ignore' }`option if the output will not be consumed.
+ *
+ * The command lookup is performed using the `options.env.PATH` environment
+ * variable if it is in the `options` object. Otherwise, `process.env.PATH` is
+ * used.
+ *
+ * On Windows, environment variables are case-insensitive. Node.js
+ * lexicographically sorts the `env` keys and uses the first one that
+ * case-insensitively matches. Only first (in lexicographic order) entry will be
+ * passed to the subprocess. This might lead to issues on Windows when passing
+ * objects to the `env` option that have multiple variants of the same key, such as`PATH` and `Path`.
+ *
+ * The {@link spawn} method spawns the child process asynchronously,
+ * without blocking the Node.js event loop. The {@link spawnSync} function provides equivalent functionality in a synchronous manner that blocks
+ * the event loop until the spawned process either exits or is terminated.
+ *
+ * For convenience, the `child_process` module provides a handful of synchronous
+ * and asynchronous alternatives to {@link spawn} and {@link spawnSync}. Each of these alternatives are implemented on
+ * top of {@link spawn} or {@link spawnSync}.
+ *
+ * * {@link exec}: spawns a shell and runs a command within that
+ * shell, passing the `stdout` and `stderr` to a callback function when
+ * complete.
+ * * {@link execFile}: similar to {@link exec} except
+ * that it spawns the command directly without first spawning a shell by
+ * default.
+ * * {@link fork}: spawns a new Node.js process and invokes a
+ * specified module with an IPC communication channel established that allows
+ * sending messages between parent and child.
+ * * {@link execSync}: a synchronous version of {@link exec} that will block the Node.js event loop.
+ * * {@link execFileSync}: a synchronous version of {@link execFile} that will block the Node.js event loop.
+ *
+ * For certain use cases, such as automating shell scripts, the `synchronous counterparts` may be more convenient. In many cases, however,
+ * the synchronous methods can have significant impact on performance due to
+ * stalling the event loop while spawned processes complete.
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/child_process.js)
+ */
+declare module 'child_process' {
+    import { ObjectEncodingOptions } from 'node:fs';
+    import { EventEmitter, Abortable } from 'node:events';
+    import * as net from 'node:net';
+    import { Writable, Readable, Stream, Pipe } from 'node:stream';
+    import { URL } from 'node:url';
+    type Serializable = string | object | number | boolean | bigint;
+    type SendHandle = net.Socket | net.Server;
+    /**
+     * Instances of the `ChildProcess` represent spawned child processes.
+     *
+     * Instances of `ChildProcess` are not intended to be created directly. Rather,
+     * use the {@link spawn}, {@link exec},{@link execFile}, or {@link fork} methods to create
+     * instances of `ChildProcess`.
+     * @since v2.2.0
+     */
+    class ChildProcess extends EventEmitter {
+        /**
+         * A `Writable Stream` that represents the child process's `stdin`.
+         *
+         * If a child process waits to read all of its input, the child will not continue
+         * until this stream has been closed via `end()`.
+         *
+         * If the child was spawned with `stdio[0]` set to anything other than `'pipe'`,
+         * then this will be `null`.
+         *
+         * `subprocess.stdin` is an alias for `subprocess.stdio[0]`. Both properties will
+         * refer to the same value.
+         *
+         * The `subprocess.stdin` property can be `undefined` if the child process could
+         * not be successfully spawned.
+         * @since v0.1.90
+         */
+        stdin: Writable | null;
+        /**
+         * A `Readable Stream` that represents the child process's `stdout`.
+         *
+         * If the child was spawned with `stdio[1]` set to anything other than `'pipe'`,
+         * then this will be `null`.
+         *
+         * `subprocess.stdout` is an alias for `subprocess.stdio[1]`. Both properties will
+         * refer to the same value.
+         *
+         * ```js
+         * const { spawn } = require('child_process');
+         *
+         * const subprocess = spawn('ls');
+         *
+         * subprocess.stdout.on('data', (data) => {
+         *   console.log(`Received chunk ${data}`);
+         * });
+         * ```
+         *
+         * The `subprocess.stdout` property can be `null` if the child process could
+         * not be successfully spawned.
+         * @since v0.1.90
+         */
+        stdout: Readable | null;
+        /**
+         * A `Readable Stream` that represents the child process's `stderr`.
+         *
+         * If the child was spawned with `stdio[2]` set to anything other than `'pipe'`,
+         * then this will be `null`.
+         *
+         * `subprocess.stderr` is an alias for `subprocess.stdio[2]`. Both properties will
+         * refer to the same value.
+         *
+         * The `subprocess.stderr` property can be `null` if the child process could
+         * not be successfully spawned.
+         * @since v0.1.90
+         */
+        stderr: Readable | null;
+        /**
+         * The `subprocess.channel` property is a reference to the child's IPC channel. If
+         * no IPC channel currently exists, this property is `undefined`.
+         * @since v7.1.0
+         */
+        readonly channel?: Pipe | null | undefined;
+        /**
+         * A sparse array of pipes to the child process, corresponding with positions in
+         * the `stdio` option passed to {@link spawn} that have been set
+         * to the value `'pipe'`. `subprocess.stdio[0]`, `subprocess.stdio[1]`, and`subprocess.stdio[2]` are also available as `subprocess.stdin`,`subprocess.stdout`, and `subprocess.stderr`,
+         * respectively.
+         *
+         * In the following example, only the child's fd `1` (stdout) is configured as a
+         * pipe, so only the parent's `subprocess.stdio[1]` is a stream, all other values
+         * in the array are `null`.
+         *
+         * ```js
+         * const assert = require('assert');
+         * const fs = require('fs');
+         * const child_process = require('child_process');
+         *
+         * const subprocess = child_process.spawn('ls', {
+         *   stdio: [
+         *     0, // Use parent's stdin for child.
+         *     'pipe', // Pipe child's stdout to parent.
+         *     fs.openSync('err.out', 'w'), // Direct child's stderr to a file.
+         *   ]
+         * });
+         *
+         * assert.strictEqual(subprocess.stdio[0], null);
+         * assert.strictEqual(subprocess.stdio[0], subprocess.stdin);
+         *
+         * assert(subprocess.stdout);
+         * assert.strictEqual(subprocess.stdio[1], subprocess.stdout);
+         *
+         * assert.strictEqual(subprocess.stdio[2], null);
+         * assert.strictEqual(subprocess.stdio[2], subprocess.stderr);
+         * ```
+         *
+         * The `subprocess.stdio` property can be `undefined` if the child process could
+         * not be successfully spawned.
+         * @since v0.7.10
+         */
+        readonly stdio: [
+            Writable | null,
+            // stdin
+            Readable | null,
+            // stdout
+            Readable | null,
+            // stderr
+            Readable | Writable | null | undefined,
+            // extra
+            Readable | Writable | null | undefined // extra
+        ];
+        /**
+         * The `subprocess.killed` property indicates whether the child process
+         * successfully received a signal from `subprocess.kill()`. The `killed` property
+         * does not indicate that the child process has been terminated.
+         * @since v0.5.10
+         */
+        readonly killed: boolean;
+        /**
+         * Returns the process identifier (PID) of the child process. If the child process
+         * fails to spawn due to errors, then the value is `undefined` and `error` is
+         * emitted.
+         *
+         * ```js
+         * const { spawn } = require('child_process');
+         * const grep = spawn('grep', ['ssh']);
+         *
+         * console.log(`Spawned child pid: ${grep.pid}`);
+         * grep.stdin.end();
+         * ```
+         * @since v0.1.90
+         */
+        readonly pid?: number | undefined;
+        /**
+         * The `subprocess.connected` property indicates whether it is still possible to
+         * send and receive messages from a child process. When `subprocess.connected` is`false`, it is no longer possible to send or receive messages.
+         * @since v0.7.2
+         */
+        readonly connected: boolean;
+        /**
+         * The `subprocess.exitCode` property indicates the exit code of the child process.
+         * If the child process is still running, the field will be `null`.
+         */
+        readonly exitCode: number | null;
+        /**
+         * The `subprocess.signalCode` property indicates the signal received by
+         * the child process if any, else `null`.
+         */
+        readonly signalCode: NodeJS.Signals | null;
+        /**
+         * The `subprocess.spawnargs` property represents the full list of command-line
+         * arguments the child process was launched with.
+         */
+        readonly spawnargs: string[];
+        /**
+         * The `subprocess.spawnfile` property indicates the executable file name of
+         * the child process that is launched.
+         *
+         * For {@link fork}, its value will be equal to `process.execPath`.
+         * For {@link spawn}, its value will be the name of
+         * the executable file.
+         * For {@link exec},  its value will be the name of the shell
+         * in which the child process is launched.
+         */
+        readonly spawnfile: string;
+        /**
+         * The `subprocess.kill()` method sends a signal to the child process. If no
+         * argument is given, the process will be sent the `'SIGTERM'` signal. See [`signal(7)`](http://man7.org/linux/man-pages/man7/signal.7.html) for a list of available signals. This function
+         * returns `true` if [`kill(2)`](http://man7.org/linux/man-pages/man2/kill.2.html) succeeds, and `false` otherwise.
+         *
+         * ```js
+         * const { spawn } = require('child_process');
+         * const grep = spawn('grep', ['ssh']);
+         *
+         * grep.on('close', (code, signal) => {
+         *   console.log(
+         *     `child process terminated due to receipt of signal ${signal}`);
+         * });
+         *
+         * // Send SIGHUP to process.
+         * grep.kill('SIGHUP');
+         * ```
+         *
+         * The `ChildProcess` object may emit an `'error'` event if the signal
+         * cannot be delivered. Sending a signal to a child process that has already exited
+         * is not an error but may have unforeseen consequences. Specifically, if the
+         * process identifier (PID) has been reassigned to another process, the signal will
+         * be delivered to that process instead which can have unexpected results.
+         *
+         * While the function is called `kill`, the signal delivered to the child process
+         * may not actually terminate the process.
+         *
+         * See [`kill(2)`](http://man7.org/linux/man-pages/man2/kill.2.html) for reference.
+         *
+         * On Windows, where POSIX signals do not exist, the `signal` argument will be
+         * ignored, and the process will be killed forcefully and abruptly (similar to`'SIGKILL'`).
+         * See `Signal Events` for more details.
+         *
+         * On Linux, child processes of child processes will not be terminated
+         * when attempting to kill their parent. This is likely to happen when running a
+         * new process in a shell or with the use of the `shell` option of `ChildProcess`:
+         *
+         * ```js
+         * 'use strict';
+         * const { spawn } = require('child_process');
+         *
+         * const subprocess = spawn(
+         *   'sh',
+         *   [
+         *     '-c',
+         *     `node -e ""setInterval(() => {
+         *       console.log(process.pid, 'is alive')
+         *     }, 500);""`,
+         *   ], {
+         *     stdio: ['inherit', 'inherit', 'inherit']
+         *   }
+         * );
+         *
+         * setTimeout(() => {
+         *   subprocess.kill(); // Does not terminate the Node.js process in the shell.
+         * }, 2000);
+         * ```
+         * @since v0.1.90
+         */
+        kill(signal?: NodeJS.Signals | number): boolean;
+        /**
+         * When an IPC channel has been established between the parent and child (
+         * i.e. when using {@link fork}), the `subprocess.send()` method can
+         * be used to send messages to the child process. When the child process is a
+         * Node.js instance, these messages can be received via the `'message'` event.
+         *
+         * The message goes through serialization and parsing. The resulting
+         * message might not be the same as what is originally sent.
+         *
+         * For example, in the parent script:
+         *
+         * ```js
+         * const cp = require('child_process');
+         * const n = cp.fork(`${__dirname}/sub.js`);
+         *
+         * n.on('message', (m) => {
+         *   console.log('PARENT got message:', m);
+         * });
+         *
+         * // Causes the child to print: CHILD got message: { hello: 'world' }
+         * n.send({ hello: 'world' });
+         * ```
+         *
+         * And then the child script, `'sub.js'` might look like this:
+         *
+         * ```js
+         * process.on('message', (m) => {
+         *   console.log('CHILD got message:', m);
+         * });
+         *
+         * // Causes the parent to print: PARENT got message: { foo: 'bar', baz: null }
+         * process.send({ foo: 'bar', baz: NaN });
+         * ```
+         *
+         * Child Node.js processes will have a `process.send()` method of their own
+         * that allows the child to send messages back to the parent.
+         *
+         * There is a special case when sending a `{cmd: 'NODE_foo'}` message. Messages
+         * containing a `NODE_` prefix in the `cmd` property are reserved for use within
+         * Node.js core and will not be emitted in the child's `'message'` event. Rather, such messages are emitted using the`'internalMessage'` event and are consumed internally by Node.js.
+         * Applications should avoid using such messages or listening for`'internalMessage'` events as it is subject to change without notice.
+         *
+         * The optional `sendHandle` argument that may be passed to `subprocess.send()` is
+         * for passing a TCP server or socket object to the child process. The child will
+         * receive the object as the second argument passed to the callback function
+         * registered on the `'message'` event. Any data that is received
+         * and buffered in the socket will not be sent to the child.
+         *
+         * The optional `callback` is a function that is invoked after the message is
+         * sent but before the child may have received it. The function is called with a
+         * single argument: `null` on success, or an `Error` object on failure.
+         *
+         * If no `callback` function is provided and the message cannot be sent, an`'error'` event will be emitted by the `ChildProcess` object. This can
+         * happen, for instance, when the child process has already exited.
+         *
+         * `subprocess.send()` will return `false` if the channel has closed or when the
+         * backlog of unsent messages exceeds a threshold that makes it unwise to send
+         * more. Otherwise, the method returns `true`. The `callback` function can be
+         * used to implement flow control.
+         *
+         * #### Example: sending a server object
+         *
+         * The `sendHandle` argument can be used, for instance, to pass the handle of
+         * a TCP server object to the child process as illustrated in the example below:
+         *
+         * ```js
+         * const subprocess = require('child_process').fork('subprocess.js');
+         *
+         * // Open up the server object and send the handle.
+         * const server = require('net').createServer();
+         * server.on('connection', (socket) => {
+         *   socket.end('handled by parent');
+         * });
+         * server.listen(1337, () => {
+         *   subprocess.send('server', server);
+         * });
+         * ```
+         *
+         * The child would then receive the server object as:
+         *
+         * ```js
+         * process.on('message', (m, server) => {
+         *   if (m === 'server') {
+         *     server.on('connection', (socket) => {
+         *       socket.end('handled by child');
+         *     });
+         *   }
+         * });
+         * ```
+         *
+         * Once the server is now shared between the parent and child, some connections
+         * can be handled by the parent and some by the child.
+         *
+         * While the example above uses a server created using the `net` module, `dgram`module servers use exactly the same workflow with the exceptions of listening on
+         * a `'message'` event instead of `'connection'` and using `server.bind()` instead
+         * of `server.listen()`. This is, however, currently only supported on Unix
+         * platforms.
+         *
+         * #### Example: sending a socket object
+         *
+         * Similarly, the `sendHandler` argument can be used to pass the handle of a
+         * socket to the child process. The example below spawns two children that each
+         * handle connections with ""normal"" or ""special"" priority:
+         *
+         * ```js
+         * const { fork } = require('child_process');
+         * const normal = fork('subprocess.js', ['normal']);
+         * const special = fork('subprocess.js', ['special']);
+         *
+         * // Open up the server and send sockets to child. Use pauseOnConnect to prevent
+         * // the sockets from being read before they are sent to the child process.
+         * const server = require('net').createServer({ pauseOnConnect: true });
+         * server.on('connection', (socket) => {
+         *
+         *   // If this is special priority...
+         *   if (socket.remoteAddress === '74.125.127.100') {
+         *     special.send('socket', socket);
+         *     return;
+         *   }
+         *   // This is normal priority.
+         *   normal.send('socket', socket);
+         * });
+         * server.listen(1337);
+         * ```
+         *
+         * The `subprocess.js` would receive the socket handle as the second argument
+         * passed to the event callback function:
+         *
+         * ```js
+         * process.on('message', (m, socket) => {
+         *   if (m === 'socket') {
+         *     if (socket) {
+         *       // Check that the client socket exists.
+         *       // It is possible for the socket to be closed between the time it is
+         *       // sent and the time it is received in the child process.
+         *       socket.end(`Request handled with ${process.argv[2]} priority`);
+         *     }
+         *   }
+         * });
+         * ```
+         *
+         * Do not use `.maxConnections` on a socket that has been passed to a subprocess.
+         * The parent cannot track when the socket is destroyed.
+         *
+         * Any `'message'` handlers in the subprocess should verify that `socket` exists,
+         * as the connection may have been closed during the time it takes to send the
+         * connection to the child.
+         * @since v0.5.9
+         * @param options The `options` argument, if present, is an object used to parameterize the sending of certain types of handles. `options` supports the following properties:
+         */
+        send(message: Serializable, callback?: (error: Error | null) => void): boolean;
+        send(message: Serializable, sendHandle?: SendHandle, callback?: (error: Error | null) => void): boolean;
+        send(message: Serializable, sendHandle?: SendHandle, options?: MessageOptions, callback?: (error: Error | null) => void): boolean;
+        /**
+         * Closes the IPC channel between parent and child, allowing the child to exit
+         * gracefully once there are no other connections keeping it alive. After calling
+         * this method the `subprocess.connected` and `process.connected` properties in
+         * both the parent and child (respectively) will be set to `false`, and it will be
+         * no longer possible to pass messages between the processes.
+         *
+         * The `'disconnect'` event will be emitted when there are no messages in the
+         * process of being received. This will most often be triggered immediately after
+         * calling `subprocess.disconnect()`.
+         *
+         * When the child process is a Node.js instance (e.g. spawned using {@link fork}), the `process.disconnect()` method can be invoked
+         * within the child process to close the IPC channel as well.
+         * @since v0.7.2
+         */
+        disconnect(): void;
+        /**
+         * By default, the parent will wait for the detached child to exit. To prevent the
+         * parent from waiting for a given `subprocess` to exit, use the`subprocess.unref()` method. Doing so will cause the parent's event loop to not
+         * include the child in its reference count, allowing the parent to exit
+         * independently of the child, unless there is an established IPC channel between
+         * the child and the parent.
+         *
+         * ```js
+         * const { spawn } = require('child_process');
+         *
+         * const subprocess = spawn(process.argv[0], ['child_program.js'], {
+         *   detached: true,
+         *   stdio: 'ignore'
+         * });
+         *
+         * subprocess.unref();
+         * ```
+         * @since v0.7.10
+         */
+        unref(): void;
+        /**
+         * Calling `subprocess.ref()` after making a call to `subprocess.unref()` will
+         * restore the removed reference count for the child process, forcing the parent
+         * to wait for the child to exit before exiting itself.
+         *
+         * ```js
+         * const { spawn } = require('child_process');
+         *
+         * const subprocess = spawn(process.argv[0], ['child_program.js'], {
+         *   detached: true,
+         *   stdio: 'ignore'
+         * });
+         *
+         * subprocess.unref();
+         * subprocess.ref();
+         * ```
+         * @since v0.7.10
+         */
+        ref(): void;
+        /**
+         * events.EventEmitter
+         * 1. close
+         * 2. disconnect
+         * 3. error
+         * 4. exit
+         * 5. message
+         * 6. spawn
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        addListener(event: 'disconnect', listener: () => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'exit', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        addListener(event: 'message', listener: (message: Serializable, sendHandle: SendHandle) => void): this;
+        addListener(event: 'spawn', listener: () => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'close', code: number | null, signal: NodeJS.Signals | null): boolean;
+        emit(event: 'disconnect'): boolean;
+        emit(event: 'error', err: Error): boolean;
+        emit(event: 'exit', code: number | null, signal: NodeJS.Signals | null): boolean;
+        emit(event: 'message', message: Serializable, sendHandle: SendHandle): boolean;
+        emit(event: 'spawn', listener: () => void): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        on(event: 'disconnect', listener: () => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'exit', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        on(event: 'message', listener: (message: Serializable, sendHandle: SendHandle) => void): this;
+        on(event: 'spawn', listener: () => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        once(event: 'disconnect', listener: () => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'exit', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        once(event: 'message', listener: (message: Serializable, sendHandle: SendHandle) => void): this;
+        once(event: 'spawn', listener: () => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        prependListener(event: 'disconnect', listener: () => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'exit', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        prependListener(event: 'message', listener: (message: Serializable, sendHandle: SendHandle) => void): this;
+        prependListener(event: 'spawn', listener: () => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        prependOnceListener(event: 'disconnect', listener: () => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'exit', listener: (code: number | null, signal: NodeJS.Signals | null) => void): this;
+        prependOnceListener(event: 'message', listener: (message: Serializable, sendHandle: SendHandle) => void): this;
+        prependOnceListener(event: 'spawn', listener: () => void): this;
+    }
+    // return this object when stdio option is undefined or not specified
+    interface ChildProcessWithoutNullStreams extends ChildProcess {
+        stdin: Writable;
+        stdout: Readable;
+        stderr: Readable;
+        readonly stdio: [
+            Writable,
+            Readable,
+            Readable,
+            // stderr
+            Readable | Writable | null | undefined,
+            // extra, no modification
+            Readable | Writable | null | undefined // extra, no modification
+        ];
+    }
+    // return this object when stdio option is a tuple of 3
+    interface ChildProcessByStdio<I extends null | Writable, O extends null | Readable, E extends null | Readable> extends ChildProcess {
+        stdin: I;
+        stdout: O;
+        stderr: E;
+        readonly stdio: [
+            I,
+            O,
+            E,
+            Readable | Writable | null | undefined,
+            // extra, no modification
+            Readable | Writable | null | undefined // extra, no modification
+        ];
+    }
+    interface MessageOptions {
+        keepOpen?: boolean | undefined;
+    }
+    type IOType = 'overlapped' | 'pipe' | 'ignore' | 'inherit';
+    type StdioOptions = IOType | Array<IOType | 'ipc' | Stream | number | null | undefined>;
+    type SerializationType = 'json' | 'advanced';
+    interface MessagingOptions extends Abortable {
+        /**
+         * Specify the kind of serialization used for sending messages between processes.
+         * @default 'json'
+         */
+        serialization?: SerializationType | undefined;
+        /**
+         * The signal value to be used when the spawned process will be killed by the abort signal.
+         * @default 'SIGTERM'
+         */
+        killSignal?: NodeJS.Signals | number | undefined;
+        /**
+         * In milliseconds the maximum amount of time the process is allowed to run.
+         */
+        timeout?: number | undefined;
+    }
+    interface ProcessEnvOptions {
+        uid?: number | undefined;
+        gid?: number | undefined;
+        cwd?: string | URL | undefined;
+        env?: NodeJS.ProcessEnv | undefined;
+    }
+    interface CommonOptions extends ProcessEnvOptions {
+        /**
+         * @default true
+         */
+        windowsHide?: boolean | undefined;
+        /**
+         * @default 0
+         */
+        timeout?: number | undefined;
+    }
+    interface CommonSpawnOptions extends CommonOptions, MessagingOptions, Abortable {
+        argv0?: string | undefined;
+        stdio?: StdioOptions | undefined;
+        shell?: boolean | string | undefined;
+        windowsVerbatimArguments?: boolean | undefined;
+    }
+    interface SpawnOptions extends CommonSpawnOptions {
+        detached?: boolean | undefined;
+    }
+    interface SpawnOptionsWithoutStdio extends SpawnOptions {
+        stdio?: StdioPipeNamed | StdioPipe[] | undefined;
+    }
+    type StdioNull = 'inherit' | 'ignore' | Stream;
+    type StdioPipeNamed = 'pipe' | 'overlapped';
+    type StdioPipe = undefined | null | StdioPipeNamed;
+    interface SpawnOptionsWithStdioTuple<Stdin extends StdioNull | StdioPipe, Stdout extends StdioNull | StdioPipe, Stderr extends StdioNull | StdioPipe> extends SpawnOptions {
+        stdio: [Stdin, Stdout, Stderr];
+    }
+    /**
+     * The `child_process.spawn()` method spawns a new process using the given`command`, with command-line arguments in `args`. If omitted, `args` defaults
+     * to an empty array.
+     *
+     * **If the `shell` option is enabled, do not pass unsanitized user input to this**
+     * **function. Any input containing shell metacharacters may be used to trigger**
+     * **arbitrary command execution.**
+     *
+     * A third argument may be used to specify additional options, with these defaults:
+     *
+     * ```js
+     * const defaults = {
+     *   cwd: undefined,
+     *   env: process.env
+     * };
+     * ```
+     *
+     * Use `cwd` to specify the working directory from which the process is spawned.
+     * If not given, the default is to inherit the current working directory. If given,
+     * but the path does not exist, the child process emits an `ENOENT` error
+     * and exits immediately. `ENOENT` is also emitted when the command
+     * does not exist.
+     *
+     * Use `env` to specify environment variables that will be visible to the new
+     * process, the default is `process.env`.
+     *
+     * `undefined` values in `env` will be ignored.
+     *
+     * Example of running `ls -lh /usr`, capturing `stdout`, `stderr`, and the
+     * exit code:
+     *
+     * ```js
+     * const { spawn } = require('child_process');
+     * const ls = spawn('ls', ['-lh', '/usr']);
+     *
+     * ls.stdout.on('data', (data) => {
+     *   console.log(`stdout: ${data}`);
+     * });
+     *
+     * ls.stderr.on('data', (data) => {
+     *   console.error(`stderr: ${data}`);
+     * });
+     *
+     * ls.on('close', (code) => {
+     *   console.log(`child process exited with code ${code}`);
+     * });
+     * ```
+     *
+     * Example: A very elaborate way to run `ps ax | grep ssh`
+     *
+     * ```js
+     * const { spawn } = require('child_process');
+     * const ps = spawn('ps', ['ax']);
+     * const grep = spawn('grep', ['ssh']);
+     *
+     * ps.stdout.on('data', (data) => {
+     *   grep.stdin.write(data);
+     * });
+     *
+     * ps.stderr.on('data', (data) => {
+     *   console.error(`ps stderr: ${data}`);
+     * });
+     *
+     * ps.on('close', (code) => {
+     *   if (code !== 0) {
+     *     console.log(`ps process exited with code ${code}`);
+     *   }
+     *   grep.stdin.end();
+     * });
+     *
+     * grep.stdout.on('data', (data) => {
+     *   console.log(data.toString());
+     * });
+     *
+     * grep.stderr.on('data', (data) => {
+     *   console.error(`grep stderr: ${data}`);
+     * });
+     *
+     * grep.on('close', (code) => {
+     *   if (code !== 0) {
+     *     console.log(`grep process exited with code ${code}`);
+     *   }
+     * });
+     * ```
+     *
+     * Example of checking for failed `spawn`:
+     *
+     * ```js
+     * const { spawn } = require('child_process');
+     * const subprocess = spawn('bad_command');
+     *
+     * subprocess.on('error', (err) => {
+     *   console.error('Failed to start subprocess.');
+     * });
+     * ```
+     *
+     * Certain platforms (macOS, Linux) will use the value of `argv[0]` for the process
+     * title while others (Windows, SunOS) will use `command`.
+     *
+     * Node.js currently overwrites `argv[0]` with `process.execPath` on startup, so`process.argv[0]` in a Node.js child process will not match the `argv0`parameter passed to `spawn` from the parent,
+     * retrieve it with the`process.argv0` property instead.
+     *
+     * If the `signal` option is enabled, calling `.abort()` on the corresponding`AbortController` is similar to calling `.kill()` on the child process except
+     * the error passed to the callback will be an `AbortError`:
+     *
+     * ```js
+     * const { spawn } = require('child_process');
+     * const controller = new AbortController();
+     * const { signal } = controller;
+     * const grep = spawn('grep', ['ssh'], { signal });
+     * grep.on('error', (err) => {
+     *   // This will be called with err being an AbortError if the controller aborts
+     * });
+     * controller.abort(); // Stops the child process
+     * ```
+     * @since v0.1.90
+     * @param command The command to run.
+     * @param args List of string arguments.
+     */
+    function spawn(command: string, options?: SpawnOptionsWithoutStdio): ChildProcessWithoutNullStreams;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioPipe, StdioPipe>): ChildProcessByStdio<Writable, Readable, Readable>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioPipe, StdioNull>): ChildProcessByStdio<Writable, Readable, null>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioNull, StdioPipe>): ChildProcessByStdio<Writable, null, Readable>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioNull, StdioPipe, StdioPipe>): ChildProcessByStdio<null, Readable, Readable>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioNull, StdioNull>): ChildProcessByStdio<Writable, null, null>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioNull, StdioPipe, StdioNull>): ChildProcessByStdio<null, Readable, null>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioNull, StdioNull, StdioPipe>): ChildProcessByStdio<null, null, Readable>;
+    function spawn(command: string, options: SpawnOptionsWithStdioTuple<StdioNull, StdioNull, StdioNull>): ChildProcessByStdio<null, null, null>;
+    function spawn(command: string, options: SpawnOptions): ChildProcess;
+    // overloads of spawn with 'args'
+    function spawn(command: string, args?: ReadonlyArray<string>, options?: SpawnOptionsWithoutStdio): ChildProcessWithoutNullStreams;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioPipe, StdioPipe>): ChildProcessByStdio<Writable, Readable, Readable>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioPipe, StdioNull>): ChildProcessByStdio<Writable, Readable, null>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioNull, StdioPipe>): ChildProcessByStdio<Writable, null, Readable>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioNull, StdioPipe, StdioPipe>): ChildProcessByStdio<null, Readable, Readable>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioPipe, StdioNull, StdioNull>): ChildProcessByStdio<Writable, null, null>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioNull, StdioPipe, StdioNull>): ChildProcessByStdio<null, Readable, null>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioNull, StdioNull, StdioPipe>): ChildProcessByStdio<null, null, Readable>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptionsWithStdioTuple<StdioNull, StdioNull, StdioNull>): ChildProcessByStdio<null, null, null>;
+    function spawn(command: string, args: ReadonlyArray<string>, options: SpawnOptions): ChildProcess;
+    interface ExecOptions extends CommonOptions {
+        shell?: string | undefined;
+        signal?: AbortSignal | undefined;
+        maxBuffer?: number | undefined;
+        killSignal?: NodeJS.Signals | number | undefined;
+    }
+    interface ExecOptionsWithStringEncoding extends ExecOptions {
+        encoding: BufferEncoding;
+    }
+    interface ExecOptionsWithBufferEncoding extends ExecOptions {
+        encoding: BufferEncoding | null; // specify `null`.
+    }
+    interface ExecException extends Error {
+        cmd?: string | undefined;
+        killed?: boolean | undefined;
+        code?: number | undefined;
+        signal?: NodeJS.Signals | undefined;
+    }
+    /**
+     * Spawns a shell then executes the `command` within that shell, buffering any
+     * generated output. The `command` string passed to the exec function is processed
+     * directly by the shell and special characters (vary based on [shell](https://en.wikipedia.org/wiki/List_of_command-line_interpreters))
+     * need to be dealt with accordingly:
+     *
+     * ```js
+     * const { exec } = require('child_process');
+     *
+     * exec('""/path/to/test file/test.sh"" arg1 arg2');
+     * // Double quotes are used so that the space in the path is not interpreted as
+     * // a delimiter of multiple arguments.
+     *
+     * exec('echo ""The \\$HOME variable is $HOME""');
+     * // The $HOME variable is escaped in the first instance, but not in the second.
+     * ```
+     *
+     * **Never pass unsanitized user input to this function. Any input containing shell**
+     * **metacharacters may be used to trigger arbitrary command execution.**
+     *
+     * If a `callback` function is provided, it is called with the arguments`(error, stdout, stderr)`. On success, `error` will be `null`. On error,`error` will be an instance of `Error`. The
+     * `error.code` property will be
+     * the exit code of the process. By convention, any exit code other than `0`indicates an error. `error.signal` will be the signal that terminated the
+     * process.
+     *
+     * The `stdout` and `stderr` arguments passed to the callback will contain the
+     * stdout and stderr output of the child process. By default, Node.js will decode
+     * the output as UTF-8 and pass strings to the callback. The `encoding` option
+     * can be used to specify the character encoding used to decode the stdout and
+     * stderr output. If `encoding` is `'buffer'`, or an unrecognized character
+     * encoding, `Buffer` objects will be passed to the callback instead.
+     *
+     * ```js
+     * const { exec } = require('child_process');
+     * exec('cat *.js missing_file | wc -l', (error, stdout, stderr) => {
+     *   if (error) {
+     *     console.error(`exec error: ${error}`);
+     *     return;
+     *   }
+     *   console.log(`stdout: ${stdout}`);
+     *   console.error(`stderr: ${stderr}`);
+     * });
+     * ```
+     *
+     * If `timeout` is greater than `0`, the parent will send the signal
+     * identified by the `killSignal` property (the default is `'SIGTERM'`) if the
+     * child runs longer than `timeout` milliseconds.
+     *
+     * Unlike the [`exec(3)`](http://man7.org/linux/man-pages/man3/exec.3.html) POSIX system call, `child_process.exec()` does not replace
+     * the existing process and uses a shell to execute the command.
+     *
+     * If this method is invoked as its `util.promisify()` ed version, it returns
+     * a `Promise` for an `Object` with `stdout` and `stderr` properties. The returned`ChildProcess` instance is attached to the `Promise` as a `child` property. In
+     * case of an error (including any error resulting in an exit code other than 0), a
+     * rejected promise is returned, with the same `error` object given in the
+     * callback, but with two additional properties `stdout` and `stderr`.
+     *
+     * ```js
+     * const util = require('util');
+     * const exec = util.promisify(require('child_process').exec);
+     *
+     * async function lsExample() {
+     *   const { stdout, stderr } = await exec('ls');
+     *   console.log('stdout:', stdout);
+     *   console.error('stderr:', stderr);
+     * }
+     * lsExample();
+     * ```
+     *
+     * If the `signal` option is enabled, calling `.abort()` on the corresponding`AbortController` is similar to calling `.kill()` on the child process except
+     * the error passed to the callback will be an `AbortError`:
+     *
+     * ```js
+     * const { exec } = require('child_process');
+     * const controller = new AbortController();
+     * const { signal } = controller;
+     * const child = exec('grep ssh', { signal }, (error) => {
+     *   console.log(error); // an AbortError
+     * });
+     * controller.abort();
+     * ```
+     * @since v0.1.90
+     * @param command The command to run, with space-separated arguments.
+     * @param callback called with the output when process terminates.
+     */
+    function exec(command: string, callback?: (error: ExecException | null, stdout: string, stderr: string) => void): ChildProcess;
+    // `options` with `""buffer""` or `null` for `encoding` means stdout/stderr are definitely `Buffer`.
+    function exec(
+        command: string,
+        options: {
+            encoding: 'buffer' | null;
+        } & ExecOptions,
+        callback?: (error: ExecException | null, stdout: Buffer, stderr: Buffer) => void
+    ): ChildProcess;
+    // `options` with well known `encoding` means stdout/stderr are definitely `string`.
+    function exec(
+        command: string,
+        options: {
+            encoding: BufferEncoding;
+        } & ExecOptions,
+        callback?: (error: ExecException | null, stdout: string, stderr: string) => void
+    ): ChildProcess;
+    // `options` with an `encoding` whose type is `string` means stdout/stderr could either be `Buffer` or `string`.
+    // There is no guarantee the `encoding` is unknown as `string` is a superset of `BufferEncoding`.
+    function exec(
+        command: string,
+        options: {
+            encoding: BufferEncoding;
+        } & ExecOptions,
+        callback?: (error: ExecException | null, stdout: string | Buffer, stderr: string | Buffer) => void
+    ): ChildProcess;
+    // `options` without an `encoding` means stdout/stderr are definitely `string`.
+    function exec(command: string, options: ExecOptions, callback?: (error: ExecException | null, stdout: string, stderr: string) => void): ChildProcess;
+    // fallback if nothing else matches. Worst case is always `string | Buffer`.
+    function exec(
+        command: string,
+        options: (ObjectEncodingOptions & ExecOptions) | undefined | null,
+        callback?: (error: ExecException | null, stdout: string | Buffer, stderr: string | Buffer) => void
+    ): ChildProcess;
+    interface PromiseWithChild<T> extends Promise<T> {
+        child: ChildProcess;
+    }
+    namespace exec {
+        function __promisify__(command: string): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            command: string,
+            options: {
+                encoding: 'buffer' | null;
+            } & ExecOptions
+        ): PromiseWithChild<{
+            stdout: Buffer;
+            stderr: Buffer;
+        }>;
+        function __promisify__(
+            command: string,
+            options: {
+                encoding: BufferEncoding;
+            } & ExecOptions
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            command: string,
+            options: ExecOptions
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            command: string,
+            options?: (ObjectEncodingOptions & ExecOptions) | null
+        ): PromiseWithChild<{
+            stdout: string | Buffer;
+            stderr: string | Buffer;
+        }>;
+    }
+    interface ExecFileOptions extends CommonOptions, Abortable {
+        maxBuffer?: number | undefined;
+        killSignal?: NodeJS.Signals | number | undefined;
+        windowsVerbatimArguments?: boolean | undefined;
+        shell?: boolean | string | undefined;
+        signal?: AbortSignal | undefined;
+    }
+    interface ExecFileOptionsWithStringEncoding extends ExecFileOptions {
+        encoding: BufferEncoding;
+    }
+    interface ExecFileOptionsWithBufferEncoding extends ExecFileOptions {
+        encoding: 'buffer' | null;
+    }
+    interface ExecFileOptionsWithOtherEncoding extends ExecFileOptions {
+        encoding: BufferEncoding;
+    }
+    type ExecFileException = ExecException & NodeJS.ErrnoException;
+    /**
+     * The `child_process.execFile()` function is similar to {@link exec} except that it does not spawn a shell by default. Rather, the specified
+     * executable `file` is spawned directly as a new process making it slightly more
+     * efficient than {@link exec}.
+     *
+     * The same options as {@link exec} are supported. Since a shell is
+     * not spawned, behaviors such as I/O redirection and file globbing are not
+     * supported.
+     *
+     * ```js
+     * const { execFile } = require('child_process');
+     * const child = execFile('node', ['--version'], (error, stdout, stderr) => {
+     *   if (error) {
+     *     throw error;
+     *   }
+     *   console.log(stdout);
+     * });
+     * ```
+     *
+     * The `stdout` and `stderr` arguments passed to the callback will contain the
+     * stdout and stderr output of the child process. By default, Node.js will decode
+     * the output as UTF-8 and pass strings to the callback. The `encoding` option
+     * can be used to specify the character encoding used to decode the stdout and
+     * stderr output. If `encoding` is `'buffer'`, or an unrecognized character
+     * encoding, `Buffer` objects will be passed to the callback instead.
+     *
+     * If this method is invoked as its `util.promisify()` ed version, it returns
+     * a `Promise` for an `Object` with `stdout` and `stderr` properties. The returned`ChildProcess` instance is attached to the `Promise` as a `child` property. In
+     * case of an error (including any error resulting in an exit code other than 0), a
+     * rejected promise is returned, with the same `error` object given in the
+     * callback, but with two additional properties `stdout` and `stderr`.
+     *
+     * ```js
+     * const util = require('util');
+     * const execFile = util.promisify(require('child_process').execFile);
+     * async function getVersion() {
+     *   const { stdout } = await execFile('node', ['--version']);
+     *   console.log(stdout);
+     * }
+     * getVersion();
+     * ```
+     *
+     * **If the `shell` option is enabled, do not pass unsanitized user input to this**
+     * **function. Any input containing shell metacharacters may be used to trigger**
+     * **arbitrary command execution.**
+     *
+     * If the `signal` option is enabled, calling `.abort()` on the corresponding`AbortController` is similar to calling `.kill()` on the child process except
+     * the error passed to the callback will be an `AbortError`:
+     *
+     * ```js
+     * const { execFile } = require('child_process');
+     * const controller = new AbortController();
+     * const { signal } = controller;
+     * const child = execFile('node', ['--version'], { signal }, (error) => {
+     *   console.log(error); // an AbortError
+     * });
+     * controller.abort();
+     * ```
+     * @since v0.1.91
+     * @param file The name or path of the executable file to run.
+     * @param args List of string arguments.
+     * @param callback Called with the output when process terminates.
+     */
+    function execFile(file: string): ChildProcess;
+    function execFile(file: string, options: (ObjectEncodingOptions & ExecFileOptions) | undefined | null): ChildProcess;
+    function execFile(file: string, args?: ReadonlyArray<string> | null): ChildProcess;
+    function execFile(file: string, args: ReadonlyArray<string> | undefined | null, options: (ObjectEncodingOptions & ExecFileOptions) | undefined | null): ChildProcess;
+    // no `options` definitely means stdout/stderr are `string`.
+    function execFile(file: string, callback: (error: ExecFileException | null, stdout: string, stderr: string) => void): ChildProcess;
+    function execFile(file: string, args: ReadonlyArray<string> | undefined | null, callback: (error: ExecFileException | null, stdout: string, stderr: string) => void): ChildProcess;
+    // `options` with `""buffer""` or `null` for `encoding` means stdout/stderr are definitely `Buffer`.
+    function execFile(file: string, options: ExecFileOptionsWithBufferEncoding, callback: (error: ExecFileException | null, stdout: Buffer, stderr: Buffer) => void): ChildProcess;
+    function execFile(
+        file: string,
+        args: ReadonlyArray<string> | undefined | null,
+        options: ExecFileOptionsWithBufferEncoding,
+        callback: (error: ExecFileException | null, stdout: Buffer, stderr: Buffer) => void
+    ): ChildProcess;
+    // `options` with well known `encoding` means stdout/stderr are definitely `string`.
+    function execFile(file: string, options: ExecFileOptionsWithStringEncoding, callback: (error: ExecFileException | null, stdout: string, stderr: string) => void): ChildProcess;
+    function execFile(
+        file: string,
+        args: ReadonlyArray<string> | undefined | null,
+        options: ExecFileOptionsWithStringEncoding,
+        callback: (error: ExecFileException | null, stdout: string, stderr: string) => void
+    ): ChildProcess;
+    // `options` with an `encoding` whose type is `string` means stdout/stderr could either be `Buffer` or `string`.
+    // There is no guarantee the `encoding` is unknown as `string` is a superset of `BufferEncoding`.
+    function execFile(file: string, options: ExecFileOptionsWithOtherEncoding, callback: (error: ExecFileException | null, stdout: string | Buffer, stderr: string | Buffer) => void): ChildProcess;
+    function execFile(
+        file: string,
+        args: ReadonlyArray<string> | undefined | null,
+        options: ExecFileOptionsWithOtherEncoding,
+        callback: (error: ExecFileException | null, stdout: string | Buffer, stderr: string | Buffer) => void
+    ): ChildProcess;
+    // `options` without an `encoding` means stdout/stderr are definitely `string`.
+    function execFile(file: string, options: ExecFileOptions, callback: (error: ExecFileException | null, stdout: string, stderr: string) => void): ChildProcess;
+    function execFile(
+        file: string,
+        args: ReadonlyArray<string> | undefined | null,
+        options: ExecFileOptions,
+        callback: (error: ExecFileException | null, stdout: string, stderr: string) => void
+    ): ChildProcess;
+    // fallback if nothing else matches. Worst case is always `string | Buffer`.
+    function execFile(
+        file: string,
+        options: (ObjectEncodingOptions & ExecFileOptions) | undefined | null,
+        callback: ((error: ExecFileException | null, stdout: string | Buffer, stderr: string | Buffer) => void) | undefined | null
+    ): ChildProcess;
+    function execFile(
+        file: string,
+        args: ReadonlyArray<string> | undefined | null,
+        options: (ObjectEncodingOptions & ExecFileOptions) | undefined | null,
+        callback: ((error: ExecFileException | null, stdout: string | Buffer, stderr: string | Buffer) => void) | undefined | null
+    ): ChildProcess;
+    namespace execFile {
+        function __promisify__(file: string): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            file: string,
+            args: ReadonlyArray<string> | undefined | null
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            file: string,
+            options: ExecFileOptionsWithBufferEncoding
+        ): PromiseWithChild<{
+            stdout: Buffer;
+            stderr: Buffer;
+        }>;
+        function __promisify__(
+            file: string,
+            args: ReadonlyArray<string> | undefined | null,
+            options: ExecFileOptionsWithBufferEncoding
+        ): PromiseWithChild<{
+            stdout: Buffer;
+            stderr: Buffer;
+        }>;
+        function __promisify__(
+            file: string,
+            options: ExecFileOptionsWithStringEncoding
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            file: string,
+            args: ReadonlyArray<string> | undefined | null,
+            options: ExecFileOptionsWithStringEncoding
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            file: string,
+            options: ExecFileOptionsWithOtherEncoding
+        ): PromiseWithChild<{
+            stdout: string | Buffer;
+            stderr: string | Buffer;
+        }>;
+        function __promisify__(
+            file: string,
+            args: ReadonlyArray<string> | undefined | null,
+            options: ExecFileOptionsWithOtherEncoding
+        ): PromiseWithChild<{
+            stdout: string | Buffer;
+            stderr: string | Buffer;
+        }>;
+        function __promisify__(
+            file: string,
+            options: ExecFileOptions
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            file: string,
+            args: ReadonlyArray<string> | undefined | null,
+            options: ExecFileOptions
+        ): PromiseWithChild<{
+            stdout: string;
+            stderr: string;
+        }>;
+        function __promisify__(
+            file: string,
+            options: (ObjectEncodingOptions & ExecFileOptions) | undefined | null
+        ): PromiseWithChild<{
+            stdout: string | Buffer;
+            stderr: string | Buffer;
+        }>;
+        function __promisify__(
+            file: string,
+            args: ReadonlyArray<string> | undefined | null,
+            options: (ObjectEncodingOptions & ExecFileOptions) | undefined | null
+        ): PromiseWithChild<{
+            stdout: string | Buffer;
+            stderr: string | Buffer;
+        }>;
+    }
+    interface ForkOptions extends ProcessEnvOptions, MessagingOptions, Abortable {
+        execPath?: string | undefined;
+        execArgv?: string[] | undefined;
+        silent?: boolean | undefined;
+        stdio?: StdioOptions | undefined;
+        detached?: boolean | undefined;
+        windowsVerbatimArguments?: boolean | undefined;
+    }
+    /**
+     * The `child_process.fork()` method is a special case of {@link spawn} used specifically to spawn new Node.js processes.
+     * Like {@link spawn}, a `ChildProcess` object is returned. The
+     * returned `ChildProcess` will have an additional communication channel
+     * built-in that allows messages to be passed back and forth between the parent and
+     * child. See `subprocess.send()` for details.
+     *
+     * Keep in mind that spawned Node.js child processes are
+     * independent of the parent with exception of the IPC communication channel
+     * that is established between the two. Each process has its own memory, with
+     * their own V8 instances. Because of the additional resource allocations
+     * required, spawning a large number of child Node.js processes is not
+     * recommended.
+     *
+     * By default, `child_process.fork()` will spawn new Node.js instances using the `process.execPath` of the parent process. The `execPath` property in the`options` object allows for an alternative
+     * execution path to be used.
+     *
+     * Node.js processes launched with a custom `execPath` will communicate with the
+     * parent process using the file descriptor (fd) identified using the
+     * environment variable `NODE_CHANNEL_FD` on the child process.
+     *
+     * Unlike the [`fork(2)`](http://man7.org/linux/man-pages/man2/fork.2.html) POSIX system call, `child_process.fork()` does not clone the
+     * current process.
+     *
+     * The `shell` option available in {@link spawn} is not supported by`child_process.fork()` and will be ignored if set.
+     *
+     * If the `signal` option is enabled, calling `.abort()` on the corresponding`AbortController` is similar to calling `.kill()` on the child process except
+     * the error passed to the callback will be an `AbortError`:
+     *
+     * ```js
+     * if (process.argv[2] === 'child') {
+     *   setTimeout(() => {
+     *     console.log(`Hello from ${process.argv[2]}!`);
+     *   }, 1_000);
+     * } else {
+     *   const { fork } = require('child_process');
+     *   const controller = new AbortController();
+     *   const { signal } = controller;
+     *   const child = fork(__filename, ['child'], { signal });
+     *   child.on('error', (err) => {
+     *     // This will be called with err being an AbortError if the controller aborts
+     *   });
+     *   controller.abort(); // Stops the child process
+     * }
+     * ```
+     * @since v0.5.0
+     * @param modulePath The module to run in the child.
+     * @param args List of string arguments.
+     */
+    function fork(modulePath: string, options?: ForkOptions): ChildProcess;
+    function fork(modulePath: string, args?: ReadonlyArray<string>, options?: ForkOptions): ChildProcess;
+    interface SpawnSyncOptions extends CommonSpawnOptions {
+        input?: string | NodeJS.ArrayBufferView | undefined;
+        maxBuffer?: number | undefined;
+        encoding?: BufferEncoding | 'buffer' | null | undefined;
+    }
+    interface SpawnSyncOptionsWithStringEncoding extends SpawnSyncOptions {
+        encoding: BufferEncoding;
+    }
+    interface SpawnSyncOptionsWithBufferEncoding extends SpawnSyncOptions {
+        encoding?: 'buffer' | null | undefined;
+    }
+    interface SpawnSyncReturns<T> {
+        pid: number;
+        output: Array<T | null>;
+        stdout: T;
+        stderr: T;
+        status: number | null;
+        signal: NodeJS.Signals | null;
+        error?: Error | undefined;
+    }
+    /**
+     * The `child_process.spawnSync()` method is generally identical to {@link spawn} with the exception that the function will not return
+     * until the child process has fully closed. When a timeout has been encountered
+     * and `killSignal` is sent, the method won't return until the process has
+     * completely exited. If the process intercepts and handles the `SIGTERM` signal
+     * and doesn't exit, the parent process will wait until the child process has
+     * exited.
+     *
+     * **If the `shell` option is enabled, do not pass unsanitized user input to this**
+     * **function. Any input containing shell metacharacters may be used to trigger**
+     * **arbitrary command execution.**
+     * @since v0.11.12
+     * @param command The command to run.
+     * @param args List of string arguments.
+     */
+    function spawnSync(command: string): SpawnSyncReturns<Buffer>;
+    function spawnSync(command: string, options: SpawnSyncOptionsWithStringEncoding): SpawnSyncReturns<string>;
+    function spawnSync(command: string, options: SpawnSyncOptionsWithBufferEncoding): SpawnSyncReturns<Buffer>;
+    function spawnSync(command: string, options?: SpawnSyncOptions): SpawnSyncReturns<string | Buffer>;
+    function spawnSync(command: string, args: ReadonlyArray<string>): SpawnSyncReturns<Buffer>;
+    function spawnSync(command: string, args: ReadonlyArray<string>, options: SpawnSyncOptionsWithStringEncoding): SpawnSyncReturns<string>;
+    function spawnSync(command: string, args: ReadonlyArray<string>, options: SpawnSyncOptionsWithBufferEncoding): SpawnSyncReturns<Buffer>;
+    function spawnSync(command: string, args?: ReadonlyArray<string>, options?: SpawnSyncOptions): SpawnSyncReturns<string | Buffer>;
+    interface CommonExecOptions extends CommonOptions {
+        input?: string | NodeJS.ArrayBufferView | undefined;
+        stdio?: StdioOptions | undefined;
+        killSignal?: NodeJS.Signals | number | undefined;
+        maxBuffer?: number | undefined;
+        encoding?: BufferEncoding | 'buffer' | null | undefined;
+    }
+    interface ExecSyncOptions extends CommonExecOptions {
+        shell?: string | undefined;
+    }
+    interface ExecSyncOptionsWithStringEncoding extends ExecSyncOptions {
+        encoding: BufferEncoding;
+    }
+    interface ExecSyncOptionsWithBufferEncoding extends ExecSyncOptions {
+        encoding?: 'buffer' | null | undefined;
+    }
+    /**
+     * The `child_process.execSync()` method is generally identical to {@link exec} with the exception that the method will not return
+     * until the child process has fully closed. When a timeout has been encountered
+     * and `killSignal` is sent, the method won't return until the process has
+     * completely exited. If the child process intercepts and handles the `SIGTERM`signal and doesn't exit, the parent process will wait until the child process
+     * has exited.
+     *
+     * If the process times out or has a non-zero exit code, this method will throw.
+     * The `Error` object will contain the entire result from {@link spawnSync}.
+     *
+     * **Never pass unsanitized user input to this function. Any input containing shell**
+     * **metacharacters may be used to trigger arbitrary command execution.**
+     * @since v0.11.12
+     * @param command The command to run.
+     * @return The stdout from the command.
+     */
+    function execSync(command: string): Buffer;
+    function execSync(command: string, options: ExecSyncOptionsWithStringEncoding): string;
+    function execSync(command: string, options: ExecSyncOptionsWithBufferEncoding): Buffer;
+    function execSync(command: string, options?: ExecSyncOptions): string | Buffer;
+    interface ExecFileSyncOptions extends CommonExecOptions {
+        shell?: boolean | string | undefined;
+    }
+    interface ExecFileSyncOptionsWithStringEncoding extends ExecFileSyncOptions {
+        encoding: BufferEncoding;
+    }
+    interface ExecFileSyncOptionsWithBufferEncoding extends ExecFileSyncOptions {
+        encoding?: 'buffer' | null; // specify `null`.
+    }
+    /**
+     * The `child_process.execFileSync()` method is generally identical to {@link execFile} with the exception that the method will not
+     * return until the child process has fully closed. When a timeout has been
+     * encountered and `killSignal` is sent, the method won't return until the process
+     * has completely exited.
+     *
+     * If the child process intercepts and handles the `SIGTERM` signal and
+     * does not exit, the parent process will still wait until the child process has
+     * exited.
+     *
+     * If the process times out or has a non-zero exit code, this method will throw an `Error` that will include the full result of the underlying {@link spawnSync}.
+     *
+     * **If the `shell` option is enabled, do not pass unsanitized user input to this**
+     * **function. Any input containing shell metacharacters may be used to trigger**
+     * **arbitrary command execution.**
+     * @since v0.11.12
+     * @param file The name or path of the executable file to run.
+     * @param args List of string arguments.
+     * @return The stdout from the command.
+     */
+    function execFileSync(file: string): Buffer;
+    function execFileSync(file: string, options: ExecFileSyncOptionsWithStringEncoding): string;
+    function execFileSync(file: string, options: ExecFileSyncOptionsWithBufferEncoding): Buffer;
+    function execFileSync(file: string, options?: ExecFileSyncOptions): string | Buffer;
+    function execFileSync(file: string, args: ReadonlyArray<string>): Buffer;
+    function execFileSync(file: string, args: ReadonlyArray<string>, options: ExecFileSyncOptionsWithStringEncoding): string;
+    function execFileSync(file: string, args: ReadonlyArray<string>, options: ExecFileSyncOptionsWithBufferEncoding): Buffer;
+    function execFileSync(file: string, args?: ReadonlyArray<string>, options?: ExecFileSyncOptions): string | Buffer;
+}
+declare module 'node:child_process' {
+    export * from 'child_process';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * A single instance of Node.js runs in a single thread. To take advantage of
+ * multi-core systems, the user will sometimes want to launch a cluster of Node.js
+ * processes to handle the load.
+ *
+ * The cluster module allows easy creation of child processes that all share
+ * server ports.
+ *
+ * ```js
+ * import cluster from 'cluster';
+ * import http from 'http';
+ * import { cpus } from 'os';
+ * import process from 'process';
+ *
+ * const numCPUs = cpus().length;
+ *
+ * if (cluster.isPrimary) {
+ *   console.log(`Primary ${process.pid} is running`);
+ *
+ *   // Fork workers.
+ *   for (let i = 0; i < numCPUs; i++) {
+ *     cluster.fork();
+ *   }
+ *
+ *   cluster.on('exit', (worker, code, signal) => {
+ *     console.log(`worker ${worker.process.pid} died`);
+ *   });
+ * } else {
+ *   // Workers can share any TCP connection
+ *   // In this case it is an HTTP server
+ *   http.createServer((req, res) => {
+ *     res.writeHead(200);
+ *     res.end('hello world\n');
+ *   }).listen(8000);
+ *
+ *   console.log(`Worker ${process.pid} started`);
+ * }
+ * ```
+ *
+ * Running Node.js will now share port 8000 between the workers:
+ *
+ * ```console
+ * $ node server.js
+ * Primary 3596 is running
+ * Worker 4324 started
+ * Worker 4520 started
+ * Worker 6056 started
+ * Worker 5644 started
+ * ```
+ *
+ * On Windows, it is not yet possible to set up a named pipe server in a worker.
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/cluster.js)
+ */
+declare module 'cluster' {
+    import * as child from 'node:child_process';
+    import EventEmitter = require('node:events');
+    import * as net from 'node:net';
+    export interface ClusterSettings {
+        execArgv?: string[] | undefined; // default: process.execArgv
+        exec?: string | undefined;
+        args?: string[] | undefined;
+        silent?: boolean | undefined;
+        stdio?: any[] | undefined;
+        uid?: number | undefined;
+        gid?: number | undefined;
+        inspectPort?: number | (() => number) | undefined;
+    }
+    export interface Address {
+        address: string;
+        port: number;
+        addressType: number | 'udp4' | 'udp6'; // 4, 6, -1, ""udp4"", ""udp6""
+    }
+    /**
+     * A `Worker` object contains all public information and method about a worker.
+     * In the primary it can be obtained using `cluster.workers`. In a worker
+     * it can be obtained using `cluster.worker`.
+     * @since v0.7.0
+     */
+    export class Worker extends EventEmitter {
+        /**
+         * Each new worker is given its own unique id, this id is stored in the`id`.
+         *
+         * While a worker is alive, this is the key that indexes it in`cluster.workers`.
+         * @since v0.8.0
+         */
+        id: number;
+        /**
+         * All workers are created using `child_process.fork()`, the returned object
+         * from this function is stored as `.process`. In a worker, the global `process`is stored.
+         *
+         * See: `Child Process module`.
+         *
+         * Workers will call `process.exit(0)` if the `'disconnect'` event occurs
+         * on `process` and `.exitedAfterDisconnect` is not `true`. This protects against
+         * accidental disconnection.
+         * @since v0.7.0
+         */
+        process: child.ChildProcess;
+        /**
+         * Send a message to a worker or primary, optionally with a handle.
+         *
+         * In the primary this sends a message to a specific worker. It is identical to `ChildProcess.send()`.
+         *
+         * In a worker this sends a message to the primary. It is identical to`process.send()`.
+         *
+         * This example will echo back all messages from the primary:
+         *
+         * ```js
+         * if (cluster.isPrimary) {
+         *   const worker = cluster.fork();
+         *   worker.send('hi there');
+         *
+         * } else if (cluster.isWorker) {
+         *   process.on('message', (msg) => {
+         *     process.send(msg);
+         *   });
+         * }
+         * ```
+         * @since v0.7.0
+         * @param options The `options` argument, if present, is an object used to parameterize the sending of certain types of handles. `options` supports the following properties:
+         */
+        send(message: child.Serializable, callback?: (error: Error | null) => void): boolean;
+        send(message: child.Serializable, sendHandle: child.SendHandle, callback?: (error: Error | null) => void): boolean;
+        send(message: child.Serializable, sendHandle: child.SendHandle, options?: child.MessageOptions, callback?: (error: Error | null) => void): boolean;
+        /**
+         * This function will kill the worker. In the primary, it does this
+         * by disconnecting the `worker.process`, and once disconnected, killing
+         * with `signal`. In the worker, it does it by disconnecting the channel,
+         * and then exiting with code `0`.
+         *
+         * Because `kill()` attempts to gracefully disconnect the worker process, it is
+         * susceptible to waiting indefinitely for the disconnect to complete. For example,
+         * if the worker enters an infinite loop, a graceful disconnect will never occur.
+         * If the graceful disconnect behavior is not needed, use `worker.process.kill()`.
+         *
+         * Causes `.exitedAfterDisconnect` to be set.
+         *
+         * This method is aliased as `worker.destroy()` for backward compatibility.
+         *
+         * In a worker, `process.kill()` exists, but it is not this function;
+         * it is `kill()`.
+         * @since v0.9.12
+         * @param [signal='SIGTERM'] Name of the kill signal to send to the worker process.
+         */
+        kill(signal?: string): void;
+        destroy(signal?: string): void;
+        /**
+         * In a worker, this function will close all servers, wait for the `'close'` event
+         * on those servers, and then disconnect the IPC channel.
+         *
+         * In the primary, an internal message is sent to the worker causing it to call`.disconnect()` on itself.
+         *
+         * Causes `.exitedAfterDisconnect` to be set.
+         *
+         * After a server is closed, it will no longer accept new connections,
+         * but connections may be accepted by any other listening worker. Existing
+         * connections will be allowed to close as usual. When no more connections exist,
+         * see `server.close()`, the IPC channel to the worker will close allowing it
+         * to die gracefully.
+         *
+         * The above applies _only_ to server connections, client connections are not
+         * automatically closed by workers, and disconnect does not wait for them to close
+         * before exiting.
+         *
+         * In a worker, `process.disconnect` exists, but it is not this function;
+         * it is `disconnect()`.
+         *
+         * Because long living server connections may block workers from disconnecting, it
+         * may be useful to send a message, so application specific actions may be taken to
+         * close them. It also may be useful to implement a timeout, killing a worker if
+         * the `'disconnect'` event has not been emitted after some time.
+         *
+         * ```js
+         * if (cluster.isPrimary) {
+         *   const worker = cluster.fork();
+         *   let timeout;
+         *
+         *   worker.on('listening', (address) => {
+         *     worker.send('shutdown');
+         *     worker.disconnect();
+         *     timeout = setTimeout(() => {
+         *       worker.kill();
+         *     }, 2000);
+         *   });
+         *
+         *   worker.on('disconnect', () => {
+         *     clearTimeout(timeout);
+         *   });
+         *
+         * } else if (cluster.isWorker) {
+         *   const net = require('net');
+         *   const server = net.createServer((socket) => {
+         *     // Connections never end
+         *   });
+         *
+         *   server.listen(8000);
+         *
+         *   process.on('message', (msg) => {
+         *     if (msg === 'shutdown') {
+         *       // Initiate graceful close of any connections to server
+         *     }
+         *   });
+         * }
+         * ```
+         * @since v0.7.7
+         * @return A reference to `worker`.
+         */
+        disconnect(): void;
+        /**
+         * This function returns `true` if the worker is connected to its primary via its
+         * IPC channel, `false` otherwise. A worker is connected to its primary after it
+         * has been created. It is disconnected after the `'disconnect'` event is emitted.
+         * @since v0.11.14
+         */
+        isConnected(): boolean;
+        /**
+         * This function returns `true` if the worker's process has terminated (either
+         * because of exiting or being signaled). Otherwise, it returns `false`.
+         *
+         * ```js
+         * import cluster from 'cluster';
+         * import http from 'http';
+         * import { cpus } from 'os';
+         * import process from 'process';
+         *
+         * const numCPUs = cpus().length;
+         *
+         * if (cluster.isPrimary) {
+         *   console.log(`Primary ${process.pid} is running`);
+         *
+         *   // Fork workers.
+         *   for (let i = 0; i < numCPUs; i++) {
+         *     cluster.fork();
+         *   }
+         *
+         *   cluster.on('fork', (worker) => {
+         *     console.log('worker is dead:', worker.isDead());
+         *   });
+         *
+         *   cluster.on('exit', (worker, code, signal) => {
+         *     console.log('worker is dead:', worker.isDead());
+         *   });
+         * } else {
+         *   // Workers can share any TCP connection. In this case, it is an HTTP server.
+         *   http.createServer((req, res) => {
+         *     res.writeHead(200);
+         *     res.end(`Current process\n ${process.pid}`);
+         *     process.kill(process.pid);
+         *   }).listen(8000);
+         * }
+         * ```
+         * @since v0.11.14
+         */
+        isDead(): boolean;
+        /**
+         * This property is `true` if the worker exited due to `.kill()` or`.disconnect()`. If the worker exited any other way, it is `false`. If the
+         * worker has not exited, it is `undefined`.
+         *
+         * The boolean `worker.exitedAfterDisconnect` allows distinguishing between
+         * voluntary and accidental exit, the primary may choose not to respawn a worker
+         * based on this value.
+         *
+         * ```js
+         * cluster.on('exit', (worker, code, signal) => {
+         *   if (worker.exitedAfterDisconnect === true) {
+         *     console.log('Oh, it was just voluntary – no need to worry');
+         *   }
+         * });
+         *
+         * // kill worker
+         * worker.kill();
+         * ```
+         * @since v6.0.0
+         */
+        exitedAfterDisconnect: boolean;
+        /**
+         * events.EventEmitter
+         *   1. disconnect
+         *   2. error
+         *   3. exit
+         *   4. listening
+         *   5. message
+         *   6. online
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'disconnect', listener: () => void): this;
+        addListener(event: 'error', listener: (error: Error) => void): this;
+        addListener(event: 'exit', listener: (code: number, signal: string) => void): this;
+        addListener(event: 'listening', listener: (address: Address) => void): this;
+        addListener(event: 'message', listener: (message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        addListener(event: 'online', listener: () => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'disconnect'): boolean;
+        emit(event: 'error', error: Error): boolean;
+        emit(event: 'exit', code: number, signal: string): boolean;
+        emit(event: 'listening', address: Address): boolean;
+        emit(event: 'message', message: any, handle: net.Socket | net.Server): boolean;
+        emit(event: 'online'): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'disconnect', listener: () => void): this;
+        on(event: 'error', listener: (error: Error) => void): this;
+        on(event: 'exit', listener: (code: number, signal: string) => void): this;
+        on(event: 'listening', listener: (address: Address) => void): this;
+        on(event: 'message', listener: (message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        on(event: 'online', listener: () => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'disconnect', listener: () => void): this;
+        once(event: 'error', listener: (error: Error) => void): this;
+        once(event: 'exit', listener: (code: number, signal: string) => void): this;
+        once(event: 'listening', listener: (address: Address) => void): this;
+        once(event: 'message', listener: (message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        once(event: 'online', listener: () => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'disconnect', listener: () => void): this;
+        prependListener(event: 'error', listener: (error: Error) => void): this;
+        prependListener(event: 'exit', listener: (code: number, signal: string) => void): this;
+        prependListener(event: 'listening', listener: (address: Address) => void): this;
+        prependListener(event: 'message', listener: (message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        prependListener(event: 'online', listener: () => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'disconnect', listener: () => void): this;
+        prependOnceListener(event: 'error', listener: (error: Error) => void): this;
+        prependOnceListener(event: 'exit', listener: (code: number, signal: string) => void): this;
+        prependOnceListener(event: 'listening', listener: (address: Address) => void): this;
+        prependOnceListener(event: 'message', listener: (message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        prependOnceListener(event: 'online', listener: () => void): this;
+    }
+    export interface Cluster extends EventEmitter {
+        disconnect(callback?: () => void): void;
+        fork(env?: any): Worker;
+        /** @deprecated since v16.0.0 - use isPrimary. */
+        readonly isMaster: boolean;
+        readonly isPrimary: boolean;
+        readonly isWorker: boolean;
+        schedulingPolicy: number;
+        readonly settings: ClusterSettings;
+        /** @deprecated since v16.0.0 - use setupPrimary. */
+        setupMaster(settings?: ClusterSettings): void;
+        /**
+         * `setupPrimary` is used to change the default 'fork' behavior. Once called, the settings will be present in cluster.settings.
+         */
+        setupPrimary(settings?: ClusterSettings): void;
+        readonly worker?: Worker | undefined;
+        readonly workers?: NodeJS.Dict<Worker> | undefined;
+        readonly SCHED_NONE: number;
+        readonly SCHED_RR: number;
+        /**
+         * events.EventEmitter
+         *   1. disconnect
+         *   2. exit
+         *   3. fork
+         *   4. listening
+         *   5. message
+         *   6. online
+         *   7. setup
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'disconnect', listener: (worker: Worker) => void): this;
+        addListener(event: 'exit', listener: (worker: Worker, code: number, signal: string) => void): this;
+        addListener(event: 'fork', listener: (worker: Worker) => void): this;
+        addListener(event: 'listening', listener: (worker: Worker, address: Address) => void): this;
+        addListener(event: 'message', listener: (worker: Worker, message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        addListener(event: 'online', listener: (worker: Worker) => void): this;
+        addListener(event: 'setup', listener: (settings: ClusterSettings) => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'disconnect', worker: Worker): boolean;
+        emit(event: 'exit', worker: Worker, code: number, signal: string): boolean;
+        emit(event: 'fork', worker: Worker): boolean;
+        emit(event: 'listening', worker: Worker, address: Address): boolean;
+        emit(event: 'message', worker: Worker, message: any, handle: net.Socket | net.Server): boolean;
+        emit(event: 'online', worker: Worker): boolean;
+        emit(event: 'setup', settings: ClusterSettings): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'disconnect', listener: (worker: Worker) => void): this;
+        on(event: 'exit', listener: (worker: Worker, code: number, signal: string) => void): this;
+        on(event: 'fork', listener: (worker: Worker) => void): this;
+        on(event: 'listening', listener: (worker: Worker, address: Address) => void): this;
+        on(event: 'message', listener: (worker: Worker, message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        on(event: 'online', listener: (worker: Worker) => void): this;
+        on(event: 'setup', listener: (settings: ClusterSettings) => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'disconnect', listener: (worker: Worker) => void): this;
+        once(event: 'exit', listener: (worker: Worker, code: number, signal: string) => void): this;
+        once(event: 'fork', listener: (worker: Worker) => void): this;
+        once(event: 'listening', listener: (worker: Worker, address: Address) => void): this;
+        once(event: 'message', listener: (worker: Worker, message: any, handle: net.Socket | net.Server) => void): this; // the handle is a net.Socket or net.Server object, or undefined.
+        once(event: 'online', listener: (worker: Worker) => void): this;
+        once(event: 'setup', listener: (settings: ClusterSettings) => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'disconnect', listener: (worker: Worker) => void): this;
+        prependListener(event: 'exit', listener: (worker: Worker, code: number, signal: string) => void): this;
+        prependListener(event: 'fork', listener: (worker: Worker) => void): this;
+        prependListener(event: 'listening', listener: (worker: Worker, address: Address) => void): this;
+        // the handle is a net.Socket or net.Server object, or undefined.
+        prependListener(event: 'message', listener: (worker: Worker, message: any, handle?: net.Socket | net.Server) => void): this;
+        prependListener(event: 'online', listener: (worker: Worker) => void): this;
+        prependListener(event: 'setup', listener: (settings: ClusterSettings) => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'disconnect', listener: (worker: Worker) => void): this;
+        prependOnceListener(event: 'exit', listener: (worker: Worker, code: number, signal: string) => void): this;
+        prependOnceListener(event: 'fork', listener: (worker: Worker) => void): this;
+        prependOnceListener(event: 'listening', listener: (worker: Worker, address: Address) => void): this;
+        // the handle is a net.Socket or net.Server object, or undefined.
+        prependOnceListener(event: 'message', listener: (worker: Worker, message: any, handle: net.Socket | net.Server) => void): this;
+        prependOnceListener(event: 'online', listener: (worker: Worker) => void): this;
+        prependOnceListener(event: 'setup', listener: (settings: ClusterSettings) => void): this;
+    }
+    const cluster: Cluster;
+    export default cluster;
+}
+declare module 'node:cluster' {
+    export * from 'cluster';
+    export { default as default } from 'cluster';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `console` module provides a simple debugging console that is similar to the
+ * JavaScript console mechanism provided by web browsers.
+ *
+ * The module exports two specific components:
+ *
+ * * A `Console` class with methods such as `console.log()`, `console.error()` and`console.warn()` that can be used to write to any Node.js stream.
+ * * A global `console` instance configured to write to `process.stdout` and `process.stderr`. The global `console` can be used without calling`require('console')`.
+ *
+ * _**Warning**_: The global console object's methods are neither consistently
+ * synchronous like the browser APIs they resemble, nor are they consistently
+ * asynchronous like all other Node.js streams. See the `note on process I/O` for
+ * more information.
+ *
+ * Example using the global `console`:
+ *
+ * ```js
+ * console.log('hello world');
+ * // Prints: hello world, to stdout
+ * console.log('hello %s', 'world');
+ * // Prints: hello world, to stdout
+ * console.error(new Error('Whoops, something bad happened'));
+ * // Prints error message and stack trace to stderr:
+ * //   Error: Whoops, something bad happened
+ * //     at [eval]:5:15
+ * //     at Script.runInThisContext (node:vm:132:18)
+ * //     at Object.runInThisContext (node:vm:309:38)
+ * //     at node:internal/process/execution:77:19
+ * //     at [eval]-wrapper:6:22
+ * //     at evalScript (node:internal/process/execution:76:60)
+ * //     at node:internal/main/eval_string:23:3
+ *
+ * const name = 'Will Robinson';
+ * console.warn(`Danger ${name}! Danger!`);
+ * // Prints: Danger Will Robinson! Danger!, to stderr
+ * ```
+ *
+ * Example using the `Console` class:
+ *
+ * ```js
+ * const out = getStreamSomehow();
+ * const err = getStreamSomehow();
+ * const myConsole = new console.Console(out, err);
+ *
+ * myConsole.log('hello world');
+ * // Prints: hello world, to out
+ * myConsole.log('hello %s', 'world');
+ * // Prints: hello world, to out
+ * myConsole.error(new Error('Whoops, something bad happened'));
+ * // Prints: [Error: Whoops, something bad happened], to err
+ *
+ * const name = 'Will Robinson';
+ * myConsole.warn(`Danger ${name}! Danger!`);
+ * // Prints: Danger Will Robinson! Danger!, to err
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/console.js)
+ */
+declare module 'console' {
+    import console = require('node:console');
+    export = console;
+}
+declare module 'node:console' {
+    import { InspectOptions } from 'node:util';
+    global {
+        // This needs to be global to avoid TS2403 in case lib.dom.d.ts is present in the same build
+        interface Console {
+            Console: console.ConsoleConstructor;
+            /**
+             * `console.assert()` writes a message if `value` is [falsy](https://developer.mozilla.org/en-US/docs/Glossary/Falsy) or omitted. It only
+             * writes a message and does not otherwise affect execution. The output always
+             * starts with `""Assertion failed""`. If provided, `message` is formatted using `util.format()`.
+             *
+             * If `value` is [truthy](https://developer.mozilla.org/en-US/docs/Glossary/Truthy), nothing happens.
+             *
+             * ```js
+             * console.assert(true, 'does nothing');
+             *
+             * console.assert(false, 'Whoops %s work', 'didn\'t');
+             * // Assertion failed: Whoops didn't work
+             *
+             * console.assert();
+             * // Assertion failed
+             * ```
+             * @since v0.1.101
+             * @param value The value tested for being truthy.
+             * @param message All arguments besides `value` are used as error message.
+             */
+            assert(value: any, message?: string, ...optionalParams: any[]): void;
+            /**
+             * When `stdout` is a TTY, calling `console.clear()` will attempt to clear the
+             * TTY. When `stdout` is not a TTY, this method does nothing.
+             *
+             * The specific operation of `console.clear()` can vary across operating systems
+             * and terminal types. For most Linux operating systems, `console.clear()`operates similarly to the `clear` shell command. On Windows, `console.clear()`will clear only the output in the
+             * current terminal viewport for the Node.js
+             * binary.
+             * @since v8.3.0
+             */
+            clear(): void;
+            /**
+             * Maintains an internal counter specific to `label` and outputs to `stdout` the
+             * number of times `console.count()` has been called with the given `label`.
+             *
+             * ```js
+             * > console.count()
+             * default: 1
+             * undefined
+             * > console.count('default')
+             * default: 2
+             * undefined
+             * > console.count('abc')
+             * abc: 1
+             * undefined
+             * > console.count('xyz')
+             * xyz: 1
+             * undefined
+             * > console.count('abc')
+             * abc: 2
+             * undefined
+             * > console.count()
+             * default: 3
+             * undefined
+             * >
+             * ```
+             * @since v8.3.0
+             * @param label The display label for the counter.
+             */
+            count(label?: string): void;
+            /**
+             * Resets the internal counter specific to `label`.
+             *
+             * ```js
+             * > console.count('abc');
+             * abc: 1
+             * undefined
+             * > console.countReset('abc');
+             * undefined
+             * > console.count('abc');
+             * abc: 1
+             * undefined
+             * >
+             * ```
+             * @since v8.3.0
+             * @param label The display label for the counter.
+             */
+            countReset(label?: string): void;
+            /**
+             * The `console.debug()` function is an alias for {@link log}.
+             * @since v8.0.0
+             */
+            debug(message?: any, ...optionalParams: any[]): void;
+            /**
+             * Uses `util.inspect()` on `obj` and prints the resulting string to `stdout`.
+             * This function bypasses any custom `inspect()` function defined on `obj`.
+             * @since v0.1.101
+             */
+            dir(obj: any, options?: InspectOptions): void;
+            /**
+             * This method calls `console.log()` passing it the arguments received.
+             * This method does not produce any XML formatting.
+             * @since v8.0.0
+             */
+            dirxml(...data: any[]): void;
+            /**
+             * Prints to `stderr` with newline. Multiple arguments can be passed, with the
+             * first used as the primary message and all additional used as substitution
+             * values similar to [`printf(3)`](http://man7.org/linux/man-pages/man3/printf.3.html) (the arguments are all passed to `util.format()`).
+             *
+             * ```js
+             * const code = 5;
+             * console.error('error #%d', code);
+             * // Prints: error #5, to stderr
+             * console.error('error', code);
+             * // Prints: error 5, to stderr
+             * ```
+             *
+             * If formatting elements (e.g. `%d`) are not found in the first string then `util.inspect()` is called on each argument and the resulting string
+             * values are concatenated. See `util.format()` for more information.
+             * @since v0.1.100
+             */
+            error(message?: any, ...optionalParams: any[]): void;
+            /**
+             * Increases indentation of subsequent lines by spaces for `groupIndentation`length.
+             *
+             * If one or more `label`s are provided, those are printed first without the
+             * additional indentation.
+             * @since v8.5.0
+             */
+            group(...label: any[]): void;
+            /**
+             * An alias for {@link group}.
+             * @since v8.5.0
+             */
+            groupCollapsed(...label: any[]): void;
+            /**
+             * Decreases indentation of subsequent lines by spaces for `groupIndentation`length.
+             * @since v8.5.0
+             */
+            groupEnd(): void;
+            /**
+             * The `console.info()` function is an alias for {@link log}.
+             * @since v0.1.100
+             */
+            info(message?: any, ...optionalParams: any[]): void;
+            /**
+             * Prints to `stdout` with newline. Multiple arguments can be passed, with the
+             * first used as the primary message and all additional used as substitution
+             * values similar to [`printf(3)`](http://man7.org/linux/man-pages/man3/printf.3.html) (the arguments are all passed to `util.format()`).
+             *
+             * ```js
+             * const count = 5;
+             * console.log('count: %d', count);
+             * // Prints: count: 5, to stdout
+             * console.log('count:', count);
+             * // Prints: count: 5, to stdout
+             * ```
+             *
+             * See `util.format()` for more information.
+             * @since v0.1.100
+             */
+            log(message?: any, ...optionalParams: any[]): void;
+            /**
+             * Try to construct a table with the columns of the properties of `tabularData`(or use `properties`) and rows of `tabularData` and log it. Falls back to just
+             * logging the argument if it can’t be parsed as tabular.
+             *
+             * ```js
+             * // These can't be parsed as tabular data
+             * console.table(Symbol());
+             * // Symbol()
+             *
+             * console.table(undefined);
+             * // undefined
+             *
+             * console.table([{ a: 1, b: 'Y' }, { a: 'Z', b: 2 }]);
+             * // ┌─────────┬─────┬─────┐
+             * // │ (index) │  a  │  b  │
+             * // ├─────────┼─────┼─────┤
+             * // │    0    │  1  │ 'Y' │
+             * // │    1    │ 'Z' │  2  │
+             * // └─────────┴─────┴─────┘
+             *
+             * console.table([{ a: 1, b: 'Y' }, { a: 'Z', b: 2 }], ['a']);
+             * // ┌─────────┬─────┐
+             * // │ (index) │  a  │
+             * // ├─────────┼─────┤
+             * // │    0    │  1  │
+             * // │    1    │ 'Z' │
+             * // └─────────┴─────┘
+             * ```
+             * @since v10.0.0
+             * @param properties Alternate properties for constructing the table.
+             */
+            table(tabularData: any, properties?: ReadonlyArray<string>): void;
+            /**
+             * Starts a timer that can be used to compute the duration of an operation. Timers
+             * are identified by a unique `label`. Use the same `label` when calling {@link timeEnd} to stop the timer and output the elapsed time in
+             * suitable time units to `stdout`. For example, if the elapsed
+             * time is 3869ms, `console.timeEnd()` displays ""3.869s"".
+             * @since v0.1.104
+             */
+            time(label?: string): void;
+            /**
+             * Stops a timer that was previously started by calling {@link time} and
+             * prints the result to `stdout`:
+             *
+             * ```js
+             * console.time('100-elements');
+             * for (let i = 0; i < 100; i++) {}
+             * console.timeEnd('100-elements');
+             * // prints 100-elements: 225.438ms
+             * ```
+             * @since v0.1.104
+             */
+            timeEnd(label?: string): void;
+            /**
+             * For a timer that was previously started by calling {@link time}, prints
+             * the elapsed time and other `data` arguments to `stdout`:
+             *
+             * ```js
+             * console.time('process');
+             * const value = expensiveProcess1(); // Returns 42
+             * console.timeLog('process', value);
+             * // Prints ""process: 365.227ms 42"".
+             * doExpensiveProcess2(value);
+             * console.timeEnd('process');
+             * ```
+             * @since v10.7.0
+             */
+            timeLog(label?: string, ...data: any[]): void;
+            /**
+             * Prints to `stderr` the string `'Trace: '`, followed by the `util.format()` formatted message and stack trace to the current position in the code.
+             *
+             * ```js
+             * console.trace('Show me');
+             * // Prints: (stack trace will vary based on where trace is called)
+             * //  Trace: Show me
+             * //    at repl:2:9
+             * //    at REPLServer.defaultEval (repl.js:248:27)
+             * //    at bound (domain.js:287:14)
+             * //    at REPLServer.runBound [as eval] (domain.js:300:12)
+             * //    at REPLServer.<anonymous> (repl.js:412:12)
+             * //    at emitOne (events.js:82:20)
+             * //    at REPLServer.emit (events.js:169:7)
+             * //    at REPLServer.Interface._onLine (readline.js:210:10)
+             * //    at REPLServer.Interface._line (readline.js:549:8)
+             * //    at REPLServer.Interface._ttyWrite (readline.js:826:14)
+             * ```
+             * @since v0.1.104
+             */
+            trace(message?: any, ...optionalParams: any[]): void;
+            /**
+             * The `console.warn()` function is an alias for {@link error}.
+             * @since v0.1.100
+             */
+            warn(message?: any, ...optionalParams: any[]): void;
+            // --- Inspector mode only ---
+            /**
+             * This method does not display anything unless used in the inspector.
+             *  Starts a JavaScript CPU profile with an optional label.
+             */
+            profile(label?: string): void;
+            /**
+             * This method does not display anything unless used in the inspector.
+             *  Stops the current JavaScript CPU profiling session if one has been started and prints the report to the Profiles panel of the inspector.
+             */
+            profileEnd(label?: string): void;
+            /**
+             * This method does not display anything unless used in the inspector.
+             *  Adds an event with the label `label` to the Timeline panel of the inspector.
+             */
+            timeStamp(label?: string): void;
+        }
+        /**
+         * The `console` module provides a simple debugging console that is similar to the
+         * JavaScript console mechanism provided by web browsers.
+         *
+         * The module exports two specific components:
+         *
+         * * A `Console` class with methods such as `console.log()`, `console.error()` and`console.warn()` that can be used to write to any Node.js stream.
+         * * A global `console` instance configured to write to `process.stdout` and `process.stderr`. The global `console` can be used without calling`require('console')`.
+         *
+         * _**Warning**_: The global console object's methods are neither consistently
+         * synchronous like the browser APIs they resemble, nor are they consistently
+         * asynchronous like all other Node.js streams. See the `note on process I/O` for
+         * more information.
+         *
+         * Example using the global `console`:
+         *
+         * ```js
+         * console.log('hello world');
+         * // Prints: hello world, to stdout
+         * console.log('hello %s', 'world');
+         * // Prints: hello world, to stdout
+         * console.error(new Error('Whoops, something bad happened'));
+         * // Prints error message and stack trace to stderr:
+         * //   Error: Whoops, something bad happened
+         * //     at [eval]:5:15
+         * //     at Script.runInThisContext (node:vm:132:18)
+         * //     at Object.runInThisContext (node:vm:309:38)
+         * //     at node:internal/process/execution:77:19
+         * //     at [eval]-wrapper:6:22
+         * //     at evalScript (node:internal/process/execution:76:60)
+         * //     at node:internal/main/eval_string:23:3
+         *
+         * const name = 'Will Robinson';
+         * console.warn(`Danger ${name}! Danger!`);
+         * // Prints: Danger Will Robinson! Danger!, to stderr
+         * ```
+         *
+         * Example using the `Console` class:
+         *
+         * ```js
+         * const out = getStreamSomehow();
+         * const err = getStreamSomehow();
+         * const myConsole = new console.Console(out, err);
+         *
+         * myConsole.log('hello world');
+         * // Prints: hello world, to out
+         * myConsole.log('hello %s', 'world');
+         * // Prints: hello world, to out
+         * myConsole.error(new Error('Whoops, something bad happened'));
+         * // Prints: [Error: Whoops, something bad happened], to err
+         *
+         * const name = 'Will Robinson';
+         * myConsole.warn(`Danger ${name}! Danger!`);
+         * // Prints: Danger Will Robinson! Danger!, to err
+         * ```
+         * @see [source](https://github.com/nodejs/node/blob/v16.4.2/lib/console.js)
+         */
+        namespace console {
+            interface ConsoleConstructorOptions {
+                stdout: NodeJS.WritableStream;
+                stderr?: NodeJS.WritableStream | undefined;
+                ignoreErrors?: boolean | undefined;
+                colorMode?: boolean | 'auto' | undefined;
+                inspectOptions?: InspectOptions | undefined;
+                /**
+                 * Set group indentation
+                 * @default 2
+                 */
+                groupIndentation?: number | undefined;
+            }
+            interface ConsoleConstructor {
+                prototype: Console;
+                new (stdout: NodeJS.WritableStream, stderr?: NodeJS.WritableStream, ignoreErrors?: boolean): Console;
+                new (options: ConsoleConstructorOptions): Console;
+            }
+        }
+        var console: Console;
+    }
+    export = globalThis.console;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/** @deprecated since v6.3.0 - use constants property exposed by the relevant module instead. */
+declare module 'constants' {
+    import { constants as osConstants, SignalConstants } from 'node:os';
+    import { constants as cryptoConstants } from 'node:crypto';
+    import { constants as fsConstants } from 'node:fs';
+
+    const exp: typeof osConstants.errno &
+        typeof osConstants.priority &
+        SignalConstants &
+        typeof cryptoConstants &
+        typeof fsConstants;
+    export = exp;
+}
+
+declare module 'node:constants' {
+    import constants = require('constants');
+    export = constants;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `dgram` module provides an implementation of UDP datagram sockets.
+ *
+ * ```js
+ * import dgram from 'dgram';
+ *
+ * const server = dgram.createSocket('udp4');
+ *
+ * server.on('error', (err) => {
+ *   console.log(`server error:\n${err.stack}`);
+ *   server.close();
+ * });
+ *
+ * server.on('message', (msg, rinfo) => {
+ *   console.log(`server got: ${msg} from ${rinfo.address}:${rinfo.port}`);
+ * });
+ *
+ * server.on('listening', () => {
+ *   const address = server.address();
+ *   console.log(`server listening ${address.address}:${address.port}`);
+ * });
+ *
+ * server.bind(41234);
+ * // Prints: server listening 0.0.0.0:41234
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/dgram.js)
+ */
+declare module 'dgram' {
+    import { AddressInfo } from 'node:net';
+    import * as dns from 'node:dns';
+    import { EventEmitter, Abortable } from 'node:events';
+    interface RemoteInfo {
+        address: string;
+        family: 'IPv4' | 'IPv6';
+        port: number;
+        size: number;
+    }
+    interface BindOptions {
+        port?: number | undefined;
+        address?: string | undefined;
+        exclusive?: boolean | undefined;
+        fd?: number | undefined;
+    }
+    type SocketType = 'udp4' | 'udp6';
+    interface SocketOptions extends Abortable {
+        type: SocketType;
+        reuseAddr?: boolean | undefined;
+        /**
+         * @default false
+         */
+        ipv6Only?: boolean | undefined;
+        recvBufferSize?: number | undefined;
+        sendBufferSize?: number | undefined;
+        lookup?: ((hostname: string, options: dns.LookupOneOptions, callback: (err: NodeJS.ErrnoException | null, address: string, family: number) => void) => void) | undefined;
+    }
+    /**
+     * Creates a `dgram.Socket` object. Once the socket is created, calling `socket.bind()` will instruct the socket to begin listening for datagram
+     * messages. When `address` and `port` are not passed to `socket.bind()` the
+     * method will bind the socket to the ""all interfaces"" address on a random port
+     * (it does the right thing for both `udp4` and `udp6` sockets). The bound address
+     * and port can be retrieved using `socket.address().address` and `socket.address().port`.
+     *
+     * If the `signal` option is enabled, calling `.abort()` on the corresponding`AbortController` is similar to calling `.close()` on the socket:
+     *
+     * ```js
+     * const controller = new AbortController();
+     * const { signal } = controller;
+     * const server = dgram.createSocket({ type: 'udp4', signal });
+     * server.on('message', (msg, rinfo) => {
+     *   console.log(`server got: ${msg} from ${rinfo.address}:${rinfo.port}`);
+     * });
+     * // Later, when you want to close the server.
+     * controller.abort();
+     * ```
+     * @since v0.11.13
+     * @param options Available options are:
+     * @param callback Attached as a listener for `'message'` events. Optional.
+     */
+    function createSocket(type: SocketType, callback?: (msg: Buffer, rinfo: RemoteInfo) => void): Socket;
+    function createSocket(options: SocketOptions, callback?: (msg: Buffer, rinfo: RemoteInfo) => void): Socket;
+    /**
+     * Encapsulates the datagram functionality.
+     *
+     * New instances of `dgram.Socket` are created using {@link createSocket}.
+     * The `new` keyword is not to be used to create `dgram.Socket` instances.
+     * @since v0.1.99
+     */
+    class Socket extends EventEmitter {
+        /**
+         * Tells the kernel to join a multicast group at the given `multicastAddress` and`multicastInterface` using the `IP_ADD_MEMBERSHIP` socket option. If the`multicastInterface` argument is not
+         * specified, the operating system will choose
+         * one interface and will add membership to it. To add membership to every
+         * available interface, call `addMembership` multiple times, once per interface.
+         *
+         * When called on an unbound socket, this method will implicitly bind to a random
+         * port, listening on all interfaces.
+         *
+         * When sharing a UDP socket across multiple `cluster` workers, the`socket.addMembership()` function must be called only once or an`EADDRINUSE` error will occur:
+         *
+         * ```js
+         * import cluster from 'cluster';
+         * import dgram from 'dgram';
+         *
+         * if (cluster.isPrimary) {
+         *   cluster.fork(); // Works ok.
+         *   cluster.fork(); // Fails with EADDRINUSE.
+         * } else {
+         *   const s = dgram.createSocket('udp4');
+         *   s.bind(1234, () => {
+         *     s.addMembership('224.0.0.114');
+         *   });
+         * }
+         * ```
+         * @since v0.6.9
+         */
+        addMembership(multicastAddress: string, multicastInterface?: string): void;
+        /**
+         * Returns an object containing the address information for a socket.
+         * For UDP sockets, this object will contain `address`, `family` and `port`properties.
+         *
+         * This method throws `EBADF` if called on an unbound socket.
+         * @since v0.1.99
+         */
+        address(): AddressInfo;
+        /**
+         * For UDP sockets, causes the `dgram.Socket` to listen for datagram
+         * messages on a named `port` and optional `address`. If `port` is not
+         * specified or is `0`, the operating system will attempt to bind to a
+         * random port. If `address` is not specified, the operating system will
+         * attempt to listen on all addresses. Once binding is complete, a`'listening'` event is emitted and the optional `callback` function is
+         * called.
+         *
+         * Specifying both a `'listening'` event listener and passing a`callback` to the `socket.bind()` method is not harmful but not very
+         * useful.
+         *
+         * A bound datagram socket keeps the Node.js process running to receive
+         * datagram messages.
+         *
+         * If binding fails, an `'error'` event is generated. In rare case (e.g.
+         * attempting to bind with a closed socket), an `Error` may be thrown.
+         *
+         * Example of a UDP server listening on port 41234:
+         *
+         * ```js
+         * import dgram from 'dgram';
+         *
+         * const server = dgram.createSocket('udp4');
+         *
+         * server.on('error', (err) => {
+         *   console.log(`server error:\n${err.stack}`);
+         *   server.close();
+         * });
+         *
+         * server.on('message', (msg, rinfo) => {
+         *   console.log(`server got: ${msg} from ${rinfo.address}:${rinfo.port}`);
+         * });
+         *
+         * server.on('listening', () => {
+         *   const address = server.address();
+         *   console.log(`server listening ${address.address}:${address.port}`);
+         * });
+         *
+         * server.bind(41234);
+         * // Prints: server listening 0.0.0.0:41234
+         * ```
+         * @since v0.1.99
+         * @param callback with no parameters. Called when binding is complete.
+         */
+        bind(port?: number, address?: string, callback?: () => void): this;
+        bind(port?: number, callback?: () => void): this;
+        bind(callback?: () => void): this;
+        bind(options: BindOptions, callback?: () => void): this;
+        /**
+         * Close the underlying socket and stop listening for data on it. If a callback is
+         * provided, it is added as a listener for the `'close'` event.
+         * @since v0.1.99
+         * @param callback Called when the socket has been closed.
+         */
+        close(callback?: () => void): this;
+        /**
+         * Associates the `dgram.Socket` to a remote address and port. Every
+         * message sent by this handle is automatically sent to that destination. Also,
+         * the socket will only receive messages from that remote peer.
+         * Trying to call `connect()` on an already connected socket will result
+         * in an `ERR_SOCKET_DGRAM_IS_CONNECTED` exception. If `address` is not
+         * provided, `'127.0.0.1'` (for `udp4` sockets) or `'::1'` (for `udp6` sockets)
+         * will be used by default. Once the connection is complete, a `'connect'` event
+         * is emitted and the optional `callback` function is called. In case of failure,
+         * the `callback` is called or, failing this, an `'error'` event is emitted.
+         * @since v12.0.0
+         * @param callback Called when the connection is completed or on error.
+         */
+        connect(port: number, address?: string, callback?: () => void): void;
+        connect(port: number, callback: () => void): void;
+        /**
+         * A synchronous function that disassociates a connected `dgram.Socket` from
+         * its remote address. Trying to call `disconnect()` on an unbound or already
+         * disconnected socket will result in an `ERR_SOCKET_DGRAM_NOT_CONNECTED` exception.
+         * @since v12.0.0
+         */
+        disconnect(): void;
+        /**
+         * Instructs the kernel to leave a multicast group at `multicastAddress` using the`IP_DROP_MEMBERSHIP` socket option. This method is automatically called by the
+         * kernel when the socket is closed or the process terminates, so most apps will
+         * never have reason to call this.
+         *
+         * If `multicastInterface` is not specified, the operating system will attempt to
+         * drop membership on all valid interfaces.
+         * @since v0.6.9
+         */
+        dropMembership(multicastAddress: string, multicastInterface?: string): void;
+        /**
+         * This method throws `ERR_SOCKET_BUFFER_SIZE` if called on an unbound socket.
+         * @since v8.7.0
+         * @return the `SO_RCVBUF` socket receive buffer size in bytes.
+         */
+        getRecvBufferSize(): number;
+        /**
+         * This method throws `ERR_SOCKET_BUFFER_SIZE` if called on an unbound socket.
+         * @since v8.7.0
+         * @return the `SO_SNDBUF` socket send buffer size in bytes.
+         */
+        getSendBufferSize(): number;
+        /**
+         * By default, binding a socket will cause it to block the Node.js process from
+         * exiting as long as the socket is open. The `socket.unref()` method can be used
+         * to exclude the socket from the reference counting that keeps the Node.js
+         * process active. The `socket.ref()` method adds the socket back to the reference
+         * counting and restores the default behavior.
+         *
+         * Calling `socket.ref()` multiples times will have no additional effect.
+         *
+         * The `socket.ref()` method returns a reference to the socket so calls can be
+         * chained.
+         * @since v0.9.1
+         */
+        ref(): this;
+        /**
+         * Returns an object containing the `address`, `family`, and `port` of the remote
+         * endpoint. This method throws an `ERR_SOCKET_DGRAM_NOT_CONNECTED` exception
+         * if the socket is not connected.
+         * @since v12.0.0
+         */
+        remoteAddress(): AddressInfo;
+        /**
+         * Broadcasts a datagram on the socket.
+         * For connectionless sockets, the destination `port` and `address` must be
+         * specified. Connected sockets, on the other hand, will use their associated
+         * remote endpoint, so the `port` and `address` arguments must not be set.
+         *
+         * The `msg` argument contains the message to be sent.
+         * Depending on its type, different behavior can apply. If `msg` is a `Buffer`,
+         * any `TypedArray` or a `DataView`,
+         * the `offset` and `length` specify the offset within the `Buffer` where the
+         * message begins and the number of bytes in the message, respectively.
+         * If `msg` is a `String`, then it is automatically converted to a `Buffer`with `'utf8'` encoding. With messages that
+         * contain multi-byte characters, `offset` and `length` will be calculated with
+         * respect to `byte length` and not the character position.
+         * If `msg` is an array, `offset` and `length` must not be specified.
+         *
+         * The `address` argument is a string. If the value of `address` is a host name,
+         * DNS will be used to resolve the address of the host. If `address` is not
+         * provided or otherwise nullish, `'127.0.0.1'` (for `udp4` sockets) or `'::1'`(for `udp6` sockets) will be used by default.
+         *
+         * If the socket has not been previously bound with a call to `bind`, the socket
+         * is assigned a random port number and is bound to the ""all interfaces"" address
+         * (`'0.0.0.0'` for `udp4` sockets, `'::0'` for `udp6` sockets.)
+         *
+         * An optional `callback` function may be specified to as a way of reporting
+         * DNS errors or for determining when it is safe to reuse the `buf` object.
+         * DNS lookups delay the time to send for at least one tick of the
+         * Node.js event loop.
+         *
+         * The only way to know for sure that the datagram has been sent is by using a`callback`. If an error occurs and a `callback` is given, the error will be
+         * passed as the first argument to the `callback`. If a `callback` is not given,
+         * the error is emitted as an `'error'` event on the `socket` object.
+         *
+         * Offset and length are optional but both _must_ be set if either are used.
+         * They are supported only when the first argument is a `Buffer`, a `TypedArray`,
+         * or a `DataView`.
+         *
+         * This method throws `ERR_SOCKET_BAD_PORT` if called on an unbound socket.
+         *
+         * Example of sending a UDP packet to a port on `localhost`;
+         *
+         * ```js
+         * import dgram from 'dgram';
+         * import { Buffer } from 'buffer';
+         *
+         * const message = Buffer.from('Some bytes');
+         * const client = dgram.createSocket('udp4');
+         * client.send(message, 41234, 'localhost', (err) => {
+         *   client.close();
+         * });
+         * ```
+         *
+         * Example of sending a UDP packet composed of multiple buffers to a port on`127.0.0.1`;
+         *
+         * ```js
+         * import dgram from 'dgram';
+         * import { Buffer } from 'buffer';
+         *
+         * const buf1 = Buffer.from('Some ');
+         * const buf2 = Buffer.from('bytes');
+         * const client = dgram.createSocket('udp4');
+         * client.send([buf1, buf2], 41234, (err) => {
+         *   client.close();
+         * });
+         * ```
+         *
+         * Sending multiple buffers might be faster or slower depending on the
+         * application and operating system. Run benchmarks to
+         * determine the optimal strategy on a case-by-case basis. Generally speaking,
+         * however, sending multiple buffers is faster.
+         *
+         * Example of sending a UDP packet using a socket connected to a port on`localhost`:
+         *
+         * ```js
+         * import dgram from 'dgram';
+         * import { Buffer } from 'buffer';
+         *
+         * const message = Buffer.from('Some bytes');
+         * const client = dgram.createSocket('udp4');
+         * client.connect(41234, 'localhost', (err) => {
+         *   client.send(message, (err) => {
+         *     client.close();
+         *   });
+         * });
+         * ```
+         * @since v0.1.99
+         * @param msg Message to be sent.
+         * @param offset Offset in the buffer where the message starts.
+         * @param length Number of bytes in the message.
+         * @param port Destination port.
+         * @param address Destination host name or IP address.
+         * @param callback Called when the message has been sent.
+         */
+        send(msg: string | Uint8Array | ReadonlyArray<any>, port?: number, address?: string, callback?: (error: Error | null, bytes: number) => void): void;
+        send(msg: string | Uint8Array | ReadonlyArray<any>, port?: number, callback?: (error: Error | null, bytes: number) => void): void;
+        send(msg: string | Uint8Array | ReadonlyArray<any>, callback?: (error: Error | null, bytes: number) => void): void;
+        send(msg: string | Uint8Array, offset: number, length: number, port?: number, address?: string, callback?: (error: Error | null, bytes: number) => void): void;
+        send(msg: string | Uint8Array, offset: number, length: number, port?: number, callback?: (error: Error | null, bytes: number) => void): void;
+        send(msg: string | Uint8Array, offset: number, length: number, callback?: (error: Error | null, bytes: number) => void): void;
+        /**
+         * Sets or clears the `SO_BROADCAST` socket option. When set to `true`, UDP
+         * packets may be sent to a local interface's broadcast address.
+         *
+         * This method throws `EBADF` if called on an unbound socket.
+         * @since v0.6.9
+         */
+        setBroadcast(flag: boolean): void;
+        /**
+         * _All references to scope in this section are referring to [IPv6 Zone Indices](https://en.wikipedia.org/wiki/IPv6_address#Scoped_literal_IPv6_addresses), which are defined by [RFC
+         * 4007](https://tools.ietf.org/html/rfc4007). In string form, an IP_
+         * _with a scope index is written as `'IP%scope'` where scope is an interface name_
+         * _or interface number._
+         *
+         * Sets the default outgoing multicast interface of the socket to a chosen
+         * interface or back to system interface selection. The `multicastInterface` must
+         * be a valid string representation of an IP from the socket's family.
+         *
+         * For IPv4 sockets, this should be the IP configured for the desired physical
+         * interface. All packets sent to multicast on the socket will be sent on the
+         * interface determined by the most recent successful use of this call.
+         *
+         * For IPv6 sockets, `multicastInterface` should include a scope to indicate the
+         * interface as in the examples that follow. In IPv6, individual `send` calls can
+         * also use explicit scope in addresses, so only packets sent to a multicast
+         * address without specifying an explicit scope are affected by the most recent
+         * successful use of this call.
+         *
+         * This method throws `EBADF` if called on an unbound socket.
+         *
+         * #### Example: IPv6 outgoing multicast interface
+         *
+         * On most systems, where scope format uses the interface name:
+         *
+         * ```js
+         * const socket = dgram.createSocket('udp6');
+         *
+         * socket.bind(1234, () => {
+         *   socket.setMulticastInterface('::%eth1');
+         * });
+         * ```
+         *
+         * On Windows, where scope format uses an interface number:
+         *
+         * ```js
+         * const socket = dgram.createSocket('udp6');
+         *
+         * socket.bind(1234, () => {
+         *   socket.setMulticastInterface('::%2');
+         * });
+         * ```
+         *
+         * #### Example: IPv4 outgoing multicast interface
+         *
+         * All systems use an IP of the host on the desired physical interface:
+         *
+         * ```js
+         * const socket = dgram.createSocket('udp4');
+         *
+         * socket.bind(1234, () => {
+         *   socket.setMulticastInterface('10.0.0.2');
+         * });
+         * ```
+         * @since v8.6.0
+         */
+        setMulticastInterface(multicastInterface: string): void;
+        /**
+         * Sets or clears the `IP_MULTICAST_LOOP` socket option. When set to `true`,
+         * multicast packets will also be received on the local interface.
+         *
+         * This method throws `EBADF` if called on an unbound socket.
+         * @since v0.3.8
+         */
+        setMulticastLoopback(flag: boolean): boolean;
+        /**
+         * Sets the `IP_MULTICAST_TTL` socket option. While TTL generally stands for
+         * ""Time to Live"", in this context it specifies the number of IP hops that a
+         * packet is allowed to travel through, specifically for multicast traffic. Each
+         * router or gateway that forwards a packet decrements the TTL. If the TTL is
+         * decremented to 0 by a router, it will not be forwarded.
+         *
+         * The `ttl` argument may be between 0 and 255\. The default on most systems is `1`.
+         *
+         * This method throws `EBADF` if called on an unbound socket.
+         * @since v0.3.8
+         */
+        setMulticastTTL(ttl: number): number;
+        /**
+         * Sets the `SO_RCVBUF` socket option. Sets the maximum socket receive buffer
+         * in bytes.
+         *
+         * This method throws `ERR_SOCKET_BUFFER_SIZE` if called on an unbound socket.
+         * @since v8.7.0
+         */
+        setRecvBufferSize(size: number): void;
+        /**
+         * Sets the `SO_SNDBUF` socket option. Sets the maximum socket send buffer
+         * in bytes.
+         *
+         * This method throws `ERR_SOCKET_BUFFER_SIZE` if called on an unbound socket.
+         * @since v8.7.0
+         */
+        setSendBufferSize(size: number): void;
+        /**
+         * Sets the `IP_TTL` socket option. While TTL generally stands for ""Time to Live"",
+         * in this context it specifies the number of IP hops that a packet is allowed to
+         * travel through. Each router or gateway that forwards a packet decrements the
+         * TTL. If the TTL is decremented to 0 by a router, it will not be forwarded.
+         * Changing TTL values is typically done for network probes or when multicasting.
+         *
+         * The `ttl` argument may be between between 1 and 255\. The default on most systems
+         * is 64.
+         *
+         * This method throws `EBADF` if called on an unbound socket.
+         * @since v0.1.101
+         */
+        setTTL(ttl: number): number;
+        /**
+         * By default, binding a socket will cause it to block the Node.js process from
+         * exiting as long as the socket is open. The `socket.unref()` method can be used
+         * to exclude the socket from the reference counting that keeps the Node.js
+         * process active, allowing the process to exit even if the socket is still
+         * listening.
+         *
+         * Calling `socket.unref()` multiple times will have no addition effect.
+         *
+         * The `socket.unref()` method returns a reference to the socket so calls can be
+         * chained.
+         * @since v0.9.1
+         */
+        unref(): this;
+        /**
+         * Tells the kernel to join a source-specific multicast channel at the given`sourceAddress` and `groupAddress`, using the `multicastInterface` with the`IP_ADD_SOURCE_MEMBERSHIP` socket
+         * option. If the `multicastInterface` argument
+         * is not specified, the operating system will choose one interface and will add
+         * membership to it. To add membership to every available interface, call`socket.addSourceSpecificMembership()` multiple times, once per interface.
+         *
+         * When called on an unbound socket, this method will implicitly bind to a random
+         * port, listening on all interfaces.
+         * @since v13.1.0, v12.16.0
+         */
+        addSourceSpecificMembership(sourceAddress: string, groupAddress: string, multicastInterface?: string): void;
+        /**
+         * Instructs the kernel to leave a source-specific multicast channel at the given`sourceAddress` and `groupAddress` using the `IP_DROP_SOURCE_MEMBERSHIP`socket option. This method is
+         * automatically called by the kernel when the
+         * socket is closed or the process terminates, so most apps will never have
+         * reason to call this.
+         *
+         * If `multicastInterface` is not specified, the operating system will attempt to
+         * drop membership on all valid interfaces.
+         * @since v13.1.0, v12.16.0
+         */
+        dropSourceSpecificMembership(sourceAddress: string, groupAddress: string, multicastInterface?: string): void;
+        /**
+         * events.EventEmitter
+         * 1. close
+         * 2. connect
+         * 3. error
+         * 4. listening
+         * 5. message
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'connect', listener: () => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'listening', listener: () => void): this;
+        addListener(event: 'message', listener: (msg: Buffer, rinfo: RemoteInfo) => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'close'): boolean;
+        emit(event: 'connect'): boolean;
+        emit(event: 'error', err: Error): boolean;
+        emit(event: 'listening'): boolean;
+        emit(event: 'message', msg: Buffer, rinfo: RemoteInfo): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'connect', listener: () => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'listening', listener: () => void): this;
+        on(event: 'message', listener: (msg: Buffer, rinfo: RemoteInfo) => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'connect', listener: () => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'listening', listener: () => void): this;
+        once(event: 'message', listener: (msg: Buffer, rinfo: RemoteInfo) => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'connect', listener: () => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'listening', listener: () => void): this;
+        prependListener(event: 'message', listener: (msg: Buffer, rinfo: RemoteInfo) => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'connect', listener: () => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'listening', listener: () => void): this;
+        prependOnceListener(event: 'message', listener: (msg: Buffer, rinfo: RemoteInfo) => void): this;
+    }
+}
+declare module 'node:dgram' {
+    export * from 'dgram';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `diagnostics_channel` module provides an API to create named channels
+ * to report arbitrary message data for diagnostics purposes.
+ *
+ * It can be accessed using:
+ *
+ * ```js
+ * import diagnostics_channel from 'diagnostics_channel';
+ * ```
+ *
+ * It is intended that a module writer wanting to report diagnostics messages
+ * will create one or many top-level channels to report messages through.
+ * Channels may also be acquired at runtime but it is not encouraged
+ * due to the additional overhead of doing so. Channels may be exported for
+ * convenience, but as long as the name is known it can be acquired anywhere.
+ *
+ * If you intend for your module to produce diagnostics data for others to
+ * consume it is recommended that you include documentation of what named
+ * channels are used along with the shape of the message data. Channel names
+ * should generally include the module name to avoid collisions with data from
+ * other modules.
+ * @experimental
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/diagnostics_channel.js)
+ */
+declare module 'diagnostics_channel' {
+    /**
+     * Check if there are active subscribers to the named channel. This is helpful if
+     * the message you want to send might be expensive to prepare.
+     *
+     * This API is optional but helpful when trying to publish messages from very
+     * performance-sensitive code.
+     *
+     * ```js
+     * import diagnostics_channel from 'diagnostics_channel';
+     *
+     * if (diagnostics_channel.hasSubscribers('my-channel')) {
+     *   // There are subscribers, prepare and publish message
+     * }
+     * ```
+     * @since v15.1.0, v14.17.0
+     * @param name The channel name
+     * @return If there are active subscribers
+     */
+    function hasSubscribers(name: string): boolean;
+    /**
+     * This is the primary entry-point for anyone wanting to interact with a named
+     * channel. It produces a channel object which is optimized to reduce overhead at
+     * publish time as much as possible.
+     *
+     * ```js
+     * import diagnostics_channel from 'diagnostics_channel';
+     *
+     * const channel = diagnostics_channel.channel('my-channel');
+     * ```
+     * @since v15.1.0, v14.17.0
+     * @param name The channel name
+     * @return The named channel object
+     */
+    function channel(name: string): Channel;
+    type ChannelListener = (message: unknown, name: string) => void;
+    /**
+     * The class `Channel` represents an individual named channel within the data
+     * pipeline. It is use to track subscribers and to publish messages when there
+     * are subscribers present. It exists as a separate object to avoid channel
+     * lookups at publish time, enabling very fast publish speeds and allowing
+     * for heavy use while incurring very minimal cost. Channels are created with {@link channel}, constructing a channel directly
+     * with `new Channel(name)` is not supported.
+     * @since v15.1.0, v14.17.0
+     */
+    class Channel {
+        readonly name: string;
+        /**
+         * Check if there are active subscribers to this channel. This is helpful if
+         * the message you want to send might be expensive to prepare.
+         *
+         * This API is optional but helpful when trying to publish messages from very
+         * performance-sensitive code.
+         *
+         * ```js
+         * import diagnostics_channel from 'diagnostics_channel';
+         *
+         * const channel = diagnostics_channel.channel('my-channel');
+         *
+         * if (channel.hasSubscribers) {
+         *   // There are subscribers, prepare and publish message
+         * }
+         * ```
+         * @since v15.1.0, v14.17.0
+         */
+        readonly hasSubscribers: boolean;
+        private constructor(name: string);
+        /**
+         * Publish a message to any subscribers to the channel. This will
+         * trigger message handlers synchronously so they will execute within
+         * the same context.
+         *
+         * ```js
+         * import diagnostics_channel from 'diagnostics_channel';
+         *
+         * const channel = diagnostics_channel.channel('my-channel');
+         *
+         * channel.publish({
+         *   some: 'message'
+         * });
+         * ```
+         * @since v15.1.0, v14.17.0
+         * @param message The message to send to the channel subscribers
+         */
+        publish(message: unknown): void;
+        /**
+         * Register a message handler to subscribe to this channel. This message handler
+         * will be run synchronously whenever a message is published to the channel. Any
+         * errors thrown in the message handler will trigger an `'uncaughtException'`.
+         *
+         * ```js
+         * import diagnostics_channel from 'diagnostics_channel';
+         *
+         * const channel = diagnostics_channel.channel('my-channel');
+         *
+         * channel.subscribe((message, name) => {
+         *   // Received data
+         * });
+         * ```
+         * @since v15.1.0, v14.17.0
+         * @param onMessage The handler to receive channel messages
+         */
+        subscribe(onMessage: ChannelListener): void;
+        /**
+         * Remove a message handler previously registered to this channel with `channel.subscribe(onMessage)`.
+         *
+         * ```js
+         * import diagnostics_channel from 'diagnostics_channel';
+         *
+         * const channel = diagnostics_channel.channel('my-channel');
+         *
+         * function onMessage(message, name) {
+         *   // Received data
+         * }
+         *
+         * channel.subscribe(onMessage);
+         *
+         * channel.unsubscribe(onMessage);
+         * ```
+         * @since v15.1.0, v14.17.0
+         * @param onMessage The previous subscribed handler to remove
+         */
+        unsubscribe(onMessage: ChannelListener): void;
+    }
+}
+declare module 'node:diagnostics_channel' {
+    export * from 'diagnostics_channel';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `dns` module enables name resolution. For example, use it to look up IP
+ * addresses of host names.
+ *
+ * Although named for the [Domain Name System (DNS)](https://en.wikipedia.org/wiki/Domain_Name_System), it does not always use the
+ * DNS protocol for lookups. {@link lookup} uses the operating system
+ * facilities to perform name resolution. It may not need to perform any network
+ * communication. To perform name resolution the way other applications on the same
+ * system do, use {@link lookup}.
+ *
+ * ```js
+ * const dns = require('dns');
+ *
+ * dns.lookup('example.org', (err, address, family) => {
+ *   console.log('address: %j family: IPv%s', address, family);
+ * });
+ * // address: ""93.184.216.34"" family: IPv4
+ * ```
+ *
+ * All other functions in the `dns` module connect to an actual DNS server to
+ * perform name resolution. They will always use the network to perform DNS
+ * queries. These functions do not use the same set of configuration files used by {@link lookup} (e.g. `/etc/hosts`). Use these functions to always perform
+ * DNS queries, bypassing other name-resolution facilities.
+ *
+ * ```js
+ * const dns = require('dns');
+ *
+ * dns.resolve4('archive.org', (err, addresses) => {
+ *   if (err) throw err;
+ *
+ *   console.log(`addresses: ${JSON.stringify(addresses)}`);
+ *
+ *   addresses.forEach((a) => {
+ *     dns.reverse(a, (err, hostnames) => {
+ *       if (err) {
+ *         throw err;
+ *       }
+ *       console.log(`reverse for ${a}: ${JSON.stringify(hostnames)}`);
+ *     });
+ *   });
+ * });
+ * ```
+ *
+ * See the `Implementation considerations section` for more information.
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/dns.js)
+ */
+declare module 'dns' {
+    import * as dnsPromises from 'node:dns/promises';
+    // Supported getaddrinfo flags.
+    export const ADDRCONFIG: number;
+    export const V4MAPPED: number;
+    /**
+     * If `dns.V4MAPPED` is specified, return resolved IPv6 addresses as
+     * well as IPv4 mapped IPv6 addresses.
+     */
+    export const ALL: number;
+    export interface LookupOptions {
+        family?: number | undefined;
+        hints?: number | undefined;
+        all?: boolean | undefined;
+        /**
+         * @default true
+         */
+        verbatim?: boolean | undefined;
+    }
+    export interface LookupOneOptions extends LookupOptions {
+        all?: false | undefined;
+    }
+    export interface LookupAllOptions extends LookupOptions {
+        all: true;
+    }
+    export interface LookupAddress {
+        address: string;
+        family: number;
+    }
+    /**
+     * Resolves a host name (e.g. `'nodejs.org'`) into the first found A (IPv4) or
+     * AAAA (IPv6) record. All `option` properties are optional. If `options` is an
+     * integer, then it must be `4` or `6` – if `options` is not provided, then IPv4
+     * and IPv6 addresses are both returned if found.
+     *
+     * With the `all` option set to `true`, the arguments for `callback` change to`(err, addresses)`, with `addresses` being an array of objects with the
+     * properties `address` and `family`.
+     *
+     * On error, `err` is an `Error` object, where `err.code` is the error code.
+     * Keep in mind that `err.code` will be set to `'ENOTFOUND'` not only when
+     * the host name does not exist but also when the lookup fails in other ways
+     * such as no available file descriptors.
+     *
+     * `dns.lookup()` does not necessarily have anything to do with the DNS protocol.
+     * The implementation uses an operating system facility that can associate names
+     * with addresses, and vice versa. This implementation can have subtle but
+     * important consequences on the behavior of any Node.js program. Please take some
+     * time to consult the `Implementation considerations section` before using`dns.lookup()`.
+     *
+     * Example usage:
+     *
+     * ```js
+     * const dns = require('dns');
+     * const options = {
+     *   family: 6,
+     *   hints: dns.ADDRCONFIG | dns.V4MAPPED,
+     * };
+     * dns.lookup('example.com', options, (err, address, family) =>
+     *   console.log('address: %j family: IPv%s', address, family));
+     * // address: ""2606:2800:220:1:248:1893:25c8:1946"" family: IPv6
+     *
+     * // When options.all is true, the result will be an Array.
+     * options.all = true;
+     * dns.lookup('example.com', options, (err, addresses) =>
+     *   console.log('addresses: %j', addresses));
+     * // addresses: [{""address"":""2606:2800:220:1:248:1893:25c8:1946"",""family"":6}]
+     * ```
+     *
+     * If this method is invoked as its `util.promisify()` ed version, and `all`is not set to `true`, it returns a `Promise` for an `Object` with `address` and`family` properties.
+     * @since v0.1.90
+     */
+    export function lookup(hostname: string, family: number, callback: (err: NodeJS.ErrnoException | null, address: string, family: number) => void): void;
+    export function lookup(hostname: string, options: LookupOneOptions, callback: (err: NodeJS.ErrnoException | null, address: string, family: number) => void): void;
+    export function lookup(hostname: string, options: LookupAllOptions, callback: (err: NodeJS.ErrnoException | null, addresses: LookupAddress[]) => void): void;
+    export function lookup(hostname: string, options: LookupOptions, callback: (err: NodeJS.ErrnoException | null, address: string | LookupAddress[], family: number) => void): void;
+    export function lookup(hostname: string, callback: (err: NodeJS.ErrnoException | null, address: string, family: number) => void): void;
+    export namespace lookup {
+        function __promisify__(hostname: string, options: LookupAllOptions): Promise<LookupAddress[]>;
+        function __promisify__(hostname: string, options?: LookupOneOptions | number): Promise<LookupAddress>;
+        function __promisify__(hostname: string, options: LookupOptions): Promise<LookupAddress | LookupAddress[]>;
+    }
+    /**
+     * Resolves the given `address` and `port` into a host name and service using
+     * the operating system's underlying `getnameinfo` implementation.
+     *
+     * If `address` is not a valid IP address, a `TypeError` will be thrown.
+     * The `port` will be coerced to a number. If it is not a legal port, a `TypeError`will be thrown.
+     *
+     * On an error, `err` is an `Error` object, where `err.code` is the error code.
+     *
+     * ```js
+     * const dns = require('dns');
+     * dns.lookupService('127.0.0.1', 22, (err, hostname, service) => {
+     *   console.log(hostname, service);
+     *   // Prints: localhost ssh
+     * });
+     * ```
+     *
+     * If this method is invoked as its `util.promisify()` ed version, it returns a`Promise` for an `Object` with `hostname` and `service` properties.
+     * @since v0.11.14
+     */
+    export function lookupService(address: string, port: number, callback: (err: NodeJS.ErrnoException | null, hostname: string, service: string) => void): void;
+    export namespace lookupService {
+        function __promisify__(
+            address: string,
+            port: number
+        ): Promise<{
+            hostname: string;
+            service: string;
+        }>;
+    }
+    export interface ResolveOptions {
+        ttl: boolean;
+    }
+    export interface ResolveWithTtlOptions extends ResolveOptions {
+        ttl: true;
+    }
+    export interface RecordWithTtl {
+        address: string;
+        ttl: number;
+    }
+    /** @deprecated Use `AnyARecord` or `AnyAaaaRecord` instead. */
+    export type AnyRecordWithTtl = AnyARecord | AnyAaaaRecord;
+    export interface AnyARecord extends RecordWithTtl {
+        type: 'A';
+    }
+    export interface AnyAaaaRecord extends RecordWithTtl {
+        type: 'AAAA';
+    }
+    export interface CaaRecord {
+        critial: number;
+        issue?: string | undefined;
+        issuewild?: string | undefined;
+        iodef?: string | undefined;
+        contactemail?: string | undefined;
+        contactphone?: string | undefined;
+    }
+    export interface MxRecord {
+        priority: number;
+        exchange: string;
+    }
+    export interface AnyMxRecord extends MxRecord {
+        type: 'MX';
+    }
+    export interface NaptrRecord {
+        flags: string;
+        service: string;
+        regexp: string;
+        replacement: string;
+        order: number;
+        preference: number;
+    }
+    export interface AnyNaptrRecord extends NaptrRecord {
+        type: 'NAPTR';
+    }
+    export interface SoaRecord {
+        nsname: string;
+        hostmaster: string;
+        serial: number;
+        refresh: number;
+        retry: number;
+        expire: number;
+        minttl: number;
+    }
+    export interface AnySoaRecord extends SoaRecord {
+        type: 'SOA';
+    }
+    export interface SrvRecord {
+        priority: number;
+        weight: number;
+        port: number;
+        name: string;
+    }
+    export interface AnySrvRecord extends SrvRecord {
+        type: 'SRV';
+    }
+    export interface AnyTxtRecord {
+        type: 'TXT';
+        entries: string[];
+    }
+    export interface AnyNsRecord {
+        type: 'NS';
+        value: string;
+    }
+    export interface AnyPtrRecord {
+        type: 'PTR';
+        value: string;
+    }
+    export interface AnyCnameRecord {
+        type: 'CNAME';
+        value: string;
+    }
+    export type AnyRecord = AnyARecord | AnyAaaaRecord | AnyCnameRecord | AnyMxRecord | AnyNaptrRecord | AnyNsRecord | AnyPtrRecord | AnySoaRecord | AnySrvRecord | AnyTxtRecord;
+    /**
+     * Uses the DNS protocol to resolve a host name (e.g. `'nodejs.org'`) into an array
+     * of the resource records. The `callback` function has arguments`(err, records)`. When successful, `records` will be an array of resource
+     * records. The type and structure of individual results varies based on `rrtype`:
+     *
+     * <omitted>
+     *
+     * On error, `err` is an `Error` object, where `err.code` is one of theDNS error codes.
+     * @since v0.1.27
+     * @param hostname Host name to resolve.
+     * @param [rrtype='A'] Resource record type.
+     */
+    export function resolve(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'A', callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'AAAA', callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'ANY', callback: (err: NodeJS.ErrnoException | null, addresses: AnyRecord[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'CNAME', callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'MX', callback: (err: NodeJS.ErrnoException | null, addresses: MxRecord[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'NAPTR', callback: (err: NodeJS.ErrnoException | null, addresses: NaptrRecord[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'NS', callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'PTR', callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'SOA', callback: (err: NodeJS.ErrnoException | null, addresses: SoaRecord) => void): void;
+    export function resolve(hostname: string, rrtype: 'SRV', callback: (err: NodeJS.ErrnoException | null, addresses: SrvRecord[]) => void): void;
+    export function resolve(hostname: string, rrtype: 'TXT', callback: (err: NodeJS.ErrnoException | null, addresses: string[][]) => void): void;
+    export function resolve(
+        hostname: string,
+        rrtype: string,
+        callback: (err: NodeJS.ErrnoException | null, addresses: string[] | MxRecord[] | NaptrRecord[] | SoaRecord | SrvRecord[] | string[][] | AnyRecord[]) => void
+    ): void;
+    export namespace resolve {
+        function __promisify__(hostname: string, rrtype?: 'A' | 'AAAA' | 'CNAME' | 'NS' | 'PTR'): Promise<string[]>;
+        function __promisify__(hostname: string, rrtype: 'ANY'): Promise<AnyRecord[]>;
+        function __promisify__(hostname: string, rrtype: 'MX'): Promise<MxRecord[]>;
+        function __promisify__(hostname: string, rrtype: 'NAPTR'): Promise<NaptrRecord[]>;
+        function __promisify__(hostname: string, rrtype: 'SOA'): Promise<SoaRecord>;
+        function __promisify__(hostname: string, rrtype: 'SRV'): Promise<SrvRecord[]>;
+        function __promisify__(hostname: string, rrtype: 'TXT'): Promise<string[][]>;
+        function __promisify__(hostname: string, rrtype: string): Promise<string[] | MxRecord[] | NaptrRecord[] | SoaRecord | SrvRecord[] | string[][] | AnyRecord[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve a IPv4 addresses (`A` records) for the`hostname`. The `addresses` argument passed to the `callback` function
+     * will contain an array of IPv4 addresses (e.g.`['74.125.79.104', '74.125.79.105', '74.125.79.106']`).
+     * @since v0.1.16
+     * @param hostname Host name to resolve.
+     */
+    export function resolve4(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve4(hostname: string, options: ResolveWithTtlOptions, callback: (err: NodeJS.ErrnoException | null, addresses: RecordWithTtl[]) => void): void;
+    export function resolve4(hostname: string, options: ResolveOptions, callback: (err: NodeJS.ErrnoException | null, addresses: string[] | RecordWithTtl[]) => void): void;
+    export namespace resolve4 {
+        function __promisify__(hostname: string): Promise<string[]>;
+        function __promisify__(hostname: string, options: ResolveWithTtlOptions): Promise<RecordWithTtl[]>;
+        function __promisify__(hostname: string, options?: ResolveOptions): Promise<string[] | RecordWithTtl[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve a IPv6 addresses (`AAAA` records) for the`hostname`. The `addresses` argument passed to the `callback` function
+     * will contain an array of IPv6 addresses.
+     * @since v0.1.16
+     * @param hostname Host name to resolve.
+     */
+    export function resolve6(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export function resolve6(hostname: string, options: ResolveWithTtlOptions, callback: (err: NodeJS.ErrnoException | null, addresses: RecordWithTtl[]) => void): void;
+    export function resolve6(hostname: string, options: ResolveOptions, callback: (err: NodeJS.ErrnoException | null, addresses: string[] | RecordWithTtl[]) => void): void;
+    export namespace resolve6 {
+        function __promisify__(hostname: string): Promise<string[]>;
+        function __promisify__(hostname: string, options: ResolveWithTtlOptions): Promise<RecordWithTtl[]>;
+        function __promisify__(hostname: string, options?: ResolveOptions): Promise<string[] | RecordWithTtl[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve `CNAME` records for the `hostname`. The`addresses` argument passed to the `callback` function
+     * will contain an array of canonical name records available for the `hostname`(e.g. `['bar.example.com']`).
+     * @since v0.3.2
+     */
+    export function resolveCname(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export namespace resolveCname {
+        function __promisify__(hostname: string): Promise<string[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve `CAA` records for the `hostname`. The`addresses` argument passed to the `callback` function
+     * will contain an array of certification authority authorization records
+     * available for the `hostname` (e.g. `[{critical: 0, iodef: 'mailto:pki@example.com'}, {critical: 128, issue: 'pki.example.com'}]`).
+     * @since v15.0.0, v14.17.0
+     */
+    export function resolveCaa(hostname: string, callback: (err: NodeJS.ErrnoException | null, records: CaaRecord[]) => void): void;
+    export namespace resolveCaa {
+        function __promisify__(hostname: string): Promise<CaaRecord[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve mail exchange records (`MX` records) for the`hostname`. The `addresses` argument passed to the `callback` function will
+     * contain an array of objects containing both a `priority` and `exchange`property (e.g. `[{priority: 10, exchange: 'mx.example.com'}, ...]`).
+     * @since v0.1.27
+     */
+    export function resolveMx(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: MxRecord[]) => void): void;
+    export namespace resolveMx {
+        function __promisify__(hostname: string): Promise<MxRecord[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve regular expression based records (`NAPTR`records) for the `hostname`. The `addresses` argument passed to the `callback`function will contain an array of
+     * objects with the following properties:
+     *
+     * * `flags`
+     * * `service`
+     * * `regexp`
+     * * `replacement`
+     * * `order`
+     * * `preference`
+     *
+     * ```js
+     * {
+     *   flags: 's',
+     *   service: 'SIP+D2U',
+     *   regexp: '',
+     *   replacement: '_sip._udp.example.com',
+     *   order: 30,
+     *   preference: 100
+     * }
+     * ```
+     * @since v0.9.12
+     */
+    export function resolveNaptr(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: NaptrRecord[]) => void): void;
+    export namespace resolveNaptr {
+        function __promisify__(hostname: string): Promise<NaptrRecord[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve name server records (`NS` records) for the`hostname`. The `addresses` argument passed to the `callback` function will
+     * contain an array of name server records available for `hostname`(e.g. `['ns1.example.com', 'ns2.example.com']`).
+     * @since v0.1.90
+     */
+    export function resolveNs(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export namespace resolveNs {
+        function __promisify__(hostname: string): Promise<string[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve pointer records (`PTR` records) for the`hostname`. The `addresses` argument passed to the `callback` function will
+     * be an array of strings containing the reply records.
+     * @since v6.0.0
+     */
+    export function resolvePtr(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[]) => void): void;
+    export namespace resolvePtr {
+        function __promisify__(hostname: string): Promise<string[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve a start of authority record (`SOA` record) for
+     * the `hostname`. The `address` argument passed to the `callback` function will
+     * be an object with the following properties:
+     *
+     * * `nsname`
+     * * `hostmaster`
+     * * `serial`
+     * * `refresh`
+     * * `retry`
+     * * `expire`
+     * * `minttl`
+     *
+     * ```js
+     * {
+     *   nsname: 'ns.example.com',
+     *   hostmaster: 'root.example.com',
+     *   serial: 2013101809,
+     *   refresh: 10000,
+     *   retry: 2400,
+     *   expire: 604800,
+     *   minttl: 3600
+     * }
+     * ```
+     * @since v0.11.10
+     */
+    export function resolveSoa(hostname: string, callback: (err: NodeJS.ErrnoException | null, address: SoaRecord) => void): void;
+    export namespace resolveSoa {
+        function __promisify__(hostname: string): Promise<SoaRecord>;
+    }
+    /**
+     * Uses the DNS protocol to resolve service records (`SRV` records) for the`hostname`. The `addresses` argument passed to the `callback` function will
+     * be an array of objects with the following properties:
+     *
+     * * `priority`
+     * * `weight`
+     * * `port`
+     * * `name`
+     *
+     * ```js
+     * {
+     *   priority: 10,
+     *   weight: 5,
+     *   port: 21223,
+     *   name: 'service.example.com'
+     * }
+     * ```
+     * @since v0.1.27
+     */
+    export function resolveSrv(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: SrvRecord[]) => void): void;
+    export namespace resolveSrv {
+        function __promisify__(hostname: string): Promise<SrvRecord[]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve text queries (`TXT` records) for the`hostname`. The `records` argument passed to the `callback` function is a
+     * two-dimensional array of the text records available for `hostname` (e.g.`[ ['v=spf1 ip4:0.0.0.0 ', '~all' ] ]`). Each sub-array contains TXT chunks of
+     * one record. Depending on the use case, these could be either joined together or
+     * treated separately.
+     * @since v0.1.27
+     */
+    export function resolveTxt(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: string[][]) => void): void;
+    export namespace resolveTxt {
+        function __promisify__(hostname: string): Promise<string[][]>;
+    }
+    /**
+     * Uses the DNS protocol to resolve all records (also known as `ANY` or `*` query).
+     * The `ret` argument passed to the `callback` function will be an array containing
+     * various types of records. Each object has a property `type` that indicates the
+     * type of the current record. And depending on the `type`, additional properties
+     * will be present on the object:
+     *
+     * <omitted>
+     *
+     * Here is an example of the `ret` object passed to the callback:
+     *
+     * ```js
+     * [ { type: 'A', address: '127.0.0.1', ttl: 299 },
+     *   { type: 'CNAME', value: 'example.com' },
+     *   { type: 'MX', exchange: 'alt4.aspmx.l.example.com', priority: 50 },
+     *   { type: 'NS', value: 'ns1.example.com' },
+     *   { type: 'TXT', entries: [ 'v=spf1 include:_spf.example.com ~all' ] },
+     *   { type: 'SOA',
+     *     nsname: 'ns1.example.com',
+     *     hostmaster: 'admin.example.com',
+     *     serial: 156696742,
+     *     refresh: 900,
+     *     retry: 900,
+     *     expire: 1800,
+     *     minttl: 60 } ]
+     * ```
+     *
+     * DNS server operators may choose not to respond to `ANY`queries. It may be better to call individual methods like {@link resolve4},{@link resolveMx}, and so on. For more details, see [RFC
+     * 8482](https://tools.ietf.org/html/rfc8482).
+     */
+    export function resolveAny(hostname: string, callback: (err: NodeJS.ErrnoException | null, addresses: AnyRecord[]) => void): void;
+    export namespace resolveAny {
+        function __promisify__(hostname: string): Promise<AnyRecord[]>;
+    }
+    /**
+     * Performs a reverse DNS query that resolves an IPv4 or IPv6 address to an
+     * array of host names.
+     *
+     * On error, `err` is an `Error` object, where `err.code` is
+     * one of the `DNS error codes`.
+     * @since v0.1.16
+     */
+    export function reverse(ip: string, callback: (err: NodeJS.ErrnoException | null, hostnames: string[]) => void): void;
+    /**
+     * Sets the IP address and port of servers to be used when performing DNS
+     * resolution. The `servers` argument is an array of [RFC 5952](https://tools.ietf.org/html/rfc5952#section-6) formatted
+     * addresses. If the port is the IANA default DNS port (53) it can be omitted.
+     *
+     * ```js
+     * dns.setServers([
+     *   '4.4.4.4',
+     *   '[2001:4860:4860::8888]',
+     *   '4.4.4.4:1053',
+     *   '[2001:4860:4860::8888]:1053',
+     * ]);
+     * ```
+     *
+     * An error will be thrown if an invalid address is provided.
+     *
+     * The `dns.setServers()` method must not be called while a DNS query is in
+     * progress.
+     *
+     * The {@link setServers} method affects only {@link resolve},`dns.resolve*()` and {@link reverse} (and specifically _not_ {@link lookup}).
+     *
+     * This method works much like [resolve.conf](https://man7.org/linux/man-pages/man5/resolv.conf.5.html).
+     * That is, if attempting to resolve with the first server provided results in a`NOTFOUND` error, the `resolve()` method will _not_ attempt to resolve with
+     * subsequent servers provided. Fallback DNS servers will only be used if the
+     * earlier ones time out or result in some other error.
+     * @since v0.11.3
+     * @param servers array of `RFC 5952` formatted addresses
+     */
+    export function setServers(servers: ReadonlyArray<string>): void;
+    /**
+     * Returns an array of IP address strings, formatted according to [RFC 5952](https://tools.ietf.org/html/rfc5952#section-6),
+     * that are currently configured for DNS resolution. A string will include a port
+     * section if a custom port is used.
+     *
+     * ```js
+     * [
+     *   '4.4.4.4',
+     *   '2001:4860:4860::8888',
+     *   '4.4.4.4:1053',
+     *   '[2001:4860:4860::8888]:1053',
+     * ]
+     * ```
+     * @since v0.11.3
+     */
+    export function getServers(): string[];
+    /**
+     * Set the default value of `verbatim` in {@link lookup} and `dnsPromises.lookup()`. The value could be:
+     *
+     * * `ipv4first`: sets default `verbatim` `false`.
+     * * `verbatim`: sets default `verbatim` `true`.
+     *
+     * The default is `ipv4first` and {@link setDefaultResultOrder} have higher
+     * priority than `--dns-result-order`. When using `worker threads`,{@link setDefaultResultOrder} from the main thread won't affect the default
+     * dns orders in workers.
+     * @since v16.4.0, v14.18.0
+     * @param order must be `'ipv4first'` or `'verbatim'`.
+     */
+    export function setDefaultResultOrder(order: 'ipv4first' | 'verbatim'): void;
+    // Error codes
+    export const NODATA: string;
+    export const FORMERR: string;
+    export const SERVFAIL: string;
+    export const NOTFOUND: string;
+    export const NOTIMP: string;
+    export const REFUSED: string;
+    export const BADQUERY: string;
+    export const BADNAME: string;
+    export const BADFAMILY: string;
+    export const BADRESP: string;
+    export const CONNREFUSED: string;
+    export const TIMEOUT: string;
+    export const EOF: string;
+    export const FILE: string;
+    export const NOMEM: string;
+    export const DESTRUCTION: string;
+    export const BADSTR: string;
+    export const BADFLAGS: string;
+    export const NONAME: string;
+    export const BADHINTS: string;
+    export const NOTINITIALIZED: string;
+    export const LOADIPHLPAPI: string;
+    export const ADDRGETNETWORKPARAMS: string;
+    export const CANCELLED: string;
+    export interface ResolverOptions {
+        timeout?: number | undefined;
+        /**
+         * @default 4
+         */
+        tries?: number;
+    }
+    /**
+     * An independent resolver for DNS requests.
+     *
+     * Creating a new resolver uses the default server settings. Setting
+     * the servers used for a resolver using `resolver.setServers()` does not affect
+     * other resolvers:
+     *
+     * ```js
+     * const { Resolver } = require('dns');
+     * const resolver = new Resolver();
+     * resolver.setServers(['4.4.4.4']);
+     *
+     * // This request will use the server at 4.4.4.4, independent of global settings.
+     * resolver.resolve4('example.org', (err, addresses) => {
+     *   // ...
+     * });
+     * ```
+     *
+     * The following methods from the `dns` module are available:
+     *
+     * * `resolver.getServers()`
+     * * `resolver.resolve()`
+     * * `resolver.resolve4()`
+     * * `resolver.resolve6()`
+     * * `resolver.resolveAny()`
+     * * `resolver.resolveCaa()`
+     * * `resolver.resolveCname()`
+     * * `resolver.resolveMx()`
+     * * `resolver.resolveNaptr()`
+     * * `resolver.resolveNs()`
+     * * `resolver.resolvePtr()`
+     * * `resolver.resolveSoa()`
+     * * `resolver.resolveSrv()`
+     * * `resolver.resolveTxt()`
+     * * `resolver.reverse()`
+     * * `resolver.setServers()`
+     * @since v8.3.0
+     */
+    export class Resolver {
+        constructor(options?: ResolverOptions);
+        /**
+         * Cancel all outstanding DNS queries made by this resolver. The corresponding
+         * callbacks will be called with an error with code `ECANCELLED`.
+         * @since v8.3.0
+         */
+        cancel(): void;
+        getServers: typeof getServers;
+        resolve: typeof resolve;
+        resolve4: typeof resolve4;
+        resolve6: typeof resolve6;
+        resolveAny: typeof resolveAny;
+        resolveCname: typeof resolveCname;
+        resolveMx: typeof resolveMx;
+        resolveNaptr: typeof resolveNaptr;
+        resolveNs: typeof resolveNs;
+        resolvePtr: typeof resolvePtr;
+        resolveSoa: typeof resolveSoa;
+        resolveSrv: typeof resolveSrv;
+        resolveTxt: typeof resolveTxt;
+        reverse: typeof reverse;
+        /**
+         * The resolver instance will send its requests from the specified IP address.
+         * This allows programs to specify outbound interfaces when used on multi-homed
+         * systems.
+         *
+         * If a v4 or v6 address is not specified, it is set to the default, and the
+         * operating system will choose a local address automatically.
+         *
+         * The resolver will use the v4 local address when making requests to IPv4 DNS
+         * servers, and the v6 local address when making requests to IPv6 DNS servers.
+         * The `rrtype` of resolution requests has no impact on the local address used.
+         * @since v15.1.0, v14.17.0
+         * @param [ipv4='0.0.0.0'] A string representation of an IPv4 address.
+         * @param [ipv6='::0'] A string representation of an IPv6 address.
+         */
+        setLocalAddress(ipv4?: string, ipv6?: string): void;
+        setServers: typeof setServers;
+    }
+    export { dnsPromises as promises };
+}
+declare module 'node:dns' {
+    export * from 'dns';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `dns.promises` API provides an alternative set of asynchronous DNS methods
+ * that return `Promise` objects rather than using callbacks. The API is accessible
+ * via `require('dns').promises` or `require('dns/promises')`.
+ * @since v10.6.0
+ */
+declare module 'dns/promises' {
+    import {
+        LookupAddress,
+        LookupOneOptions,
+        LookupAllOptions,
+        LookupOptions,
+        AnyRecord,
+        CaaRecord,
+        MxRecord,
+        NaptrRecord,
+        SoaRecord,
+        SrvRecord,
+        ResolveWithTtlOptions,
+        RecordWithTtl,
+        ResolveOptions,
+        ResolverOptions,
+    } from 'node:dns';
+    /**
+     * Returns an array of IP address strings, formatted according to [RFC 5952](https://tools.ietf.org/html/rfc5952#section-6),
+     * that are currently configured for DNS resolution. A string will include a port
+     * section if a custom port is used.
+     *
+     * ```js
+     * [
+     *   '4.4.4.4',
+     *   '2001:4860:4860::8888',
+     *   '4.4.4.4:1053',
+     *   '[2001:4860:4860::8888]:1053',
+     * ]
+     * ```
+     * @since v10.6.0
+     */
+    function getServers(): string[];
+    /**
+     * Resolves a host name (e.g. `'nodejs.org'`) into the first found A (IPv4) or
+     * AAAA (IPv6) record. All `option` properties are optional. If `options` is an
+     * integer, then it must be `4` or `6` – if `options` is not provided, then IPv4
+     * and IPv6 addresses are both returned if found.
+     *
+     * With the `all` option set to `true`, the `Promise` is resolved with `addresses`being an array of objects with the properties `address` and `family`.
+     *
+     * On error, the `Promise` is rejected with an `Error` object, where `err.code`is the error code.
+     * Keep in mind that `err.code` will be set to `'ENOTFOUND'` not only when
+     * the host name does not exist but also when the lookup fails in other ways
+     * such as no available file descriptors.
+     *
+     * `dnsPromises.lookup()` does not necessarily have anything to do with the DNS
+     * protocol. The implementation uses an operating system facility that can
+     * associate names with addresses, and vice versa. This implementation can have
+     * subtle but important consequences on the behavior of any Node.js program. Please
+     * take some time to consult the `Implementation considerations section` before
+     * using `dnsPromises.lookup()`.
+     *
+     * Example usage:
+     *
+     * ```js
+     * const dns = require('dns');
+     * const dnsPromises = dns.promises;
+     * const options = {
+     *   family: 6,
+     *   hints: dns.ADDRCONFIG | dns.V4MAPPED,
+     * };
+     *
+     * dnsPromises.lookup('example.com', options).then((result) => {
+     *   console.log('address: %j family: IPv%s', result.address, result.family);
+     *   // address: ""2606:2800:220:1:248:1893:25c8:1946"" family: IPv6
+     * });
+     *
+     * // When options.all is true, the result will be an Array.
+     * options.all = true;
+     * dnsPromises.lookup('example.com', options).then((result) => {
+     *   console.log('addresses: %j', result);
+     *   // addresses: [{""address"":""2606:2800:220:1:248:1893:25c8:1946"",""family"":6}]
+     * });
+     * ```
+     * @since v10.6.0
+     */
+    function lookup(hostname: string, family: number): Promise<LookupAddress>;
+    function lookup(hostname: string, options: LookupOneOptions): Promise<LookupAddress>;
+    function lookup(hostname: string, options: LookupAllOptions): Promise<LookupAddress[]>;
+    function lookup(hostname: string, options: LookupOptions): Promise<LookupAddress | LookupAddress[]>;
+    function lookup(hostname: string): Promise<LookupAddress>;
+    /**
+     * Resolves the given `address` and `port` into a host name and service using
+     * the operating system's underlying `getnameinfo` implementation.
+     *
+     * If `address` is not a valid IP address, a `TypeError` will be thrown.
+     * The `port` will be coerced to a number. If it is not a legal port, a `TypeError`will be thrown.
+     *
+     * On error, the `Promise` is rejected with an `Error` object, where `err.code`is the error code.
+     *
+     * ```js
+     * const dnsPromises = require('dns').promises;
+     * dnsPromises.lookupService('127.0.0.1', 22).then((result) => {
+     *   console.log(result.hostname, result.service);
+     *   // Prints: localhost ssh
+     * });
+     * ```
+     * @since v10.6.0
+     */
+    function lookupService(
+        address: string,
+        port: number
+    ): Promise<{
+        hostname: string;
+        service: string;
+    }>;
+    /**
+     * Uses the DNS protocol to resolve a host name (e.g. `'nodejs.org'`) into an array
+     * of the resource records. When successful, the `Promise` is resolved with an
+     * array of resource records. The type and structure of individual results vary
+     * based on `rrtype`:
+     *
+     * <omitted>
+     *
+     * On error, the `Promise` is rejected with an `Error` object, where `err.code`is one of the DNS error codes.
+     * @since v10.6.0
+     * @param hostname Host name to resolve.
+     * @param [rrtype='A'] Resource record type.
+     */
+    function resolve(hostname: string): Promise<string[]>;
+    function resolve(hostname: string, rrtype: 'A'): Promise<string[]>;
+    function resolve(hostname: string, rrtype: 'AAAA'): Promise<string[]>;
+    function resolve(hostname: string, rrtype: 'ANY'): Promise<AnyRecord[]>;
+    function resolve(hostname: string, rrtype: 'CAA'): Promise<CaaRecord[]>;
+    function resolve(hostname: string, rrtype: 'CNAME'): Promise<string[]>;
+    function resolve(hostname: string, rrtype: 'MX'): Promise<MxRecord[]>;
+    function resolve(hostname: string, rrtype: 'NAPTR'): Promise<NaptrRecord[]>;
+    function resolve(hostname: string, rrtype: 'NS'): Promise<string[]>;
+    function resolve(hostname: string, rrtype: 'PTR'): Promise<string[]>;
+    function resolve(hostname: string, rrtype: 'SOA'): Promise<SoaRecord>;
+    function resolve(hostname: string, rrtype: 'SRV'): Promise<SrvRecord[]>;
+    function resolve(hostname: string, rrtype: 'TXT'): Promise<string[][]>;
+    function resolve(hostname: string, rrtype: string): Promise<string[] | MxRecord[] | NaptrRecord[] | SoaRecord | SrvRecord[] | string[][] | AnyRecord[]>;
+    /**
+     * Uses the DNS protocol to resolve IPv4 addresses (`A` records) for the`hostname`. On success, the `Promise` is resolved with an array of IPv4
+     * addresses (e.g. `['74.125.79.104', '74.125.79.105', '74.125.79.106']`).
+     * @since v10.6.0
+     * @param hostname Host name to resolve.
+     */
+    function resolve4(hostname: string): Promise<string[]>;
+    function resolve4(hostname: string, options: ResolveWithTtlOptions): Promise<RecordWithTtl[]>;
+    function resolve4(hostname: string, options: ResolveOptions): Promise<string[] | RecordWithTtl[]>;
+    /**
+     * Uses the DNS protocol to resolve IPv6 addresses (`AAAA` records) for the`hostname`. On success, the `Promise` is resolved with an array of IPv6
+     * addresses.
+     * @since v10.6.0
+     * @param hostname Host name to resolve.
+     */
+    function resolve6(hostname: string): Promise<string[]>;
+    function resolve6(hostname: string, options: ResolveWithTtlOptions): Promise<RecordWithTtl[]>;
+    function resolve6(hostname: string, options: ResolveOptions): Promise<string[] | RecordWithTtl[]>;
+    /**
+     * Uses the DNS protocol to resolve all records (also known as `ANY` or `*` query).
+     * On success, the `Promise` is resolved with an array containing various types of
+     * records. Each object has a property `type` that indicates the type of the
+     * current record. And depending on the `type`, additional properties will be
+     * present on the object:
+     *
+     * <omitted>
+     *
+     * Here is an example of the result object:
+     *
+     * ```js
+     * [ { type: 'A', address: '127.0.0.1', ttl: 299 },
+     *   { type: 'CNAME', value: 'example.com' },
+     *   { type: 'MX', exchange: 'alt4.aspmx.l.example.com', priority: 50 },
+     *   { type: 'NS', value: 'ns1.example.com' },
+     *   { type: 'TXT', entries: [ 'v=spf1 include:_spf.example.com ~all' ] },
+     *   { type: 'SOA',
+     *     nsname: 'ns1.example.com',
+     *     hostmaster: 'admin.example.com',
+     *     serial: 156696742,
+     *     refresh: 900,
+     *     retry: 900,
+     *     expire: 1800,
+     *     minttl: 60 } ]
+     * ```
+     * @since v10.6.0
+     */
+    function resolveAny(hostname: string): Promise<AnyRecord[]>;
+    /**
+     * Uses the DNS protocol to resolve `CAA` records for the `hostname`. On success,
+     * the `Promise` is resolved with an array of objects containing available
+     * certification authority authorization records available for the `hostname`(e.g. `[{critical: 0, iodef: 'mailto:pki@example.com'},{critical: 128, issue: 'pki.example.com'}]`).
+     * @since v15.0.0, v14.17.0
+     */
+    function resolveCaa(hostname: string): Promise<CaaRecord[]>;
+    /**
+     * Uses the DNS protocol to resolve `CNAME` records for the `hostname`. On success,
+     * the `Promise` is resolved with an array of canonical name records available for
+     * the `hostname` (e.g. `['bar.example.com']`).
+     * @since v10.6.0
+     */
+    function resolveCname(hostname: string): Promise<string[]>;
+    /**
+     * Uses the DNS protocol to resolve mail exchange records (`MX` records) for the`hostname`. On success, the `Promise` is resolved with an array of objects
+     * containing both a `priority` and `exchange` property (e.g.`[{priority: 10, exchange: 'mx.example.com'}, ...]`).
+     * @since v10.6.0
+     */
+    function resolveMx(hostname: string): Promise<MxRecord[]>;
+    /**
+     * Uses the DNS protocol to resolve regular expression based records (`NAPTR`records) for the `hostname`. On success, the `Promise` is resolved with an array
+     * of objects with the following properties:
+     *
+     * * `flags`
+     * * `service`
+     * * `regexp`
+     * * `replacement`
+     * * `order`
+     * * `preference`
+     *
+     * ```js
+     * {
+     *   flags: 's',
+     *   service: 'SIP+D2U',
+     *   regexp: '',
+     *   replacement: '_sip._udp.example.com',
+     *   order: 30,
+     *   preference: 100
+     * }
+     * ```
+     * @since v10.6.0
+     */
+    function resolveNaptr(hostname: string): Promise<NaptrRecord[]>;
+    /**
+     * Uses the DNS protocol to resolve name server records (`NS` records) for the`hostname`. On success, the `Promise` is resolved with an array of name server
+     * records available for `hostname` (e.g.`['ns1.example.com', 'ns2.example.com']`).
+     * @since v10.6.0
+     */
+    function resolveNs(hostname: string): Promise<string[]>;
+    /**
+     * Uses the DNS protocol to resolve pointer records (`PTR` records) for the`hostname`. On success, the `Promise` is resolved with an array of strings
+     * containing the reply records.
+     * @since v10.6.0
+     */
+    function resolvePtr(hostname: string): Promise<string[]>;
+    /**
+     * Uses the DNS protocol to resolve a start of authority record (`SOA` record) for
+     * the `hostname`. On success, the `Promise` is resolved with an object with the
+     * following properties:
+     *
+     * * `nsname`
+     * * `hostmaster`
+     * * `serial`
+     * * `refresh`
+     * * `retry`
+     * * `expire`
+     * * `minttl`
+     *
+     * ```js
+     * {
+     *   nsname: 'ns.example.com',
+     *   hostmaster: 'root.example.com',
+     *   serial: 2013101809,
+     *   refresh: 10000,
+     *   retry: 2400,
+     *   expire: 604800,
+     *   minttl: 3600
+     * }
+     * ```
+     * @since v10.6.0
+     */
+    function resolveSoa(hostname: string): Promise<SoaRecord>;
+    /**
+     * Uses the DNS protocol to resolve service records (`SRV` records) for the`hostname`. On success, the `Promise` is resolved with an array of objects with
+     * the following properties:
+     *
+     * * `priority`
+     * * `weight`
+     * * `port`
+     * * `name`
+     *
+     * ```js
+     * {
+     *   priority: 10,
+     *   weight: 5,
+     *   port: 21223,
+     *   name: 'service.example.com'
+     * }
+     * ```
+     * @since v10.6.0
+     */
+    function resolveSrv(hostname: string): Promise<SrvRecord[]>;
+    /**
+     * Uses the DNS protocol to resolve text queries (`TXT` records) for the`hostname`. On success, the `Promise` is resolved with a two-dimensional array
+     * of the text records available for `hostname` (e.g.`[ ['v=spf1 ip4:0.0.0.0 ', '~all' ] ]`). Each sub-array contains TXT chunks of
+     * one record. Depending on the use case, these could be either joined together or
+     * treated separately.
+     * @since v10.6.0
+     */
+    function resolveTxt(hostname: string): Promise<string[][]>;
+    /**
+     * Performs a reverse DNS query that resolves an IPv4 or IPv6 address to an
+     * array of host names.
+     *
+     * On error, the `Promise` is rejected with an `Error` object, where `err.code`is one of the DNS error codes.
+     * @since v10.6.0
+     */
+    function reverse(ip: string): Promise<string[]>;
+    /**
+     * Sets the IP address and port of servers to be used when performing DNS
+     * resolution. The `servers` argument is an array of [RFC 5952](https://tools.ietf.org/html/rfc5952#section-6) formatted
+     * addresses. If the port is the IANA default DNS port (53) it can be omitted.
+     *
+     * ```js
+     * dnsPromises.setServers([
+     *   '4.4.4.4',
+     *   '[2001:4860:4860::8888]',
+     *   '4.4.4.4:1053',
+     *   '[2001:4860:4860::8888]:1053',
+     * ]);
+     * ```
+     *
+     * An error will be thrown if an invalid address is provided.
+     *
+     * The `dnsPromises.setServers()` method must not be called while a DNS query is in
+     * progress.
+     *
+     * This method works much like [resolve.conf](https://man7.org/linux/man-pages/man5/resolv.conf.5.html).
+     * That is, if attempting to resolve with the first server provided results in a`NOTFOUND` error, the `resolve()` method will _not_ attempt to resolve with
+     * subsequent servers provided. Fallback DNS servers will only be used if the
+     * earlier ones time out or result in some other error.
+     * @since v10.6.0
+     * @param servers array of `RFC 5952` formatted addresses
+     */
+    function setServers(servers: ReadonlyArray<string>): void;
+    /**
+     * Set the default value of `verbatim` in `dns.lookup()` and `dnsPromises.lookup()`. The value could be:
+     *
+     * * `ipv4first`: sets default `verbatim` `false`.
+     * * `verbatim`: sets default `verbatim` `true`.
+     *
+     * The default is `ipv4first` and `dnsPromises.setDefaultResultOrder()` have
+     * higher priority than `--dns-result-order`. When using `worker threads`,`dnsPromises.setDefaultResultOrder()` from the main thread won't affect the
+     * default dns orders in workers.
+     * @since v16.4.0, v14.18.0
+     * @param order must be `'ipv4first'` or `'verbatim'`.
+     */
+    function setDefaultResultOrder(order: 'ipv4first' | 'verbatim'): void;
+    class Resolver {
+        constructor(options?: ResolverOptions);
+        cancel(): void;
+        getServers: typeof getServers;
+        resolve: typeof resolve;
+        resolve4: typeof resolve4;
+        resolve6: typeof resolve6;
+        resolveAny: typeof resolveAny;
+        resolveCname: typeof resolveCname;
+        resolveMx: typeof resolveMx;
+        resolveNaptr: typeof resolveNaptr;
+        resolveNs: typeof resolveNs;
+        resolvePtr: typeof resolvePtr;
+        resolveSoa: typeof resolveSoa;
+        resolveSrv: typeof resolveSrv;
+        resolveTxt: typeof resolveTxt;
+        reverse: typeof reverse;
+        setLocalAddress(ipv4?: string, ipv6?: string): void;
+        setServers: typeof setServers;
+    }
+}
+declare module 'node:dns/promises' {
+    export * from 'dns/promises';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * **This module is pending deprecation.** Once a replacement API has been
+ * finalized, this module will be fully deprecated. Most developers should**not** have cause to use this module. Users who absolutely must have
+ * the functionality that domains provide may rely on it for the time being
+ * but should expect to have to migrate to a different solution
+ * in the future.
+ *
+ * Domains provide a way to handle multiple different IO operations as a
+ * single group. If any of the event emitters or callbacks registered to a
+ * domain emit an `'error'` event, or throw an error, then the domain object
+ * will be notified, rather than losing the context of the error in the`process.on('uncaughtException')` handler, or causing the program to
+ * exit immediately with an error code.
+ * @deprecated Since v1.4.2 - Deprecated
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/domain.js)
+ */
+declare module 'domain' {
+    import EventEmitter = require('node:events');
+    /**
+     * The `Domain` class encapsulates the functionality of routing errors and
+     * uncaught exceptions to the active `Domain` object.
+     *
+     * To handle the errors that it catches, listen to its `'error'` event.
+     */
+    class Domain extends EventEmitter {
+        /**
+         * An array of timers and event emitters that have been explicitly added
+         * to the domain.
+         */
+        members: Array<EventEmitter | NodeJS.Timer>;
+        /**
+         * The `enter()` method is plumbing used by the `run()`, `bind()`, and`intercept()` methods to set the active domain. It sets `domain.active` and`process.domain` to the domain, and implicitly
+         * pushes the domain onto the domain
+         * stack managed by the domain module (see {@link exit} for details on the
+         * domain stack). The call to `enter()` delimits the beginning of a chain of
+         * asynchronous calls and I/O operations bound to a domain.
+         *
+         * Calling `enter()` changes only the active domain, and does not alter the domain
+         * itself. `enter()` and `exit()` can be called an arbitrary number of times on a
+         * single domain.
+         */
+        enter(): void;
+        /**
+         * The `exit()` method exits the current domain, popping it off the domain stack.
+         * Any time execution is going to switch to the context of a different chain of
+         * asynchronous calls, it's important to ensure that the current domain is exited.
+         * The call to `exit()` delimits either the end of or an interruption to the chain
+         * of asynchronous calls and I/O operations bound to a domain.
+         *
+         * If there are multiple, nested domains bound to the current execution context,`exit()` will exit any domains nested within this domain.
+         *
+         * Calling `exit()` changes only the active domain, and does not alter the domain
+         * itself. `enter()` and `exit()` can be called an arbitrary number of times on a
+         * single domain.
+         */
+        exit(): void;
+        /**
+         * Run the supplied function in the context of the domain, implicitly
+         * binding all event emitters, timers, and lowlevel requests that are
+         * created in that context. Optionally, arguments can be passed to
+         * the function.
+         *
+         * This is the most basic way to use a domain.
+         *
+         * ```js
+         * const domain = require('domain');
+         * const fs = require('fs');
+         * const d = domain.create();
+         * d.on('error', (er) => {
+         *   console.error('Caught error!', er);
+         * });
+         * d.run(() => {
+         *   process.nextTick(() => {
+         *     setTimeout(() => { // Simulating some various async stuff
+         *       fs.open('non-existent file', 'r', (er, fd) => {
+         *         if (er) throw er;
+         *         // proceed...
+         *       });
+         *     }, 100);
+         *   });
+         * });
+         * ```
+         *
+         * In this example, the `d.on('error')` handler will be triggered, rather
+         * than crashing the program.
+         */
+        run<T>(fn: (...args: any[]) => T, ...args: any[]): T;
+        /**
+         * Explicitly adds an emitter to the domain. If any event handlers called by
+         * the emitter throw an error, or if the emitter emits an `'error'` event, it
+         * will be routed to the domain's `'error'` event, just like with implicit
+         * binding.
+         *
+         * This also works with timers that are returned from `setInterval()` and `setTimeout()`. If their callback function throws, it will be caught by
+         * the domain `'error'` handler.
+         *
+         * If the Timer or `EventEmitter` was already bound to a domain, it is removed
+         * from that one, and bound to this one instead.
+         * @param emitter emitter or timer to be added to the domain
+         */
+        add(emitter: EventEmitter | NodeJS.Timer): void;
+        /**
+         * The opposite of {@link add}. Removes domain handling from the
+         * specified emitter.
+         * @param emitter emitter or timer to be removed from the domain
+         */
+        remove(emitter: EventEmitter | NodeJS.Timer): void;
+        /**
+         * The returned function will be a wrapper around the supplied callback
+         * function. When the returned function is called, any errors that are
+         * thrown will be routed to the domain's `'error'` event.
+         *
+         * ```js
+         * const d = domain.create();
+         *
+         * function readSomeFile(filename, cb) {
+         *   fs.readFile(filename, 'utf8', d.bind((er, data) => {
+         *     // If this throws, it will also be passed to the domain.
+         *     return cb(er, data ? JSON.parse(data) : null);
+         *   }));
+         * }
+         *
+         * d.on('error', (er) => {
+         *   // An error occurred somewhere. If we throw it now, it will crash the program
+         *   // with the normal line number and stack message.
+         * });
+         * ```
+         * @param callback The callback function
+         * @return The bound function
+         */
+        bind<T extends Function>(callback: T): T;
+        /**
+         * This method is almost identical to {@link bind}. However, in
+         * addition to catching thrown errors, it will also intercept `Error` objects sent as the first argument to the function.
+         *
+         * In this way, the common `if (err) return callback(err);` pattern can be replaced
+         * with a single error handler in a single place.
+         *
+         * ```js
+         * const d = domain.create();
+         *
+         * function readSomeFile(filename, cb) {
+         *   fs.readFile(filename, 'utf8', d.intercept((data) => {
+         *     // Note, the first argument is never passed to the
+         *     // callback since it is assumed to be the 'Error' argument
+         *     // and thus intercepted by the domain.
+         *
+         *     // If this throws, it will also be passed to the domain
+         *     // so the error-handling logic can be moved to the 'error'
+         *     // event on the domain instead of being repeated throughout
+         *     // the program.
+         *     return cb(null, JSON.parse(data));
+         *   }));
+         * }
+         *
+         * d.on('error', (er) => {
+         *   // An error occurred somewhere. If we throw it now, it will crash the program
+         *   // with the normal line number and stack message.
+         * });
+         * ```
+         * @param callback The callback function
+         * @return The intercepted function
+         */
+        intercept<T extends Function>(callback: T): T;
+    }
+    function create(): Domain;
+}
+declare module 'node:domain' {
+    export * from 'domain';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * Much of the Node.js core API is built around an idiomatic asynchronous
+ * event-driven architecture in which certain kinds of objects (called ""emitters"")
+ * emit named events that cause `Function` objects (""listeners"") to be called.
+ *
+ * For instance: a `net.Server` object emits an event each time a peer
+ * connects to it; a `fs.ReadStream` emits an event when the file is opened;
+ * a `stream` emits an event whenever data is available to be read.
+ *
+ * All objects that emit events are instances of the `EventEmitter` class. These
+ * objects expose an `eventEmitter.on()` function that allows one or more
+ * functions to be attached to named events emitted by the object. Typically,
+ * event names are camel-cased strings but any valid JavaScript property key
+ * can be used.
+ *
+ * When the `EventEmitter` object emits an event, all of the functions attached
+ * to that specific event are called _synchronously_. Any values returned by the
+ * called listeners are _ignored_ and discarded.
+ *
+ * The following example shows a simple `EventEmitter` instance with a single
+ * listener. The `eventEmitter.on()` method is used to register listeners, while
+ * the `eventEmitter.emit()` method is used to trigger the event.
+ *
+ * ```js
+ * const EventEmitter = require('events');
+ *
+ * class MyEmitter extends EventEmitter {}
+ *
+ * const myEmitter = new MyEmitter();
+ * myEmitter.on('event', () => {
+ *   console.log('an event occurred!');
+ * });
+ * myEmitter.emit('event');
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/events.js)
+ */
+declare module 'events' {
+    interface EventEmitterOptions {
+        /**
+         * Enables automatic capturing of promise rejection.
+         */
+        captureRejections?: boolean | undefined;
+    }
+    interface NodeEventTarget {
+        once(eventName: string | symbol, listener: (...args: any[]) => void): this;
+    }
+    interface DOMEventTarget {
+        addEventListener(
+            eventName: string,
+            listener: (...args: any[]) => void,
+            opts?: {
+                once: boolean;
+            },
+        ): any;
+    }
+    interface StaticEventEmitterOptions {
+        signal?: AbortSignal | undefined;
+    }
+    interface EventEmitter extends NodeJS.EventEmitter {}
+    /**
+     * The `EventEmitter` class is defined and exposed by the `events` module:
+     *
+     * ```js
+     * const EventEmitter = require('events');
+     * ```
+     *
+     * All `EventEmitter`s emit the event `'newListener'` when new listeners are
+     * added and `'removeListener'` when existing listeners are removed.
+     *
+     * It supports the following option:
+     * @since v0.1.26
+     */
+    class EventEmitter {
+        constructor(options?: EventEmitterOptions);
+        /**
+         * Creates a `Promise` that is fulfilled when the `EventEmitter` emits the given
+         * event or that is rejected if the `EventEmitter` emits `'error'` while waiting.
+         * The `Promise` will resolve with an array of all the arguments emitted to the
+         * given event.
+         *
+         * This method is intentionally generic and works with the web platform [EventTarget](https://dom.spec.whatwg.org/#interface-eventtarget) interface, which has no special`'error'` event
+         * semantics and does not listen to the `'error'` event.
+         *
+         * ```js
+         * const { once, EventEmitter } = require('events');
+         *
+         * async function run() {
+         *   const ee = new EventEmitter();
+         *
+         *   process.nextTick(() => {
+         *     ee.emit('myevent', 42);
+         *   });
+         *
+         *   const [value] = await once(ee, 'myevent');
+         *   console.log(value);
+         *
+         *   const err = new Error('kaboom');
+         *   process.nextTick(() => {
+         *     ee.emit('error', err);
+         *   });
+         *
+         *   try {
+         *     await once(ee, 'myevent');
+         *   } catch (err) {
+         *     console.log('error happened', err);
+         *   }
+         * }
+         *
+         * run();
+         * ```
+         *
+         * The special handling of the `'error'` event is only used when `events.once()`is used to wait for another event. If `events.once()` is used to wait for the
+         * '`error'` event itself, then it is treated as any other kind of event without
+         * special handling:
+         *
+         * ```js
+         * const { EventEmitter, once } = require('events');
+         *
+         * const ee = new EventEmitter();
+         *
+         * once(ee, 'error')
+         *   .then(([err]) => console.log('ok', err.message))
+         *   .catch((err) => console.log('error', err.message));
+         *
+         * ee.emit('error', new Error('boom'));
+         *
+         * // Prints: ok boom
+         * ```
+         *
+         * An `AbortSignal` can be used to cancel waiting for the event:
+         *
+         * ```js
+         * const { EventEmitter, once } = require('events');
+         *
+         * const ee = new EventEmitter();
+         * const ac = new AbortController();
+         *
+         * async function foo(emitter, event, signal) {
+         *   try {
+         *     await once(emitter, event, { signal });
+         *     console.log('event emitted!');
+         *   } catch (error) {
+         *     if (error.name === 'AbortError') {
+         *       console.error('Waiting for the event was canceled!');
+         *     } else {
+         *       console.error('There was an error', error.message);
+         *     }
+         *   }
+         * }
+         *
+         * foo(ee, 'foo', ac.signal);
+         * ac.abort(); // Abort waiting for the event
+         * ee.emit('foo'); // Prints: Waiting for the event was canceled!
+         * ```
+         * @since v11.13.0, v10.16.0
+         */
+        static once(
+            emitter: NodeEventTarget,
+            eventName: string | symbol,
+            options?: StaticEventEmitterOptions,
+        ): Promise<any[]>;
+        static once(emitter: DOMEventTarget, eventName: string, options?: StaticEventEmitterOptions): Promise<any[]>;
+        /**
+         * ```js
+         * const { on, EventEmitter } = require('events');
+         *
+         * (async () => {
+         *   const ee = new EventEmitter();
+         *
+         *   // Emit later on
+         *   process.nextTick(() => {
+         *     ee.emit('foo', 'bar');
+         *     ee.emit('foo', 42);
+         *   });
+         *
+         *   for await (const event of on(ee, 'foo')) {
+         *     // The execution of this inner block is synchronous and it
+         *     // processes one event at a time (even with await). Do not use
+         *     // if concurrent execution is required.
+         *     console.log(event); // prints ['bar'] [42]
+         *   }
+         *   // Unreachable here
+         * })();
+         * ```
+         *
+         * Returns an `AsyncIterator` that iterates `eventName` events. It will throw
+         * if the `EventEmitter` emits `'error'`. It removes all listeners when
+         * exiting the loop. The `value` returned by each iteration is an array
+         * composed of the emitted event arguments.
+         *
+         * An `AbortSignal` can be used to cancel waiting on events:
+         *
+         * ```js
+         * const { on, EventEmitter } = require('events');
+         * const ac = new AbortController();
+         *
+         * (async () => {
+         *   const ee = new EventEmitter();
+         *
+         *   // Emit later on
+         *   process.nextTick(() => {
+         *     ee.emit('foo', 'bar');
+         *     ee.emit('foo', 42);
+         *   });
+         *
+         *   for await (const event of on(ee, 'foo', { signal: ac.signal })) {
+         *     // The execution of this inner block is synchronous and it
+         *     // processes one event at a time (even with await). Do not use
+         *     // if concurrent execution is required.
+         *     console.log(event); // prints ['bar'] [42]
+         *   }
+         *   // Unreachable here
+         * })();
+         *
+         * process.nextTick(() => ac.abort());
+         * ```
+         * @since v13.6.0, v12.16.0
+         * @param eventName The name of the event being listened for
+         * @return that iterates `eventName` events emitted by the `emitter`
+         */
+        static on(
+            emitter: NodeJS.EventEmitter,
+            eventName: string,
+            options?: StaticEventEmitterOptions,
+        ): AsyncIterableIterator<any>;
+        /**
+         * A class method that returns the number of listeners for the given `eventName`registered on the given `emitter`.
+         *
+         * ```js
+         * const { EventEmitter, listenerCount } = require('events');
+         * const myEmitter = new EventEmitter();
+         * myEmitter.on('event', () => {});
+         * myEmitter.on('event', () => {});
+         * console.log(listenerCount(myEmitter, 'event'));
+         * // Prints: 2
+         * ```
+         * @since v0.9.12
+         * @deprecated Since v3.2.0 - Use `listenerCount` instead.
+         * @param emitter The emitter to query
+         * @param eventName The event name
+         */
+        static listenerCount(emitter: NodeJS.EventEmitter, eventName: string | symbol): number;
+        /**
+         * Returns a copy of the array of listeners for the event named `eventName`.
+         *
+         * For `EventEmitter`s this behaves exactly the same as calling `.listeners` on
+         * the emitter.
+         *
+         * For `EventTarget`s this is the only way to get the event listeners for the
+         * event target. This is useful for debugging and diagnostic purposes.
+         *
+         * ```js
+         * const { getEventListeners, EventEmitter } = require('events');
+         *
+         * {
+         *   const ee = new EventEmitter();
+         *   const listener = () => console.log('Events are fun');
+         *   ee.on('foo', listener);
+         *   getEventListeners(ee, 'foo'); // [listener]
+         * }
+         * {
+         *   const et = new EventTarget();
+         *   const listener = () => console.log('Events are fun');
+         *   et.addEventListener('foo', listener);
+         *   getEventListeners(et, 'foo'); // [listener]
+         * }
+         * ```
+         * @since v15.2.0, v14.17.0
+         */
+        static getEventListeners(emitter: DOMEventTarget | NodeJS.EventEmitter, name: string | symbol): Function[];
+        /**
+         * By default `EventEmitter`s will print a warning if more than `10` listeners are
+         * added for a particular event. This is a useful default that helps finding
+         * memory leaks. The `EventEmitter.setMaxListeners()` method allows the default limit to be
+         * modified (if eventTargets is empty) or modify the limit specified in every `EventTarget` | `EventEmitter` passed as arguments.
+         * The value can be set to`Infinity` (or `0`) to indicate an unlimited number of listeners.
+         *
+         * ```js
+         * EventEmitter.setMaxListeners(20);
+         * // Equivalent to
+         * EventEmitter.defaultMaxListeners = 20;
+         *
+         * const eventTarget = new EventTarget();
+         * // Only way to increase limit for `EventTarget` instances
+         * // as these doesn't expose its own `setMaxListeners` method
+         * EventEmitter.setMaxListeners(20, eventTarget);
+         * ```
+         * @since v15.3.0, v14.17.0
+         */
+        static setMaxListeners(n?: number, ...eventTargets: Array<DOMEventTarget | NodeJS.EventEmitter>): void;
+        /**
+         * This symbol shall be used to install a listener for only monitoring `'error'`
+         * events. Listeners installed using this symbol are called before the regular
+         * `'error'` listeners are called.
+         *
+         * Installing a listener using this symbol does not change the behavior once an
+         * `'error'` event is emitted, therefore the process will still crash if no
+         * regular `'error'` listener is installed.
+         */
+        static readonly errorMonitor: unique symbol;
+        static readonly captureRejectionSymbol: unique symbol;
+        /**
+         * Sets or gets the default captureRejection value for all emitters.
+         */
+        // TODO: These should be described using static getter/setter pairs:
+        static captureRejections: boolean;
+        static defaultMaxListeners: number;
+    }
+    import internal = require('node:events');
+    namespace EventEmitter {
+        // Should just be `export { EventEmitter }`, but that doesn't work in TypeScript 3.4
+        export { internal as EventEmitter };
+        export interface Abortable {
+            /**
+             * When provided the corresponding `AbortController` can be used to cancel an asynchronous action.
+             */
+            signal?: AbortSignal | undefined;
+        }
+    }
+    global {
+        namespace NodeJS {
+            interface EventEmitter {
+                /**
+                 * Alias for `emitter.on(eventName, listener)`.
+                 * @since v0.1.26
+                 */
+                addListener(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Adds the `listener` function to the end of the listeners array for the
+                 * event named `eventName`. No checks are made to see if the `listener` has
+                 * already been added. Multiple calls passing the same combination of `eventName`and `listener` will result in the `listener` being added, and called, multiple
+                 * times.
+                 *
+                 * ```js
+                 * server.on('connection', (stream) => {
+                 *   console.log('someone connected!');
+                 * });
+                 * ```
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 *
+                 * By default, event listeners are invoked in the order they are added. The`emitter.prependListener()` method can be used as an alternative to add the
+                 * event listener to the beginning of the listeners array.
+                 *
+                 * ```js
+                 * const myEE = new EventEmitter();
+                 * myEE.on('foo', () => console.log('a'));
+                 * myEE.prependListener('foo', () => console.log('b'));
+                 * myEE.emit('foo');
+                 * // Prints:
+                 * //   b
+                 * //   a
+                 * ```
+                 * @since v0.1.101
+                 * @param eventName The name of the event.
+                 * @param listener The callback function
+                 */
+                on(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Adds a **one-time**`listener` function for the event named `eventName`. The
+                 * next time `eventName` is triggered, this listener is removed and then invoked.
+                 *
+                 * ```js
+                 * server.once('connection', (stream) => {
+                 *   console.log('Ah, we have our first user!');
+                 * });
+                 * ```
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 *
+                 * By default, event listeners are invoked in the order they are added. The`emitter.prependOnceListener()` method can be used as an alternative to add the
+                 * event listener to the beginning of the listeners array.
+                 *
+                 * ```js
+                 * const myEE = new EventEmitter();
+                 * myEE.once('foo', () => console.log('a'));
+                 * myEE.prependOnceListener('foo', () => console.log('b'));
+                 * myEE.emit('foo');
+                 * // Prints:
+                 * //   b
+                 * //   a
+                 * ```
+                 * @since v0.3.0
+                 * @param eventName The name of the event.
+                 * @param listener The callback function
+                 */
+                once(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Removes the specified `listener` from the listener array for the event named`eventName`.
+                 *
+                 * ```js
+                 * const callback = (stream) => {
+                 *   console.log('someone connected!');
+                 * };
+                 * server.on('connection', callback);
+                 * // ...
+                 * server.removeListener('connection', callback);
+                 * ```
+                 *
+                 * `removeListener()` will remove, at most, one instance of a listener from the
+                 * listener array. If any single listener has been added multiple times to the
+                 * listener array for the specified `eventName`, then `removeListener()` must be
+                 * called multiple times to remove each instance.
+                 *
+                 * Once an event is emitted, all listeners attached to it at the
+                 * time of emitting are called in order. This implies that any`removeListener()` or `removeAllListeners()` calls _after_ emitting and_before_ the last listener finishes execution will
+                 * not remove them from`emit()` in progress. Subsequent events behave as expected.
+                 *
+                 * ```js
+                 * const myEmitter = new MyEmitter();
+                 *
+                 * const callbackA = () => {
+                 *   console.log('A');
+                 *   myEmitter.removeListener('event', callbackB);
+                 * };
+                 *
+                 * const callbackB = () => {
+                 *   console.log('B');
+                 * };
+                 *
+                 * myEmitter.on('event', callbackA);
+                 *
+                 * myEmitter.on('event', callbackB);
+                 *
+                 * // callbackA removes listener callbackB but it will still be called.
+                 * // Internal listener array at time of emit [callbackA, callbackB]
+                 * myEmitter.emit('event');
+                 * // Prints:
+                 * //   A
+                 * //   B
+                 *
+                 * // callbackB is now removed.
+                 * // Internal listener array [callbackA]
+                 * myEmitter.emit('event');
+                 * // Prints:
+                 * //   A
+                 * ```
+                 *
+                 * Because listeners are managed using an internal array, calling this will
+                 * change the position indices of any listener registered _after_ the listener
+                 * being removed. This will not impact the order in which listeners are called,
+                 * but it means that any copies of the listener array as returned by
+                 * the `emitter.listeners()` method will need to be recreated.
+                 *
+                 * When a single function has been added as a handler multiple times for a single
+                 * event (as in the example below), `removeListener()` will remove the most
+                 * recently added instance. In the example the `once('ping')`listener is removed:
+                 *
+                 * ```js
+                 * const ee = new EventEmitter();
+                 *
+                 * function pong() {
+                 *   console.log('pong');
+                 * }
+                 *
+                 * ee.on('ping', pong);
+                 * ee.once('ping', pong);
+                 * ee.removeListener('ping', pong);
+                 *
+                 * ee.emit('ping');
+                 * ee.emit('ping');
+                 * ```
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 * @since v0.1.26
+                 */
+                removeListener(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Alias for `emitter.removeListener()`.
+                 * @since v10.0.0
+                 */
+                off(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Removes all listeners, or those of the specified `eventName`.
+                 *
+                 * It is bad practice to remove listeners added elsewhere in the code,
+                 * particularly when the `EventEmitter` instance was created by some other
+                 * component or module (e.g. sockets or file streams).
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 * @since v0.1.26
+                 */
+                removeAllListeners(event?: string | symbol): this;
+                /**
+                 * By default `EventEmitter`s will print a warning if more than `10` listeners are
+                 * added for a particular event. This is a useful default that helps finding
+                 * memory leaks. The `emitter.setMaxListeners()` method allows the limit to be
+                 * modified for this specific `EventEmitter` instance. The value can be set to`Infinity` (or `0`) to indicate an unlimited number of listeners.
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 * @since v0.3.5
+                 */
+                setMaxListeners(n: number): this;
+                /**
+                 * Returns the current max listener value for the `EventEmitter` which is either
+                 * set by `emitter.setMaxListeners(n)` or defaults to {@link defaultMaxListeners}.
+                 * @since v1.0.0
+                 */
+                getMaxListeners(): number;
+                /**
+                 * Returns a copy of the array of listeners for the event named `eventName`.
+                 *
+                 * ```js
+                 * server.on('connection', (stream) => {
+                 *   console.log('someone connected!');
+                 * });
+                 * console.log(util.inspect(server.listeners('connection')));
+                 * // Prints: [ [Function] ]
+                 * ```
+                 * @since v0.1.26
+                 */
+                listeners(eventName: string | symbol): Function[];
+                /**
+                 * Returns a copy of the array of listeners for the event named `eventName`,
+                 * including any wrappers (such as those created by `.once()`).
+                 *
+                 * ```js
+                 * const emitter = new EventEmitter();
+                 * emitter.once('log', () => console.log('log once'));
+                 *
+                 * // Returns a new Array with a function `onceWrapper` which has a property
+                 * // `listener` which contains the original listener bound above
+                 * const listeners = emitter.rawListeners('log');
+                 * const logFnWrapper = listeners[0];
+                 *
+                 * // Logs ""log once"" to the console and does not unbind the `once` event
+                 * logFnWrapper.listener();
+                 *
+                 * // Logs ""log once"" to the console and removes the listener
+                 * logFnWrapper();
+                 *
+                 * emitter.on('log', () => console.log('log persistently'));
+                 * // Will return a new Array with a single function bound by `.on()` above
+                 * const newListeners = emitter.rawListeners('log');
+                 *
+                 * // Logs ""log persistently"" twice
+                 * newListeners[0]();
+                 * emitter.emit('log');
+                 * ```
+                 * @since v9.4.0
+                 */
+                rawListeners(eventName: string | symbol): Function[];
+                /**
+                 * Synchronously calls each of the listeners registered for the event named`eventName`, in the order they were registered, passing the supplied arguments
+                 * to each.
+                 *
+                 * Returns `true` if the event had listeners, `false` otherwise.
+                 *
+                 * ```js
+                 * const EventEmitter = require('events');
+                 * const myEmitter = new EventEmitter();
+                 *
+                 * // First listener
+                 * myEmitter.on('event', function firstListener() {
+                 *   console.log('Helloooo! first listener');
+                 * });
+                 * // Second listener
+                 * myEmitter.on('event', function secondListener(arg1, arg2) {
+                 *   console.log(`event with parameters ${arg1}, ${arg2} in second listener`);
+                 * });
+                 * // Third listener
+                 * myEmitter.on('event', function thirdListener(...args) {
+                 *   const parameters = args.join(', ');
+                 *   console.log(`event with parameters ${parameters} in third listener`);
+                 * });
+                 *
+                 * console.log(myEmitter.listeners('event'));
+                 *
+                 * myEmitter.emit('event', 1, 2, 3, 4, 5);
+                 *
+                 * // Prints:
+                 * // [
+                 * //   [Function: firstListener],
+                 * //   [Function: secondListener],
+                 * //   [Function: thirdListener]
+                 * // ]
+                 * // Helloooo! first listener
+                 * // event with parameters 1, 2 in second listener
+                 * // event with parameters 1, 2, 3, 4, 5 in third listener
+                 * ```
+                 * @since v0.1.26
+                 */
+                emit(eventName: string | symbol, ...args: any[]): boolean;
+                /**
+                 * Returns the number of listeners listening to the event named `eventName`.
+                 * @since v3.2.0
+                 * @param eventName The name of the event being listened for
+                 */
+                listenerCount(eventName: string | symbol): number;
+                /**
+                 * Adds the `listener` function to the _beginning_ of the listeners array for the
+                 * event named `eventName`. No checks are made to see if the `listener` has
+                 * already been added. Multiple calls passing the same combination of `eventName`and `listener` will result in the `listener` being added, and called, multiple
+                 * times.
+                 *
+                 * ```js
+                 * server.prependListener('connection', (stream) => {
+                 *   console.log('someone connected!');
+                 * });
+                 * ```
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 * @since v6.0.0
+                 * @param eventName The name of the event.
+                 * @param listener The callback function
+                 */
+                prependListener(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Adds a **one-time**`listener` function for the event named `eventName` to the_beginning_ of the listeners array. The next time `eventName` is triggered, this
+                 * listener is removed, and then invoked.
+                 *
+                 * ```js
+                 * server.prependOnceListener('connection', (stream) => {
+                 *   console.log('Ah, we have our first user!');
+                 * });
+                 * ```
+                 *
+                 * Returns a reference to the `EventEmitter`, so that calls can be chained.
+                 * @since v6.0.0
+                 * @param eventName The name of the event.
+                 * @param listener The callback function
+                 */
+                prependOnceListener(eventName: string | symbol, listener: (...args: any[]) => void): this;
+                /**
+                 * Returns an array listing the events for which the emitter has registered
+                 * listeners. The values in the array are strings or `Symbol`s.
+                 *
+                 * ```js
+                 * const EventEmitter = require('events');
+                 * const myEE = new EventEmitter();
+                 * myEE.on('foo', () => {});
+                 * myEE.on('bar', () => {});
+                 *
+                 * const sym = Symbol('symbol');
+                 * myEE.on(sym, () => {});
+                 *
+                 * console.log(myEE.eventNames());
+                 * // Prints: [ 'foo', 'bar', Symbol(symbol) ]
+                 * ```
+                 * @since v6.0.0
+                 */
+                eventNames(): Array<string | symbol>;
+            }
+        }
+    }
+    export = EventEmitter;
+}
+declare module 'node:events' {
+    import events = require('events');
+    export = events;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `fs/promises` API provides asynchronous file system methods that return
+ * promises.
+ *
+ * The promise APIs use the underlying Node.js threadpool to perform file
+ * system operations off the event loop thread. These operations are not
+ * synchronized or threadsafe. Care must be taken when performing multiple
+ * concurrent modifications on the same file or data corruption may occur.
+ * @since v10.0.0
+ */
+declare module 'fs/promises' {
+    import { Abortable } from 'node:events';
+    import { Stream } from 'node:stream';
+    import {
+        Stats,
+        BigIntStats,
+        StatOptions,
+        WriteVResult,
+        ReadVResult,
+        PathLike,
+        RmDirOptions,
+        RmOptions,
+        MakeDirectoryOptions,
+        Dirent,
+        OpenDirOptions,
+        Dir,
+        ObjectEncodingOptions,
+        BufferEncodingOption,
+        OpenMode,
+        Mode,
+        WatchOptions,
+        WatchEventType,
+        CopyOptions,
+        ReadStream,
+        WriteStream,
+    } from 'node:fs';
+    interface FileChangeInfo<T extends string | Buffer> {
+        eventType: WatchEventType;
+        filename: T;
+    }
+    interface FlagAndOpenMode {
+        mode?: Mode | undefined;
+        flag?: OpenMode | undefined;
+    }
+    interface FileReadResult<T extends NodeJS.ArrayBufferView> {
+        bytesRead: number;
+        buffer: T;
+    }
+    interface FileReadOptions<T extends NodeJS.ArrayBufferView = Buffer> {
+        /**
+         * @default `Buffer.alloc(0xffff)`
+         */
+        buffer?: T;
+        /**
+         * @default 0
+         */
+        offset?: number | null;
+        /**
+         * @default `buffer.byteLength`
+         */
+        length?: number | null;
+        position?: number | null;
+    }
+    interface CreateReadStreamOptions {
+        encoding?: BufferEncoding | null | undefined;
+        autoClose?: boolean | undefined;
+        emitClose?: boolean | undefined;
+        start?: number | undefined;
+        end?: number | undefined;
+        highWaterMark?: number | undefined;
+    }
+    interface CreateWriteStreamOptions {
+        encoding?: BufferEncoding | null | undefined;
+        autoClose?: boolean | undefined;
+        emitClose?: boolean | undefined;
+        start?: number | undefined;
+    }
+    // TODO: Add `EventEmitter` close
+    interface FileHandle {
+        /**
+         * The numeric file descriptor managed by the {FileHandle} object.
+         * @since v10.0.0
+         */
+        readonly fd: number;
+        /**
+         * Alias of `filehandle.writeFile()`.
+         *
+         * When operating on file handles, the mode cannot be changed from what it was set
+         * to with `fsPromises.open()`. Therefore, this is equivalent to `filehandle.writeFile()`.
+         * @since v10.0.0
+         * @return Fulfills with `undefined` upon success.
+         */
+        appendFile(data: string | Uint8Array, options?: (ObjectEncodingOptions & FlagAndOpenMode) | BufferEncoding | null): Promise<void>;
+        /**
+         * Changes the ownership of the file. A wrapper for [`chown(2)`](http://man7.org/linux/man-pages/man2/chown.2.html).
+         * @since v10.0.0
+         * @param uid The file's new owner's user id.
+         * @param gid The file's new group's group id.
+         * @return Fulfills with `undefined` upon success.
+         */
+        chown(uid: number, gid: number): Promise<void>;
+        /**
+         * Modifies the permissions on the file. See [`chmod(2)`](http://man7.org/linux/man-pages/man2/chmod.2.html).
+         * @since v10.0.0
+         * @param mode the file mode bit mask.
+         * @return Fulfills with `undefined` upon success.
+         */
+        chmod(mode: Mode): Promise<void>;
+        /**
+         * Unlike the 16 kb default `highWaterMark` for a `stream.Readable`, the stream
+         * returned by this method has a default `highWaterMark` of 64 kb.
+         *
+         * `options` can include `start` and `end` values to read a range of bytes from
+         * the file instead of the entire file. Both `start` and `end` are inclusive and
+         * start counting at 0, allowed values are in the
+         * \[0, [`Number.MAX_SAFE_INTEGER`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/MAX_SAFE_INTEGER)\] range. If `start` is
+         * omitted or `undefined`, `filehandle.createReadStream()` reads sequentially from
+         * the current file position. The `encoding` can be any one of those accepted by `Buffer`.
+         *
+         * If the `FileHandle` points to a character device that only supports blocking
+         * reads (such as keyboard or sound card), read operations do not finish until data
+         * is available. This can prevent the process from exiting and the stream from
+         * closing naturally.
+         *
+         * By default, the stream will emit a `'close'` event after it has been
+         * destroyed.  Set the `emitClose` option to `false` to change this behavior.
+         *
+         * ```js
+         * import { open } from 'fs/promises';
+         *
+         * const fd = await open('/dev/input/event0');
+         * // Create a stream from some character device.
+         * const stream = fd.createReadStream();
+         * setTimeout(() => {
+         *   stream.close(); // This may not close the stream.
+         *   // Artificially marking end-of-stream, as if the underlying resource had
+         *   // indicated end-of-file by itself, allows the stream to close.
+         *   // This does not cancel pending read operations, and if there is such an
+         *   // operation, the process may still not be able to exit successfully
+         *   // until it finishes.
+         *   stream.push(null);
+         *   stream.read(0);
+         * }, 100);
+         * ```
+         *
+         * If `autoClose` is false, then the file descriptor won't be closed, even if
+         * there's an error. It is the application's responsibility to close it and make
+         * sure there's no file descriptor leak. If `autoClose` is set to true (default
+         * behavior), on `'error'` or `'end'` the file descriptor will be closed
+         * automatically.
+         *
+         * An example to read the last 10 bytes of a file which is 100 bytes long:
+         *
+         * ```js
+         * import { open } from 'fs/promises';
+         *
+         * const fd = await open('sample.txt');
+         * fd.createReadStream({ start: 90, end: 99 });
+         * ```
+         * @since v16.11.0
+         */
+        createReadStream(options?: CreateReadStreamOptions): ReadStream;
+        /**
+         * `options` may also include a `start` option to allow writing data at some
+         * position past the beginning of the file, allowed values are in the
+         * \[0, [`Number.MAX_SAFE_INTEGER`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/MAX_SAFE_INTEGER)\] range. Modifying a file rather than replacing
+         * it may require the `flags` `open` option to be set to `r+` rather than the
+         * default `r`. The `encoding` can be any one of those accepted by `Buffer`.
+         *
+         * If `autoClose` is set to true (default behavior) on `'error'` or `'finish'`the file descriptor will be closed automatically. If `autoClose` is false,
+         * then the file descriptor won't be closed, even if there's an error.
+         * It is the application's responsibility to close it and make sure there's no
+         * file descriptor leak.
+         *
+         * By default, the stream will emit a `'close'` event after it has been
+         * destroyed.  Set the `emitClose` option to `false` to change this behavior.
+         * @since v16.11.0
+         */
+        createWriteStream(options?: CreateWriteStreamOptions): WriteStream;
+        /**
+         * Forces all currently queued I/O operations associated with the file to the
+         * operating system's synchronized I/O completion state. Refer to the POSIX [`fdatasync(2)`](http://man7.org/linux/man-pages/man2/fdatasync.2.html) documentation for details.
+         *
+         * Unlike `filehandle.sync` this method does not flush modified metadata.
+         * @since v10.0.0
+         * @return Fulfills with `undefined` upon success.
+         */
+        datasync(): Promise<void>;
+        /**
+         * Request that all data for the open file descriptor is flushed to the storage
+         * device. The specific implementation is operating system and device specific.
+         * Refer to the POSIX [`fsync(2)`](http://man7.org/linux/man-pages/man2/fsync.2.html) documentation for more detail.
+         * @since v10.0.0
+         * @return Fufills with `undefined` upon success.
+         */
+        sync(): Promise<void>;
+        /**
+         * Reads data from the file and stores that in the given buffer.
+         *
+         * If the file is not modified concurrently, the end-of-file is reached when the
+         * number of bytes read is zero.
+         * @since v10.0.0
+         * @param buffer A buffer that will be filled with the file data read.
+         * @param offset The location in the buffer at which to start filling.
+         * @param length The number of bytes to read.
+         * @param position The location where to begin reading data from the file. If `null`, data will be read from the current file position, and the position will be updated. If `position` is an
+         * integer, the current file position will remain unchanged.
+         * @return Fulfills upon success with an object with two properties:
+         */
+        read<T extends NodeJS.ArrayBufferView>(buffer: T, offset?: number | null, length?: number | null, position?: number | null): Promise<FileReadResult<T>>;
+        read<T extends NodeJS.ArrayBufferView = Buffer>(options?: FileReadOptions<T>): Promise<FileReadResult<T>>;
+        /**
+         * Asynchronously reads the entire contents of a file.
+         *
+         * If `options` is a string, then it specifies the `encoding`.
+         *
+         * The `FileHandle` has to support reading.
+         *
+         * If one or more `filehandle.read()` calls are made on a file handle and then a`filehandle.readFile()` call is made, the data will be read from the current
+         * position till the end of the file. It doesn't always read from the beginning
+         * of the file.
+         * @since v10.0.0
+         * @return Fulfills upon a successful read with the contents of the file. If no encoding is specified (using `options.encoding`), the data is returned as a {Buffer} object. Otherwise, the
+         * data will be a string.
+         */
+        readFile(
+            options?: {
+                encoding?: null | undefined;
+                flag?: OpenMode | undefined;
+            } | null
+        ): Promise<Buffer>;
+        /**
+         * Asynchronously reads the entire contents of a file. The underlying file will _not_ be closed automatically.
+         * The `FileHandle` must have been opened for reading.
+         * @param options An object that may contain an optional flag.
+         * If a flag is not provided, it defaults to `'r'`.
+         */
+        readFile(
+            options:
+                | {
+                      encoding: BufferEncoding;
+                      flag?: OpenMode | undefined;
+                  }
+                | BufferEncoding
+        ): Promise<string>;
+        /**
+         * Asynchronously reads the entire contents of a file. The underlying file will _not_ be closed automatically.
+         * The `FileHandle` must have been opened for reading.
+         * @param options An object that may contain an optional flag.
+         * If a flag is not provided, it defaults to `'r'`.
+         */
+        readFile(
+            options?:
+                | (ObjectEncodingOptions & {
+                      flag?: OpenMode | undefined;
+                  })
+                | BufferEncoding
+                | null
+        ): Promise<string | Buffer>;
+        /**
+         * @since v10.0.0
+         * @return Fulfills with an {fs.Stats} for the file.
+         */
+        stat(
+            opts?: StatOptions & {
+                bigint?: false | undefined;
+            }
+        ): Promise<Stats>;
+        stat(
+            opts: StatOptions & {
+                bigint: true;
+            }
+        ): Promise<BigIntStats>;
+        stat(opts?: StatOptions): Promise<Stats | BigIntStats>;
+        /**
+         * Truncates the file.
+         *
+         * If the file was larger than `len` bytes, only the first `len` bytes will be
+         * retained in the file.
+         *
+         * The following example retains only the first four bytes of the file:
+         *
+         * ```js
+         * import { open } from 'fs/promises';
+         *
+         * let filehandle = null;
+         * try {
+         *   filehandle = await open('temp.txt', 'r+');
+         *   await filehandle.truncate(4);
+         * } finally {
+         *   await filehandle?.close();
+         * }
+         * ```
+         *
+         * If the file previously was shorter than `len` bytes, it is extended, and the
+         * extended part is filled with null bytes (`'\0'`):
+         *
+         * If `len` is negative then `0` will be used.
+         * @since v10.0.0
+         * @param [len=0]
+         * @return Fulfills with `undefined` upon success.
+         */
+        truncate(len?: number): Promise<void>;
+        /**
+         * Change the file system timestamps of the object referenced by the `FileHandle` then resolves the promise with no arguments upon success.
+         * @since v10.0.0
+         */
+        utimes(atime: string | number | Date, mtime: string | number | Date): Promise<void>;
+        /**
+         * Asynchronously writes data to a file, replacing the file if it already exists.`data` can be a string, a buffer, an
+         * [AsyncIterable](https://tc39.github.io/ecma262/#sec-asynciterable-interface) or
+         * [Iterable](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#The_iterable_protocol) object, or an
+         * object with an own `toString` function
+         * property. The promise is resolved with no arguments upon success.
+         *
+         * If `options` is a string, then it specifies the `encoding`.
+         *
+         * The `FileHandle` has to support writing.
+         *
+         * It is unsafe to use `filehandle.writeFile()` multiple times on the same file
+         * without waiting for the promise to be resolved (or rejected).
+         *
+         * If one or more `filehandle.write()` calls are made on a file handle and then a`filehandle.writeFile()` call is made, the data will be written from the
+         * current position till the end of the file. It doesn't always write from the
+         * beginning of the file.
+         * @since v10.0.0
+         */
+        writeFile(data: string | Uint8Array, options?: (ObjectEncodingOptions & FlagAndOpenMode & Abortable) | BufferEncoding | null): Promise<void>;
+        /**
+         * Write `buffer` to the file.
+         *
+         * If `buffer` is a plain object, it must have an own (not inherited) `toString`function property.
+         *
+         * The promise is resolved with an object containing two properties:
+         *
+         * It is unsafe to use `filehandle.write()` multiple times on the same file
+         * without waiting for the promise to be resolved (or rejected). For this
+         * scenario, use `fs.createWriteStream()`.
+         *
+         * On Linux, positional writes do not work when the file is opened in append mode.
+         * The kernel ignores the position argument and always appends the data to
+         * the end of the file.
+         * @since v10.0.0
+         * @param [offset=0] The start position from within `buffer` where the data to write begins.
+         * @param [length=buffer.byteLength] The number of bytes from `buffer` to write.
+         * @param position The offset from the beginning of the file where the data from `buffer` should be written. If `position` is not a `number`, the data will be written at the current position.
+         * See the POSIX pwrite(2) documentation for more detail.
+         */
+        write<TBuffer extends Uint8Array>(
+            buffer: TBuffer,
+            offset?: number | null,
+            length?: number | null,
+            position?: number | null
+        ): Promise<{
+            bytesWritten: number;
+            buffer: TBuffer;
+        }>;
+        write(
+            data: string,
+            position?: number | null,
+            encoding?: BufferEncoding | null
+        ): Promise<{
+            bytesWritten: number;
+            buffer: string;
+        }>;
+        /**
+         * Write an array of [ArrayBufferView](https://developer.mozilla.org/en-US/docs/Web/API/ArrayBufferView) s to the file.
+         *
+         * The promise is resolved with an object containing a two properties:
+         *
+         * It is unsafe to call `writev()` multiple times on the same file without waiting
+         * for the promise to be resolved (or rejected).
+         *
+         * On Linux, positional writes don't work when the file is opened in append mode.
+         * The kernel ignores the position argument and always appends the data to
+         * the end of the file.
+         * @since v12.9.0
+         * @param position The offset from the beginning of the file where the data from `buffers` should be written. If `position` is not a `number`, the data will be written at the current
+         * position.
+         */
+        writev(buffers: ReadonlyArray<NodeJS.ArrayBufferView>, position?: number): Promise<WriteVResult>;
+        /**
+         * Read from a file and write to an array of [ArrayBufferView](https://developer.mozilla.org/en-US/docs/Web/API/ArrayBufferView) s
+         * @since v13.13.0, v12.17.0
+         * @param position The offset from the beginning of the file where the data should be read from. If `position` is not a `number`, the data will be read from the current position.
+         * @return Fulfills upon success an object containing two properties:
+         */
+        readv(buffers: ReadonlyArray<NodeJS.ArrayBufferView>, position?: number): Promise<ReadVResult>;
+        /**
+         * Closes the file handle after waiting for any pending operation on the handle to
+         * complete.
+         *
+         * ```js
+         * import { open } from 'fs/promises';
+         *
+         * let filehandle;
+         * try {
+         *   filehandle = await open('thefile.txt', 'r');
+         * } finally {
+         *   await filehandle?.close();
+         * }
+         * ```
+         * @since v10.0.0
+         * @return Fulfills with `undefined` upon success.
+         */
+        close(): Promise<void>;
+    }
+    /**
+     * Tests a user's permissions for the file or directory specified by `path`.
+     * The `mode` argument is an optional integer that specifies the accessibility
+     * checks to be performed. Check `File access constants` for possible values
+     * of `mode`. It is possible to create a mask consisting of the bitwise OR of
+     * two or more values (e.g. `fs.constants.W_OK | fs.constants.R_OK`).
+     *
+     * If the accessibility check is successful, the promise is resolved with no
+     * value. If any of the accessibility checks fail, the promise is rejected
+     * with an [Error](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error) object. The following example checks if the file`/etc/passwd` can be read and
+     * written by the current process.
+     *
+     * ```js
+     * import { access } from 'fs/promises';
+     * import { constants } from 'fs';
+     *
+     * try {
+     *   await access('/etc/passwd', constants.R_OK | constants.W_OK);
+     *   console.log('can access');
+     * } catch {
+     *   console.error('cannot access');
+     * }
+     * ```
+     *
+     * Using `fsPromises.access()` to check for the accessibility of a file before
+     * calling `fsPromises.open()` is not recommended. Doing so introduces a race
+     * condition, since other processes may change the file's state between the two
+     * calls. Instead, user code should open/read/write the file directly and handle
+     * the error raised if the file is not accessible.
+     * @since v10.0.0
+     * @param [mode=fs.constants.F_OK]
+     * @return Fulfills with `undefined` upon success.
+     */
+    function access(path: PathLike, mode?: number): Promise<void>;
+    /**
+     * Asynchronously copies `src` to `dest`. By default, `dest` is overwritten if it
+     * already exists.
+     *
+     * No guarantees are made about the atomicity of the copy operation. If an
+     * error occurs after the destination file has been opened for writing, an attempt
+     * will be made to remove the destination.
+     *
+     * ```js
+     * import { constants } from 'fs';
+     * import { copyFile } from 'fs/promises';
+     *
+     * try {
+     *   await copyFile('source.txt', 'destination.txt');
+     *   console.log('source.txt was copied to destination.txt');
+     * } catch {
+     *   console.log('The file could not be copied');
+     * }
+     *
+     * // By using COPYFILE_EXCL, the operation will fail if destination.txt exists.
+     * try {
+     *   await copyFile('source.txt', 'destination.txt', constants.COPYFILE_EXCL);
+     *   console.log('source.txt was copied to destination.txt');
+     * } catch {
+     *   console.log('The file could not be copied');
+     * }
+     * ```
+     * @since v10.0.0
+     * @param src source filename to copy
+     * @param dest destination filename of the copy operation
+     * @param [mode=0] Optional modifiers that specify the behavior of the copy operation. It is possible to create a mask consisting of the bitwise OR of two or more values (e.g.
+     * `fs.constants.COPYFILE_EXCL | fs.constants.COPYFILE_FICLONE`)
+     * @return Fulfills with `undefined` upon success.
+     */
+    function copyFile(src: PathLike, dest: PathLike, mode?: number): Promise<void>;
+    /**
+     * Opens a `FileHandle`.
+     *
+     * Refer to the POSIX [`open(2)`](http://man7.org/linux/man-pages/man2/open.2.html) documentation for more detail.
+     *
+     * Some characters (`< > : "" / \ | ? *`) are reserved under Windows as documented
+     * by [Naming Files, Paths, and Namespaces](https://docs.microsoft.com/en-us/windows/desktop/FileIO/naming-a-file). Under NTFS, if the filename contains
+     * a colon, Node.js will open a file system stream, as described by [this MSDN page](https://docs.microsoft.com/en-us/windows/desktop/FileIO/using-streams).
+     * @since v10.0.0
+     * @param [flags='r'] See `support of file system `flags``.
+     * @param [mode=0o666] Sets the file mode (permission and sticky bits) if the file is created.
+     * @return Fulfills with a {FileHandle} object.
+     */
+    function open(path: PathLike, flags: string | number, mode?: Mode): Promise<FileHandle>;
+    /**
+     * Renames `oldPath` to `newPath`.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function rename(oldPath: PathLike, newPath: PathLike): Promise<void>;
+    /**
+     * Truncates (shortens or extends the length) of the content at `path` to `len`bytes.
+     * @since v10.0.0
+     * @param [len=0]
+     * @return Fulfills with `undefined` upon success.
+     */
+    function truncate(path: PathLike, len?: number): Promise<void>;
+    /**
+     * Removes the directory identified by `path`.
+     *
+     * Using `fsPromises.rmdir()` on a file (not a directory) results in the
+     * promise being rejected with an `ENOENT` error on Windows and an `ENOTDIR`error on POSIX.
+     *
+     * To get a behavior similar to the `rm -rf` Unix command, use `fsPromises.rm()` with options `{ recursive: true, force: true }`.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function rmdir(path: PathLike, options?: RmDirOptions): Promise<void>;
+    /**
+     * Removes files and directories (modeled on the standard POSIX `rm` utility).
+     * @since v14.14.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function rm(path: PathLike, options?: RmOptions): Promise<void>;
+    /**
+     * Asynchronously creates a directory.
+     *
+     * The optional `options` argument can be an integer specifying `mode` (permission
+     * and sticky bits), or an object with a `mode` property and a `recursive`property indicating whether parent directories should be created. Calling`fsPromises.mkdir()` when `path` is a directory
+     * that exists results in a
+     * rejection only when `recursive` is false.
+     * @since v10.0.0
+     * @return Upon success, fulfills with `undefined` if `recursive` is `false`, or the first directory path created if `recursive` is `true`.
+     */
+    function mkdir(
+        path: PathLike,
+        options: MakeDirectoryOptions & {
+            recursive: true;
+        }
+    ): Promise<string | undefined>;
+    /**
+     * Asynchronous mkdir(2) - create a directory.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options Either the file mode, or an object optionally specifying the file mode and whether parent folders
+     * should be created. If a string is passed, it is parsed as an octal integer. If not specified, defaults to `0o777`.
+     */
+    function mkdir(
+        path: PathLike,
+        options?:
+            | Mode
+            | (MakeDirectoryOptions & {
+                  recursive?: false | undefined;
+              })
+            | null
+    ): Promise<void>;
+    /**
+     * Asynchronous mkdir(2) - create a directory.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options Either the file mode, or an object optionally specifying the file mode and whether parent folders
+     * should be created. If a string is passed, it is parsed as an octal integer. If not specified, defaults to `0o777`.
+     */
+    function mkdir(path: PathLike, options?: Mode | MakeDirectoryOptions | null): Promise<string | undefined>;
+    /**
+     * Reads the contents of a directory.
+     *
+     * The optional `options` argument can be a string specifying an encoding, or an
+     * object with an `encoding` property specifying the character encoding to use for
+     * the filenames. If the `encoding` is set to `'buffer'`, the filenames returned
+     * will be passed as `Buffer` objects.
+     *
+     * If `options.withFileTypes` is set to `true`, the resolved array will contain `fs.Dirent` objects.
+     *
+     * ```js
+     * import { readdir } from 'fs/promises';
+     *
+     * try {
+     *   const files = await readdir(path);
+     *   for (const file of files)
+     *     console.log(file);
+     * } catch (err) {
+     *   console.error(err);
+     * }
+     * ```
+     * @since v10.0.0
+     * @return Fulfills with an array of the names of the files in the directory excluding `'.'` and `'..'`.
+     */
+    function readdir(
+        path: PathLike,
+        options?:
+            | (ObjectEncodingOptions & {
+                  withFileTypes?: false | undefined;
+              })
+            | BufferEncoding
+            | null
+    ): Promise<string[]>;
+    /**
+     * Asynchronous readdir(3) - read a directory.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function readdir(
+        path: PathLike,
+        options:
+            | {
+                  encoding: 'buffer';
+                  withFileTypes?: false | undefined;
+              }
+            | 'buffer'
+    ): Promise<Buffer[]>;
+    /**
+     * Asynchronous readdir(3) - read a directory.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function readdir(
+        path: PathLike,
+        options?:
+            | (ObjectEncodingOptions & {
+                  withFileTypes?: false | undefined;
+              })
+            | BufferEncoding
+            | null
+    ): Promise<string[] | Buffer[]>;
+    /**
+     * Asynchronous readdir(3) - read a directory.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options If called with `withFileTypes: true` the result data will be an array of Dirent.
+     */
+    function readdir(
+        path: PathLike,
+        options: ObjectEncodingOptions & {
+            withFileTypes: true;
+        }
+    ): Promise<Dirent[]>;
+    /**
+     * Reads the contents of the symbolic link referred to by `path`. See the POSIX [`readlink(2)`](http://man7.org/linux/man-pages/man2/readlink.2.html) documentation for more detail. The promise is
+     * resolved with the`linkString` upon success.
+     *
+     * The optional `options` argument can be a string specifying an encoding, or an
+     * object with an `encoding` property specifying the character encoding to use for
+     * the link path returned. If the `encoding` is set to `'buffer'`, the link path
+     * returned will be passed as a `Buffer` object.
+     * @since v10.0.0
+     * @return Fulfills with the `linkString` upon success.
+     */
+    function readlink(path: PathLike, options?: ObjectEncodingOptions | BufferEncoding | null): Promise<string>;
+    /**
+     * Asynchronous readlink(2) - read value of a symbolic link.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function readlink(path: PathLike, options: BufferEncodingOption): Promise<Buffer>;
+    /**
+     * Asynchronous readlink(2) - read value of a symbolic link.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function readlink(path: PathLike, options?: ObjectEncodingOptions | string | null): Promise<string | Buffer>;
+    /**
+     * Creates a symbolic link.
+     *
+     * The `type` argument is only used on Windows platforms and can be one of `'dir'`,`'file'`, or `'junction'`. Windows junction points require the destination path
+     * to be absolute. When using `'junction'`, the `target` argument will
+     * automatically be normalized to absolute path.
+     * @since v10.0.0
+     * @param [type='file']
+     * @return Fulfills with `undefined` upon success.
+     */
+    function symlink(target: PathLike, path: PathLike, type?: string | null): Promise<void>;
+    /**
+     * Equivalent to `fsPromises.stat()` unless `path` refers to a symbolic link,
+     * in which case the link itself is stat-ed, not the file that it refers to.
+     * Refer to the POSIX [`lstat(2)`](http://man7.org/linux/man-pages/man2/lstat.2.html) document for more detail.
+     * @since v10.0.0
+     * @return Fulfills with the {fs.Stats} object for the given symbolic link `path`.
+     */
+    function lstat(
+        path: PathLike,
+        opts?: StatOptions & {
+            bigint?: false | undefined;
+        }
+    ): Promise<Stats>;
+    function lstat(
+        path: PathLike,
+        opts: StatOptions & {
+            bigint: true;
+        }
+    ): Promise<BigIntStats>;
+    function lstat(path: PathLike, opts?: StatOptions): Promise<Stats | BigIntStats>;
+    /**
+     * @since v10.0.0
+     * @return Fulfills with the {fs.Stats} object for the given `path`.
+     */
+    function stat(
+        path: PathLike,
+        opts?: StatOptions & {
+            bigint?: false | undefined;
+        }
+    ): Promise<Stats>;
+    function stat(
+        path: PathLike,
+        opts: StatOptions & {
+            bigint: true;
+        }
+    ): Promise<BigIntStats>;
+    function stat(path: PathLike, opts?: StatOptions): Promise<Stats | BigIntStats>;
+    /**
+     * Creates a new link from the `existingPath` to the `newPath`. See the POSIX [`link(2)`](http://man7.org/linux/man-pages/man2/link.2.html) documentation for more detail.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function link(existingPath: PathLike, newPath: PathLike): Promise<void>;
+    /**
+     * If `path` refers to a symbolic link, then the link is removed without affecting
+     * the file or directory to which that link refers. If the `path` refers to a file
+     * path that is not a symbolic link, the file is deleted. See the POSIX [`unlink(2)`](http://man7.org/linux/man-pages/man2/unlink.2.html) documentation for more detail.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function unlink(path: PathLike): Promise<void>;
+    /**
+     * Changes the permissions of a file.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function chmod(path: PathLike, mode: Mode): Promise<void>;
+    /**
+     * Changes the permissions on a symbolic link.
+     *
+     * This method is only implemented on macOS.
+     * @deprecated Since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function lchmod(path: PathLike, mode: Mode): Promise<void>;
+    /**
+     * Changes the ownership on a symbolic link.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function lchown(path: PathLike, uid: number, gid: number): Promise<void>;
+    /**
+     * Changes the access and modification times of a file in the same way as `fsPromises.utimes()`, with the difference that if the path refers to a
+     * symbolic link, then the link is not dereferenced: instead, the timestamps of
+     * the symbolic link itself are changed.
+     * @since v14.5.0, v12.19.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function lutimes(path: PathLike, atime: string | number | Date, mtime: string | number | Date): Promise<void>;
+    /**
+     * Changes the ownership of a file.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function chown(path: PathLike, uid: number, gid: number): Promise<void>;
+    /**
+     * Change the file system timestamps of the object referenced by `path`.
+     *
+     * The `atime` and `mtime` arguments follow these rules:
+     *
+     * * Values can be either numbers representing Unix epoch time, `Date`s, or a
+     * numeric string like `'123456789.0'`.
+     * * If the value can not be converted to a number, or is `NaN`, `Infinity` or`-Infinity`, an `Error` will be thrown.
+     * @since v10.0.0
+     * @return Fulfills with `undefined` upon success.
+     */
+    function utimes(path: PathLike, atime: string | number | Date, mtime: string | number | Date): Promise<void>;
+    /**
+     * Determines the actual location of `path` using the same semantics as the`fs.realpath.native()` function.
+     *
+     * Only paths that can be converted to UTF8 strings are supported.
+     *
+     * The optional `options` argument can be a string specifying an encoding, or an
+     * object with an `encoding` property specifying the character encoding to use for
+     * the path. If the `encoding` is set to `'buffer'`, the path returned will be
+     * passed as a `Buffer` object.
+     *
+     * On Linux, when Node.js is linked against musl libc, the procfs file system must
+     * be mounted on `/proc` in order for this function to work. Glibc does not have
+     * this restriction.
+     * @since v10.0.0
+     * @return Fulfills with the resolved path upon success.
+     */
+    function realpath(path: PathLike, options?: ObjectEncodingOptions | BufferEncoding | null): Promise<string>;
+    /**
+     * Asynchronous realpath(3) - return the canonicalized absolute pathname.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function realpath(path: PathLike, options: BufferEncodingOption): Promise<Buffer>;
+    /**
+     * Asynchronous realpath(3) - return the canonicalized absolute pathname.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function realpath(path: PathLike, options?: ObjectEncodingOptions | BufferEncoding | null): Promise<string | Buffer>;
+    /**
+     * Creates a unique temporary directory. A unique directory name is generated by
+     * appending six random characters to the end of the provided `prefix`. Due to
+     * platform inconsistencies, avoid trailing `X` characters in `prefix`. Some
+     * platforms, notably the BSDs, can return more than six random characters, and
+     * replace trailing `X` characters in `prefix` with random characters.
+     *
+     * The optional `options` argument can be a string specifying an encoding, or an
+     * object with an `encoding` property specifying the character encoding to use.
+     *
+     * ```js
+     * import { mkdtemp } from 'fs/promises';
+     *
+     * try {
+     *   await mkdtemp(path.join(os.tmpdir(), 'foo-'));
+     * } catch (err) {
+     *   console.error(err);
+     * }
+     * ```
+     *
+     * The `fsPromises.mkdtemp()` method will append the six randomly selected
+     * characters directly to the `prefix` string. For instance, given a directory`/tmp`, if the intention is to create a temporary directory _within_`/tmp`, the`prefix` must end with a trailing
+     * platform-specific path separator
+     * (`require('path').sep`).
+     * @since v10.0.0
+     * @return Fulfills with a string containing the filesystem path of the newly created temporary directory.
+     */
+    function mkdtemp(prefix: string, options?: ObjectEncodingOptions | BufferEncoding | null): Promise<string>;
+    /**
+     * Asynchronously creates a unique temporary directory.
+     * Generates six random characters to be appended behind a required `prefix` to create a unique temporary directory.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function mkdtemp(prefix: string, options: BufferEncodingOption): Promise<Buffer>;
+    /**
+     * Asynchronously creates a unique temporary directory.
+     * Generates six random characters to be appended behind a required `prefix` to create a unique temporary directory.
+     * @param options The encoding (or an object specifying the encoding), used as the encoding of the result. If not provided, `'utf8'` is used.
+     */
+    function mkdtemp(prefix: string, options?: ObjectEncodingOptions | BufferEncoding | null): Promise<string | Buffer>;
+    /**
+     * Asynchronously writes data to a file, replacing the file if it already exists.`data` can be a string, a `Buffer`, or, an object with an own (not inherited)`toString` function property.
+     *
+     * The `encoding` option is ignored if `data` is a buffer.
+     *
+     * If `options` is a string, then it specifies the encoding.
+     *
+     * The `mode` option only affects the newly created file. See `fs.open()` for more details.
+     *
+     * Any specified `FileHandle` has to support writing.
+     *
+     * It is unsafe to use `fsPromises.writeFile()` multiple times on the same file
+     * without waiting for the promise to be settled.
+     *
+     * Similarly to `fsPromises.readFile` \- `fsPromises.writeFile` is a convenience
+     * method that performs multiple `write` calls internally to write the buffer
+     * passed to it. For performance sensitive code consider using `fs.createWriteStream()`.
+     *
+     * It is possible to use an `AbortSignal` to cancel an `fsPromises.writeFile()`.
+     * Cancelation is ""best effort"", and some amount of data is likely still
+     * to be written.
+     *
+     * ```js
+     * import { writeFile } from 'fs/promises';
+     * import { Buffer } from 'buffer';
+     *
+     * try {
+     *   const controller = new AbortController();
+     *   const { signal } = controller;
+     *   const data = new Uint8Array(Buffer.from('Hello Node.js'));
+     *   const promise = writeFile('message.txt', data, { signal });
+     *
+     *   // Abort the request before the promise settles.
+     *   controller.abort();
+     *
+     *   await promise;
+     * } catch (err) {
+     *   // When a request is aborted - err is an AbortError
+     *   console.error(err);
+     * }
+     * ```
+     *
+     * Aborting an ongoing request does not abort individual operating
+     * system requests but rather the internal buffering `fs.writeFile` performs.
+     * @since v10.0.0
+     * @param file filename or `FileHandle`
+     * @return Fulfills with `undefined` upon success.
+     */
+    function writeFile(
+        file: PathLike | FileHandle,
+        data: string | NodeJS.ArrayBufferView | Iterable<string | NodeJS.ArrayBufferView> | AsyncIterable<string | NodeJS.ArrayBufferView> | Stream,
+        options?:
+            | (ObjectEncodingOptions & {
+                  mode?: Mode | undefined;
+                  flag?: OpenMode | undefined;
+              } & Abortable)
+            | BufferEncoding
+            | null
+    ): Promise<void>;
+    /**
+     * Asynchronously append data to a file, creating the file if it does not yet
+     * exist. `data` can be a string or a `Buffer`.
+     *
+     * If `options` is a string, then it specifies the `encoding`.
+     *
+     * The `mode` option only affects the newly created file. See `fs.open()` for more details.
+     *
+     * The `path` may be specified as a `FileHandle` that has been opened
+     * for appending (using `fsPromises.open()`).
+     * @since v10.0.0
+     * @param path filename or {FileHandle}
+     * @return Fulfills with `undefined` upon success.
+     */
+    function appendFile(path: PathLike | FileHandle, data: string | Uint8Array, options?: (ObjectEncodingOptions & FlagAndOpenMode) | BufferEncoding | null): Promise<void>;
+    /**
+     * Asynchronously reads the entire contents of a file.
+     *
+     * If no encoding is specified (using `options.encoding`), the data is returned
+     * as a `Buffer` object. Otherwise, the data will be a string.
+     *
+     * If `options` is a string, then it specifies the encoding.
+     *
+     * When the `path` is a directory, the behavior of `fsPromises.readFile()` is
+     * platform-specific. On macOS, Linux, and Windows, the promise will be rejected
+     * with an error. On FreeBSD, a representation of the directory's contents will be
+     * returned.
+     *
+     * It is possible to abort an ongoing `readFile` using an `AbortSignal`. If a
+     * request is aborted the promise returned is rejected with an `AbortError`:
+     *
+     * ```js
+     * import { readFile } from 'fs/promises';
+     *
+     * try {
+     *   const controller = new AbortController();
+     *   const { signal } = controller;
+     *   const promise = readFile(fileName, { signal });
+     *
+     *   // Abort the request before the promise settles.
+     *   controller.abort();
+     *
+     *   await promise;
+     * } catch (err) {
+     *   // When a request is aborted - err is an AbortError
+     *   console.error(err);
+     * }
+     * ```
+     *
+     * Aborting an ongoing request does not abort individual operating
+     * system requests but rather the internal buffering `fs.readFile` performs.
+     *
+     * Any specified `FileHandle` has to support reading.
+     * @since v10.0.0
+     * @param path filename or `FileHandle`
+     * @return Fulfills with the contents of the file.
+     */
+    function readFile(
+        path: PathLike | FileHandle,
+        options?:
+            | ({
+                  encoding?: null | undefined;
+                  flag?: OpenMode | undefined;
+              } & Abortable)
+            | null
+    ): Promise<Buffer>;
+    /**
+     * Asynchronously reads the entire contents of a file.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * If a `FileHandle` is provided, the underlying file will _not_ be closed automatically.
+     * @param options An object that may contain an optional flag.
+     * If a flag is not provided, it defaults to `'r'`.
+     */
+    function readFile(
+        path: PathLike | FileHandle,
+        options:
+            | ({
+                  encoding: BufferEncoding;
+                  flag?: OpenMode | undefined;
+              } & Abortable)
+            | BufferEncoding
+    ): Promise<string>;
+    /**
+     * Asynchronously reads the entire contents of a file.
+     * @param path A path to a file. If a URL is provided, it must use the `file:` protocol.
+     * If a `FileHandle` is provided, the underlying file will _not_ be closed automatically.
+     * @param options An object that may contain an optional flag.
+     * If a flag is not provided, it defaults to `'r'`.
+     */
+    function readFile(
+        path: PathLike | FileHandle,
+        options?:
+            | (ObjectEncodingOptions &
+                  Abortable & {
+                      flag?: OpenMode | undefined;
+                  })
+            | BufferEncoding
+            | null
+    ): Promise<string | Buffer>;
+    /**
+     * Asynchronously open a directory for iterative scanning. See the POSIX [`opendir(3)`](http://man7.org/linux/man-pages/man3/opendir.3.html) documentation for more detail.
+     *
+     * Creates an `fs.Dir`, which contains all further functions for reading from
+     * and cleaning up the directory.
+     *
+     * The `encoding` option sets the encoding for the `path` while opening the
+     * directory and subsequent read operations.
+     *
+     * Example using async iteration:
+     *
+     * ```js
+     * import { opendir } from 'fs/promises';
+     *
+     * try {
+     *   const dir = await opendir('./');
+     *   for await (const dirent of dir)
+     *     console.log(dirent.name);
+     * } catch (err) {
+     *   console.error(err);
+     * }
+     * ```
+     *
+     * When using the async iterator, the `fs.Dir` object will be automatically
+     * closed after the iterator exits.
+     * @since v12.12.0
+     * @return Fulfills with an {fs.Dir}.
+     */
+    function opendir(path: PathLike, options?: OpenDirOptions): Promise<Dir>;
+    /**
+     * Returns an async iterator that watches for changes on `filename`, where `filename`is either a file or a directory.
+     *
+     * ```js
+     * const { watch } = require('fs/promises');
+     *
+     * const ac = new AbortController();
+     * const { signal } = ac;
+     * setTimeout(() => ac.abort(), 10000);
+     *
+     * (async () => {
+     *   try {
+     *     const watcher = watch(__filename, { signal });
+     *     for await (const event of watcher)
+     *       console.log(event);
+     *   } catch (err) {
+     *     if (err.name === 'AbortError')
+     *       return;
+     *     throw err;
+     *   }
+     * })();
+     * ```
+     *
+     * On most platforms, `'rename'` is emitted whenever a filename appears or
+     * disappears in the directory.
+     *
+     * All the `caveats` for `fs.watch()` also apply to `fsPromises.watch()`.
+     * @since v15.9.0, v14.18.0
+     * @return of objects with the properties:
+     */
+    function watch(
+        filename: PathLike,
+        options:
+            | (WatchOptions & {
+                  encoding: 'buffer';
+              })
+            | 'buffer'
+    ): AsyncIterable<FileChangeInfo<Buffer>>;
+    /**
+     * Watch for changes on `filename`, where `filename` is either a file or a directory, returning an `FSWatcher`.
+     * @param filename A path to a file or directory. If a URL is provided, it must use the `file:` protocol.
+     * @param options Either the encoding for the filename provided to the listener, or an object optionally specifying encoding, persistent, and recursive options.
+     * If `encoding` is not supplied, the default of `'utf8'` is used.
+     * If `persistent` is not supplied, the default of `true` is used.
+     * If `recursive` is not supplied, the default of `false` is used.
+     */
+    function watch(filename: PathLike, options?: WatchOptions | BufferEncoding): AsyncIterable<FileChangeInfo<string>>;
+    /**
+     * Watch for changes on `filename`, where `filename` is either a file or a directory, returning an `FSWatcher`.
+     * @param filename A path to a file or directory. If a URL is provided, it must use the `file:` protocol.
+     * @param options Either the encoding for the filename provided to the listener, or an object optionally specifying encoding, persistent, and recursive options.
+     * If `encoding` is not supplied, the default of `'utf8'` is used.
+     * If `persistent` is not supplied, the default of `true` is used.
+     * If `recursive` is not supplied, the default of `false` is used.
+     */
+    function watch(filename: PathLike, options: WatchOptions | string): AsyncIterable<FileChangeInfo<string>> | AsyncIterable<FileChangeInfo<Buffer>>;
+    /**
+     * Asynchronously copies the entire directory structure from `src` to `dest`,
+     * including subdirectories and files.
+     *
+     * When copying a directory to another directory, globs are not supported and
+     * behavior is similar to `cp dir1/ dir2/`.
+     * @since v16.7.0
+     * @experimental
+     * @param src source path to copy.
+     * @param dest destination path to copy to.
+     * @return Fulfills with `undefined` upon success.
+     */
+    function cp(source: string, destination: string, opts?: CopyOptions): Promise<void>;
+}
+declare module 'node:fs/promises' {
+    export * from 'fs/promises';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+// Declare ""static"" methods in Error
+interface ErrorConstructor {
+    /** Create .stack property on a target object */
+    captureStackTrace(targetObject: object, constructorOpt?: Function): void;
+
+    /**
+     * Optional override for formatting stack traces
+     *
+     * @see https://v8.dev/docs/stack-trace-api#customizing-stack-traces
+     */
+    prepareStackTrace?: ((err: Error, stackTraces: NodeJS.CallSite[]) => any) | undefined;
+
+    stackTraceLimit: number;
+}
+
+/*-----------------------------------------------*
+ *                                               *
+ *                   GLOBAL                      *
+ *                                               *
+ ------------------------------------------------*/
+
+// For backwards compability
+interface NodeRequire extends NodeJS.Require { }
+interface RequireResolve extends NodeJS.RequireResolve { }
+interface NodeModule extends NodeJS.Module { }
+
+declare var process: NodeJS.Process;
+declare var console: Console;
+
+declare var __filename: string;
+declare var __dirname: string;
+
+declare var require: NodeRequire;
+declare var module: NodeModule;
+
+// Same as module.exports
+declare var exports: any;
+
+/**
+ * Only available if `--expose-gc` is passed to the process.
+ */
+declare var gc: undefined | (() => void);
+
+//#region borrowed
+// from https://github.com/microsoft/TypeScript/blob/38da7c600c83e7b31193a62495239a0fe478cb67/lib/lib.webworker.d.ts#L633 until moved to separate lib
+/** A controller object that allows you to abort one or more DOM requests as and when desired. */
+interface AbortController {
+    /**
+     * Returns the AbortSignal object associated with this object.
+     */
+
+    readonly signal: AbortSignal;
+    /**
+     * Invoking this method will set this object's AbortSignal's aborted flag and signal to any observers that the associated activity is to be aborted.
+     */
+    abort(): void;
+}
+
+/** A signal object that allows you to communicate with a DOM request (such as a Fetch) and abort it if required via an AbortController object. */
+interface AbortSignal {
+    /**
+     * Returns true if this AbortSignal's AbortController has signaled to abort, and false otherwise.
+     */
+    readonly aborted: boolean;
+}
+
+declare var AbortController: {
+    prototype: AbortController;
+    new(): AbortController;
+};
+
+declare var AbortSignal: {
+    prototype: AbortSignal;
+    new(): AbortSignal;
+    // TODO: Add abort() static
+};
+//#endregion borrowed
+
+//#region ArrayLike.at()
+interface RelativeIndexable<T> {
+    /**
+     * Takes an integer value and returns the item at that index,
+     * allowing for positive and negative integers.
+     * Negative integers count back from the last item in the array.
+     */
+    at(index: number): T | undefined;
+}
+interface String extends RelativeIndexable<string> {}
+interface Array<T> extends RelativeIndexable<T> {}
+interface Int8Array extends RelativeIndexable<number> {}
+interface Uint8Array extends RelativeIndexable<number> {}
+interface Uint8ClampedArray extends RelativeIndexable<number> {}
+interface Int16Array extends RelativeIndexable<number> {}
+interface Uint16Array extends RelativeIndexable<number> {}
+interface Int32Array extends RelativeIndexable<number> {}
+interface Uint32Array extends RelativeIndexable<number> {}
+interface Float32Array extends RelativeIndexable<number> {}
+interface Float64Array extends RelativeIndexable<number> {}
+interface BigInt64Array extends RelativeIndexable<bigint> {}
+interface BigUint64Array extends RelativeIndexable<bigint> {}
+//#endregion ArrayLike.at() end
+
+/**
+ * @since v17.0.0
+ *
+ * Creates a deep clone of an object.
+ */
+declare function structuredClone<T>(
+    value: T,
+    transfer?: { transfer: ReadonlyArray<import('worker_threads').TransferListItem> },
+): T;
+
+/*----------------------------------------------*
+*                                               *
+*               GLOBAL INTERFACES               *
+*                                               *
+*-----------------------------------------------*/
+declare namespace NodeJS {
+    interface CallSite {
+        /**
+         * Value of ""this""
+         */
+        getThis(): unknown;
+
+        /**
+         * Type of ""this"" as a string.
+         * This is the name of the function stored in the constructor field of
+         * ""this"", if available.  Otherwise the object's [[Class]] internal
+         * property.
+         */
+        getTypeName(): string | null;
+
+        /**
+         * Current function
+         */
+        getFunction(): Function | undefined;
+
+        /**
+         * Name of the current function, typically its name property.
+         * If a name property is not available an attempt will be made to try
+         * to infer a name from the function's context.
+         */
+        getFunctionName(): string | null;
+
+        /**
+         * Name of the property [of ""this"" or one of its prototypes] that holds
+         * the current function
+         */
+        getMethodName(): string | null;
+
+        /**
+         * Name of the script [if this function was defined in a script]
+         */
+        getFileName(): string | null;
+
+        /**
+         * Current line number [if this function was defined in a script]
+         */
+        getLineNumber(): number | null;
+
+        /**
+         * Current column number [if this function was defined in a script]
+         */
+        getColumnNumber(): number | null;
+
+        /**
+         * A call site object representing the location where eval was called
+         * [if this function was created using a call to eval]
+         */
+        getEvalOrigin(): string | undefined;
+
+        /**
+         * Is this a toplevel invocation, that is, is ""this"" the global object?
+         */
+        isToplevel(): boolean;
+
+        /**
+         * Does this call take place in code defined by a call to eval?
+         */
+        isEval(): boolean;
+
+        /**
+         * Is this call in native V8 code?
+         */
+        isNative(): boolean;
+
+        /**
+         * Is this a constructor call?
+         */
+        isConstructor(): boolean;
+    }
+
+    interface ErrnoException extends Error {
+        errno?: number | undefined;
+        code?: string | undefined;
+        path?: string | undefined;
+        syscall?: string | undefined;
+    }
+
+    interface ReadableStream extends EventEmitter {
+        readable: boolean;
+        read(size?: number): string | Buffer;
+        setEncoding(encoding: BufferEncoding): this;
+        pause(): this;
+        resume(): this;
+        isPaused(): boolean;
+        pipe<T extends WritableStream>(destination: T, options?: { end?: boolean | undefined; }): T;
+        unpipe(destination?: WritableStream): this;
+        unshift(chunk: string | Uint8Array, encoding?: BufferEncoding): void;
+        wrap(oldStream: ReadableStream): this;
+        [Symbol.asyncIterator](): AsyncIterableIterator<string | Buffer>;
+    }
+
+    interface WritableStream extends EventEmitter {
+        writable: boolean;
+        write(buffer: Uint8Array | string, cb?: (err?: Error | null) => void): boolean;
+        write(str: string, encoding?: BufferEncoding, cb?: (err?: Error | null) => void): boolean;
+        end(cb?: () => void): this;
+        end(data: string | Uint8Array, cb?: () => void): this;
+        end(str: string, encoding?: BufferEncoding, cb?: () => void): this;
+    }
+
+    interface ReadWriteStream extends ReadableStream, WritableStream { }
+
+    interface RefCounted {
+        ref(): this;
+        unref(): this;
+    }
+
+    type TypedArray =
+        | Uint8Array
+        | Uint8ClampedArray
+        | Uint16Array
+        | Uint32Array
+        | Int8Array
+        | Int16Array
+        | Int32Array
+        | BigUint64Array
+        | BigInt64Array
+        | Float32Array
+        | Float64Array;
+    type ArrayBufferView = TypedArray | DataView;
+
+    interface Require {
+        (id: string): any;
+        resolve: RequireResolve;
+        cache: Dict<NodeModule>;
+        /**
+         * @deprecated
+         */
+        extensions: RequireExtensions;
+        main: Module | undefined;
+    }
+
+    interface RequireResolve {
+        (id: string, options?: { paths?: string[] | undefined; }): string;
+        paths(request: string): string[] | null;
+    }
+
+    interface RequireExtensions extends Dict<(m: Module, filename: string) => any> {
+        '.js': (m: Module, filename: string) => any;
+        '.json': (m: Module, filename: string) => any;
+        '.node': (m: Module, filename: string) => any;
+    }
+    interface Module {
+        /**
+         * `true` if the module is running during the Node.js preload
+         */
+        isPreloading: boolean;
+        exports: any;
+        require: Require;
+        id: string;
+        filename: string;
+        loaded: boolean;
+        /** @deprecated since v14.6.0 Please use `require.main` and `module.children` instead. */
+        parent: Module | null | undefined;
+        children: Module[];
+        /**
+         * @since v11.14.0
+         *
+         * The directory name of the module. This is usually the same as the path.dirname() of the module.id.
+         */
+        path: string;
+        paths: string[];
+    }
+
+    interface Dict<T> {
+        [key: string]: T | undefined;
+    }
+
+    interface ReadOnlyDict<T> {
+        readonly [key: string]: T | undefined;
+    }
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+declare var global: typeof globalThis;"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * To use the HTTP server and client one must `require('http')`.
+ *
+ * The HTTP interfaces in Node.js are designed to support many features
+ * of the protocol which have been traditionally difficult to use.
+ * In particular, large, possibly chunk-encoded, messages. The interface is
+ * careful to never buffer entire requests or responses, so the
+ * user is able to stream data.
+ *
+ * HTTP message headers are represented by an object like this:
+ *
+ * ```js
+ * { 'content-length': '123',
+ *   'content-type': 'text/plain',
+ *   'connection': 'keep-alive',
+ *   'host': 'mysite.com',
+ *   'accept': '*' }
+ * ```
+ *
+ * Keys are lowercased. Values are not modified.
+ *
+ * In order to support the full spectrum of possible HTTP applications, the Node.js
+ * HTTP API is very low-level. It deals with stream handling and message
+ * parsing only. It parses a message into headers and body but it does not
+ * parse the actual headers or the body.
+ *
+ * See `message.headers` for details on how duplicate headers are handled.
+ *
+ * The raw headers as they were received are retained in the `rawHeaders`property, which is an array of `[key, value, key2, value2, ...]`. For
+ * example, the previous message header object might have a `rawHeaders`list like the following:
+ *
+ * ```js
+ * [ 'ConTent-Length', '123456',
+ *   'content-LENGTH', '123',
+ *   'content-type', 'text/plain',
+ *   'CONNECTION', 'keep-alive',
+ *   'Host', 'mysite.com',
+ *   'accepT', '*' ]
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/http.js)
+ */
+declare module 'http' {
+    import * as stream from 'node:stream';
+    import { URL } from 'node:url';
+    import { TcpSocketConnectOpts, Socket, Server as NetServer, LookupFunction } from 'node:net';
+    // incoming headers will never contain number
+    interface IncomingHttpHeaders extends NodeJS.Dict<string | string[]> {
+        accept?: string | undefined;
+        'accept-language'?: string | undefined;
+        'accept-patch'?: string | undefined;
+        'accept-ranges'?: string | undefined;
+        'access-control-allow-credentials'?: string | undefined;
+        'access-control-allow-headers'?: string | undefined;
+        'access-control-allow-methods'?: string | undefined;
+        'access-control-allow-origin'?: string | undefined;
+        'access-control-expose-headers'?: string | undefined;
+        'access-control-max-age'?: string | undefined;
+        'access-control-request-headers'?: string | undefined;
+        'access-control-request-method'?: string | undefined;
+        age?: string | undefined;
+        allow?: string | undefined;
+        'alt-svc'?: string | undefined;
+        authorization?: string | undefined;
+        'cache-control'?: string | undefined;
+        connection?: string | undefined;
+        'content-disposition'?: string | undefined;
+        'content-encoding'?: string | undefined;
+        'content-language'?: string | undefined;
+        'content-length'?: string | undefined;
+        'content-location'?: string | undefined;
+        'content-range'?: string | undefined;
+        'content-type'?: string | undefined;
+        cookie?: string | undefined;
+        date?: string | undefined;
+        etag?: string | undefined;
+        expect?: string | undefined;
+        expires?: string | undefined;
+        forwarded?: string | undefined;
+        from?: string | undefined;
+        host?: string | undefined;
+        'if-match'?: string | undefined;
+        'if-modified-since'?: string | undefined;
+        'if-none-match'?: string | undefined;
+        'if-unmodified-since'?: string | undefined;
+        'last-modified'?: string | undefined;
+        location?: string | undefined;
+        origin?: string | undefined;
+        pragma?: string | undefined;
+        'proxy-authenticate'?: string | undefined;
+        'proxy-authorization'?: string | undefined;
+        'public-key-pins'?: string | undefined;
+        range?: string | undefined;
+        referer?: string | undefined;
+        'retry-after'?: string | undefined;
+        'sec-websocket-accept'?: string | undefined;
+        'sec-websocket-extensions'?: string | undefined;
+        'sec-websocket-key'?: string | undefined;
+        'sec-websocket-protocol'?: string | undefined;
+        'sec-websocket-version'?: string | undefined;
+        'set-cookie'?: string[] | undefined;
+        'strict-transport-security'?: string | undefined;
+        tk?: string | undefined;
+        trailer?: string | undefined;
+        'transfer-encoding'?: string | undefined;
+        upgrade?: string | undefined;
+        'user-agent'?: string | undefined;
+        vary?: string | undefined;
+        via?: string | undefined;
+        warning?: string | undefined;
+        'www-authenticate'?: string | undefined;
+    }
+    // outgoing headers allows numbers (as they are converted internally to strings)
+    type OutgoingHttpHeader = number | string | string[];
+    interface OutgoingHttpHeaders extends NodeJS.Dict<OutgoingHttpHeader> {}
+    interface ClientRequestArgs {
+        signal?: AbortSignal | undefined;
+        protocol?: string | null | undefined;
+        host?: string | null | undefined;
+        hostname?: string | null | undefined;
+        family?: number | undefined;
+        port?: number | string | null | undefined;
+        defaultPort?: number | string | undefined;
+        localAddress?: string | undefined;
+        socketPath?: string | undefined;
+        /**
+         * @default 8192
+         */
+        maxHeaderSize?: number | undefined;
+        method?: string | undefined;
+        path?: string | null | undefined;
+        headers?: OutgoingHttpHeaders | undefined;
+        auth?: string | null | undefined;
+        agent?: Agent | boolean | undefined;
+        _defaultAgent?: Agent | undefined;
+        timeout?: number | undefined;
+        setHost?: boolean | undefined;
+        // https://github.com/nodejs/node/blob/master/lib/_http_client.js#L278
+        createConnection?: ((options: ClientRequestArgs, oncreate: (err: Error, socket: Socket) => void) => Socket) | undefined;
+        lookup?: LookupFunction | undefined;
+    }
+    interface ServerOptions {
+        IncomingMessage?: typeof IncomingMessage | undefined;
+        ServerResponse?: typeof ServerResponse | undefined;
+        /**
+         * Optionally overrides the value of
+         * `--max-http-header-size` for requests received by this server, i.e.
+         * the maximum length of request headers in bytes.
+         * @default 8192
+         */
+        maxHeaderSize?: number | undefined;
+        /**
+         * Use an insecure HTTP parser that accepts invalid HTTP headers when true.
+         * Using the insecure parser should be avoided.
+         * See --insecure-http-parser for more information.
+         * @default false
+         */
+        insecureHTTPParser?: boolean | undefined;
+    }
+    type RequestListener = (req: IncomingMessage, res: ServerResponse) => void;
+    /**
+     * @since v0.1.17
+     */
+    class Server extends NetServer {
+        constructor(requestListener?: RequestListener);
+        constructor(options: ServerOptions, requestListener?: RequestListener);
+        /**
+         * Sets the timeout value for sockets, and emits a `'timeout'` event on
+         * the Server object, passing the socket as an argument, if a timeout
+         * occurs.
+         *
+         * If there is a `'timeout'` event listener on the Server object, then it
+         * will be called with the timed-out socket as an argument.
+         *
+         * By default, the Server does not timeout sockets. However, if a callback
+         * is assigned to the Server's `'timeout'` event, timeouts must be handled
+         * explicitly.
+         * @since v0.9.12
+         * @param [msecs=0 (no timeout)]
+         */
+        setTimeout(msecs?: number, callback?: () => void): this;
+        setTimeout(callback: () => void): this;
+        /**
+         * Limits maximum incoming headers count. If set to 0, no limit will be applied.
+         * @since v0.7.0
+         */
+        maxHeadersCount: number | null;
+        /**
+         * The maximum number of requests socket can handle
+         * before closing keep alive connection.
+         *
+         * A value of `0` will disable the limit.
+         *
+         * When the limit is reached it will set the `Connection` header value to `close`,
+         * but will not actually close the connection, subsequent requests sent
+         * after the limit is reached will get `503 Service Unavailable` as a response.
+         * @since v16.10.0
+         */
+        maxRequestsPerSocket: number | null;
+        /**
+         * The number of milliseconds of inactivity before a socket is presumed
+         * to have timed out.
+         *
+         * A value of `0` will disable the timeout behavior on incoming connections.
+         *
+         * The socket timeout logic is set up on connection, so changing this
+         * value only affects new connections to the server, not any existing connections.
+         * @since v0.9.12
+         */
+        timeout: number;
+        /**
+         * Limit the amount of time the parser will wait to receive the complete HTTP
+         * headers.
+         *
+         * In case of inactivity, the rules defined in `server.timeout` apply. However,
+         * that inactivity based timeout would still allow the connection to be kept open
+         * if the headers are being sent very slowly (by default, up to a byte per 2
+         * minutes). In order to prevent this, whenever header data arrives an additional
+         * check is made that more than `server.headersTimeout` milliseconds has not
+         * passed since the connection was established. If the check fails, a `'timeout'`event is emitted on the server object, and (by default) the socket is destroyed.
+         * See `server.timeout` for more information on how timeout behavior can be
+         * customized.
+         * @since v11.3.0, v10.14.0
+         */
+        headersTimeout: number;
+        /**
+         * The number of milliseconds of inactivity a server needs to wait for additional
+         * incoming data, after it has finished writing the last response, before a socket
+         * will be destroyed. If the server receives new data before the keep-alive
+         * timeout has fired, it will reset the regular inactivity timeout, i.e.,`server.timeout`.
+         *
+         * A value of `0` will disable the keep-alive timeout behavior on incoming
+         * connections.
+         * A value of `0` makes the http server behave similarly to Node.js versions prior
+         * to 8.0.0, which did not have a keep-alive timeout.
+         *
+         * The socket timeout logic is set up on connection, so changing this value only
+         * affects new connections to the server, not any existing connections.
+         * @since v8.0.0
+         */
+        keepAliveTimeout: number;
+        /**
+         * Sets the timeout value in milliseconds for receiving the entire request from
+         * the client.
+         *
+         * If the timeout expires, the server responds with status 408 without
+         * forwarding the request to the request listener and then closes the connection.
+         *
+         * It must be set to a non-zero value (e.g. 120 seconds) to protect against
+         * potential Denial-of-Service attacks in case the server is deployed without a
+         * reverse proxy in front.
+         * @since v14.11.0
+         */
+        requestTimeout: number;
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'connection', listener: (socket: Socket) => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'listening', listener: () => void): this;
+        addListener(event: 'checkContinue', listener: RequestListener): this;
+        addListener(event: 'checkExpectation', listener: RequestListener): this;
+        addListener(event: 'clientError', listener: (err: Error, socket: stream.Duplex) => void): this;
+        addListener(event: 'connect', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        addListener(event: 'request', listener: RequestListener): this;
+        addListener(event: 'upgrade', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        emit(event: string, ...args: any[]): boolean;
+        emit(event: 'close'): boolean;
+        emit(event: 'connection', socket: Socket): boolean;
+        emit(event: 'error', err: Error): boolean;
+        emit(event: 'listening'): boolean;
+        emit(event: 'checkContinue', req: IncomingMessage, res: ServerResponse): boolean;
+        emit(event: 'checkExpectation', req: IncomingMessage, res: ServerResponse): boolean;
+        emit(event: 'clientError', err: Error, socket: stream.Duplex): boolean;
+        emit(event: 'connect', req: IncomingMessage, socket: stream.Duplex, head: Buffer): boolean;
+        emit(event: 'request', req: IncomingMessage, res: ServerResponse): boolean;
+        emit(event: 'upgrade', req: IncomingMessage, socket: stream.Duplex, head: Buffer): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'connection', listener: (socket: Socket) => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'listening', listener: () => void): this;
+        on(event: 'checkContinue', listener: RequestListener): this;
+        on(event: 'checkExpectation', listener: RequestListener): this;
+        on(event: 'clientError', listener: (err: Error, socket: stream.Duplex) => void): this;
+        on(event: 'connect', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        on(event: 'request', listener: RequestListener): this;
+        on(event: 'upgrade', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'connection', listener: (socket: Socket) => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'listening', listener: () => void): this;
+        once(event: 'checkContinue', listener: RequestListener): this;
+        once(event: 'checkExpectation', listener: RequestListener): this;
+        once(event: 'clientError', listener: (err: Error, socket: stream.Duplex) => void): this;
+        once(event: 'connect', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        once(event: 'request', listener: RequestListener): this;
+        once(event: 'upgrade', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'connection', listener: (socket: Socket) => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'listening', listener: () => void): this;
+        prependListener(event: 'checkContinue', listener: RequestListener): this;
+        prependListener(event: 'checkExpectation', listener: RequestListener): this;
+        prependListener(event: 'clientError', listener: (err: Error, socket: stream.Duplex) => void): this;
+        prependListener(event: 'connect', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        prependListener(event: 'request', listener: RequestListener): this;
+        prependListener(event: 'upgrade', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'connection', listener: (socket: Socket) => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'listening', listener: () => void): this;
+        prependOnceListener(event: 'checkContinue', listener: RequestListener): this;
+        prependOnceListener(event: 'checkExpectation', listener: RequestListener): this;
+        prependOnceListener(event: 'clientError', listener: (err: Error, socket: stream.Duplex) => void): this;
+        prependOnceListener(event: 'connect', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+        prependOnceListener(event: 'request', listener: RequestListener): this;
+        prependOnceListener(event: 'upgrade', listener: (req: IncomingMessage, socket: stream.Duplex, head: Buffer) => void): this;
+    }
+    /**
+     * This class serves as the parent class of {@link ClientRequest} and {@link ServerResponse}. It is an abstract of outgoing message from
+     * the perspective of the participants of HTTP transaction.
+     * @since v0.1.17
+     */
+    class OutgoingMessage extends stream.Writable {
+        readonly req: IncomingMessage;
+        chunkedEncoding: boolean;
+        shouldKeepAlive: boolean;
+        useChunkedEncodingByDefault: boolean;
+        sendDate: boolean;
+        /**
+         * @deprecated Use `writableEnded` instead.
+         */
+        finished: boolean;
+        /**
+         * Read-only. `true` if the headers were sent, otherwise `false`.
+         * @since v0.9.3
+         */
+        readonly headersSent: boolean;
+        /**
+         * Aliases of `outgoingMessage.socket`
+         * @since v0.3.0
+         * @deprecated Since v15.12.0,v14.17.1 - Use `socket` instead.
+         */
+        readonly connection: Socket | null;
+        /**
+         * Reference to the underlying socket. Usually, users will not want to access
+         * this property.
+         *
+         * After calling `outgoingMessage.end()`, this property will be nulled.
+         * @since v0.3.0
+         */
+        readonly socket: Socket | null;
+        constructor();
+        /**
+         * Once a socket is associated with the message and is connected,`socket.setTimeout()` will be called with `msecs` as the first parameter.
+         * @since v0.9.12
+         * @param callback Optional function to be called when a timeout occurs. Same as binding to the `timeout` event.
+         */
+        setTimeout(msecs: number, callback?: () => void): this;
+        /**
+         * Sets a single header value for the header object.
+         * @since v0.4.0
+         * @param name Header name
+         * @param value Header value
+         */
+        setHeader(name: string, value: number | string | ReadonlyArray<string>): this;
+        /**
+         * Gets the value of HTTP header with the given name. If such a name doesn't
+         * exist in message, it will be `undefined`.
+         * @since v0.4.0
+         * @param name Name of header
+         */
+        getHeader(name: string): number | string | string[] | undefined;
+        /**
+         * Returns a shallow copy of the current outgoing headers. Since a shallow
+         * copy is used, array values may be mutated without additional calls to
+         * various header-related HTTP module methods. The keys of the returned
+         * object are the header names and the values are the respective header
+         * values. All header names are lowercase.
+         *
+         * The object returned by the `outgoingMessage.getHeaders()` method does
+         * not prototypically inherit from the JavaScript Object. This means that
+         * typical Object methods such as `obj.toString()`, `obj.hasOwnProperty()`,
+         * and others are not defined and will not work.
+         *
+         * ```js
+         * outgoingMessage.setHeader('Foo', 'bar');
+         * outgoingMessage.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);
+         *
+         * const headers = outgoingMessage.getHeaders();
+         * // headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] }
+         * ```
+         * @since v8.0.0
+         */
+        getHeaders(): OutgoingHttpHeaders;
+        /**
+         * Returns an array of names of headers of the outgoing outgoingMessage. All
+         * names are lowercase.
+         * @since v8.0.0
+         */
+        getHeaderNames(): string[];
+        /**
+         * Returns `true` if the header identified by `name` is currently set in the
+         * outgoing headers. The header name is case-insensitive.
+         *
+         * ```js
+         * const hasContentType = outgoingMessage.hasHeader('content-type');
+         * ```
+         * @since v8.0.0
+         */
+        hasHeader(name: string): boolean;
+        /**
+         * Removes a header that is queued for implicit sending.
+         *
+         * ```js
+         * outgoingMessage.removeHeader('Content-Encoding');
+         * ```
+         * @since v0.4.0
+         */
+        removeHeader(name: string): void;
+        /**
+         * Adds HTTP trailers (headers but at the end of the message) to the message.
+         *
+         * Trailers are **only** be emitted if the message is chunked encoded. If not,
+         * the trailer will be silently discarded.
+         *
+         * HTTP requires the `Trailer` header to be sent to emit trailers,
+         * with a list of header fields in its value, e.g.
+         *
+         * ```js
+         * message.writeHead(200, { 'Content-Type': 'text/plain',
+         *                          'Trailer': 'Content-MD5' });
+         * message.write(fileData);
+         * message.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
+         * message.end();
+         * ```
+         *
+         * Attempting to set a header field name or value that contains invalid characters
+         * will result in a `TypeError` being thrown.
+         * @since v0.3.0
+         */
+        addTrailers(headers: OutgoingHttpHeaders | ReadonlyArray<[string, string]>): void;
+        /**
+         * Compulsorily flushes the message headers
+         *
+         * For efficiency reason, Node.js normally buffers the message headers
+         * until `outgoingMessage.end()` is called or the first chunk of message data
+         * is written. It then tries to pack the headers and data into a single TCP
+         * packet.
+         *
+         * It is usually desired (it saves a TCP round-trip), but not when the first
+         * data is not sent until possibly much later. `outgoingMessage.flushHeaders()`bypasses the optimization and kickstarts the request.
+         * @since v1.6.0
+         */
+        flushHeaders(): void;
+    }
+    /**
+     * This object is created internally by an HTTP server, not by the user. It is
+     * passed as the second parameter to the `'request'` event.
+     * @since v0.1.17
+     */
+    class ServerResponse extends OutgoingMessage {
+        /**
+         * When using implicit headers (not calling `response.writeHead()` explicitly),
+         * this property controls the status code that will be sent to the client when
+         * the headers get flushed.
+         *
+         * ```js
+         * response.statusCode = 404;
+         * ```
+         *
+         * After response header was sent to the client, this property indicates the
+         * status code which was sent out.
+         * @since v0.4.0
+         */
+        statusCode: number;
+        /**
+         * When using implicit headers (not calling `response.writeHead()` explicitly),
+         * this property controls the status message that will be sent to the client when
+         * the headers get flushed. If this is left as `undefined` then the standard
+         * message for the status code will be used.
+         *
+         * ```js
+         * response.statusMessage = 'Not found';
+         * ```
+         *
+         * After response header was sent to the client, this property indicates the
+         * status message which was sent out.
+         * @since v0.11.8
+         */
+        statusMessage: string;
+        constructor(req: IncomingMessage);
+        assignSocket(socket: Socket): void;
+        detachSocket(socket: Socket): void;
+        /**
+         * Sends a HTTP/1.1 100 Continue message to the client, indicating that
+         * the request body should be sent. See the `'checkContinue'` event on`Server`.
+         * @since v0.3.0
+         */
+        writeContinue(callback?: () => void): void;
+        /**
+         * Sends a response header to the request. The status code is a 3-digit HTTP
+         * status code, like `404`. The last argument, `headers`, are the response headers.
+         * Optionally one can give a human-readable `statusMessage` as the second
+         * argument.
+         *
+         * `headers` may be an `Array` where the keys and values are in the same list.
+         * It is _not_ a list of tuples. So, the even-numbered offsets are key values,
+         * and the odd-numbered offsets are the associated values. The array is in the same
+         * format as `request.rawHeaders`.
+         *
+         * Returns a reference to the `ServerResponse`, so that calls can be chained.
+         *
+         * ```js
+         * const body = 'hello world';
+         * response
+         *   .writeHead(200, {
+         *     'Content-Length': Buffer.byteLength(body),
+         *     'Content-Type': 'text/plain'
+         *   })
+         *   .end(body);
+         * ```
+         *
+         * This method must only be called once on a message and it must
+         * be called before `response.end()` is called.
+         *
+         * If `response.write()` or `response.end()` are called before calling
+         * this, the implicit/mutable headers will be calculated and call this function.
+         *
+         * When headers have been set with `response.setHeader()`, they will be merged
+         * with any headers passed to `response.writeHead()`, with the headers passed
+         * to `response.writeHead()` given precedence.
+         *
+         * If this method is called and `response.setHeader()` has not been called,
+         * it will directly write the supplied header values onto the network channel
+         * without caching internally, and the `response.getHeader()` on the header
+         * will not yield the expected result. If progressive population of headers is
+         * desired with potential future retrieval and modification, use `response.setHeader()` instead.
+         *
+         * ```js
+         * // Returns content-type = text/plain
+         * const server = http.createServer((req, res) => {
+         *   res.setHeader('Content-Type', 'text/html');
+         *   res.setHeader('X-Foo', 'bar');
+         *   res.writeHead(200, { 'Content-Type': 'text/plain' });
+         *   res.end('ok');
+         * });
+         * ```
+         *
+         * `Content-Length` is given in bytes, not characters. Use `Buffer.byteLength()` to determine the length of the body in bytes. Node.js
+         * does not check whether `Content-Length` and the length of the body which has
+         * been transmitted are equal or not.
+         *
+         * Attempting to set a header field name or value that contains invalid characters
+         * will result in a `TypeError` being thrown.
+         * @since v0.1.30
+         */
+        writeHead(statusCode: number, statusMessage?: string, headers?: OutgoingHttpHeaders | OutgoingHttpHeader[]): this;
+        writeHead(statusCode: number, headers?: OutgoingHttpHeaders | OutgoingHttpHeader[]): this;
+        /**
+         * Sends a HTTP/1.1 102 Processing message to the client, indicating that
+         * the request body should be sent.
+         * @since v10.0.0
+         */
+        writeProcessing(): void;
+    }
+    interface InformationEvent {
+        statusCode: number;
+        statusMessage: string;
+        httpVersion: string;
+        httpVersionMajor: number;
+        httpVersionMinor: number;
+        headers: IncomingHttpHeaders;
+        rawHeaders: string[];
+    }
+    /**
+     * This object is created internally and returned from {@link request}. It
+     * represents an _in-progress_ request whose header has already been queued. The
+     * header is still mutable using the `setHeader(name, value)`,`getHeader(name)`, `removeHeader(name)` API. The actual header will
+     * be sent along with the first data chunk or when calling `request.end()`.
+     *
+     * To get the response, add a listener for `'response'` to the request object.`'response'` will be emitted from the request object when the response
+     * headers have been received. The `'response'` event is executed with one
+     * argument which is an instance of {@link IncomingMessage}.
+     *
+     * During the `'response'` event, one can add listeners to the
+     * response object; particularly to listen for the `'data'` event.
+     *
+     * If no `'response'` handler is added, then the response will be
+     * entirely discarded. However, if a `'response'` event handler is added,
+     * then the data from the response object **must** be consumed, either by
+     * calling `response.read()` whenever there is a `'readable'` event, or
+     * by adding a `'data'` handler, or by calling the `.resume()` method.
+     * Until the data is consumed, the `'end'` event will not fire. Also, until
+     * the data is read it will consume memory that can eventually lead to a
+     * 'process out of memory' error.
+     *
+     * For backward compatibility, `res` will only emit `'error'` if there is an`'error'` listener registered.
+     *
+     * Node.js does not check whether Content-Length and the length of the
+     * body which has been transmitted are equal or not.
+     * @since v0.1.17
+     */
+    class ClientRequest extends OutgoingMessage {
+        /**
+         * The `request.aborted` property will be `true` if the request has
+         * been aborted.
+         * @since v0.11.14
+         * @deprecated Since v17.0.0 - Check `destroyed` instead.
+         */
+        aborted: boolean;
+        /**
+         * The request host.
+         * @since v14.5.0, v12.19.0
+         */
+        host: string;
+        /**
+         * The request protocol.
+         * @since v14.5.0, v12.19.0
+         */
+        protocol: string;
+        /**
+         * Whether the request is send through a reused socket.
+         * @since v13.0.0, v12.16.0
+         */
+        reusedSocket: boolean;
+        /**
+         * Limits maximum response headers count. If set to 0, no limit will be applied.
+         * @default 2000
+         */
+        maxHeadersCount: number;
+        constructor(url: string | URL | ClientRequestArgs, cb?: (res: IncomingMessage) => void);
+        /**
+         * The request method.
+         * @since v0.1.97
+         */
+        method: string;
+        /**
+         * The request path.
+         * @since v0.4.0
+         */
+        path: string;
+        /**
+         * Marks the request as aborting. Calling this will cause remaining data
+         * in the response to be dropped and the socket to be destroyed.
+         * @since v0.3.8
+         * @deprecated Since v14.1.0,v13.14.0 - Use `destroy` instead.
+         */
+        abort(): void;
+        onSocket(socket: Socket): void;
+        /**
+         * Once a socket is assigned to this request and is connected `socket.setTimeout()` will be called.
+         * @since v0.5.9
+         * @param timeout Milliseconds before a request times out.
+         * @param callback Optional function to be called when a timeout occurs. Same as binding to the `'timeout'` event.
+         */
+        setTimeout(timeout: number, callback?: () => void): this;
+        /**
+         * Once a socket is assigned to this request and is connected `socket.setNoDelay()` will be called.
+         * @since v0.5.9
+         */
+        setNoDelay(noDelay?: boolean): void;
+        /**
+         * Once a socket is assigned to this request and is connected `socket.setKeepAlive()` will be called.
+         * @since v0.5.9
+         */
+        setSocketKeepAlive(enable?: boolean, initialDelay?: number): void;
+        /**
+         * Returns an array containing the unique names of the current outgoing raw
+         * headers. Header names are returned with their exact casing being set.
+         *
+         * ```js
+         * request.setHeader('Foo', 'bar');
+         * request.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);
+         *
+         * const headerNames = request.getRawHeaderNames();
+         * // headerNames === ['Foo', 'Set-Cookie']
+         * ```
+         * @since v15.13.0, v14.17.0
+         */
+        getRawHeaderNames(): string[];
+        /**
+         * @deprecated
+         */
+        addListener(event: 'abort', listener: () => void): this;
+        addListener(event: 'connect', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        addListener(event: 'continue', listener: () => void): this;
+        addListener(event: 'information', listener: (info: InformationEvent) => void): this;
+        addListener(event: 'response', listener: (response: IncomingMessage) => void): this;
+        addListener(event: 'socket', listener: (socket: Socket) => void): this;
+        addListener(event: 'timeout', listener: () => void): this;
+        addListener(event: 'upgrade', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'drain', listener: () => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'finish', listener: () => void): this;
+        addListener(event: 'pipe', listener: (src: stream.Readable) => void): this;
+        addListener(event: 'unpipe', listener: (src: stream.Readable) => void): this;
+        addListener(event: string | symbol, listener: (...args: any[]) => void): this;
+        /**
+         * @deprecated
+         */
+        on(event: 'abort', listener: () => void): this;
+        on(event: 'connect', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        on(event: 'continue', listener: () => void): this;
+        on(event: 'information', listener: (info: InformationEvent) => void): this;
+        on(event: 'response', listener: (response: IncomingMessage) => void): this;
+        on(event: 'socket', listener: (socket: Socket) => void): this;
+        on(event: 'timeout', listener: () => void): this;
+        on(event: 'upgrade', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'drain', listener: () => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'finish', listener: () => void): this;
+        on(event: 'pipe', listener: (src: stream.Readable) => void): this;
+        on(event: 'unpipe', listener: (src: stream.Readable) => void): this;
+        on(event: string | symbol, listener: (...args: any[]) => void): this;
+        /**
+         * @deprecated
+         */
+        once(event: 'abort', listener: () => void): this;
+        once(event: 'connect', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        once(event: 'continue', listener: () => void): this;
+        once(event: 'information', listener: (info: InformationEvent) => void): this;
+        once(event: 'response', listener: (response: IncomingMessage) => void): this;
+        once(event: 'socket', listener: (socket: Socket) => void): this;
+        once(event: 'timeout', listener: () => void): this;
+        once(event: 'upgrade', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'drain', listener: () => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'finish', listener: () => void): this;
+        once(event: 'pipe', listener: (src: stream.Readable) => void): this;
+        once(event: 'unpipe', listener: (src: stream.Readable) => void): this;
+        once(event: string | symbol, listener: (...args: any[]) => void): this;
+        /**
+         * @deprecated
+         */
+        prependListener(event: 'abort', listener: () => void): this;
+        prependListener(event: 'connect', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        prependListener(event: 'continue', listener: () => void): this;
+        prependListener(event: 'information', listener: (info: InformationEvent) => void): this;
+        prependListener(event: 'response', listener: (response: IncomingMessage) => void): this;
+        prependListener(event: 'socket', listener: (socket: Socket) => void): this;
+        prependListener(event: 'timeout', listener: () => void): this;
+        prependListener(event: 'upgrade', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'drain', listener: () => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'finish', listener: () => void): this;
+        prependListener(event: 'pipe', listener: (src: stream.Readable) => void): this;
+        prependListener(event: 'unpipe', listener: (src: stream.Readable) => void): this;
+        prependListener(event: string | symbol, listener: (...args: any[]) => void): this;
+        /**
+         * @deprecated
+         */
+        prependOnceListener(event: 'abort', listener: () => void): this;
+        prependOnceListener(event: 'connect', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        prependOnceListener(event: 'continue', listener: () => void): this;
+        prependOnceListener(event: 'information', listener: (info: InformationEvent) => void): this;
+        prependOnceListener(event: 'response', listener: (response: IncomingMessage) => void): this;
+        prependOnceListener(event: 'socket', listener: (socket: Socket) => void): this;
+        prependOnceListener(event: 'timeout', listener: () => void): this;
+        prependOnceListener(event: 'upgrade', listener: (response: IncomingMessage, socket: Socket, head: Buffer) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'drain', listener: () => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'finish', listener: () => void): this;
+        prependOnceListener(event: 'pipe', listener: (src: stream.Readable) => void): this;
+        prependOnceListener(event: 'unpipe', listener: (src: stream.Readable) => void): this;
+        prependOnceListener(event: string | symbol, listener: (...args: any[]) => void): this;
+    }
+    /**
+     * An `IncomingMessage` object is created by {@link Server} or {@link ClientRequest} and passed as the first argument to the `'request'` and `'response'` event respectively. It may be used to
+     * access response
+     * status, headers and data.
+     *
+     * Different from its `socket` value which is a subclass of `stream.Duplex`, the`IncomingMessage` itself extends `stream.Readable` and is created separately to
+     * parse and emit the incoming HTTP headers and payload, as the underlying socket
+     * may be reused multiple times in case of keep-alive.
+     * @since v0.1.17
+     */
+    class IncomingMessage extends stream.Readable {
+        constructor(socket: Socket);
+        /**
+         * The `message.aborted` property will be `true` if the request has
+         * been aborted.
+         * @since v10.1.0
+         * @deprecated Since v17.0.0 - Check `message.destroyed` from [stream.Readable](https://nodejs.org/dist/latest-v17.x/docs/api/stream.html#class-streamreadable).
+         */
+        aborted: boolean;
+        /**
+         * In case of server request, the HTTP version sent by the client. In the case of
+         * client response, the HTTP version of the connected-to server.
+         * Probably either `'1.1'` or `'1.0'`.
+         *
+         * Also `message.httpVersionMajor` is the first integer and`message.httpVersionMinor` is the second.
+         * @since v0.1.1
+         */
+        httpVersion: string;
+        httpVersionMajor: number;
+        httpVersionMinor: number;
+        /**
+         * The `message.complete` property will be `true` if a complete HTTP message has
+         * been received and successfully parsed.
+         *
+         * This property is particularly useful as a means of determining if a client or
+         * server fully transmitted a message before a connection was terminated:
+         *
+         * ```js
+         * const req = http.request({
+         *   host: '127.0.0.1',
+         *   port: 8080,
+         *   method: 'POST'
+         * }, (res) => {
+         *   res.resume();
+         *   res.on('end', () => {
+         *     if (!res.complete)
+         *       console.error(
+         *         'The connection was terminated while the message was still being sent');
+         *   });
+         * });
+         * ```
+         * @since v0.3.0
+         */
+        complete: boolean;
+        /**
+         * Alias for `message.socket`.
+         * @since v0.1.90
+         * @deprecated Since v16.0.0 - Use `socket`.
+         */
+        connection: Socket;
+        /**
+         * The `net.Socket` object associated with the connection.
+         *
+         * With HTTPS support, use `request.socket.getPeerCertificate()` to obtain the
+         * client's authentication details.
+         *
+         * This property is guaranteed to be an instance of the `net.Socket` class,
+         * a subclass of `stream.Duplex`, unless the user specified a socket
+         * type other than `net.Socket`.
+         * @since v0.3.0
+         */
+        socket: Socket;
+        /**
+         * The request/response headers object.
+         *
+         * Key-value pairs of header names and values. Header names are lower-cased.
+         *
+         * ```js
+         * // Prints something like:
+         * //
+         * // { 'user-agent': 'curl/7.22.0',
+         * //   host: '127.0.0.1:8000',
+         * //   accept: '*' }
+         * console.log(request.headers);
+         * ```
+         *
+         * Duplicates in raw headers are handled in the following ways, depending on the
+         * header name:
+         *
+         * * Duplicates of `age`, `authorization`, `content-length`, `content-type`,`etag`, `expires`, `from`, `host`, `if-modified-since`, `if-unmodified-since`,`last-modified`, `location`,
+         * `max-forwards`, `proxy-authorization`, `referer`,`retry-after`, `server`, or `user-agent` are discarded.
+         * * `set-cookie` is always an array. Duplicates are added to the array.
+         * * For duplicate `cookie` headers, the values are joined together with '; '.
+         * * For all other headers, the values are joined together with ', '.
+         * @since v0.1.5
+         */
+        headers: IncomingHttpHeaders;
+        /**
+         * The raw request/response headers list exactly as they were received.
+         *
+         * The keys and values are in the same list. It is _not_ a
+         * list of tuples. So, the even-numbered offsets are key values, and the
+         * odd-numbered offsets are the associated values.
+         *
+         * Header names are not lowercased, and duplicates are not merged.
+         *
+         * ```js
+         * // Prints something like:
+         * //
+         * // [ 'user-agent',
+         * //   'this is invalid because there can be only one',
+         * //   'User-Agent',
+         * //   'curl/7.22.0',
+         * //   'Host',
+         * //   '127.0.0.1:8000',
+         * //   'ACCEPT',
+         * //   '*' ]
+         * console.log(request.rawHeaders);
+         * ```
+         * @since v0.11.6
+         */
+        rawHeaders: string[];
+        /**
+         * The request/response trailers object. Only populated at the `'end'` event.
+         * @since v0.3.0
+         */
+        trailers: NodeJS.Dict<string>;
+        /**
+         * The raw request/response trailer keys and values exactly as they were
+         * received. Only populated at the `'end'` event.
+         * @since v0.11.6
+         */
+        rawTrailers: string[];
+        /**
+         * Calls `message.socket.setTimeout(msecs, callback)`.
+         * @since v0.5.9
+         */
+        setTimeout(msecs: number, callback?: () => void): this;
+        /**
+         * **Only valid for request obtained from {@link Server}.**
+         *
+         * The request method as a string. Read only. Examples: `'GET'`, `'DELETE'`.
+         * @since v0.1.1
+         */
+        method?: string | undefined;
+        /**
+         * **Only valid for request obtained from {@link Server}.**
+         *
+         * Request URL string. This contains only the URL that is present in the actual
+         * HTTP request. Take the following request:
+         *
+         * ```http
+         * GET /status?name=ryan HTTP/1.1
+         * Accept: text/plain
+         * ```
+         *
+         * To parse the URL into its parts:
+         *
+         * ```js
+         * new URL(request.url, `http://${request.headers.host}`);
+         * ```
+         *
+         * When `request.url` is `'/status?name=ryan'` and`request.headers.host` is `'localhost:3000'`:
+         *
+         * ```console
+         * $ node
+         * > new URL(request.url, `http://${request.headers.host}`)
+         * URL {
+         *   href: 'http://localhost:3000/status?name=ryan',
+         *   origin: 'http://localhost:3000',
+         *   protocol: 'http:',
+         *   username: '',
+         *   password: '',
+         *   host: 'localhost:3000',
+         *   hostname: 'localhost',
+         *   port: '3000',
+         *   pathname: '/status',
+         *   search: '?name=ryan',
+         *   searchParams: URLSearchParams { 'name' => 'ryan' },
+         *   hash: ''
+         * }
+         * ```
+         * @since v0.1.90
+         */
+        url?: string | undefined;
+        /**
+         * **Only valid for response obtained from {@link ClientRequest}.**
+         *
+         * The 3-digit HTTP response status code. E.G. `404`.
+         * @since v0.1.1
+         */
+        statusCode?: number | undefined;
+        /**
+         * **Only valid for response obtained from {@link ClientRequest}.**
+         *
+         * The HTTP response status message (reason phrase). E.G. `OK` or `Internal Server Error`.
+         * @since v0.11.10
+         */
+        statusMessage?: string | undefined;
+        /**
+         * Calls `destroy()` on the socket that received the `IncomingMessage`. If `error`is provided, an `'error'` event is emitted on the socket and `error` is passed
+         * as an argument to any listeners on the event.
+         * @since v0.3.0
+         */
+        destroy(error?: Error): this;
+    }
+    interface AgentOptions extends Partial<TcpSocketConnectOpts> {
+        /**
+         * Keep sockets around in a pool to be used by other requests in the future. Default = false
+         */
+        keepAlive?: boolean | undefined;
+        /**
+         * When using HTTP KeepAlive, how often to send TCP KeepAlive packets over sockets being kept alive. Default = 1000.
+         * Only relevant if keepAlive is set to true.
+         */
+        keepAliveMsecs?: number | undefined;
+        /**
+         * Maximum number of sockets to allow per host. Default for Node 0.10 is 5, default for Node 0.12 is Infinity
+         */
+        maxSockets?: number | undefined;
+        /**
+         * Maximum number of sockets allowed for all hosts in total. Each request will use a new socket until the maximum is reached. Default: Infinity.
+         */
+        maxTotalSockets?: number | undefined;
+        /**
+         * Maximum number of sockets to leave open in a free state. Only relevant if keepAlive is set to true. Default = 256.
+         */
+        maxFreeSockets?: number | undefined;
+        /**
+         * Socket timeout in milliseconds. This will set the timeout after the socket is connected.
+         */
+        timeout?: number | undefined;
+        /**
+         * Scheduling strategy to apply when picking the next free socket to use.
+         * @default `lifo`
+         */
+        scheduling?: 'fifo' | 'lifo' | undefined;
+    }
+    /**
+     * An `Agent` is responsible for managing connection persistence
+     * and reuse for HTTP clients. It maintains a queue of pending requests
+     * for a given host and port, reusing a single socket connection for each
+     * until the queue is empty, at which time the socket is either destroyed
+     * or put into a pool where it is kept to be used again for requests to the
+     * same host and port. Whether it is destroyed or pooled depends on the`keepAlive` `option`.
+     *
+     * Pooled connections have TCP Keep-Alive enabled for them, but servers may
+     * still close idle connections, in which case they will be removed from the
+     * pool and a new connection will be made when a new HTTP request is made for
+     * that host and port. Servers may also refuse to allow multiple requests
+     * over the same connection, in which case the connection will have to be
+     * remade for every request and cannot be pooled. The `Agent` will still make
+     * the requests to that server, but each one will occur over a new connection.
+     *
+     * When a connection is closed by the client or the server, it is removed
+     * from the pool. Any unused sockets in the pool will be unrefed so as not
+     * to keep the Node.js process running when there are no outstanding requests.
+     * (see `socket.unref()`).
+     *
+     * It is good practice, to `destroy()` an `Agent` instance when it is no
+     * longer in use, because unused sockets consume OS resources.
+     *
+     * Sockets are removed from an agent when the socket emits either
+     * a `'close'` event or an `'agentRemove'` event. When intending to keep one
+     * HTTP request open for a long time without keeping it in the agent, something
+     * like the following may be done:
+     *
+     * ```js
+     * http.get(options, (res) => {
+     *   // Do stuff
+     * }).on('socket', (socket) => {
+     *   socket.emit('agentRemove');
+     * });
+     * ```
+     *
+     * An agent may also be used for an individual request. By providing`{agent: false}` as an option to the `http.get()` or `http.request()`functions, a one-time use `Agent` with default options
+     * will be used
+     * for the client connection.
+     *
+     * `agent:false`:
+     *
+     * ```js
+     * http.get({
+     *   hostname: 'localhost',
+     *   port: 80,
+     *   path: '/',
+     *   agent: false  // Create a new agent just for this one request
+     * }, (res) => {
+     *   // Do stuff with response
+     * });
+     * ```
+     * @since v0.3.4
+     */
+    class Agent {
+        /**
+         * By default set to 256\. For agents with `keepAlive` enabled, this
+         * sets the maximum number of sockets that will be left open in the free
+         * state.
+         * @since v0.11.7
+         */
+        maxFreeSockets: number;
+        /**
+         * By default set to `Infinity`. Determines how many concurrent sockets the agent
+         * can have open per origin. Origin is the returned value of `agent.getName()`.
+         * @since v0.3.6
+         */
+        maxSockets: number;
+        /**
+         * By default set to `Infinity`. Determines how many concurrent sockets the agent
+         * can have open. Unlike `maxSockets`, this parameter applies across all origins.
+         * @since v14.5.0, v12.19.0
+         */
+        maxTotalSockets: number;
+        /**
+         * An object which contains arrays of sockets currently awaiting use by
+         * the agent when `keepAlive` is enabled. Do not modify.
+         *
+         * Sockets in the `freeSockets` list will be automatically destroyed and
+         * removed from the array on `'timeout'`.
+         * @since v0.11.4
+         */
+        readonly freeSockets: NodeJS.ReadOnlyDict<Socket[]>;
+        /**
+         * An object which contains arrays of sockets currently in use by the
+         * agent. Do not modify.
+         * @since v0.3.6
+         */
+        readonly sockets: NodeJS.ReadOnlyDict<Socket[]>;
+        /**
+         * An object which contains queues of requests that have not yet been assigned to
+         * sockets. Do not modify.
+         * @since v0.5.9
+         */
+        readonly requests: NodeJS.ReadOnlyDict<IncomingMessage[]>;
+        constructor(opts?: AgentOptions);
+        /**
+         * Destroy any sockets that are currently in use by the agent.
+         *
+         * It is usually not necessary to do this. However, if using an
+         * agent with `keepAlive` enabled, then it is best to explicitly shut down
+         * the agent when it is no longer needed. Otherwise,
+         * sockets might stay open for quite a long time before the server
+         * terminates them.
+         * @since v0.11.4
+         */
+        destroy(): void;
+    }
+    const METHODS: string[];
+    const STATUS_CODES: {
+        [errorCode: number]: string | undefined;
+        [errorCode: string]: string | undefined;
+    };
+    /**
+     * Returns a new instance of {@link Server}.
+     *
+     * The `requestListener` is a function which is automatically
+     * added to the `'request'` event.
+     * @since v0.1.13
+     */
+    function createServer(requestListener?: RequestListener): Server;
+    function createServer(options: ServerOptions, requestListener?: RequestListener): Server;
+    // although RequestOptions are passed as ClientRequestArgs to ClientRequest directly,
+    // create interface RequestOptions would make the naming more clear to developers
+    interface RequestOptions extends ClientRequestArgs {}
+    /**
+     * Node.js maintains several connections per server to make HTTP requests.
+     * This function allows one to transparently issue requests.
+     *
+     * `url` can be a string or a `URL` object. If `url` is a
+     * string, it is automatically parsed with `new URL()`. If it is a `URL` object, it will be automatically converted to an ordinary `options` object.
+     *
+     * If both `url` and `options` are specified, the objects are merged, with the`options` properties taking precedence.
+     *
+     * The optional `callback` parameter will be added as a one-time listener for
+     * the `'response'` event.
+     *
+     * `http.request()` returns an instance of the {@link ClientRequest} class. The `ClientRequest` instance is a writable stream. If one needs to
+     * upload a file with a POST request, then write to the `ClientRequest` object.
+     *
+     * ```js
+     * const http = require('http');
+     *
+     * const postData = JSON.stringify({
+     *   'msg': 'Hello World!'
+     * });
+     *
+     * const options = {
+     *   hostname: 'www.google.com',
+     *   port: 80,
+     *   path: '/upload',
+     *   method: 'POST',
+     *   headers: {
+     *     'Content-Type': 'application/json',
+     *     'Content-Length': Buffer.byteLength(postData)
+     *   }
+     * };
+     *
+     * const req = http.request(options, (res) => {
+     *   console.log(`STATUS: ${res.statusCode}`);
+     *   console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
+     *   res.setEncoding('utf8');
+     *   res.on('data', (chunk) => {
+     *     console.log(`BODY: ${chunk}`);
+     *   });
+     *   res.on('end', () => {
+     *     console.log('No more data in response.');
+     *   });
+     * });
+     *
+     * req.on('error', (e) => {
+     *   console.error(`problem with request: ${e.message}`);
+     * });
+     *
+     * // Write data to request body
+     * req.write(postData);
+     * req.end();
+     * ```
+     *
+     * In the example `req.end()` was called. With `http.request()` one
+     * must always call `req.end()` to signify the end of the request -
+     * even if there is no data being written to the request body.
+     *
+     * If any error is encountered during the request (be that with DNS resolution,
+     * TCP level errors, or actual HTTP parse errors) an `'error'` event is emitted
+     * on the returned request object. As with all `'error'` events, if no listeners
+     * are registered the error will be thrown.
+     *
+     * There are a few special headers that should be noted.
+     *
+     * * Sending a 'Connection: keep-alive' will notify Node.js that the connection to
+     * the server should be persisted until the next request.
+     * * Sending a 'Content-Length' header will disable the default chunked encoding.
+     * * Sending an 'Expect' header will immediately send the request headers.
+     * Usually, when sending 'Expect: 100-continue', both a timeout and a listener
+     * for the `'continue'` event should be set. See RFC 2616 Section 8.2.3 for more
+     * information.
+     * * Sending an Authorization header will override using the `auth` option
+     * to compute basic authentication.
+     *
+     * Example using a `URL` as `options`:
+     *
+     * ```js
+     * const options = new URL('http://abc:xyz@example.com');
+     *
+     * const req = http.request(options, (res) => {
+     *   // ...
+     * });
+     * ```
+     *
+     * In a successful request, the following events will be emitted in the following
+     * order:
+     *
+     * * `'socket'`
+     * * `'response'`
+     *    * `'data'` any number of times, on the `res` object
+     *    (`'data'` will not be emitted at all if the response body is empty, for
+     *    instance, in most redirects)
+     *    * `'end'` on the `res` object
+     * * `'close'`
+     *
+     * In the case of a connection error, the following events will be emitted:
+     *
+     * * `'socket'`
+     * * `'error'`
+     * * `'close'`
+     *
+     * In the case of a premature connection close before the response is received,
+     * the following events will be emitted in the following order:
+     *
+     * * `'socket'`
+     * * `'error'` with an error with message `'Error: socket hang up'` and code`'ECONNRESET'`
+     * * `'close'`
+     *
+     * In the case of a premature connection close after the response is received,
+     * the following events will be emitted in the following order:
+     *
+     * * `'socket'`
+     * * `'response'`
+     *    * `'data'` any number of times, on the `res` object
+     * * (connection closed here)
+     * * `'aborted'` on the `res` object
+     * * `'error'` on the `res` object with an error with message`'Error: aborted'` and code `'ECONNRESET'`.
+     * * `'close'`
+     * * `'close'` on the `res` object
+     *
+     * If `req.destroy()` is called before a socket is assigned, the following
+     * events will be emitted in the following order:
+     *
+     * * (`req.destroy()` called here)
+     * * `'error'` with an error with message `'Error: socket hang up'` and code`'ECONNRESET'`
+     * * `'close'`
+     *
+     * If `req.destroy()` is called before the connection succeeds, the following
+     * events will be emitted in the following order:
+     *
+     * * `'socket'`
+     * * (`req.destroy()` called here)
+     * * `'error'` with an error with message `'Error: socket hang up'` and code`'ECONNRESET'`
+     * * `'close'`
+     *
+     * If `req.destroy()` is called after the response is received, the following
+     * events will be emitted in the following order:
+     *
+     * * `'socket'`
+     * * `'response'`
+     *    * `'data'` any number of times, on the `res` object
+     * * (`req.destroy()` called here)
+     * * `'aborted'` on the `res` object
+     * * `'error'` on the `res` object with an error with message`'Error: aborted'` and code `'ECONNRESET'`.
+     * * `'close'`
+     * * `'close'` on the `res` object
+     *
+     * If `req.abort()` is called before a socket is assigned, the following
+     * events will be emitted in the following order:
+     *
+     * * (`req.abort()` called here)
+     * * `'abort'`
+     * * `'close'`
+     *
+     * If `req.abort()` is called before the connection succeeds, the following
+     * events will be emitted in the following order:
+     *
+     * * `'socket'`
+     * * (`req.abort()` called here)
+     * * `'abort'`
+     * * `'error'` with an error with message `'Error: socket hang up'` and code`'ECONNRESET'`
+     * * `'close'`
+     *
+     * If `req.abort()` is called after the response is received, the following
+     * events will be emitted in the following order:
+     *
+     * * `'socket'`
+     * * `'response'`
+     *    * `'data'` any number of times, on the `res` object
+     * * (`req.abort()` called here)
+     * * `'abort'`
+     * * `'aborted'` on the `res` object
+     * * `'error'` on the `res` object with an error with message`'Error: aborted'` and code `'ECONNRESET'`.
+     * * `'close'`
+     * * `'close'` on the `res` object
+     *
+     * Setting the `timeout` option or using the `setTimeout()` function will
+     * not abort the request or do anything besides add a `'timeout'` event.
+     *
+     * Passing an `AbortSignal` and then calling `abort` on the corresponding`AbortController` will behave the same way as calling `.destroy()` on the
+     * request itself.
+     * @since v0.3.6
+     */
+    function request(options: RequestOptions | string | URL, callback?: (res: IncomingMessage) => void): ClientRequest;
+    function request(url: string | URL, options: RequestOptions, callback?: (res: IncomingMessage) => void): ClientRequest;
+    /**
+     * Since most requests are GET requests without bodies, Node.js provides this
+     * convenience method. The only difference between this method and {@link request} is that it sets the method to GET and calls `req.end()`automatically. The callback must take care to consume the
+     * response
+     * data for reasons stated in {@link ClientRequest} section.
+     *
+     * The `callback` is invoked with a single argument that is an instance of {@link IncomingMessage}.
+     *
+     * JSON fetching example:
+     *
+     * ```js
+     * http.get('http://localhost:8000/', (res) => {
+     *   const { statusCode } = res;
+     *   const contentType = res.headers['content-type'];
+     *
+     *   let error;
+     *   // Any 2xx status code signals a successful response but
+     *   // here we're only checking for 200.
+     *   if (statusCode !== 200) {
+     *     error = new Error('Request Failed.\n' +
+     *                       `Status Code: ${statusCode}`);
+     *   } else if (!/^application\/json/.test(contentType)) {
+     *     error = new Error('Invalid content-type.\n' +
+     *                       `Expected application/json but received ${contentType}`);
+     *   }
+     *   if (error) {
+     *     console.error(error.message);
+     *     // Consume response data to free up memory
+     *     res.resume();
+     *     return;
+     *   }
+     *
+     *   res.setEncoding('utf8');
+     *   let rawData = '';
+     *   res.on('data', (chunk) => { rawData += chunk; });
+     *   res.on('end', () => {
+     *     try {
+     *       const parsedData = JSON.parse(rawData);
+     *       console.log(parsedData);
+     *     } catch (e) {
+     *       console.error(e.message);
+     *     }
+     *   });
+     * }).on('error', (e) => {
+     *   console.error(`Got error: ${e.message}`);
+     * });
+     *
+     * // Create a local server to receive data from
+     * const server = http.createServer((req, res) => {
+     *   res.writeHead(200, { 'Content-Type': 'application/json' });
+     *   res.end(JSON.stringify({
+     *     data: 'Hello World!'
+     *   }));
+     * });
+     *
+     * server.listen(8000);
+     * ```
+     * @since v0.3.6
+     * @param options Accepts the same `options` as {@link request}, with the `method` always set to `GET`. Properties that are inherited from the prototype are ignored.
+     */
+    function get(options: RequestOptions | string | URL, callback?: (res: IncomingMessage) => void): ClientRequest;
+    function get(url: string | URL, options: RequestOptions, callback?: (res: IncomingMessage) => void): ClientRequest;
+    let globalAgent: Agent;
+    /**
+     * Read-only property specifying the maximum allowed size of HTTP headers in bytes.
+     * Defaults to 16KB. Configurable using the `--max-http-header-size` CLI option.
+     */
+    const maxHeaderSize: number;
+}
+declare module 'node:http' {
+    export * from 'http';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * HTTPS is the HTTP protocol over TLS/SSL. In Node.js this is implemented as a
+ * separate module.
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/https.js)
+ */
+declare module 'https' {
+    import { Duplex } from 'node:stream';
+    import * as tls from 'node:tls';
+    import * as http from 'node:http';
+    import { URL } from 'node:url';
+    type ServerOptions = tls.SecureContextOptions & tls.TlsOptions & http.ServerOptions;
+    type RequestOptions = http.RequestOptions &
+        tls.SecureContextOptions & {
+            rejectUnauthorized?: boolean | undefined; // Defaults to true
+            servername?: string | undefined; // SNI TLS Extension
+        };
+    interface AgentOptions extends http.AgentOptions, tls.ConnectionOptions {
+        rejectUnauthorized?: boolean | undefined;
+        maxCachedSessions?: number | undefined;
+    }
+    /**
+     * An `Agent` object for HTTPS similar to `http.Agent`. See {@link request} for more information.
+     * @since v0.4.5
+     */
+    class Agent extends http.Agent {
+        constructor(options?: AgentOptions);
+        options: AgentOptions;
+    }
+    interface Server extends http.Server {}
+    /**
+     * See `http.Server` for more information.
+     * @since v0.3.4
+     */
+    class Server extends tls.Server {
+        constructor(requestListener?: http.RequestListener);
+        constructor(options: ServerOptions, requestListener?: http.RequestListener);
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'keylog', listener: (line: Buffer, tlsSocket: tls.TLSSocket) => void): this;
+        addListener(event: 'newSession', listener: (sessionId: Buffer, sessionData: Buffer, callback: (err: Error, resp: Buffer) => void) => void): this;
+        addListener(event: 'OCSPRequest', listener: (certificate: Buffer, issuer: Buffer, callback: (err: Error | null, resp: Buffer) => void) => void): this;
+        addListener(event: 'resumeSession', listener: (sessionId: Buffer, callback: (err: Error, sessionData: Buffer) => void) => void): this;
+        addListener(event: 'secureConnection', listener: (tlsSocket: tls.TLSSocket) => void): this;
+        addListener(event: 'tlsClientError', listener: (err: Error, tlsSocket: tls.TLSSocket) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'connection', listener: (socket: Duplex) => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'listening', listener: () => void): this;
+        addListener(event: 'checkContinue', listener: http.RequestListener): this;
+        addListener(event: 'checkExpectation', listener: http.RequestListener): this;
+        addListener(event: 'clientError', listener: (err: Error, socket: Duplex) => void): this;
+        addListener(event: 'connect', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        addListener(event: 'request', listener: http.RequestListener): this;
+        addListener(event: 'upgrade', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        emit(event: string, ...args: any[]): boolean;
+        emit(event: 'keylog', line: Buffer, tlsSocket: tls.TLSSocket): boolean;
+        emit(event: 'newSession', sessionId: Buffer, sessionData: Buffer, callback: (err: Error, resp: Buffer) => void): boolean;
+        emit(event: 'OCSPRequest', certificate: Buffer, issuer: Buffer, callback: (err: Error | null, resp: Buffer) => void): boolean;
+        emit(event: 'resumeSession', sessionId: Buffer, callback: (err: Error, sessionData: Buffer) => void): boolean;
+        emit(event: 'secureConnection', tlsSocket: tls.TLSSocket): boolean;
+        emit(event: 'tlsClientError', err: Error, tlsSocket: tls.TLSSocket): boolean;
+        emit(event: 'close'): boolean;
+        emit(event: 'connection', socket: Duplex): boolean;
+        emit(event: 'error', err: Error): boolean;
+        emit(event: 'listening'): boolean;
+        emit(event: 'checkContinue', req: http.IncomingMessage, res: http.ServerResponse): boolean;
+        emit(event: 'checkExpectation', req: http.IncomingMessage, res: http.ServerResponse): boolean;
+        emit(event: 'clientError', err: Error, socket: Duplex): boolean;
+        emit(event: 'connect', req: http.IncomingMessage, socket: Duplex, head: Buffer): boolean;
+        emit(event: 'request', req: http.IncomingMessage, res: http.ServerResponse): boolean;
+        emit(event: 'upgrade', req: http.IncomingMessage, socket: Duplex, head: Buffer): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'keylog', listener: (line: Buffer, tlsSocket: tls.TLSSocket) => void): this;
+        on(event: 'newSession', listener: (sessionId: Buffer, sessionData: Buffer, callback: (err: Error, resp: Buffer) => void) => void): this;
+        on(event: 'OCSPRequest', listener: (certificate: Buffer, issuer: Buffer, callback: (err: Error | null, resp: Buffer) => void) => void): this;
+        on(event: 'resumeSession', listener: (sessionId: Buffer, callback: (err: Error, sessionData: Buffer) => void) => void): this;
+        on(event: 'secureConnection', listener: (tlsSocket: tls.TLSSocket) => void): this;
+        on(event: 'tlsClientError', listener: (err: Error, tlsSocket: tls.TLSSocket) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'connection', listener: (socket: Duplex) => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'listening', listener: () => void): this;
+        on(event: 'checkContinue', listener: http.RequestListener): this;
+        on(event: 'checkExpectation', listener: http.RequestListener): this;
+        on(event: 'clientError', listener: (err: Error, socket: Duplex) => void): this;
+        on(event: 'connect', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        on(event: 'request', listener: http.RequestListener): this;
+        on(event: 'upgrade', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'keylog', listener: (line: Buffer, tlsSocket: tls.TLSSocket) => void): this;
+        once(event: 'newSession', listener: (sessionId: Buffer, sessionData: Buffer, callback: (err: Error, resp: Buffer) => void) => void): this;
+        once(event: 'OCSPRequest', listener: (certificate: Buffer, issuer: Buffer, callback: (err: Error | null, resp: Buffer) => void) => void): this;
+        once(event: 'resumeSession', listener: (sessionId: Buffer, callback: (err: Error, sessionData: Buffer) => void) => void): this;
+        once(event: 'secureConnection', listener: (tlsSocket: tls.TLSSocket) => void): this;
+        once(event: 'tlsClientError', listener: (err: Error, tlsSocket: tls.TLSSocket) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'connection', listener: (socket: Duplex) => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'listening', listener: () => void): this;
+        once(event: 'checkContinue', listener: http.RequestListener): this;
+        once(event: 'checkExpectation', listener: http.RequestListener): this;
+        once(event: 'clientError', listener: (err: Error, socket: Duplex) => void): this;
+        once(event: 'connect', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        once(event: 'request', listener: http.RequestListener): this;
+        once(event: 'upgrade', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'keylog', listener: (line: Buffer, tlsSocket: tls.TLSSocket) => void): this;
+        prependListener(event: 'newSession', listener: (sessionId: Buffer, sessionData: Buffer, callback: (err: Error, resp: Buffer) => void) => void): this;
+        prependListener(event: 'OCSPRequest', listener: (certificate: Buffer, issuer: Buffer, callback: (err: Error | null, resp: Buffer) => void) => void): this;
+        prependListener(event: 'resumeSession', listener: (sessionId: Buffer, callback: (err: Error, sessionData: Buffer) => void) => void): this;
+        prependListener(event: 'secureConnection', listener: (tlsSocket: tls.TLSSocket) => void): this;
+        prependListener(event: 'tlsClientError', listener: (err: Error, tlsSocket: tls.TLSSocket) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'connection', listener: (socket: Duplex) => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'listening', listener: () => void): this;
+        prependListener(event: 'checkContinue', listener: http.RequestListener): this;
+        prependListener(event: 'checkExpectation', listener: http.RequestListener): this;
+        prependListener(event: 'clientError', listener: (err: Error, socket: Duplex) => void): this;
+        prependListener(event: 'connect', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        prependListener(event: 'request', listener: http.RequestListener): this;
+        prependListener(event: 'upgrade', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'keylog', listener: (line: Buffer, tlsSocket: tls.TLSSocket) => void): this;
+        prependOnceListener(event: 'newSession', listener: (sessionId: Buffer, sessionData: Buffer, callback: (err: Error, resp: Buffer) => void) => void): this;
+        prependOnceListener(event: 'OCSPRequest', listener: (certificate: Buffer, issuer: Buffer, callback: (err: Error | null, resp: Buffer) => void) => void): this;
+        prependOnceListener(event: 'resumeSession', listener: (sessionId: Buffer, callback: (err: Error, sessionData: Buffer) => void) => void): this;
+        prependOnceListener(event: 'secureConnection', listener: (tlsSocket: tls.TLSSocket) => void): this;
+        prependOnceListener(event: 'tlsClientError', listener: (err: Error, tlsSocket: tls.TLSSocket) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'connection', listener: (socket: Duplex) => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'listening', listener: () => void): this;
+        prependOnceListener(event: 'checkContinue', listener: http.RequestListener): this;
+        prependOnceListener(event: 'checkExpectation', listener: http.RequestListener): this;
+        prependOnceListener(event: 'clientError', listener: (err: Error, socket: Duplex) => void): this;
+        prependOnceListener(event: 'connect', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+        prependOnceListener(event: 'request', listener: http.RequestListener): this;
+        prependOnceListener(event: 'upgrade', listener: (req: http.IncomingMessage, socket: Duplex, head: Buffer) => void): this;
+    }
+    /**
+     * ```js
+     * // curl -k https://localhost:8000/
+     * const https = require('https');
+     * const fs = require('fs');
+     *
+     * const options = {
+     *   key: fs.readFileSync('test/fixtures/keys/agent2-key.pem'),
+     *   cert: fs.readFileSync('test/fixtures/keys/agent2-cert.pem')
+     * };
+     *
+     * https.createServer(options, (req, res) => {
+     *   res.writeHead(200);
+     *   res.end('hello world\n');
+     * }).listen(8000);
+     * ```
+     *
+     * Or
+     *
+     * ```js
+     * const https = require('https');
+     * const fs = require('fs');
+     *
+     * const options = {
+     *   pfx: fs.readFileSync('test/fixtures/test_cert.pfx'),
+     *   passphrase: 'sample'
+     * };
+     *
+     * https.createServer(options, (req, res) => {
+     *   res.writeHead(200);
+     *   res.end('hello world\n');
+     * }).listen(8000);
+     * ```
+     * @since v0.3.4
+     * @param options Accepts `options` from `createServer`, `createSecureContext` and `createServer`.
+     * @param requestListener A listener to be added to the `'request'` event.
+     */
+    function createServer(requestListener?: http.RequestListener): Server;
+    function createServer(options: ServerOptions, requestListener?: http.RequestListener): Server;
+    /**
+     * Makes a request to a secure web server.
+     *
+     * The following additional `options` from `tls.connect()` are also accepted:`ca`, `cert`, `ciphers`, `clientCertEngine`, `crl`, `dhparam`, `ecdhCurve`,`honorCipherOrder`, `key`, `passphrase`,
+     * `pfx`, `rejectUnauthorized`,`secureOptions`, `secureProtocol`, `servername`, `sessionIdContext`,`highWaterMark`.
+     *
+     * `options` can be an object, a string, or a `URL` object. If `options` is a
+     * string, it is automatically parsed with `new URL()`. If it is a `URL` object, it will be automatically converted to an ordinary `options` object.
+     *
+     * `https.request()` returns an instance of the `http.ClientRequest` class. The `ClientRequest` instance is a writable stream. If one needs to
+     * upload a file with a POST request, then write to the `ClientRequest` object.
+     *
+     * ```js
+     * const https = require('https');
+     *
+     * const options = {
+     *   hostname: 'encrypted.google.com',
+     *   port: 443,
+     *   path: '/',
+     *   method: 'GET'
+     * };
+     *
+     * const req = https.request(options, (res) => {
+     *   console.log('statusCode:', res.statusCode);
+     *   console.log('headers:', res.headers);
+     *
+     *   res.on('data', (d) => {
+     *     process.stdout.write(d);
+     *   });
+     * });
+     *
+     * req.on('error', (e) => {
+     *   console.error(e);
+     * });
+     * req.end();
+     * ```
+     *
+     * Example using options from `tls.connect()`:
+     *
+     * ```js
+     * const options = {
+     *   hostname: 'encrypted.google.com',
+     *   port: 443,
+     *   path: '/',
+     *   method: 'GET',
+     *   key: fs.readFileSync('test/fixtures/keys/agent2-key.pem'),
+     *   cert: fs.readFileSync('test/fixtures/keys/agent2-cert.pem')
+     * };
+     * options.agent = new https.Agent(options);
+     *
+     * const req = https.request(options, (res) => {
+     *   // ...
+     * });
+     * ```
+     *
+     * Alternatively, opt out of connection pooling by not using an `Agent`.
+     *
+     * ```js
+     * const options = {
+     *   hostname: 'encrypted.google.com',
+     *   port: 443,
+     *   path: '/',
+     *   method: 'GET',
+     *   key: fs.readFileSync('test/fixtures/keys/agent2-key.pem'),
+     *   cert: fs.readFileSync('test/fixtures/keys/agent2-cert.pem'),
+     *   agent: false
+     * };
+     *
+     * const req = https.request(options, (res) => {
+     *   // ...
+     * });
+     * ```
+     *
+     * Example using a `URL` as `options`:
+     *
+     * ```js
+     * const options = new URL('https://abc:xyz@example.com');
+     *
+     * const req = https.request(options, (res) => {
+     *   // ...
+     * });
+     * ```
+     *
+     * Example pinning on certificate fingerprint, or the public key (similar to`pin-sha256`):
+     *
+     * ```js
+     * const tls = require('tls');
+     * const https = require('https');
+     * const crypto = require('crypto');
+     *
+     * function sha256(s) {
+     *   return crypto.createHash('sha256').update(s).digest('base64');
+     * }
+     * const options = {
+     *   hostname: 'github.com',
+     *   port: 443,
+     *   path: '/',
+     *   method: 'GET',
+     *   checkServerIdentity: function(host, cert) {
+     *     // Make sure the certificate is issued to the host we are connected to
+     *     const err = tls.checkServerIdentity(host, cert);
+     *     if (err) {
+     *       return err;
+     *     }
+     *
+     *     // Pin the public key, similar to HPKP pin-sha25 pinning
+     *     const pubkey256 = 'pL1+qb9HTMRZJmuC/bB/ZI9d302BYrrqiVuRyW+DGrU=';
+     *     if (sha256(cert.pubkey) !== pubkey256) {
+     *       const msg = 'Certificate verification error: ' +
+     *         `The public key of '${cert.subject.CN}' ` +
+     *         'does not match our pinned fingerprint';
+     *       return new Error(msg);
+     *     }
+     *
+     *     // Pin the exact certificate, rather than the pub key
+     *     const cert256 = '25:FE:39:32:D9:63:8C:8A:FC:A1:9A:29:87:' +
+     *       'D8:3E:4C:1D:98:DB:71:E4:1A:48:03:98:EA:22:6A:BD:8B:93:16';
+     *     if (cert.fingerprint256 !== cert256) {
+     *       const msg = 'Certificate verification error: ' +
+     *         `The certificate of '${cert.subject.CN}' ` +
+     *         'does not match our pinned fingerprint';
+     *       return new Error(msg);
+     *     }
+     *
+     *     // This loop is informational only.
+     *     // Print the certificate and public key fingerprints of all certs in the
+     *     // chain. Its common to pin the public key of the issuer on the public
+     *     // internet, while pinning the public key of the service in sensitive
+     *     // environments.
+     *     do {
+     *       console.log('Subject Common Name:', cert.subject.CN);
+     *       console.log('  Certificate SHA256 fingerprint:', cert.fingerprint256);
+     *
+     *       hash = crypto.createHash('sha256');
+     *       console.log('  Public key ping-sha256:', sha256(cert.pubkey));
+     *
+     *       lastprint256 = cert.fingerprint256;
+     *       cert = cert.issuerCertificate;
+     *     } while (cert.fingerprint256 !== lastprint256);
+     *
+     *   },
+     * };
+     *
+     * options.agent = new https.Agent(options);
+     * const req = https.request(options, (res) => {
+     *   console.log('All OK. Server matched our pinned cert or public key');
+     *   console.log('statusCode:', res.statusCode);
+     *   // Print the HPKP values
+     *   console.log('headers:', res.headers['public-key-pins']);
+     *
+     *   res.on('data', (d) => {});
+     * });
+     *
+     * req.on('error', (e) => {
+     *   console.error(e.message);
+     * });
+     * req.end();
+     * ```
+     *
+     * Outputs for example:
+     *
+     * ```text
+     * Subject Common Name: github.com
+     *   Certificate SHA256 fingerprint: 25:FE:39:32:D9:63:8C:8A:FC:A1:9A:29:87:D8:3E:4C:1D:98:DB:71:E4:1A:48:03:98:EA:22:6A:BD:8B:93:16
+     *   Public key ping-sha256: pL1+qb9HTMRZJmuC/bB/ZI9d302BYrrqiVuRyW+DGrU=
+     * Subject Common Name: DigiCert SHA2 Extended Validation Server CA
+     *   Certificate SHA256 fingerprint: 40:3E:06:2A:26:53:05:91:13:28:5B:AF:80:A0:D4:AE:42:2C:84:8C:9F:78:FA:D0:1F:C9:4B:C5:B8:7F:EF:1A
+     *   Public key ping-sha256: RRM1dGqnDFsCJXBTHky16vi1obOlCgFFn/yOhI/y+ho=
+     * Subject Common Name: DigiCert High Assurance EV Root CA
+     *   Certificate SHA256 fingerprint: 74:31:E5:F4:C3:C1:CE:46:90:77:4F:0B:61:E0:54:40:88:3B:A9:A0:1E:D0:0B:A6:AB:D7:80:6E:D3:B1:18:CF
+     *   Public key ping-sha256: WoiWRyIOVNa9ihaBciRSC7XHjliYS9VwUGOIud4PB18=
+     * All OK. Server matched our pinned cert or public key
+     * statusCode: 200
+     * headers: max-age=0; pin-sha256=""WoiWRyIOVNa9ihaBciRSC7XHjliYS9VwUGOIud4PB18=""; pin-sha256=""RRM1dGqnDFsCJXBTHky16vi1obOlCgFFn/yOhI/y+ho="";
+     * pin-sha256=""k2v657xBsOVe1PQRwOsHsw3bsGT2VzIqz5K+59sNQws=""; pin-sha256=""K87oWBWM9UZfyddvDfoxL+8lpNyoUB2ptGtn0fv6G2Q=""; pin-sha256=""IQBnNBEiFuhj+8x6X8XLgh01V9Ic5/V3IRQLNFFc7v4="";
+     * pin-sha256=""iie1VXtL7HzAMF+/PVPR9xzT80kQxdZeJ+zduCB3uj0=""; pin-sha256=""LvRiGEjRqfzurezaWuj8Wie2gyHMrW5Q06LspMnox7A=""; includeSubDomains
+     * ```
+     * @since v0.3.6
+     * @param options Accepts all `options` from `request`, with some differences in default values:
+     */
+    function request(options: RequestOptions | string | URL, callback?: (res: http.IncomingMessage) => void): http.ClientRequest;
+    function request(url: string | URL, options: RequestOptions, callback?: (res: http.IncomingMessage) => void): http.ClientRequest;
+    /**
+     * Like `http.get()` but for HTTPS.
+     *
+     * `options` can be an object, a string, or a `URL` object. If `options` is a
+     * string, it is automatically parsed with `new URL()`. If it is a `URL` object, it will be automatically converted to an ordinary `options` object.
+     *
+     * ```js
+     * const https = require('https');
+     *
+     * https.get('https://encrypted.google.com/', (res) => {
+     *   console.log('statusCode:', res.statusCode);
+     *   console.log('headers:', res.headers);
+     *
+     *   res.on('data', (d) => {
+     *     process.stdout.write(d);
+     *   });
+     *
+     * }).on('error', (e) => {
+     *   console.error(e);
+     * });
+     * ```
+     * @since v0.3.6
+     * @param options Accepts the same `options` as {@link request}, with the `method` always set to `GET`.
+     */
+    function get(options: RequestOptions | string | URL, callback?: (res: http.IncomingMessage) => void): http.ClientRequest;
+    function get(url: string | URL, options: RequestOptions, callback?: (res: http.IncomingMessage) => void): http.ClientRequest;
+    let globalAgent: Agent;
+}
+declare module 'node:https' {
+    export * from 'https';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+// Type definitions for non-npm package Node.js 17.0
+// Project: https://nodejs.org/
+// Definitions by: Microsoft TypeScript <https://github.com/Microsoft>
+//                 DefinitelyTyped <https://github.com/DefinitelyTyped>
+//                 Alberto Schiabel <https://github.com/jkomyno>
+//                 Alvis HT Tang <https://github.com/alvis>
+//                 Andrew Makarov <https://github.com/r3nya>
+//                 Benjamin Toueg <https://github.com/btoueg>
+//                 Chigozirim C. <https://github.com/smac89>
+//                 David Junger <https://github.com/touffy>
+//                 Deividas Bakanas <https://github.com/DeividasBakanas>
+//                 Eugene Y. Q. Shen <https://github.com/eyqs>
+//                 Hannes Magnusson <https://github.com/Hannes-Magnusson-CK>
+//                 Huw <https://github.com/hoo29>
+//                 Kelvin Jin <https://github.com/kjin>
+//                 Klaus Meinhardt <https://github.com/ajafff>
+//                 Lishude <https://github.com/islishude>
+//                 Mariusz Wiktorczyk <https://github.com/mwiktorczyk>
+//                 Mohsen Azimi <https://github.com/mohsen1>
+//                 Nicolas Even <https://github.com/n-e>
+//                 Nikita Galkin <https://github.com/galkin>
+//                 Parambir Singh <https://github.com/parambirs>
+//                 Sebastian Silbermann <https://github.com/eps1lon>
+//                 Simon Schick <https://github.com/SimonSchick>
+//                 Thomas den Hollander <https://github.com/ThomasdenH>
+//                 Wilco Bakker <https://github.com/WilcoBakker>
+//                 wwwy3y3 <https://github.com/wwwy3y3>
+//                 Samuel Ainsworth <https://github.com/samuela>
+//                 Kyle Uehlein <https://github.com/kuehlein>
+//                 Thanik Bhongbhibhat <https://github.com/bhongy>
+//                 Marcin Kopacz <https://github.com/chyzwar>
+//                 Trivikram Kamat <https://github.com/trivikr>
+//                 Junxiao Shi <https://github.com/yoursunny>
+//                 Ilia Baryshnikov <https://github.com/qwelias>
+//                 ExE Boss <https://github.com/ExE-Boss>
+//                 Piotr Błażejewicz <https://github.com/peterblazejewicz>
+//                 Anna Henningsen <https://github.com/addaleax>
+//                 Victor Perin <https://github.com/victorperin>
+//                 Yongsheng Zhang <https://github.com/ZYSzys>
+//                 NodeJS Contributors <https://github.com/NodeJS>
+//                 Linus Unnebäck <https://github.com/LinusU>
+//                 wafuwafu13 <https://github.com/wafuwafu13>
+// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped
+
+/**
+ * License for programmatically and manually incorporated
+ * documentation aka. `JSDoc` from https://github.com/nodejs/node/tree/master/doc
+ *
+ * Copyright Node.js contributors. All rights reserved.
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the ""Software""), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+// NOTE: These definitions support NodeJS and TypeScript 3.7+.
+
+// Reference required types from the default lib:
+/// <reference lib=""es2020"" />
+/// <reference lib=""esnext.asynciterable"" />
+/// <reference lib=""esnext.intl"" />
+/// <reference lib=""esnext.bigint"" />
+
+// Base definitions for all NodeJS modules that are not specific to any version of TypeScript:
+/// <reference path=""assert.d.ts"" />
+/// <reference path=""assert/strict.d.ts"" />
+/// <reference path=""globals.d.ts"" />
+/// <reference path=""async_hooks.d.ts"" />
+/// <reference path=""buffer.d.ts"" />
+/// <reference path=""child_process.d.ts"" />
+/// <reference path=""cluster.d.ts"" />
+/// <reference path=""console.d.ts"" />
+/// <reference path=""constants.d.ts"" />
+/// <reference path=""crypto.d.ts"" />
+/// <reference path=""dgram.d.ts"" />
+/// <reference path=""diagnostics_channel.d.ts"" />
+/// <reference path=""dns.d.ts"" />
+/// <reference path=""dns/promises.d.ts"" />
+/// <reference path=""dns/promises.d.ts"" />
+/// <reference path=""domain.d.ts"" />
+/// <reference path=""events.d.ts"" />
+/// <reference path=""fs.d.ts"" />
+/// <reference path=""fs/promises.d.ts"" />
+/// <reference path=""http.d.ts"" />
+/// <reference path=""http2.d.ts"" />
+/// <reference path=""https.d.ts"" />
+/// <reference path=""inspector.d.ts"" />
+/// <reference path=""module.d.ts"" />
+/// <reference path=""net.d.ts"" />
+/// <reference path=""os.d.ts"" />
+/// <reference path=""path.d.ts"" />
+/// <reference path=""perf_hooks.d.ts"" />
+/// <reference path=""process.d.ts"" />
+/// <reference path=""punycode.d.ts"" />
+/// <reference path=""querystring.d.ts"" />
+/// <reference path=""readline.d.ts"" />
+/// <reference path=""repl.d.ts"" />
+/// <reference path=""stream.d.ts"" />
+/// <reference path=""stream/promises.d.ts"" />
+/// <reference path=""stream/consumers.d.ts"" />
+/// <reference path=""stream/web.d.ts"" />
+/// <reference path=""string_decoder.d.ts"" />
+/// <reference path=""timers.d.ts"" />
+/// <reference path=""timers/promises.d.ts"" />
+/// <reference path=""tls.d.ts"" />
+/// <reference path=""trace_events.d.ts"" />
+/// <reference path=""tty.d.ts"" />
+/// <reference path=""url.d.ts"" />
+/// <reference path=""util.d.ts"" />
+/// <reference path=""v8.d.ts"" />
+/// <reference path=""vm.d.ts"" />
+/// <reference path=""wasi.d.ts"" />
+/// <reference path=""worker_threads.d.ts"" />
+/// <reference path=""zlib.d.ts"" />
+
+/// <reference path=""globals.global.d.ts"" />"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * @since v0.3.7
+ */
+declare module 'module' {
+    import { URL } from 'node:url';
+    namespace Module {
+        /**
+         * The `module.syncBuiltinESMExports()` method updates all the live bindings for
+         * builtin `ES Modules` to match the properties of the `CommonJS` exports. It
+         * does not add or remove exported names from the `ES Modules`.
+         *
+         * ```js
+         * const fs = require('fs');
+         * const assert = require('assert');
+         * const { syncBuiltinESMExports } = require('module');
+         *
+         * fs.readFile = newAPI;
+         *
+         * delete fs.readFileSync;
+         *
+         * function newAPI() {
+         *   // ...
+         * }
+         *
+         * fs.newAPI = newAPI;
+         *
+         * syncBuiltinESMExports();
+         *
+         * import('fs').then((esmFS) => {
+         *   // It syncs the existing readFile property with the new value
+         *   assert.strictEqual(esmFS.readFile, newAPI);
+         *   // readFileSync has been deleted from the required fs
+         *   assert.strictEqual('readFileSync' in fs, false);
+         *   // syncBuiltinESMExports() does not remove readFileSync from esmFS
+         *   assert.strictEqual('readFileSync' in esmFS, true);
+         *   // syncBuiltinESMExports() does not add names
+         *   assert.strictEqual(esmFS.newAPI, undefined);
+         * });
+         * ```
+         * @since v12.12.0
+         */
+        function syncBuiltinESMExports(): void;
+        /**
+         * `path` is the resolved path for the file for which a corresponding source map
+         * should be fetched.
+         * @since v13.7.0, v12.17.0
+         */
+        function findSourceMap(path: string, error?: Error): SourceMap;
+        interface SourceMapPayload {
+            file: string;
+            version: number;
+            sources: string[];
+            sourcesContent: string[];
+            names: string[];
+            mappings: string;
+            sourceRoot: string;
+        }
+        interface SourceMapping {
+            generatedLine: number;
+            generatedColumn: number;
+            originalSource: string;
+            originalLine: number;
+            originalColumn: number;
+        }
+        /**
+         * @since v13.7.0, v12.17.0
+         */
+        class SourceMap {
+            /**
+             * Getter for the payload used to construct the `SourceMap` instance.
+             */
+            readonly payload: SourceMapPayload;
+            constructor(payload: SourceMapPayload);
+            /**
+             * Given a line number and column number in the generated source file, returns
+             * an object representing the position in the original file. The object returned
+             * consists of the following keys:
+             */
+            findEntry(line: number, column: number): SourceMapping;
+        }
+    }
+    interface Module extends NodeModule {}
+    class Module {
+        static runMain(): void;
+        static wrap(code: string): string;
+        static createRequire(path: string | URL): NodeRequire;
+        static builtinModules: string[];
+        static Module: typeof Module;
+        constructor(id: string, parent?: Module);
+    }
+    global {
+        interface ImportMeta {
+            url: string;
+            /**
+             * @experimental
+             * This feature is only available with the `--experimental-import-meta-resolve`
+             * command flag enabled.
+             *
+             * Provides a module-relative resolution function scoped to each module, returning
+             * the URL string.
+             *
+             * @param specified The module specifier to resolve relative to `parent`.
+             * @param parent The absolute parent module URL to resolve from. If none
+             * is specified, the value of `import.meta.url` is used as the default.
+             */
+            resolve?(specified: string, parent?: string | URL): Promise<string>;
+        }
+    }
+    export = Module;
+}
+declare module 'node:module' {
+    import module = require('module');
+    export = module;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * > Stability: 2 - Stable
+ *
+ * The `net` module provides an asynchronous network API for creating stream-based
+ * TCP or `IPC` servers ({@link createServer}) and clients
+ * ({@link createConnection}).
+ *
+ * It can be accessed using:
+ *
+ * ```js
+ * const net = require('net');
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/net.js)
+ */
+declare module 'net' {
+    import * as stream from 'node:stream';
+    import { Abortable, EventEmitter } from 'node:events';
+    import * as dns from 'node:dns';
+    type LookupFunction = (hostname: string, options: dns.LookupOneOptions, callback: (err: NodeJS.ErrnoException | null, address: string, family: number) => void) => void;
+    interface AddressInfo {
+        address: string;
+        family: string;
+        port: number;
+    }
+    interface SocketConstructorOpts {
+        fd?: number | undefined;
+        allowHalfOpen?: boolean | undefined;
+        readable?: boolean | undefined;
+        writable?: boolean | undefined;
+        signal?: AbortSignal;
+    }
+    interface OnReadOpts {
+        buffer: Uint8Array | (() => Uint8Array);
+        /**
+         * This function is called for every chunk of incoming data.
+         * Two arguments are passed to it: the number of bytes written to buffer and a reference to buffer.
+         * Return false from this function to implicitly pause() the socket.
+         */
+        callback(bytesWritten: number, buf: Uint8Array): boolean;
+    }
+    interface ConnectOpts {
+        /**
+         * If specified, incoming data is stored in a single buffer and passed to the supplied callback when data arrives on the socket.
+         * Note: this will cause the streaming functionality to not provide any data, however events like 'error', 'end', and 'close' will
+         * still be emitted as normal and methods like pause() and resume() will also behave as expected.
+         */
+        onread?: OnReadOpts | undefined;
+    }
+    interface TcpSocketConnectOpts extends ConnectOpts {
+        port: number;
+        host?: string | undefined;
+        localAddress?: string | undefined;
+        localPort?: number | undefined;
+        hints?: number | undefined;
+        family?: number | undefined;
+        lookup?: LookupFunction | undefined;
+    }
+    interface IpcSocketConnectOpts extends ConnectOpts {
+        path: string;
+    }
+    type SocketConnectOpts = TcpSocketConnectOpts | IpcSocketConnectOpts;
+    type SocketReadyState = 'opening' | 'open' | 'readOnly' | 'writeOnly' | 'closed';
+    /**
+     * This class is an abstraction of a TCP socket or a streaming `IPC` endpoint
+     * (uses named pipes on Windows, and Unix domain sockets otherwise). It is also
+     * an `EventEmitter`.
+     *
+     * A `net.Socket` can be created by the user and used directly to interact with
+     * a server. For example, it is returned by {@link createConnection},
+     * so the user can use it to talk to the server.
+     *
+     * It can also be created by Node.js and passed to the user when a connection
+     * is received. For example, it is passed to the listeners of a `'connection'` event emitted on a {@link Server}, so the user can use
+     * it to interact with the client.
+     * @since v0.3.4
+     */
+    class Socket extends stream.Duplex {
+        constructor(options?: SocketConstructorOpts);
+        /**
+         * Sends data on the socket. The second parameter specifies the encoding in the
+         * case of a string. It defaults to UTF8 encoding.
+         *
+         * Returns `true` if the entire data was flushed successfully to the kernel
+         * buffer. Returns `false` if all or part of the data was queued in user memory.`'drain'` will be emitted when the buffer is again free.
+         *
+         * The optional `callback` parameter will be executed when the data is finally
+         * written out, which may not be immediately.
+         *
+         * See `Writable` stream `write()` method for more
+         * information.
+         * @since v0.1.90
+         * @param [encoding='utf8'] Only used when data is `string`.
+         */
+        write(buffer: Uint8Array | string, cb?: (err?: Error) => void): boolean;
+        write(str: Uint8Array | string, encoding?: BufferEncoding, cb?: (err?: Error) => void): boolean;
+        /**
+         * Initiate a connection on a given socket.
+         *
+         * Possible signatures:
+         *
+         * * `socket.connect(options[, connectListener])`
+         * * `socket.connect(path[, connectListener])` for `IPC` connections.
+         * * `socket.connect(port[, host][, connectListener])` for TCP connections.
+         * * Returns: `net.Socket` The socket itself.
+         *
+         * This function is asynchronous. When the connection is established, the `'connect'` event will be emitted. If there is a problem connecting,
+         * instead of a `'connect'` event, an `'error'` event will be emitted with
+         * the error passed to the `'error'` listener.
+         * The last parameter `connectListener`, if supplied, will be added as a listener
+         * for the `'connect'` event **once**.
+         *
+         * This function should only be used for reconnecting a socket after`'close'` has been emitted or otherwise it may lead to undefined
+         * behavior.
+         */
+        connect(options: SocketConnectOpts, connectionListener?: () => void): this;
+        connect(port: number, host: string, connectionListener?: () => void): this;
+        connect(port: number, connectionListener?: () => void): this;
+        connect(path: string, connectionListener?: () => void): this;
+        /**
+         * Set the encoding for the socket as a `Readable Stream`. See `readable.setEncoding()` for more information.
+         * @since v0.1.90
+         * @return The socket itself.
+         */
+        setEncoding(encoding?: BufferEncoding): this;
+        /**
+         * Pauses the reading of data. That is, `'data'` events will not be emitted.
+         * Useful to throttle back an upload.
+         * @return The socket itself.
+         */
+        pause(): this;
+        /**
+         * Resumes reading after a call to `socket.pause()`.
+         * @return The socket itself.
+         */
+        resume(): this;
+        /**
+         * Sets the socket to timeout after `timeout` milliseconds of inactivity on
+         * the socket. By default `net.Socket` do not have a timeout.
+         *
+         * When an idle timeout is triggered the socket will receive a `'timeout'` event but the connection will not be severed. The user must manually call `socket.end()` or `socket.destroy()` to
+         * end the connection.
+         *
+         * ```js
+         * socket.setTimeout(3000);
+         * socket.on('timeout', () => {
+         *   console.log('socket timeout');
+         *   socket.end();
+         * });
+         * ```
+         *
+         * If `timeout` is 0, then the existing idle timeout is disabled.
+         *
+         * The optional `callback` parameter will be added as a one-time listener for the `'timeout'` event.
+         * @since v0.1.90
+         * @return The socket itself.
+         */
+        setTimeout(timeout: number, callback?: () => void): this;
+        /**
+         * Enable/disable the use of Nagle's algorithm.
+         *
+         * When a TCP connection is created, it will have Nagle's algorithm enabled.
+         *
+         * Nagle's algorithm delays data before it is sent via the network. It attempts
+         * to optimize throughput at the expense of latency.
+         *
+         * Passing `true` for `noDelay` or not passing an argument will disable Nagle's
+         * algorithm for the socket. Passing `false` for `noDelay` will enable Nagle's
+         * algorithm.
+         * @since v0.1.90
+         * @param [noDelay=true]
+         * @return The socket itself.
+         */
+        setNoDelay(noDelay?: boolean): this;
+        /**
+         * Enable/disable keep-alive functionality, and optionally set the initial
+         * delay before the first keepalive probe is sent on an idle socket.
+         *
+         * Set `initialDelay` (in milliseconds) to set the delay between the last
+         * data packet received and the first keepalive probe. Setting `0` for`initialDelay` will leave the value unchanged from the default
+         * (or previous) setting.
+         *
+         * Enabling the keep-alive functionality will set the following socket options:
+         *
+         * * `SO_KEEPALIVE=1`
+         * * `TCP_KEEPIDLE=initialDelay`
+         * * `TCP_KEEPCNT=10`
+         * * `TCP_KEEPINTVL=1`
+         * @since v0.1.92
+         * @param [enable=false]
+         * @param [initialDelay=0]
+         * @return The socket itself.
+         */
+        setKeepAlive(enable?: boolean, initialDelay?: number): this;
+        /**
+         * Returns the bound `address`, the address `family` name and `port` of the
+         * socket as reported by the operating system:`{ port: 12346, family: 'IPv4', address: '127.0.0.1' }`
+         * @since v0.1.90
+         */
+        address(): AddressInfo | {};
+        /**
+         * Calling `unref()` on a socket will allow the program to exit if this is the only
+         * active socket in the event system. If the socket is already `unref`ed calling`unref()` again will have no effect.
+         * @since v0.9.1
+         * @return The socket itself.
+         */
+        unref(): this;
+        /**
+         * Opposite of `unref()`, calling `ref()` on a previously `unref`ed socket will_not_ let the program exit if it's the only socket left (the default behavior).
+         * If the socket is `ref`ed calling `ref` again will have no effect.
+         * @since v0.9.1
+         * @return The socket itself.
+         */
+        ref(): this;
+        /**
+         * This property shows the number of characters buffered for writing. The buffer
+         * may contain strings whose length after encoding is not yet known. So this number
+         * is only an approximation of the number of bytes in the buffer.
+         *
+         * `net.Socket` has the property that `socket.write()` always works. This is to
+         * help users get up and running quickly. The computer cannot always keep up
+         * with the amount of data that is written to a socket. The network connection
+         * simply might be too slow. Node.js will internally queue up the data written to a
+         * socket and send it out over the wire when it is possible.
+         *
+         * The consequence of this internal buffering is that memory may grow.
+         * Users who experience large or growing `bufferSize` should attempt to
+         * ""throttle"" the data flows in their program with `socket.pause()` and `socket.resume()`.
+         * @since v0.3.8
+         * @deprecated Since v14.6.0 - Use `writableLength` instead.
+         */
+        readonly bufferSize: number;
+        /**
+         * The amount of received bytes.
+         * @since v0.5.3
+         */
+        readonly bytesRead: number;
+        /**
+         * The amount of bytes sent.
+         * @since v0.5.3
+         */
+        readonly bytesWritten: number;
+        /**
+         * If `true`,`socket.connect(options[, connectListener])` was
+         * called and has not yet finished. It will stay `true` until the socket becomes
+         * connected, then it is set to `false` and the `'connect'` event is emitted. Note
+         * that the `socket.connect(options[, connectListener])` callback is a listener for the `'connect'` event.
+         * @since v6.1.0
+         */
+        readonly connecting: boolean;
+        /**
+         * See `writable.destroyed` for further details.
+         */
+        readonly destroyed: boolean;
+        /**
+         * The string representation of the local IP address the remote client is
+         * connecting on. For example, in a server listening on `'0.0.0.0'`, if a client
+         * connects on `'192.168.1.1'`, the value of `socket.localAddress` would be`'192.168.1.1'`.
+         * @since v0.9.6
+         */
+        readonly localAddress?: string;
+        /**
+         * The numeric representation of the local port. For example, `80` or `21`.
+         * @since v0.9.6
+         */
+        readonly localPort?: number;
+        /**
+         * This property represents the state of the connection as a string.
+         * @see {https://nodejs.org/api/net.html#socketreadystate}
+         * @since v0.5.0
+         */
+        readonly readyState: SocketReadyState;
+        /**
+         * The string representation of the remote IP address. For example,`'74.125.127.100'` or `'2001:4860:a005::68'`. Value may be `undefined` if
+         * the socket is destroyed (for example, if the client disconnected).
+         * @since v0.5.10
+         */
+        readonly remoteAddress?: string | undefined;
+        /**
+         * The string representation of the remote IP family. `'IPv4'` or `'IPv6'`.
+         * @since v0.11.14
+         */
+        readonly remoteFamily?: string | undefined;
+        /**
+         * The numeric representation of the remote port. For example, `80` or `21`.
+         * @since v0.5.10
+         */
+        readonly remotePort?: number | undefined;
+        /**
+         * Half-closes the socket. i.e., it sends a FIN packet. It is possible the
+         * server will still send some data.
+         *
+         * See `writable.end()` for further details.
+         * @since v0.1.90
+         * @param [encoding='utf8'] Only used when data is `string`.
+         * @param callback Optional callback for when the socket is finished.
+         * @return The socket itself.
+         */
+        end(callback?: () => void): this;
+        end(buffer: Uint8Array | string, callback?: () => void): this;
+        end(str: Uint8Array | string, encoding?: BufferEncoding, callback?: () => void): this;
+        /**
+         * events.EventEmitter
+         *   1. close
+         *   2. connect
+         *   3. data
+         *   4. drain
+         *   5. end
+         *   6. error
+         *   7. lookup
+         *   8. timeout
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: (hadError: boolean) => void): this;
+        addListener(event: 'connect', listener: () => void): this;
+        addListener(event: 'data', listener: (data: Buffer) => void): this;
+        addListener(event: 'drain', listener: () => void): this;
+        addListener(event: 'end', listener: () => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'lookup', listener: (err: Error, address: string, family: string | number, host: string) => void): this;
+        addListener(event: 'ready', listener: () => void): this;
+        addListener(event: 'timeout', listener: () => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'close', hadError: boolean): boolean;
+        emit(event: 'connect'): boolean;
+        emit(event: 'data', data: Buffer): boolean;
+        emit(event: 'drain'): boolean;
+        emit(event: 'end'): boolean;
+        emit(event: 'error', err: Error): boolean;
+        emit(event: 'lookup', err: Error, address: string, family: string | number, host: string): boolean;
+        emit(event: 'ready'): boolean;
+        emit(event: 'timeout'): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: (hadError: boolean) => void): this;
+        on(event: 'connect', listener: () => void): this;
+        on(event: 'data', listener: (data: Buffer) => void): this;
+        on(event: 'drain', listener: () => void): this;
+        on(event: 'end', listener: () => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'lookup', listener: (err: Error, address: string, family: string | number, host: string) => void): this;
+        on(event: 'ready', listener: () => void): this;
+        on(event: 'timeout', listener: () => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: (hadError: boolean) => void): this;
+        once(event: 'connect', listener: () => void): this;
+        once(event: 'data', listener: (data: Buffer) => void): this;
+        once(event: 'drain', listener: () => void): this;
+        once(event: 'end', listener: () => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'lookup', listener: (err: Error, address: string, family: string | number, host: string) => void): this;
+        once(event: 'ready', listener: () => void): this;
+        once(event: 'timeout', listener: () => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: (hadError: boolean) => void): this;
+        prependListener(event: 'connect', listener: () => void): this;
+        prependListener(event: 'data', listener: (data: Buffer) => void): this;
+        prependListener(event: 'drain', listener: () => void): this;
+        prependListener(event: 'end', listener: () => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'lookup', listener: (err: Error, address: string, family: string | number, host: string) => void): this;
+        prependListener(event: 'ready', listener: () => void): this;
+        prependListener(event: 'timeout', listener: () => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: (hadError: boolean) => void): this;
+        prependOnceListener(event: 'connect', listener: () => void): this;
+        prependOnceListener(event: 'data', listener: (data: Buffer) => void): this;
+        prependOnceListener(event: 'drain', listener: () => void): this;
+        prependOnceListener(event: 'end', listener: () => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'lookup', listener: (err: Error, address: string, family: string | number, host: string) => void): this;
+        prependOnceListener(event: 'ready', listener: () => void): this;
+        prependOnceListener(event: 'timeout', listener: () => void): this;
+    }
+    interface ListenOptions extends Abortable {
+        port?: number | undefined;
+        host?: string | undefined;
+        backlog?: number | undefined;
+        path?: string | undefined;
+        exclusive?: boolean | undefined;
+        readableAll?: boolean | undefined;
+        writableAll?: boolean | undefined;
+        /**
+         * @default false
+         */
+        ipv6Only?: boolean | undefined;
+    }
+    interface ServerOpts {
+        /**
+         * Indicates whether half-opened TCP connections are allowed.
+         * @default false
+         */
+        allowHalfOpen?: boolean | undefined;
+        /**
+         * Indicates whether the socket should be paused on incoming connections.
+         * @default false
+         */
+        pauseOnConnect?: boolean | undefined;
+    }
+    /**
+     * This class is used to create a TCP or `IPC` server.
+     * @since v0.1.90
+     */
+    class Server extends EventEmitter {
+        constructor(connectionListener?: (socket: Socket) => void);
+        constructor(options?: ServerOpts, connectionListener?: (socket: Socket) => void);
+        /**
+         * Start a server listening for connections. A `net.Server` can be a TCP or
+         * an `IPC` server depending on what it listens to.
+         *
+         * Possible signatures:
+         *
+         * * `server.listen(handle[, backlog][, callback])`
+         * * `server.listen(options[, callback])`
+         * * `server.listen(path[, backlog][, callback])` for `IPC` servers
+         * * `server.listen([port[, host[, backlog]]][, callback])` for TCP servers
+         *
+         * This function is asynchronous. When the server starts listening, the `'listening'` event will be emitted. The last parameter `callback`will be added as a listener for the `'listening'`
+         * event.
+         *
+         * All `listen()` methods can take a `backlog` parameter to specify the maximum
+         * length of the queue of pending connections. The actual length will be determined
+         * by the OS through sysctl settings such as `tcp_max_syn_backlog` and `somaxconn`on Linux. The default value of this parameter is 511 (not 512).
+         *
+         * All {@link Socket} are set to `SO_REUSEADDR` (see [`socket(7)`](https://man7.org/linux/man-pages/man7/socket.7.html) for
+         * details).
+         *
+         * The `server.listen()` method can be called again if and only if there was an
+         * error during the first `server.listen()` call or `server.close()` has been
+         * called. Otherwise, an `ERR_SERVER_ALREADY_LISTEN` error will be thrown.
+         *
+         * One of the most common errors raised when listening is `EADDRINUSE`.
+         * This happens when another server is already listening on the requested`port`/`path`/`handle`. One way to handle this would be to retry
+         * after a certain amount of time:
+         *
+         * ```js
+         * server.on('error', (e) => {
+         *   if (e.code === 'EADDRINUSE') {
+         *     console.log('Address in use, retrying...');
+         *     setTimeout(() => {
+         *       server.close();
+         *       server.listen(PORT, HOST);
+         *     }, 1000);
+         *   }
+         * });
+         * ```
+         */
+        listen(port?: number, hostname?: string, backlog?: number, listeningListener?: () => void): this;
+        listen(port?: number, hostname?: string, listeningListener?: () => void): this;
+        listen(port?: number, backlog?: number, listeningListener?: () => void): this;
+        listen(port?: number, listeningListener?: () => void): this;
+        listen(path: string, backlog?: number, listeningListener?: () => void): this;
+        listen(path: string, listeningListener?: () => void): this;
+        listen(options: ListenOptions, listeningListener?: () => void): this;
+        listen(handle: any, backlog?: number, listeningListener?: () => void): this;
+        listen(handle: any, listeningListener?: () => void): this;
+        /**
+         * Stops the server from accepting new connections and keeps existing
+         * connections. This function is asynchronous, the server is finally closed
+         * when all connections are ended and the server emits a `'close'` event.
+         * The optional `callback` will be called once the `'close'` event occurs. Unlike
+         * that event, it will be called with an `Error` as its only argument if the server
+         * was not open when it was closed.
+         * @since v0.1.90
+         * @param callback Called when the server is closed.
+         */
+        close(callback?: (err?: Error) => void): this;
+        /**
+         * Returns the bound `address`, the address `family` name, and `port` of the server
+         * as reported by the operating system if listening on an IP socket
+         * (useful to find which port was assigned when getting an OS-assigned address):`{ port: 12346, family: 'IPv4', address: '127.0.0.1' }`.
+         *
+         * For a server listening on a pipe or Unix domain socket, the name is returned
+         * as a string.
+         *
+         * ```js
+         * const server = net.createServer((socket) => {
+         *   socket.end('goodbye\n');
+         * }).on('error', (err) => {
+         *   // Handle errors here.
+         *   throw err;
+         * });
+         *
+         * // Grab an arbitrary unused port.
+         * server.listen(() => {
+         *   console.log('opened server on', server.address());
+         * });
+         * ```
+         *
+         * `server.address()` returns `null` before the `'listening'` event has been
+         * emitted or after calling `server.close()`.
+         * @since v0.1.90
+         */
+        address(): AddressInfo | string | null;
+        /**
+         * Asynchronously get the number of concurrent connections on the server. Works
+         * when sockets were sent to forks.
+         *
+         * Callback should take two arguments `err` and `count`.
+         * @since v0.9.7
+         */
+        getConnections(cb: (error: Error | null, count: number) => void): void;
+        /**
+         * Opposite of `unref()`, calling `ref()` on a previously `unref`ed server will_not_ let the program exit if it's the only server left (the default behavior).
+         * If the server is `ref`ed calling `ref()` again will have no effect.
+         * @since v0.9.1
+         */
+        ref(): this;
+        /**
+         * Calling `unref()` on a server will allow the program to exit if this is the only
+         * active server in the event system. If the server is already `unref`ed calling`unref()` again will have no effect.
+         * @since v0.9.1
+         */
+        unref(): this;
+        /**
+         * Set this property to reject connections when the server's connection count gets
+         * high.
+         *
+         * It is not recommended to use this option once a socket has been sent to a child
+         * with `child_process.fork()`.
+         * @since v0.2.0
+         */
+        maxConnections: number;
+        connections: number;
+        /**
+         * Indicates whether or not the server is listening for connections.
+         * @since v5.7.0
+         */
+        listening: boolean;
+        /**
+         * events.EventEmitter
+         *   1. close
+         *   2. connection
+         *   3. error
+         *   4. listening
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'connection', listener: (socket: Socket) => void): this;
+        addListener(event: 'error', listener: (err: Error) => void): this;
+        addListener(event: 'listening', listener: () => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'close'): boolean;
+        emit(event: 'connection', socket: Socket): boolean;
+        emit(event: 'error', err: Error): boolean;
+        emit(event: 'listening'): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'connection', listener: (socket: Socket) => void): this;
+        on(event: 'error', listener: (err: Error) => void): this;
+        on(event: 'listening', listener: () => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'connection', listener: (socket: Socket) => void): this;
+        once(event: 'error', listener: (err: Error) => void): this;
+        once(event: 'listening', listener: () => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'connection', listener: (socket: Socket) => void): this;
+        prependListener(event: 'error', listener: (err: Error) => void): this;
+        prependListener(event: 'listening', listener: () => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'connection', listener: (socket: Socket) => void): this;
+        prependOnceListener(event: 'error', listener: (err: Error) => void): this;
+        prependOnceListener(event: 'listening', listener: () => void): this;
+    }
+    type IPVersion = 'ipv4' | 'ipv6';
+    /**
+     * The `BlockList` object can be used with some network APIs to specify rules for
+     * disabling inbound or outbound access to specific IP addresses, IP ranges, or
+     * IP subnets.
+     * @since v15.0.0, v14.18.0
+     */
+    class BlockList {
+        /**
+         * Adds a rule to block the given IP address.
+         * @since v15.0.0, v14.18.0
+         * @param address An IPv4 or IPv6 address.
+         * @param [type='ipv4'] Either `'ipv4'` or `'ipv6'`.
+         */
+        addAddress(address: string, type?: IPVersion): void;
+        addAddress(address: SocketAddress): void;
+        /**
+         * Adds a rule to block a range of IP addresses from `start` (inclusive) to`end` (inclusive).
+         * @since v15.0.0, v14.18.0
+         * @param start The starting IPv4 or IPv6 address in the range.
+         * @param end The ending IPv4 or IPv6 address in the range.
+         * @param [type='ipv4'] Either `'ipv4'` or `'ipv6'`.
+         */
+        addRange(start: string, end: string, type?: IPVersion): void;
+        addRange(start: SocketAddress, end: SocketAddress): void;
+        /**
+         * Adds a rule to block a range of IP addresses specified as a subnet mask.
+         * @since v15.0.0, v14.18.0
+         * @param net The network IPv4 or IPv6 address.
+         * @param prefix The number of CIDR prefix bits. For IPv4, this must be a value between `0` and `32`. For IPv6, this must be between `0` and `128`.
+         * @param [type='ipv4'] Either `'ipv4'` or `'ipv6'`.
+         */
+        addSubnet(net: SocketAddress, prefix: number): void;
+        addSubnet(net: string, prefix: number, type?: IPVersion): void;
+        /**
+         * Returns `true` if the given IP address matches any of the rules added to the`BlockList`.
+         *
+         * ```js
+         * const blockList = new net.BlockList();
+         * blockList.addAddress('123.123.123.123');
+         * blockList.addRange('10.0.0.1', '10.0.0.10');
+         * blockList.addSubnet('8592:757c:efae:4e45::', 64, 'ipv6');
+         *
+         * console.log(blockList.check('123.123.123.123'));  // Prints: true
+         * console.log(blockList.check('10.0.0.3'));  // Prints: true
+         * console.log(blockList.check('222.111.111.222'));  // Prints: false
+         *
+         * // IPv6 notation for IPv4 addresses works:
+         * console.log(blockList.check('::ffff:7b7b:7b7b', 'ipv6')); // Prints: true
+         * console.log(blockList.check('::ffff:123.123.123.123', 'ipv6')); // Prints: true
+         * ```
+         * @since v15.0.0, v14.18.0
+         * @param address The IP address to check
+         * @param [type='ipv4'] Either `'ipv4'` or `'ipv6'`.
+         */
+        check(address: SocketAddress): boolean;
+        check(address: string, type?: IPVersion): boolean;
+    }
+    interface TcpNetConnectOpts extends TcpSocketConnectOpts, SocketConstructorOpts {
+        timeout?: number | undefined;
+    }
+    interface IpcNetConnectOpts extends IpcSocketConnectOpts, SocketConstructorOpts {
+        timeout?: number | undefined;
+    }
+    type NetConnectOpts = TcpNetConnectOpts | IpcNetConnectOpts;
+    /**
+     * Creates a new TCP or `IPC` server.
+     *
+     * If `allowHalfOpen` is set to `true`, when the other end of the socket
+     * signals the end of transmission, the server will only send back the end of
+     * transmission when `socket.end()` is explicitly called. For example, in the
+     * context of TCP, when a FIN packed is received, a FIN packed is sent
+     * back only when `socket.end()` is explicitly called. Until then the
+     * connection is half-closed (non-readable but still writable). See `'end'` event and [RFC 1122](https://tools.ietf.org/html/rfc1122) (section 4.2.2.13) for more information.
+     *
+     * If `pauseOnConnect` is set to `true`, then the socket associated with each
+     * incoming connection will be paused, and no data will be read from its handle.
+     * This allows connections to be passed between processes without any data being
+     * read by the original process. To begin reading data from a paused socket, call `socket.resume()`.
+     *
+     * The server can be a TCP server or an `IPC` server, depending on what it `listen()` to.
+     *
+     * Here is an example of an TCP echo server which listens for connections
+     * on port 8124:
+     *
+     * ```js
+     * const net = require('net');
+     * const server = net.createServer((c) => {
+     *   // 'connection' listener.
+     *   console.log('client connected');
+     *   c.on('end', () => {
+     *     console.log('client disconnected');
+     *   });
+     *   c.write('hello\r\n');
+     *   c.pipe(c);
+     * });
+     * server.on('error', (err) => {
+     *   throw err;
+     * });
+     * server.listen(8124, () => {
+     *   console.log('server bound');
+     * });
+     * ```
+     *
+     * Test this by using `telnet`:
+     *
+     * ```console
+     * $ telnet localhost 8124
+     * ```
+     *
+     * To listen on the socket `/tmp/echo.sock`:
+     *
+     * ```js
+     * server.listen('/tmp/echo.sock', () => {
+     *   console.log('server bound');
+     * });
+     * ```
+     *
+     * Use `nc` to connect to a Unix domain socket server:
+     *
+     * ```console
+     * $ nc -U /tmp/echo.sock
+     * ```
+     * @since v0.5.0
+     * @param connectionListener Automatically set as a listener for the {@link 'connection'} event.
+     */
+    function createServer(connectionListener?: (socket: Socket) => void): Server;
+    function createServer(options?: ServerOpts, connectionListener?: (socket: Socket) => void): Server;
+    /**
+     * Aliases to {@link createConnection}.
+     *
+     * Possible signatures:
+     *
+     * * {@link connect}
+     * * {@link connect} for `IPC` connections.
+     * * {@link connect} for TCP connections.
+     */
+    function connect(options: NetConnectOpts, connectionListener?: () => void): Socket;
+    function connect(port: number, host?: string, connectionListener?: () => void): Socket;
+    function connect(path: string, connectionListener?: () => void): Socket;
+    /**
+     * A factory function, which creates a new {@link Socket},
+     * immediately initiates connection with `socket.connect()`,
+     * then returns the `net.Socket` that starts the connection.
+     *
+     * When the connection is established, a `'connect'` event will be emitted
+     * on the returned socket. The last parameter `connectListener`, if supplied,
+     * will be added as a listener for the `'connect'` event **once**.
+     *
+     * Possible signatures:
+     *
+     * * {@link createConnection}
+     * * {@link createConnection} for `IPC` connections.
+     * * {@link createConnection} for TCP connections.
+     *
+     * The {@link connect} function is an alias to this function.
+     */
+    function createConnection(options: NetConnectOpts, connectionListener?: () => void): Socket;
+    function createConnection(port: number, host?: string, connectionListener?: () => void): Socket;
+    function createConnection(path: string, connectionListener?: () => void): Socket;
+    /**
+     * Tests if input is an IP address. Returns `0` for invalid strings,
+     * returns `4` for IP version 4 addresses, and returns `6` for IP version 6
+     * addresses.
+     * @since v0.3.0
+     */
+    function isIP(input: string): number;
+    /**
+     * Returns `true` if input is a version 4 IP address, otherwise returns `false`.
+     * @since v0.3.0
+     */
+    function isIPv4(input: string): boolean;
+    /**
+     * Returns `true` if input is a version 6 IP address, otherwise returns `false`.
+     * @since v0.3.0
+     */
+    function isIPv6(input: string): boolean;
+    interface SocketAddressInitOptions {
+        /**
+         * The network address as either an IPv4 or IPv6 string.
+         * @default 127.0.0.1
+         */
+        address?: string | undefined;
+        /**
+         * @default `'ipv4'`
+         */
+        family?: IPVersion | undefined;
+        /**
+         * An IPv6 flow-label used only if `family` is `'ipv6'`.
+         * @default 0
+         */
+        flowlabel?: number | undefined;
+        /**
+         * An IP port.
+         * @default 0
+         */
+        port?: number | undefined;
+    }
+    /**
+     * @since v15.14.0, v14.18.0
+     */
+    class SocketAddress {
+        constructor(options: SocketAddressInitOptions);
+        /**
+         * Either \`'ipv4'\` or \`'ipv6'\`.
+         * @since v15.14.0, v14.18.0
+         */
+        readonly address: string;
+        /**
+         * Either \`'ipv4'\` or \`'ipv6'\`.
+         * @since v15.14.0, v14.18.0
+         */
+        readonly family: IPVersion;
+        /**
+         * @since v15.14.0, v14.18.0
+         */
+        readonly port: number;
+        /**
+         * @since v15.14.0, v14.18.0
+         */
+        readonly flowlabel: number;
+    }
+}
+declare module 'node:net' {
+    export * from 'net';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `os` module provides operating system-related utility methods and
+ * properties. It can be accessed using:
+ *
+ * ```js
+ * const os = require('os');
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/os.js)
+ */
+declare module 'os' {
+    interface CpuInfo {
+        model: string;
+        speed: number;
+        times: {
+            user: number;
+            nice: number;
+            sys: number;
+            idle: number;
+            irq: number;
+        };
+    }
+    interface NetworkInterfaceBase {
+        address: string;
+        netmask: string;
+        mac: string;
+        internal: boolean;
+        cidr: string | null;
+    }
+    interface NetworkInterfaceInfoIPv4 extends NetworkInterfaceBase {
+        family: 'IPv4';
+    }
+    interface NetworkInterfaceInfoIPv6 extends NetworkInterfaceBase {
+        family: 'IPv6';
+        scopeid: number;
+    }
+    interface UserInfo<T> {
+        username: T;
+        uid: number;
+        gid: number;
+        shell: T;
+        homedir: T;
+    }
+    type NetworkInterfaceInfo = NetworkInterfaceInfoIPv4 | NetworkInterfaceInfoIPv6;
+    /**
+     * Returns the host name of the operating system as a string.
+     * @since v0.3.3
+     */
+    function hostname(): string;
+    /**
+     * Returns an array containing the 1, 5, and 15 minute load averages.
+     *
+     * The load average is a measure of system activity calculated by the operating
+     * system and expressed as a fractional number.
+     *
+     * The load average is a Unix-specific concept. On Windows, the return value is
+     * always `[0, 0, 0]`.
+     * @since v0.3.3
+     */
+    function loadavg(): number[];
+    /**
+     * Returns the system uptime in number of seconds.
+     * @since v0.3.3
+     */
+    function uptime(): number;
+    /**
+     * Returns the amount of free system memory in bytes as an integer.
+     * @since v0.3.3
+     */
+    function freemem(): number;
+    /**
+     * Returns the total amount of system memory in bytes as an integer.
+     * @since v0.3.3
+     */
+    function totalmem(): number;
+    /**
+     * Returns an array of objects containing information about each logical CPU core.
+     *
+     * The properties included on each object include:
+     *
+     * ```js
+     * [
+     *   {
+     *     model: 'Intel(R) Core(TM) i7 CPU         860  @ 2.80GHz',
+     *     speed: 2926,
+     *     times: {
+     *       user: 252020,
+     *       nice: 0,
+     *       sys: 30340,
+     *       idle: 1070356870,
+     *       irq: 0
+     *     }
+     *   },
+     *   {
+     *     model: 'Intel(R) Core(TM) i7 CPU         860  @ 2.80GHz',
+     *     speed: 2926,
+     *     times: {
+     *       user: 306960,
+     *       nice: 0,
+     *       sys: 26980,
+     *       idle: 1071569080,
+     *       irq: 0
+     *     }
+     *   },
+     *   {
+     *     model: 'Intel(R) Core(TM) i7 CPU         860  @ 2.80GHz',
+     *     speed: 2926,
+     *     times: {
+     *       user: 248450,
+     *       nice: 0,
+     *       sys: 21750,
+     *       idle: 1070919370,
+     *       irq: 0
+     *     }
+     *   },
+     *   {
+     *     model: 'Intel(R) Core(TM) i7 CPU         860  @ 2.80GHz',
+     *     speed: 2926,
+     *     times: {
+     *       user: 256880,
+     *       nice: 0,
+     *       sys: 19430,
+     *       idle: 1070905480,
+     *       irq: 20
+     *     }
+     *   },
+     * ]
+     * ```
+     *
+     * `nice` values are POSIX-only. On Windows, the `nice` values of all processors
+     * are always 0.
+     * @since v0.3.3
+     */
+    function cpus(): CpuInfo[];
+    /**
+     * Returns the operating system name as returned by [`uname(3)`](https://linux.die.net/man/3/uname). For example, it
+     * returns `'Linux'` on Linux, `'Darwin'` on macOS, and `'Windows_NT'` on Windows.
+     *
+     * See [https://en.wikipedia.org/wiki/Uname#Examples](https://en.wikipedia.org/wiki/Uname#Examples) for additional information
+     * about the output of running [`uname(3)`](https://linux.die.net/man/3/uname) on various operating systems.
+     * @since v0.3.3
+     */
+    function type(): string;
+    /**
+     * Returns the operating system as a string.
+     *
+     * On POSIX systems, the operating system release is determined by calling [`uname(3)`](https://linux.die.net/man/3/uname). On Windows, `GetVersionExW()` is used. See
+     * [https://en.wikipedia.org/wiki/Uname#Examples](https://en.wikipedia.org/wiki/Uname#Examples) for more information.
+     * @since v0.3.3
+     */
+    function release(): string;
+    /**
+     * Returns an object containing network interfaces that have been assigned a
+     * network address.
+     *
+     * Each key on the returned object identifies a network interface. The associated
+     * value is an array of objects that each describe an assigned network address.
+     *
+     * The properties available on the assigned network address object include:
+     *
+     * ```js
+     * {
+     *   lo: [
+     *     {
+     *       address: '127.0.0.1',
+     *       netmask: '255.0.0.0',
+     *       family: 'IPv4',
+     *       mac: '00:00:00:00:00:00',
+     *       internal: true,
+     *       cidr: '127.0.0.1/8'
+     *     },
+     *     {
+     *       address: '::1',
+     *       netmask: 'ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff',
+     *       family: 'IPv6',
+     *       mac: '00:00:00:00:00:00',
+     *       scopeid: 0,
+     *       internal: true,
+     *       cidr: '::1/128'
+     *     }
+     *   ],
+     *   eth0: [
+     *     {
+     *       address: '192.168.1.108',
+     *       netmask: '255.255.255.0',
+     *       family: 'IPv4',
+     *       mac: '01:02:03:0a:0b:0c',
+     *       internal: false,
+     *       cidr: '192.168.1.108/24'
+     *     },
+     *     {
+     *       address: 'fe80::a00:27ff:fe4e:66a1',
+     *       netmask: 'ffff:ffff:ffff:ffff::',
+     *       family: 'IPv6',
+     *       mac: '01:02:03:0a:0b:0c',
+     *       scopeid: 1,
+     *       internal: false,
+     *       cidr: 'fe80::a00:27ff:fe4e:66a1/64'
+     *     }
+     *   ]
+     * }
+     * ```
+     * @since v0.6.0
+     */
+    function networkInterfaces(): NodeJS.Dict<NetworkInterfaceInfo[]>;
+    /**
+     * Returns the string path of the current user's home directory.
+     *
+     * On POSIX, it uses the `$HOME` environment variable if defined. Otherwise it
+     * uses the [effective UID](https://en.wikipedia.org/wiki/User_identifier#Effective_user_ID) to look up the user's home directory.
+     *
+     * On Windows, it uses the `USERPROFILE` environment variable if defined.
+     * Otherwise it uses the path to the profile directory of the current user.
+     * @since v2.3.0
+     */
+    function homedir(): string;
+    /**
+     * Returns information about the currently effective user. On POSIX platforms,
+     * this is typically a subset of the password file. The returned object includes
+     * the `username`, `uid`, `gid`, `shell`, and `homedir`. On Windows, the `uid` and`gid` fields are `-1`, and `shell` is `null`.
+     *
+     * The value of `homedir` returned by `os.userInfo()` is provided by the operating
+     * system. This differs from the result of `os.homedir()`, which queries
+     * environment variables for the home directory before falling back to the
+     * operating system response.
+     *
+     * Throws a `SystemError` if a user has no `username` or `homedir`.
+     * @since v6.0.0
+     */
+    function userInfo(options: { encoding: 'buffer' }): UserInfo<Buffer>;
+    function userInfo(options?: { encoding: BufferEncoding }): UserInfo<string>;
+    type SignalConstants = {
+        [key in NodeJS.Signals]: number;
+    };
+    namespace constants {
+        const UV_UDP_REUSEADDR: number;
+        namespace signals {}
+        const signals: SignalConstants;
+        namespace errno {
+            const E2BIG: number;
+            const EACCES: number;
+            const EADDRINUSE: number;
+            const EADDRNOTAVAIL: number;
+            const EAFNOSUPPORT: number;
+            const EAGAIN: number;
+            const EALREADY: number;
+            const EBADF: number;
+            const EBADMSG: number;
+            const EBUSY: number;
+            const ECANCELED: number;
+            const ECHILD: number;
+            const ECONNABORTED: number;
+            const ECONNREFUSED: number;
+            const ECONNRESET: number;
+            const EDEADLK: number;
+            const EDESTADDRREQ: number;
+            const EDOM: number;
+            const EDQUOT: number;
+            const EEXIST: number;
+            const EFAULT: number;
+            const EFBIG: number;
+            const EHOSTUNREACH: number;
+            const EIDRM: number;
+            const EILSEQ: number;
+            const EINPROGRESS: number;
+            const EINTR: number;
+            const EINVAL: number;
+            const EIO: number;
+            const EISCONN: number;
+            const EISDIR: number;
+            const ELOOP: number;
+            const EMFILE: number;
+            const EMLINK: number;
+            const EMSGSIZE: number;
+            const EMULTIHOP: number;
+            const ENAMETOOLONG: number;
+            const ENETDOWN: number;
+            const ENETRESET: number;
+            const ENETUNREACH: number;
+            const ENFILE: number;
+            const ENOBUFS: number;
+            const ENODATA: number;
+            const ENODEV: number;
+            const ENOENT: number;
+            const ENOEXEC: number;
+            const ENOLCK: number;
+            const ENOLINK: number;
+            const ENOMEM: number;
+            const ENOMSG: number;
+            const ENOPROTOOPT: number;
+            const ENOSPC: number;
+            const ENOSR: number;
+            const ENOSTR: number;
+            const ENOSYS: number;
+            const ENOTCONN: number;
+            const ENOTDIR: number;
+            const ENOTEMPTY: number;
+            const ENOTSOCK: number;
+            const ENOTSUP: number;
+            const ENOTTY: number;
+            const ENXIO: number;
+            const EOPNOTSUPP: number;
+            const EOVERFLOW: number;
+            const EPERM: number;
+            const EPIPE: number;
+            const EPROTO: number;
+            const EPROTONOSUPPORT: number;
+            const EPROTOTYPE: number;
+            const ERANGE: number;
+            const EROFS: number;
+            const ESPIPE: number;
+            const ESRCH: number;
+            const ESTALE: number;
+            const ETIME: number;
+            const ETIMEDOUT: number;
+            const ETXTBSY: number;
+            const EWOULDBLOCK: number;
+            const EXDEV: number;
+            const WSAEINTR: number;
+            const WSAEBADF: number;
+            const WSAEACCES: number;
+            const WSAEFAULT: number;
+            const WSAEINVAL: number;
+            const WSAEMFILE: number;
+            const WSAEWOULDBLOCK: number;
+            const WSAEINPROGRESS: number;
+            const WSAEALREADY: number;
+            const WSAENOTSOCK: number;
+            const WSAEDESTADDRREQ: number;
+            const WSAEMSGSIZE: number;
+            const WSAEPROTOTYPE: number;
+            const WSAENOPROTOOPT: number;
+            const WSAEPROTONOSUPPORT: number;
+            const WSAESOCKTNOSUPPORT: number;
+            const WSAEOPNOTSUPP: number;
+            const WSAEPFNOSUPPORT: number;
+            const WSAEAFNOSUPPORT: number;
+            const WSAEADDRINUSE: number;
+            const WSAEADDRNOTAVAIL: number;
+            const WSAENETDOWN: number;
+            const WSAENETUNREACH: number;
+            const WSAENETRESET: number;
+            const WSAECONNABORTED: number;
+            const WSAECONNRESET: number;
+            const WSAENOBUFS: number;
+            const WSAEISCONN: number;
+            const WSAENOTCONN: number;
+            const WSAESHUTDOWN: number;
+            const WSAETOOMANYREFS: number;
+            const WSAETIMEDOUT: number;
+            const WSAECONNREFUSED: number;
+            const WSAELOOP: number;
+            const WSAENAMETOOLONG: number;
+            const WSAEHOSTDOWN: number;
+            const WSAEHOSTUNREACH: number;
+            const WSAENOTEMPTY: number;
+            const WSAEPROCLIM: number;
+            const WSAEUSERS: number;
+            const WSAEDQUOT: number;
+            const WSAESTALE: number;
+            const WSAEREMOTE: number;
+            const WSASYSNOTREADY: number;
+            const WSAVERNOTSUPPORTED: number;
+            const WSANOTINITIALISED: number;
+            const WSAEDISCON: number;
+            const WSAENOMORE: number;
+            const WSAECANCELLED: number;
+            const WSAEINVALIDPROCTABLE: number;
+            const WSAEINVALIDPROVIDER: number;
+            const WSAEPROVIDERFAILEDINIT: number;
+            const WSASYSCALLFAILURE: number;
+            const WSASERVICE_NOT_FOUND: number;
+            const WSATYPE_NOT_FOUND: number;
+            const WSA_E_NO_MORE: number;
+            const WSA_E_CANCELLED: number;
+            const WSAEREFUSED: number;
+        }
+        namespace priority {
+            const PRIORITY_LOW: number;
+            const PRIORITY_BELOW_NORMAL: number;
+            const PRIORITY_NORMAL: number;
+            const PRIORITY_ABOVE_NORMAL: number;
+            const PRIORITY_HIGH: number;
+            const PRIORITY_HIGHEST: number;
+        }
+    }
+    const devNull: string;
+    const EOL: string;
+    /**
+     * Returns the operating system CPU architecture for which the Node.js binary was
+     * compiled. Possible values are `'arm'`, `'arm64'`, `'ia32'`, `'mips'`,`'mipsel'`, `'ppc'`, `'ppc64'`, `'s390'`, `'s390x'`, `'x32'`, and `'x64'`.
+     *
+     * The return value is equivalent to `process.arch`.
+     * @since v0.5.0
+     */
+    function arch(): string;
+    /**
+     * Returns a string identifying the kernel version.
+     *
+     * On POSIX systems, the operating system release is determined by calling [`uname(3)`](https://linux.die.net/man/3/uname). On Windows, `RtlGetVersion()` is used, and if it is not
+     * available, `GetVersionExW()` will be used. See [https://en.wikipedia.org/wiki/Uname#Examples](https://en.wikipedia.org/wiki/Uname#Examples) for more information.
+     * @since v13.11.0, v12.17.0
+     */
+    function version(): string;
+    /**
+     * Returns a string identifying the operating system platform. The value is set
+     * at compile time. Possible values are `'aix'`, `'darwin'`, `'freebsd'`,`'linux'`, `'openbsd'`, `'sunos'`, and `'win32'`.
+     *
+     * The return value is equivalent to `process.platform`.
+     *
+     * The value `'android'` may also be returned if Node.js is built on the Android
+     * operating system. [Android support is experimental](https://github.com/nodejs/node/blob/HEAD/BUILDING.md#androidandroid-based-devices-eg-firefox-os).
+     * @since v0.5.0
+     */
+    function platform(): NodeJS.Platform;
+    /**
+     * Returns the operating system's default directory for temporary files as a
+     * string.
+     * @since v0.9.9
+     */
+    function tmpdir(): string;
+    /**
+     * Returns a string identifying the endianness of the CPU for which the Node.js
+     * binary was compiled.
+     *
+     * Possible values are `'BE'` for big endian and `'LE'` for little endian.
+     * @since v0.9.4
+     */
+    function endianness(): 'BE' | 'LE';
+    /**
+     * Returns the scheduling priority for the process specified by `pid`. If `pid` is
+     * not provided or is `0`, the priority of the current process is returned.
+     * @since v10.10.0
+     * @param [pid=0] The process ID to retrieve scheduling priority for.
+     */
+    function getPriority(pid?: number): number;
+    /**
+     * Attempts to set the scheduling priority for the process specified by `pid`. If`pid` is not provided or is `0`, the process ID of the current process is used.
+     *
+     * The `priority` input must be an integer between `-20` (high priority) and `19`(low priority). Due to differences between Unix priority levels and Windows
+     * priority classes, `priority` is mapped to one of six priority constants in`os.constants.priority`. When retrieving a process priority level, this range
+     * mapping may cause the return value to be slightly different on Windows. To avoid
+     * confusion, set `priority` to one of the priority constants.
+     *
+     * On Windows, setting priority to `PRIORITY_HIGHEST` requires elevated user
+     * privileges. Otherwise the set priority will be silently reduced to`PRIORITY_HIGH`.
+     * @since v10.10.0
+     * @param [pid=0] The process ID to set scheduling priority for.
+     * @param priority The scheduling priority to assign to the process.
+     */
+    function setPriority(priority: number): void;
+    function setPriority(pid: number, priority: number): void;
+}
+declare module 'node:os' {
+    export * from 'os';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+{
+  ""_from"": ""@types/node@*"",
+  ""_id"": ""@types/node@17.0.31"",
+  ""_inBundle"": false,
+  ""_integrity"": ""sha512-AR0x5HbXGqkEx9CadRH3EBYx/VkiUgZIhP4wvPn/+5KIsgpNoyFaRlVe0Zlx9gRtg8fA06a9tskE2MSN7TcG4Q=="",
+  ""_location"": ""/@types/node"",
+  ""_phantomChildren"": {},
+  ""_requested"": {
+    ""type"": ""range"",
+    ""registry"": true,
+    ""raw"": ""@types/node@*"",
+    ""name"": ""@types/node"",
+    ""escapedName"": ""@types%2fnode"",
+    ""scope"": ""@types"",
+    ""rawSpec"": ""*"",
+    ""saveSpec"": null,
+    ""fetchSpec"": ""*""
+  },
+  ""_requiredBy"": [
+    ""/@types/cacheable-request"",
+    ""/@types/keyv"",
+    ""/@types/responselike""
+  ],
+  ""_resolved"": ""https://registry.npmjs.org/@types/node/-/node-17.0.31.tgz"",
+  ""_shasum"": ""a5bb84ecfa27eec5e1c802c6bbf8139bdb163a5d"",
+  ""_spec"": ""@types/node@*"",
+  ""_where"": ""D:\\kindle\\translateWordInPDF\\node_modules\\@types\\cacheable-request"",
+  ""bugs"": {
+    ""url"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/issues""
+  },
+  ""bundleDependencies"": false,
+  ""contributors"": [
+    {
+      ""name"": ""Microsoft TypeScript"",
+      ""url"": ""https://github.com/Microsoft""
+    },
+    {
+      ""name"": ""DefinitelyTyped"",
+      ""url"": ""https://github.com/DefinitelyTyped""
+    },
+    {
+      ""name"": ""Alberto Schiabel"",
+      ""url"": ""https://github.com/jkomyno""
+    },
+    {
+      ""name"": ""Alvis HT Tang"",
+      ""url"": ""https://github.com/alvis""
+    },
+    {
+      ""name"": ""Andrew Makarov"",
+      ""url"": ""https://github.com/r3nya""
+    },
+    {
+      ""name"": ""Benjamin Toueg"",
+      ""url"": ""https://github.com/btoueg""
+    },
+    {
+      ""name"": ""Chigozirim C."",
+      ""url"": ""https://github.com/smac89""
+    },
+    {
+      ""name"": ""David Junger"",
+      ""url"": ""https://github.com/touffy""
+    },
+    {
+      ""name"": ""Deividas Bakanas"",
+      ""url"": ""https://github.com/DeividasBakanas""
+    },
+    {
+      ""name"": ""Eugene Y. Q. Shen"",
+      ""url"": ""https://github.com/eyqs""
+    },
+    {
+      ""name"": ""Hannes Magnusson"",
+      ""url"": ""https://github.com/Hannes-Magnusson-CK""
+    },
+    {
+      ""name"": ""Huw"",
+      ""url"": ""https://github.com/hoo29""
+    },
+    {
+      ""name"": ""Kelvin Jin"",
+      ""url"": ""https://github.com/kjin""
+    },
+    {
+      ""name"": ""Klaus Meinhardt"",
+      ""url"": ""https://github.com/ajafff""
+    },
+    {
+      ""name"": ""Lishude"",
+      ""url"": ""https://github.com/islishude""
+    },
+    {
+      ""name"": ""Mariusz Wiktorczyk"",
+      ""url"": ""https://github.com/mwiktorczyk""
+    },
+    {
+      ""name"": ""Mohsen Azimi"",
+      ""url"": ""https://github.com/mohsen1""
+    },
+    {
+      ""name"": ""Nicolas Even"",
+      ""url"": ""https://github.com/n-e""
+    },
+    {
+      ""name"": ""Nikita Galkin"",
+      ""url"": ""https://github.com/galkin""
+    },
+    {
+      ""name"": ""Parambir Singh"",
+      ""url"": ""https://github.com/parambirs""
+    },
+    {
+      ""name"": ""Sebastian Silbermann"",
+      ""url"": ""https://github.com/eps1lon""
+    },
+    {
+      ""name"": ""Simon Schick"",
+      ""url"": ""https://github.com/SimonSchick""
+    },
+    {
+      ""name"": ""Thomas den Hollander"",
+      ""url"": ""https://github.com/ThomasdenH""
+    },
+    {
+      ""name"": ""Wilco Bakker"",
+      ""url"": ""https://github.com/WilcoBakker""
+    },
+    {
+      ""name"": ""wwwy3y3"",
+      ""url"": ""https://github.com/wwwy3y3""
+    },
+    {
+      ""name"": ""Samuel Ainsworth"",
+      ""url"": ""https://github.com/samuela""
+    },
+    {
+      ""name"": ""Kyle Uehlein"",
+      ""url"": ""https://github.com/kuehlein""
+    },
+    {
+      ""name"": ""Thanik Bhongbhibhat"",
+      ""url"": ""https://github.com/bhongy""
+    },
+    {
+      ""name"": ""Marcin Kopacz"",
+      ""url"": ""https://github.com/chyzwar""
+    },
+    {
+      ""name"": ""Trivikram Kamat"",
+      ""url"": ""https://github.com/trivikr""
+    },
+    {
+      ""name"": ""Junxiao Shi"",
+      ""url"": ""https://github.com/yoursunny""
+    },
+    {
+      ""name"": ""Ilia Baryshnikov"",
+      ""url"": ""https://github.com/qwelias""
+    },
+    {
+      ""name"": ""ExE Boss"",
+      ""url"": ""https://github.com/ExE-Boss""
+    },
+    {
+      ""name"": ""Piotr Błażejewicz"",
+      ""url"": ""https://github.com/peterblazejewicz""
+    },
+    {
+      ""name"": ""Anna Henningsen"",
+      ""url"": ""https://github.com/addaleax""
+    },
+    {
+      ""name"": ""Victor Perin"",
+      ""url"": ""https://github.com/victorperin""
+    },
+    {
+      ""name"": ""Yongsheng Zhang"",
+      ""url"": ""https://github.com/ZYSzys""
+    },
+    {
+      ""name"": ""NodeJS Contributors"",
+      ""url"": ""https://github.com/NodeJS""
+    },
+    {
+      ""name"": ""Linus Unnebäck"",
+      ""url"": ""https://github.com/LinusU""
+    },
+    {
+      ""name"": ""wafuwafu13"",
+      ""url"": ""https://github.com/wafuwafu13""
+    }
+  ],
+  ""dependencies"": {},
+  ""deprecated"": false,
+  ""description"": ""TypeScript definitions for Node.js"",
+  ""homepage"": ""https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/node"",
+  ""license"": ""MIT"",
+  ""main"": """",
+  ""name"": ""@types/node"",
+  ""repository"": {
+    ""type"": ""git"",
+    ""url"": ""git+https://github.com/DefinitelyTyped/DefinitelyTyped.git"",
+    ""directory"": ""types/node""
+  },
+  ""scripts"": {},
+  ""typeScriptVersion"": ""3.9"",
+  ""types"": ""index.d.ts"",
+  ""typesPublisherContentHash"": ""187e51e75f3e87938751363254f337ef4fab9306b9bb5a7d36cb895b7c95c5e2"",
+  ""version"": ""17.0.31""
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+declare module 'path/posix' {
+    import path = require('path');
+    export = path;
+}
+declare module 'path/win32' {
+    import path = require('path');
+    export = path;
+}
+/**
+ * The `path` module provides utilities for working with file and directory paths.
+ * It can be accessed using:
+ *
+ * ```js
+ * const path = require('path');
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/path.js)
+ */
+declare module 'path' {
+    namespace path {
+        /**
+         * A parsed path object generated by path.parse() or consumed by path.format().
+         */
+        interface ParsedPath {
+            /**
+             * The root of the path such as '/' or 'c:\'
+             */
+            root: string;
+            /**
+             * The full directory path such as '/home/user/dir' or 'c:\path\dir'
+             */
+            dir: string;
+            /**
+             * The file name including extension (if any) such as 'index.html'
+             */
+            base: string;
+            /**
+             * The file extension (if any) such as '.html'
+             */
+            ext: string;
+            /**
+             * The file name without extension (if any) such as 'index'
+             */
+            name: string;
+        }
+        interface FormatInputPathObject {
+            /**
+             * The root of the path such as '/' or 'c:\'
+             */
+            root?: string | undefined;
+            /**
+             * The full directory path such as '/home/user/dir' or 'c:\path\dir'
+             */
+            dir?: string | undefined;
+            /**
+             * The file name including extension (if any) such as 'index.html'
+             */
+            base?: string | undefined;
+            /**
+             * The file extension (if any) such as '.html'
+             */
+            ext?: string | undefined;
+            /**
+             * The file name without extension (if any) such as 'index'
+             */
+            name?: string | undefined;
+        }
+        interface PlatformPath {
+            /**
+             * Normalize a string path, reducing '..' and '.' parts.
+             * When multiple slashes are found, they're replaced by a single one; when the path contains a trailing slash, it is preserved. On Windows backslashes are used.
+             *
+             * @param p string path to normalize.
+             */
+            normalize(p: string): string;
+            /**
+             * Join all arguments together and normalize the resulting path.
+             * Arguments must be strings. In v0.8, non-string arguments were silently ignored. In v0.10 and up, an exception is thrown.
+             *
+             * @param paths paths to join.
+             */
+            join(...paths: string[]): string;
+            /**
+             * The right-most parameter is considered {to}.  Other parameters are considered an array of {from}.
+             *
+             * Starting from leftmost {from} parameter, resolves {to} to an absolute path.
+             *
+             * If {to} isn't already absolute, {from} arguments are prepended in right to left order,
+             * until an absolute path is found. If after using all {from} paths still no absolute path is found,
+             * the current working directory is used as well. The resulting path is normalized,
+             * and trailing slashes are removed unless the path gets resolved to the root directory.
+             *
+             * @param pathSegments string paths to join.  Non-string arguments are ignored.
+             */
+            resolve(...pathSegments: string[]): string;
+            /**
+             * Determines whether {path} is an absolute path. An absolute path will always resolve to the same location, regardless of the working directory.
+             *
+             * @param path path to test.
+             */
+            isAbsolute(p: string): boolean;
+            /**
+             * Solve the relative path from {from} to {to}.
+             * At times we have two absolute paths, and we need to derive the relative path from one to the other. This is actually the reverse transform of path.resolve.
+             */
+            relative(from: string, to: string): string;
+            /**
+             * Return the directory name of a path. Similar to the Unix dirname command.
+             *
+             * @param p the path to evaluate.
+             */
+            dirname(p: string): string;
+            /**
+             * Return the last portion of a path. Similar to the Unix basename command.
+             * Often used to extract the file name from a fully qualified path.
+             *
+             * @param p the path to evaluate.
+             * @param ext optionally, an extension to remove from the result.
+             */
+            basename(p: string, ext?: string): string;
+            /**
+             * Return the extension of the path, from the last '.' to end of string in the last portion of the path.
+             * If there is no '.' in the last portion of the path or the first character of it is '.', then it returns an empty string
+             *
+             * @param p the path to evaluate.
+             */
+            extname(p: string): string;
+            /**
+             * The platform-specific file separator. '\\' or '/'.
+             */
+            readonly sep: string;
+            /**
+             * The platform-specific file delimiter. ';' or ':'.
+             */
+            readonly delimiter: string;
+            /**
+             * Returns an object from a path string - the opposite of format().
+             *
+             * @param pathString path to evaluate.
+             */
+            parse(p: string): ParsedPath;
+            /**
+             * Returns a path string from an object - the opposite of parse().
+             *
+             * @param pathString path to evaluate.
+             */
+            format(pP: FormatInputPathObject): string;
+            /**
+             * On Windows systems only, returns an equivalent namespace-prefixed path for the given path.
+             * If path is not a string, path will be returned without modifications.
+             * This method is meaningful only on Windows system.
+             * On POSIX systems, the method is non-operational and always returns path without modifications.
+             */
+            toNamespacedPath(path: string): string;
+            /**
+             * Posix specific pathing.
+             * Same as parent object on posix.
+             */
+            readonly posix: PlatformPath;
+            /**
+             * Windows specific pathing.
+             * Same as parent object on windows
+             */
+            readonly win32: PlatformPath;
+        }
+    }
+    const path: path.PlatformPath;
+    export = path;
+}
+declare module 'node:path' {
+    import path = require('path');
+    export = path;
+}
+declare module 'node:path/posix' {
+    import path = require('path/posix');
+    export = path;
+}
+declare module 'node:path/win32' {
+    import path = require('path/win32');
+    export = path;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * This module provides an implementation of a subset of the W3C [Web Performance APIs](https://w3c.github.io/perf-timing-primer/) as well as additional APIs for
+ * Node.js-specific performance measurements.
+ *
+ * Node.js supports the following [Web Performance APIs](https://w3c.github.io/perf-timing-primer/):
+ *
+ * * [High Resolution Time](https://www.w3.org/TR/hr-time-2)
+ * * [Performance Timeline](https://w3c.github.io/performance-timeline/)
+ * * [User Timing](https://www.w3.org/TR/user-timing/)
+ *
+ * ```js
+ * const { PerformanceObserver, performance } = require('perf_hooks');
+ *
+ * const obs = new PerformanceObserver((items) => {
+ *   console.log(items.getEntries()[0].duration);
+ *   performance.clearMarks();
+ * });
+ * obs.observe({ type: 'measure' });
+ * performance.measure('Start to Now');
+ *
+ * performance.mark('A');
+ * doSomeLongRunningProcess(() => {
+ *   performance.measure('A to Now', 'A');
+ *
+ *   performance.mark('B');
+ *   performance.measure('A to B', 'A', 'B');
+ * });
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/perf_hooks.js)
+ */
+declare module 'perf_hooks' {
+    import { AsyncResource } from 'node:async_hooks';
+    type EntryType = 'node' | 'mark' | 'measure' | 'gc' | 'function' | 'http2' | 'http';
+    interface NodeGCPerformanceDetail {
+        /**
+         * When `performanceEntry.entryType` is equal to 'gc', `the performance.kind` property identifies
+         * the type of garbage collection operation that occurred.
+         * See perf_hooks.constants for valid values.
+         */
+        readonly kind?: number | undefined;
+        /**
+         * When `performanceEntry.entryType` is equal to 'gc', the `performance.flags`
+         * property contains additional information about garbage collection operation.
+         * See perf_hooks.constants for valid values.
+         */
+        readonly flags?: number | undefined;
+    }
+    /**
+     * @since v8.5.0
+     */
+    class PerformanceEntry {
+        protected constructor();
+        /**
+         * The total number of milliseconds elapsed for this entry. This value will not
+         * be meaningful for all Performance Entry types.
+         * @since v8.5.0
+         */
+        readonly duration: number;
+        /**
+         * The name of the performance entry.
+         * @since v8.5.0
+         */
+        readonly name: string;
+        /**
+         * The high resolution millisecond timestamp marking the starting time of the
+         * Performance Entry.
+         * @since v8.5.0
+         */
+        readonly startTime: number;
+        /**
+         * The type of the performance entry. It may be one of:
+         *
+         * * `'node'` (Node.js only)
+         * * `'mark'` (available on the Web)
+         * * `'measure'` (available on the Web)
+         * * `'gc'` (Node.js only)
+         * * `'function'` (Node.js only)
+         * * `'http2'` (Node.js only)
+         * * `'http'` (Node.js only)
+         * @since v8.5.0
+         */
+        readonly entryType: EntryType;
+        /**
+         * Additional detail specific to the `entryType`.
+         * @since v16.0.0
+         */
+        readonly detail?: NodeGCPerformanceDetail | unknown | undefined; // TODO: Narrow this based on entry type.
+    }
+    /**
+     * _This property is an extension by Node.js. It is not available in Web browsers._
+     *
+     * Provides timing details for Node.js itself. The constructor of this class
+     * is not exposed to users.
+     * @since v8.5.0
+     */
+    class PerformanceNodeTiming extends PerformanceEntry {
+        /**
+         * The high resolution millisecond timestamp at which the Node.js process
+         * completed bootstrapping. If bootstrapping has not yet finished, the property
+         * has the value of -1.
+         * @since v8.5.0
+         */
+        readonly bootstrapComplete: number;
+        /**
+         * The high resolution millisecond timestamp at which the Node.js environment was
+         * initialized.
+         * @since v8.5.0
+         */
+        readonly environment: number;
+        /**
+         * The high resolution millisecond timestamp of the amount of time the event loop
+         * has been idle within the event loop's event provider (e.g. `epoll_wait`). This
+         * does not take CPU usage into consideration. If the event loop has not yet
+         * started (e.g., in the first tick of the main script), the property has the
+         * value of 0.
+         * @since v14.10.0, v12.19.0
+         */
+        readonly idleTime: number;
+        /**
+         * The high resolution millisecond timestamp at which the Node.js event loop
+         * exited. If the event loop has not yet exited, the property has the value of -1\.
+         * It can only have a value of not -1 in a handler of the `'exit'` event.
+         * @since v8.5.0
+         */
+        readonly loopExit: number;
+        /**
+         * The high resolution millisecond timestamp at which the Node.js event loop
+         * started. If the event loop has not yet started (e.g., in the first tick of the
+         * main script), the property has the value of -1.
+         * @since v8.5.0
+         */
+        readonly loopStart: number;
+        /**
+         * The high resolution millisecond timestamp at which the V8 platform was
+         * initialized.
+         * @since v8.5.0
+         */
+        readonly v8Start: number;
+    }
+    interface EventLoopUtilization {
+        idle: number;
+        active: number;
+        utilization: number;
+    }
+    /**
+     * @param util1 The result of a previous call to eventLoopUtilization()
+     * @param util2 The result of a previous call to eventLoopUtilization() prior to util1
+     */
+    type EventLoopUtilityFunction = (util1?: EventLoopUtilization, util2?: EventLoopUtilization) => EventLoopUtilization;
+    interface MarkOptions {
+        /**
+         * Additional optional detail to include with the mark.
+         */
+        detail?: unknown | undefined;
+        /**
+         * An optional timestamp to be used as the mark time.
+         * @default `performance.now()`.
+         */
+        startTime?: number | undefined;
+    }
+    interface MeasureOptions {
+        /**
+         * Additional optional detail to include with the mark.
+         */
+        detail?: unknown | undefined;
+        /**
+         * Duration between start and end times.
+         */
+        duration?: number | undefined;
+        /**
+         * Timestamp to be used as the end time, or a string identifying a previously recorded mark.
+         */
+        end?: number | string | undefined;
+        /**
+         * Timestamp to be used as the start time, or a string identifying a previously recorded mark.
+         */
+        start?: number | string | undefined;
+    }
+    interface TimerifyOptions {
+        /**
+         * A histogram object created using
+         * `perf_hooks.createHistogram()` that will record runtime durations in
+         * nanoseconds.
+         */
+        histogram?: RecordableHistogram | undefined;
+    }
+    interface Performance {
+        /**
+         * If name is not provided, removes all PerformanceMark objects from the Performance Timeline.
+         * If name is provided, removes only the named mark.
+         * @param name
+         */
+        clearMarks(name?: string): void;
+        /**
+         * Creates a new PerformanceMark entry in the Performance Timeline.
+         * A PerformanceMark is a subclass of PerformanceEntry whose performanceEntry.entryType is always 'mark',
+         * and whose performanceEntry.duration is always 0.
+         * Performance marks are used to mark specific significant moments in the Performance Timeline.
+         * @param name
+         */
+        mark(name?: string, options?: MarkOptions): void;
+        /**
+         * Creates a new PerformanceMeasure entry in the Performance Timeline.
+         * A PerformanceMeasure is a subclass of PerformanceEntry whose performanceEntry.entryType is always 'measure',
+         * and whose performanceEntry.duration measures the number of milliseconds elapsed since startMark and endMark.
+         *
+         * The startMark argument may identify any existing PerformanceMark in the the Performance Timeline, or may identify
+         * any of the timestamp properties provided by the PerformanceNodeTiming class. If the named startMark does not exist,
+         * then startMark is set to timeOrigin by default.
+         *
+         * The endMark argument must identify any existing PerformanceMark in the the Performance Timeline or any of the timestamp
+         * properties provided by the PerformanceNodeTiming class. If the named endMark does not exist, an error will be thrown.
+         * @param name
+         * @param startMark
+         * @param endMark
+         */
+        measure(name: string, startMark?: string, endMark?: string): void;
+        measure(name: string, options: MeasureOptions): void;
+        /**
+         * An instance of the PerformanceNodeTiming class that provides performance metrics for specific Node.js operational milestones.
+         */
+        readonly nodeTiming: PerformanceNodeTiming;
+        /**
+         * @return the current high resolution millisecond timestamp
+         */
+        now(): number;
+        /**
+         * The timeOrigin specifies the high resolution millisecond timestamp from which all performance metric durations are measured.
+         */
+        readonly timeOrigin: number;
+        /**
+         * Wraps a function within a new function that measures the running time of the wrapped function.
+         * A PerformanceObserver must be subscribed to the 'function' event type in order for the timing details to be accessed.
+         * @param fn
+         */
+        timerify<T extends (...params: any[]) => any>(fn: T, options?: TimerifyOptions): T;
+        /**
+         * eventLoopUtilization is similar to CPU utilization except that it is calculated using high precision wall-clock time.
+         * It represents the percentage of time the event loop has spent outside the event loop's event provider (e.g. epoll_wait).
+         * No other CPU idle time is taken into consideration.
+         */
+        eventLoopUtilization: EventLoopUtilityFunction;
+    }
+    interface PerformanceObserverEntryList {
+        /**
+         * Returns a list of `PerformanceEntry` objects in chronological order
+         * with respect to `performanceEntry.startTime`.
+         *
+         * ```js
+         * const {
+         *   performance,
+         *   PerformanceObserver
+         * } = require('perf_hooks');
+         *
+         * const obs = new PerformanceObserver((perfObserverList, observer) => {
+         *   console.log(perfObserverList.getEntries());
+         *
+         *    * [
+         *    *   PerformanceEntry {
+         *    *     name: 'test',
+         *    *     entryType: 'mark',
+         *    *     startTime: 81.465639,
+         *    *     duration: 0
+         *    *   },
+         *    *   PerformanceEntry {
+         *    *     name: 'meow',
+         *    *     entryType: 'mark',
+         *    *     startTime: 81.860064,
+         *    *     duration: 0
+         *    *   }
+         *    * ]
+         *
+         *   observer.disconnect();
+         * });
+         * obs.observe({ type: 'mark' });
+         *
+         * performance.mark('test');
+         * performance.mark('meow');
+         * ```
+         * @since v8.5.0
+         */
+        getEntries(): PerformanceEntry[];
+        /**
+         * Returns a list of `PerformanceEntry` objects in chronological order
+         * with respect to `performanceEntry.startTime` whose `performanceEntry.name` is
+         * equal to `name`, and optionally, whose `performanceEntry.entryType` is equal to`type`.
+         *
+         * ```js
+         * const {
+         *   performance,
+         *   PerformanceObserver
+         * } = require('perf_hooks');
+         *
+         * const obs = new PerformanceObserver((perfObserverList, observer) => {
+         *   console.log(perfObserverList.getEntriesByName('meow'));
+         *
+         *    * [
+         *    *   PerformanceEntry {
+         *    *     name: 'meow',
+         *    *     entryType: 'mark',
+         *    *     startTime: 98.545991,
+         *    *     duration: 0
+         *    *   }
+         *    * ]
+         *
+         *   console.log(perfObserverList.getEntriesByName('nope')); // []
+         *
+         *   console.log(perfObserverList.getEntriesByName('test', 'mark'));
+         *
+         *    * [
+         *    *   PerformanceEntry {
+         *    *     name: 'test',
+         *    *     entryType: 'mark',
+         *    *     startTime: 63.518931,
+         *    *     duration: 0
+         *    *   }
+         *    * ]
+         *
+         *   console.log(perfObserverList.getEntriesByName('test', 'measure')); // []
+         *   observer.disconnect();
+         * });
+         * obs.observe({ entryTypes: ['mark', 'measure'] });
+         *
+         * performance.mark('test');
+         * performance.mark('meow');
+         * ```
+         * @since v8.5.0
+         */
+        getEntriesByName(name: string, type?: EntryType): PerformanceEntry[];
+        /**
+         * Returns a list of `PerformanceEntry` objects in chronological order
+         * with respect to `performanceEntry.startTime` whose `performanceEntry.entryType`is equal to `type`.
+         *
+         * ```js
+         * const {
+         *   performance,
+         *   PerformanceObserver
+         * } = require('perf_hooks');
+         *
+         * const obs = new PerformanceObserver((perfObserverList, observer) => {
+         *   console.log(perfObserverList.getEntriesByType('mark'));
+         *
+         *    * [
+         *    *   PerformanceEntry {
+         *    *     name: 'test',
+         *    *     entryType: 'mark',
+         *    *     startTime: 55.897834,
+         *    *     duration: 0
+         *    *   },
+         *    *   PerformanceEntry {
+         *    *     name: 'meow',
+         *    *     entryType: 'mark',
+         *    *     startTime: 56.350146,
+         *    *     duration: 0
+         *    *   }
+         *    * ]
+         *
+         *   observer.disconnect();
+         * });
+         * obs.observe({ type: 'mark' });
+         *
+         * performance.mark('test');
+         * performance.mark('meow');
+         * ```
+         * @since v8.5.0
+         */
+        getEntriesByType(type: EntryType): PerformanceEntry[];
+    }
+    type PerformanceObserverCallback = (list: PerformanceObserverEntryList, observer: PerformanceObserver) => void;
+    class PerformanceObserver extends AsyncResource {
+        constructor(callback: PerformanceObserverCallback);
+        /**
+         * Disconnects the `PerformanceObserver` instance from all notifications.
+         * @since v8.5.0
+         */
+        disconnect(): void;
+        /**
+         * Subscribes the `PerformanceObserver` instance to notifications of new `PerformanceEntry` instances identified either by `options.entryTypes`or `options.type`:
+         *
+         * ```js
+         * const {
+         *   performance,
+         *   PerformanceObserver
+         * } = require('perf_hooks');
+         *
+         * const obs = new PerformanceObserver((list, observer) => {
+         *   // Called three times synchronously. `list` contains one item.
+         * });
+         * obs.observe({ type: 'mark' });
+         *
+         * for (let n = 0; n < 3; n++)
+         *   performance.mark(`test${n}`);
+         * ```
+         * @since v8.5.0
+         */
+        observe(
+            options:
+                | {
+                      entryTypes: ReadonlyArray<EntryType>;
+                      buffered?: boolean | undefined;
+                  }
+                | {
+                      type: EntryType;
+                      buffered?: boolean | undefined;
+                  }
+        ): void;
+    }
+    namespace constants {
+        const NODE_PERFORMANCE_GC_MAJOR: number;
+        const NODE_PERFORMANCE_GC_MINOR: number;
+        const NODE_PERFORMANCE_GC_INCREMENTAL: number;
+        const NODE_PERFORMANCE_GC_WEAKCB: number;
+        const NODE_PERFORMANCE_GC_FLAGS_NO: number;
+        const NODE_PERFORMANCE_GC_FLAGS_CONSTRUCT_RETAINED: number;
+        const NODE_PERFORMANCE_GC_FLAGS_FORCED: number;
+        const NODE_PERFORMANCE_GC_FLAGS_SYNCHRONOUS_PHANTOM_PROCESSING: number;
+        const NODE_PERFORMANCE_GC_FLAGS_ALL_AVAILABLE_GARBAGE: number;
+        const NODE_PERFORMANCE_GC_FLAGS_ALL_EXTERNAL_MEMORY: number;
+        const NODE_PERFORMANCE_GC_FLAGS_SCHEDULE_IDLE: number;
+    }
+    const performance: Performance;
+    interface EventLoopMonitorOptions {
+        /**
+         * The sampling rate in milliseconds.
+         * Must be greater than zero.
+         * @default 10
+         */
+        resolution?: number | undefined;
+    }
+    interface Histogram {
+        /**
+         * Returns a `Map` object detailing the accumulated percentile distribution.
+         * @since v11.10.0
+         */
+        readonly percentiles: Map<number, number>;
+        /**
+         * The number of times the event loop delay exceeded the maximum 1 hour event
+         * loop delay threshold.
+         * @since v11.10.0
+         */
+        readonly exceeds: number;
+        /**
+         * The minimum recorded event loop delay.
+         * @since v11.10.0
+         */
+        readonly min: number;
+        /**
+         * The maximum recorded event loop delay.
+         * @since v11.10.0
+         */
+        readonly max: number;
+        /**
+         * The mean of the recorded event loop delays.
+         * @since v11.10.0
+         */
+        readonly mean: number;
+        /**
+         * The standard deviation of the recorded event loop delays.
+         * @since v11.10.0
+         */
+        readonly stddev: number;
+        /**
+         * Resets the collected histogram data.
+         * @since v11.10.0
+         */
+        reset(): void;
+        /**
+         * Returns the value at the given percentile.
+         * @since v11.10.0
+         * @param percentile A percentile value in the range (0, 100].
+         */
+        percentile(percentile: number): number;
+    }
+    interface IntervalHistogram extends Histogram {
+        /**
+         * Enables the update interval timer. Returns `true` if the timer was
+         * started, `false` if it was already started.
+         * @since v11.10.0
+         */
+        enable(): boolean;
+        /**
+         * Disables the update interval timer. Returns `true` if the timer was
+         * stopped, `false` if it was already stopped.
+         * @since v11.10.0
+         */
+        disable(): boolean;
+    }
+    interface RecordableHistogram extends Histogram {
+        /**
+         * @since v15.9.0, v14.18.0
+         * @param val The amount to record in the histogram.
+         */
+        record(val: number | bigint): void;
+        /**
+         * Calculates the amount of time (in nanoseconds) that has passed since the
+         * previous call to `recordDelta()` and records that amount in the histogram.
+         *
+         * ## Examples
+         * @since v15.9.0, v14.18.0
+         */
+        recordDelta(): void;
+    }
+    /**
+     * _This property is an extension by Node.js. It is not available in Web browsers._
+     *
+     * Creates an `IntervalHistogram` object that samples and reports the event loop
+     * delay over time. The delays will be reported in nanoseconds.
+     *
+     * Using a timer to detect approximate event loop delay works because the
+     * execution of timers is tied specifically to the lifecycle of the libuv
+     * event loop. That is, a delay in the loop will cause a delay in the execution
+     * of the timer, and those delays are specifically what this API is intended to
+     * detect.
+     *
+     * ```js
+     * const { monitorEventLoopDelay } = require('perf_hooks');
+     * const h = monitorEventLoopDelay({ resolution: 20 });
+     * h.enable();
+     * // Do something.
+     * h.disable();
+     * console.log(h.min);
+     * console.log(h.max);
+     * console.log(h.mean);
+     * console.log(h.stddev);
+     * console.log(h.percentiles);
+     * console.log(h.percentile(50));
+     * console.log(h.percentile(99));
+     * ```
+     * @since v11.10.0
+     */
+    function monitorEventLoopDelay(options?: EventLoopMonitorOptions): IntervalHistogram;
+    interface CreateHistogramOptions {
+        /**
+         * The minimum recordable value. Must be an integer value greater than 0.
+         * @default 1
+         */
+        min?: number | bigint | undefined;
+        /**
+         * The maximum recordable value. Must be an integer value greater than min.
+         * @default Number.MAX_SAFE_INTEGER
+         */
+        max?: number | bigint | undefined;
+        /**
+         * The number of accuracy digits. Must be a number between 1 and 5.
+         * @default 3
+         */
+        figures?: number | undefined;
+    }
+    /**
+     * Returns a `RecordableHistogram`.
+     * @since v15.9.0, v14.18.0
+     */
+    function createHistogram(options?: CreateHistogramOptions): RecordableHistogram;
+}
+declare module 'node:perf_hooks' {
+    export * from 'perf_hooks';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+declare module 'process' {
+    import * as tty from 'node:tty';
+    import { Worker } from 'node:worker_threads';
+    global {
+        var process: NodeJS.Process;
+        namespace NodeJS {
+            // this namespace merge is here because these are specifically used
+            // as the type for process.stdin, process.stdout, and process.stderr.
+            // they can't live in tty.d.ts because we need to disambiguate the imported name.
+            interface ReadStream extends tty.ReadStream {}
+            interface WriteStream extends tty.WriteStream {}
+            interface MemoryUsageFn {
+                /**
+                 * The `process.memoryUsage()` method iterate over each page to gather informations about memory
+                 * usage which can be slow depending on the program memory allocations.
+                 */
+                (): MemoryUsage;
+                /**
+                 * method returns an integer representing the Resident Set Size (RSS) in bytes.
+                 */
+                rss(): number;
+            }
+            interface MemoryUsage {
+                rss: number;
+                heapTotal: number;
+                heapUsed: number;
+                external: number;
+                arrayBuffers: number;
+            }
+            interface CpuUsage {
+                user: number;
+                system: number;
+            }
+            interface ProcessRelease {
+                name: string;
+                sourceUrl?: string | undefined;
+                headersUrl?: string | undefined;
+                libUrl?: string | undefined;
+                lts?: string | undefined;
+            }
+            interface ProcessVersions extends Dict<string> {
+                http_parser: string;
+                node: string;
+                v8: string;
+                ares: string;
+                uv: string;
+                zlib: string;
+                modules: string;
+                openssl: string;
+            }
+            type Platform = 'aix' | 'android' | 'darwin' | 'freebsd' | 'haiku' | 'linux' | 'openbsd' | 'sunos' | 'win32' | 'cygwin' | 'netbsd';
+            type Signals =
+                | 'SIGABRT'
+                | 'SIGALRM'
+                | 'SIGBUS'
+                | 'SIGCHLD'
+                | 'SIGCONT'
+                | 'SIGFPE'
+                | 'SIGHUP'
+                | 'SIGILL'
+                | 'SIGINT'
+                | 'SIGIO'
+                | 'SIGIOT'
+                | 'SIGKILL'
+                | 'SIGPIPE'
+                | 'SIGPOLL'
+                | 'SIGPROF'
+                | 'SIGPWR'
+                | 'SIGQUIT'
+                | 'SIGSEGV'
+                | 'SIGSTKFLT'
+                | 'SIGSTOP'
+                | 'SIGSYS'
+                | 'SIGTERM'
+                | 'SIGTRAP'
+                | 'SIGTSTP'
+                | 'SIGTTIN'
+                | 'SIGTTOU'
+                | 'SIGUNUSED'
+                | 'SIGURG'
+                | 'SIGUSR1'
+                | 'SIGUSR2'
+                | 'SIGVTALRM'
+                | 'SIGWINCH'
+                | 'SIGXCPU'
+                | 'SIGXFSZ'
+                | 'SIGBREAK'
+                | 'SIGLOST'
+                | 'SIGINFO';
+            type UncaughtExceptionOrigin = 'uncaughtException' | 'unhandledRejection';
+            type MultipleResolveType = 'resolve' | 'reject';
+            type BeforeExitListener = (code: number) => void;
+            type DisconnectListener = () => void;
+            type ExitListener = (code: number) => void;
+            type RejectionHandledListener = (promise: Promise<unknown>) => void;
+            type UncaughtExceptionListener = (error: Error, origin: UncaughtExceptionOrigin) => void;
+            /**
+             * Most of the time the unhandledRejection will be an Error, but this should not be relied upon
+             * as *anything* can be thrown/rejected, it is therefore unsafe to assume the the value is an Error.
+             */
+            type UnhandledRejectionListener = (reason: unknown, promise: Promise<unknown>) => void;
+            type WarningListener = (warning: Error) => void;
+            type MessageListener = (message: unknown, sendHandle: unknown) => void;
+            type SignalsListener = (signal: Signals) => void;
+            type MultipleResolveListener = (type: MultipleResolveType, promise: Promise<unknown>, value: unknown) => void;
+            type WorkerListener = (worker: Worker) => void;
+            interface Socket extends ReadWriteStream {
+                isTTY?: true | undefined;
+            }
+            // Alias for compatibility
+            interface ProcessEnv extends Dict<string> {
+                /**
+                 * Can be used to change the default timezone at runtime
+                 */
+                TZ?: string;
+            }
+            interface HRTime {
+                (time?: [number, number]): [number, number];
+                bigint(): bigint;
+            }
+            interface ProcessReport {
+                /**
+                 * Directory where the report is written.
+                 * working directory of the Node.js process.
+                 * @default '' indicating that reports are written to the current
+                 */
+                directory: string;
+                /**
+                 * Filename where the report is written.
+                 * The default value is the empty string.
+                 * @default '' the output filename will be comprised of a timestamp,
+                 * PID, and sequence number.
+                 */
+                filename: string;
+                /**
+                 * Returns a JSON-formatted diagnostic report for the running process.
+                 * The report's JavaScript stack trace is taken from err, if present.
+                 */
+                getReport(err?: Error): string;
+                /**
+                 * If true, a diagnostic report is generated on fatal errors,
+                 * such as out of memory errors or failed C++ assertions.
+                 * @default false
+                 */
+                reportOnFatalError: boolean;
+                /**
+                 * If true, a diagnostic report is generated when the process
+                 * receives the signal specified by process.report.signal.
+                 * @default false
+                 */
+                reportOnSignal: boolean;
+                /**
+                 * If true, a diagnostic report is generated on uncaught exception.
+                 * @default false
+                 */
+                reportOnUncaughtException: boolean;
+                /**
+                 * The signal used to trigger the creation of a diagnostic report.
+                 * @default 'SIGUSR2'
+                 */
+                signal: Signals;
+                /**
+                 * Writes a diagnostic report to a file. If filename is not provided, the default filename
+                 * includes the date, time, PID, and a sequence number.
+                 * The report's JavaScript stack trace is taken from err, if present.
+                 *
+                 * @param fileName Name of the file where the report is written.
+                 * This should be a relative path, that will be appended to the directory specified in
+                 * `process.report.directory`, or the current working directory of the Node.js process,
+                 * if unspecified.
+                 * @param error A custom error used for reporting the JavaScript stack.
+                 * @return Filename of the generated report.
+                 */
+                writeReport(fileName?: string): string;
+                writeReport(error?: Error): string;
+                writeReport(fileName?: string, err?: Error): string;
+            }
+            interface ResourceUsage {
+                fsRead: number;
+                fsWrite: number;
+                involuntaryContextSwitches: number;
+                ipcReceived: number;
+                ipcSent: number;
+                majorPageFault: number;
+                maxRSS: number;
+                minorPageFault: number;
+                sharedMemorySize: number;
+                signalsCount: number;
+                swappedOut: number;
+                systemCPUTime: number;
+                unsharedDataSize: number;
+                unsharedStackSize: number;
+                userCPUTime: number;
+                voluntaryContextSwitches: number;
+            }
+            interface EmitWarningOptions {
+                /**
+                 * When `warning` is a `string`, `type` is the name to use for the _type_ of warning being emitted.
+                 *
+                 * @default 'Warning'
+                 */
+                type?: string | undefined;
+                /**
+                 * A unique identifier for the warning instance being emitted.
+                 */
+                code?: string | undefined;
+                /**
+                 * When `warning` is a `string`, `ctor` is an optional function used to limit the generated stack trace.
+                 *
+                 * @default process.emitWarning
+                 */
+                ctor?: Function | undefined;
+                /**
+                 * Additional text to include with the error.
+                 */
+                detail?: string | undefined;
+            }
+            interface ProcessConfig {
+                readonly target_defaults: {
+                    readonly cflags: any[];
+                    readonly default_configuration: string;
+                    readonly defines: string[];
+                    readonly include_dirs: string[];
+                    readonly libraries: string[];
+                };
+                readonly variables: {
+                    readonly clang: number;
+                    readonly host_arch: string;
+                    readonly node_install_npm: boolean;
+                    readonly node_install_waf: boolean;
+                    readonly node_prefix: string;
+                    readonly node_shared_openssl: boolean;
+                    readonly node_shared_v8: boolean;
+                    readonly node_shared_zlib: boolean;
+                    readonly node_use_dtrace: boolean;
+                    readonly node_use_etw: boolean;
+                    readonly node_use_openssl: boolean;
+                    readonly target_arch: string;
+                    readonly v8_no_strict_aliasing: number;
+                    readonly v8_use_snapshot: boolean;
+                    readonly visibility: string;
+                };
+            }
+            interface Process extends EventEmitter {
+                /**
+                 * The `process.stdout` property returns a stream connected to`stdout` (fd `1`). It is a `net.Socket` (which is a `Duplex` stream) unless fd `1` refers to a file, in which case it is
+                 * a `Writable` stream.
+                 *
+                 * For example, to copy `process.stdin` to `process.stdout`:
+                 *
+                 * ```js
+                 * import { stdin, stdout } from 'process';
+                 *
+                 * stdin.pipe(stdout);
+                 * ```
+                 *
+                 * `process.stdout` differs from other Node.js streams in important ways. See `note on process I/O` for more information.
+                 */
+                stdout: WriteStream & {
+                    fd: 1;
+                };
+                /**
+                 * The `process.stderr` property returns a stream connected to`stderr` (fd `2`). It is a `net.Socket` (which is a `Duplex` stream) unless fd `2` refers to a file, in which case it is
+                 * a `Writable` stream.
+                 *
+                 * `process.stderr` differs from other Node.js streams in important ways. See `note on process I/O` for more information.
+                 */
+                stderr: WriteStream & {
+                    fd: 2;
+                };
+                /**
+                 * The `process.stdin` property returns a stream connected to`stdin` (fd `0`). It is a `net.Socket` (which is a `Duplex` stream) unless fd `0` refers to a file, in which case it is
+                 * a `Readable` stream.
+                 *
+                 * For details of how to read from `stdin` see `readable.read()`.
+                 *
+                 * As a `Duplex` stream, `process.stdin` can also be used in ""old"" mode that
+                 * is compatible with scripts written for Node.js prior to v0.10\.
+                 * For more information see `Stream compatibility`.
+                 *
+                 * In ""old"" streams mode the `stdin` stream is paused by default, so one
+                 * must call `process.stdin.resume()` to read from it. Note also that calling`process.stdin.resume()` itself would switch stream to ""old"" mode.
+                 */
+                stdin: ReadStream & {
+                    fd: 0;
+                };
+                openStdin(): Socket;
+                /**
+                 * The `process.argv` property returns an array containing the command-line
+                 * arguments passed when the Node.js process was launched. The first element will
+                 * be {@link execPath}. See `process.argv0` if access to the original value
+                 * of `argv[0]` is needed. The second element will be the path to the JavaScript
+                 * file being executed. The remaining elements will be any additional command-line
+                 * arguments.
+                 *
+                 * For example, assuming the following script for `process-args.js`:
+                 *
+                 * ```js
+                 * import { argv } from 'process';
+                 *
+                 * // print process.argv
+                 * argv.forEach((val, index) => {
+                 *   console.log(`${index}: ${val}`);
+                 * });
+                 * ```
+                 *
+                 * Launching the Node.js process as:
+                 *
+                 * ```console
+                 * $ node process-args.js one two=three four
+                 * ```
+                 *
+                 * Would generate the output:
+                 *
+                 * ```text
+                 * 0: /usr/local/bin/node
+                 * 1: /Users/mjr/work/node/process-args.js
+                 * 2: one
+                 * 3: two=three
+                 * 4: four
+                 * ```
+                 * @since v0.1.27
+                 */
+                argv: string[];
+                /**
+                 * The `process.argv0` property stores a read-only copy of the original value of`argv[0]` passed when Node.js starts.
+                 *
+                 * ```console
+                 * $ bash -c 'exec -a customArgv0 ./node'
+                 * > process.argv[0]
+                 * '/Volumes/code/external/node/out/Release/node'
+                 * > process.argv0
+                 * 'customArgv0'
+                 * ```
+                 * @since v6.4.0
+                 */
+                argv0: string;
+                /**
+                 * The `process.execArgv` property returns the set of Node.js-specific command-line
+                 * options passed when the Node.js process was launched. These options do not
+                 * appear in the array returned by the {@link argv} property, and do not
+                 * include the Node.js executable, the name of the script, or any options following
+                 * the script name. These options are useful in order to spawn child processes with
+                 * the same execution environment as the parent.
+                 *
+                 * ```console
+                 * $ node --harmony script.js --version
+                 * ```
+                 *
+                 * Results in `process.execArgv`:
+                 *
+                 * ```js
+                 * ['--harmony']
+                 * ```
+                 *
+                 * And `process.argv`:
+                 *
+                 * ```js
+                 * ['/usr/local/bin/node', 'script.js', '--version']
+                 * ```
+                 *
+                 * Refer to `Worker constructor` for the detailed behavior of worker
+                 * threads with this property.
+                 * @since v0.7.7
+                 */
+                execArgv: string[];
+                /**
+                 * The `process.execPath` property returns the absolute pathname of the executable
+                 * that started the Node.js process. Symbolic links, if any, are resolved.
+                 *
+                 * ```js
+                 * '/usr/local/bin/node'
+                 * ```
+                 * @since v0.1.100
+                 */
+                execPath: string;
+                /**
+                 * The `process.abort()` method causes the Node.js process to exit immediately and
+                 * generate a core file.
+                 *
+                 * This feature is not available in `Worker` threads.
+                 * @since v0.7.0
+                 */
+                abort(): never;
+                /**
+                 * The `process.chdir()` method changes the current working directory of the
+                 * Node.js process or throws an exception if doing so fails (for instance, if
+                 * the specified `directory` does not exist).
+                 *
+                 * ```js
+                 * import { chdir, cwd } from 'process';
+                 *
+                 * console.log(`Starting directory: ${cwd()}`);
+                 * try {
+                 *   chdir('/tmp');
+                 *   console.log(`New directory: ${cwd()}`);
+                 * } catch (err) {
+                 *   console.error(`chdir: ${err}`);
+                 * }
+                 * ```
+                 *
+                 * This feature is not available in `Worker` threads.
+                 * @since v0.1.17
+                 */
+                chdir(directory: string): void;
+                /**
+                 * The `process.cwd()` method returns the current working directory of the Node.js
+                 * process.
+                 *
+                 * ```js
+                 * import { cwd } from 'process';
+                 *
+                 * console.log(`Current directory: ${cwd()}`);
+                 * ```
+                 * @since v0.1.8
+                 */
+                cwd(): string;
+                /**
+                 * The port used by the Node.js debugger when enabled.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * process.debugPort = 5858;
+                 * ```
+                 * @since v0.7.2
+                 */
+                debugPort: number;
+                /**
+                 * The `process.emitWarning()` method can be used to emit custom or application
+                 * specific process warnings. These can be listened for by adding a handler to the `'warning'` event.
+                 *
+                 * ```js
+                 * import { emitWarning } from 'process';
+                 *
+                 * // Emit a warning with a code and additional detail.
+                 * emitWarning('Something happened!', {
+                 *   code: 'MY_WARNING',
+                 *   detail: 'This is some additional information'
+                 * });
+                 * // Emits:
+                 * // (node:56338) [MY_WARNING] Warning: Something happened!
+                 * // This is some additional information
+                 * ```
+                 *
+                 * In this example, an `Error` object is generated internally by`process.emitWarning()` and passed through to the `'warning'` handler.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * process.on('warning', (warning) => {
+                 *   console.warn(warning.name);    // 'Warning'
+                 *   console.warn(warning.message); // 'Something happened!'
+                 *   console.warn(warning.code);    // 'MY_WARNING'
+                 *   console.warn(warning.stack);   // Stack trace
+                 *   console.warn(warning.detail);  // 'This is some additional information'
+                 * });
+                 * ```
+                 *
+                 * If `warning` is passed as an `Error` object, the `options` argument is ignored.
+                 * @since v8.0.0
+                 * @param warning The warning to emit.
+                 */
+                emitWarning(warning: string | Error, ctor?: Function): void;
+                emitWarning(warning: string | Error, type?: string, ctor?: Function): void;
+                emitWarning(warning: string | Error, type?: string, code?: string, ctor?: Function): void;
+                emitWarning(warning: string | Error, options?: EmitWarningOptions): void;
+                /**
+                 * The `process.env` property returns an object containing the user environment.
+                 * See [`environ(7)`](http://man7.org/linux/man-pages/man7/environ.7.html).
+                 *
+                 * An example of this object looks like:
+                 *
+                 * ```js
+                 * {
+                 *   TERM: 'xterm-256color',
+                 *   SHELL: '/usr/local/bin/bash',
+                 *   USER: 'maciej',
+                 *   PATH: '~/.bin/:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin',
+                 *   PWD: '/Users/maciej',
+                 *   EDITOR: 'vim',
+                 *   SHLVL: '1',
+                 *   HOME: '/Users/maciej',
+                 *   LOGNAME: 'maciej',
+                 *   _: '/usr/local/bin/node'
+                 * }
+                 * ```
+                 *
+                 * It is possible to modify this object, but such modifications will not be
+                 * reflected outside the Node.js process, or (unless explicitly requested)
+                 * to other `Worker` threads.
+                 * In other words, the following example would not work:
+                 *
+                 * ```console
+                 * $ node -e 'process.env.foo = ""bar""' &#x26;&#x26; echo $foo
+                 * ```
+                 *
+                 * While the following will:
+                 *
+                 * ```js
+                 * import { env } from 'process';
+                 *
+                 * env.foo = 'bar';
+                 * console.log(env.foo);
+                 * ```
+                 *
+                 * Assigning a property on `process.env` will implicitly convert the value
+                 * to a string. **This behavior is deprecated.** Future versions of Node.js may
+                 * throw an error when the value is not a string, number, or boolean.
+                 *
+                 * ```js
+                 * import { env } from 'process';
+                 *
+                 * env.test = null;
+                 * console.log(env.test);
+                 * // => 'null'
+                 * env.test = undefined;
+                 * console.log(env.test);
+                 * // => 'undefined'
+                 * ```
+                 *
+                 * Use `delete` to delete a property from `process.env`.
+                 *
+                 * ```js
+                 * import { env } from 'process';
+                 *
+                 * env.TEST = 1;
+                 * delete env.TEST;
+                 * console.log(env.TEST);
+                 * // => undefined
+                 * ```
+                 *
+                 * On Windows operating systems, environment variables are case-insensitive.
+                 *
+                 * ```js
+                 * import { env } from 'process';
+                 *
+                 * env.TEST = 1;
+                 * console.log(env.test);
+                 * // => 1
+                 * ```
+                 *
+                 * Unless explicitly specified when creating a `Worker` instance,
+                 * each `Worker` thread has its own copy of `process.env`, based on its
+                 * parent thread’s `process.env`, or whatever was specified as the `env` option
+                 * to the `Worker` constructor. Changes to `process.env` will not be visible
+                 * across `Worker` threads, and only the main thread can make changes that
+                 * are visible to the operating system or to native add-ons.
+                 * @since v0.1.27
+                 */
+                env: ProcessEnv;
+                /**
+                 * The `process.exit()` method instructs Node.js to terminate the process
+                 * synchronously with an exit status of `code`. If `code` is omitted, exit uses
+                 * either the 'success' code `0` or the value of `process.exitCode` if it has been
+                 * set. Node.js will not terminate until all the `'exit'` event listeners are
+                 * called.
+                 *
+                 * To exit with a 'failure' code:
+                 *
+                 * ```js
+                 * import { exit } from 'process';
+                 *
+                 * exit(1);
+                 * ```
+                 *
+                 * The shell that executed Node.js should see the exit code as `1`.
+                 *
+                 * Calling `process.exit()` will force the process to exit as quickly as possible
+                 * even if there are still asynchronous operations pending that have not yet
+                 * completed fully, including I/O operations to `process.stdout` and`process.stderr`.
+                 *
+                 * In most situations, it is not actually necessary to call `process.exit()`explicitly. The Node.js process will exit on its own _if there is no additional_
+                 * _work pending_ in the event loop. The `process.exitCode` property can be set to
+                 * tell the process which exit code to use when the process exits gracefully.
+                 *
+                 * For instance, the following example illustrates a _misuse_ of the`process.exit()` method that could lead to data printed to stdout being
+                 * truncated and lost:
+                 *
+                 * ```js
+                 * import { exit } from 'process';
+                 *
+                 * // This is an example of what *not* to do:
+                 * if (someConditionNotMet()) {
+                 *   printUsageToStdout();
+                 *   exit(1);
+                 * }
+                 * ```
+                 *
+                 * The reason this is problematic is because writes to `process.stdout` in Node.js
+                 * are sometimes _asynchronous_ and may occur over multiple ticks of the Node.js
+                 * event loop. Calling `process.exit()`, however, forces the process to exit_before_ those additional writes to `stdout` can be performed.
+                 *
+                 * Rather than calling `process.exit()` directly, the code _should_ set the`process.exitCode` and allow the process to exit naturally by avoiding
+                 * scheduling any additional work for the event loop:
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * // How to properly set the exit code while letting
+                 * // the process exit gracefully.
+                 * if (someConditionNotMet()) {
+                 *   printUsageToStdout();
+                 *   process.exitCode = 1;
+                 * }
+                 * ```
+                 *
+                 * If it is necessary to terminate the Node.js process due to an error condition,
+                 * throwing an _uncaught_ error and allowing the process to terminate accordingly
+                 * is safer than calling `process.exit()`.
+                 *
+                 * In `Worker` threads, this function stops the current thread rather
+                 * than the current process.
+                 * @since v0.1.13
+                 * @param [code=0] The exit code.
+                 */
+                exit(code?: number): never;
+                /**
+                 * A number which will be the process exit code, when the process either
+                 * exits gracefully, or is exited via {@link exit} without specifying
+                 * a code.
+                 *
+                 * Specifying a code to {@link exit} will override any
+                 * previous setting of `process.exitCode`.
+                 * @since v0.11.8
+                 */
+                exitCode?: number | undefined;
+                /**
+                 * The `process.getgid()` method returns the numerical group identity of the
+                 * process. (See [`getgid(2)`](http://man7.org/linux/man-pages/man2/getgid.2.html).)
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getgid) {
+                 *   console.log(`Current gid: ${process.getgid()}`);
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * @since v0.1.31
+                 */
+                getgid(): number;
+                /**
+                 * The `process.setgid()` method sets the group identity of the process. (See [`setgid(2)`](http://man7.org/linux/man-pages/man2/setgid.2.html).) The `id` can be passed as either a
+                 * numeric ID or a group name
+                 * string. If a group name is specified, this method blocks while resolving the
+                 * associated numeric ID.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getgid &#x26;&#x26; process.setgid) {
+                 *   console.log(`Current gid: ${process.getgid()}`);
+                 *   try {
+                 *     process.setgid(501);
+                 *     console.log(`New gid: ${process.getgid()}`);
+                 *   } catch (err) {
+                 *     console.log(`Failed to set gid: ${err}`);
+                 *   }
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * This feature is not available in `Worker` threads.
+                 * @since v0.1.31
+                 * @param id The group name or ID
+                 */
+                setgid(id: number | string): void;
+                /**
+                 * The `process.getuid()` method returns the numeric user identity of the process.
+                 * (See [`getuid(2)`](http://man7.org/linux/man-pages/man2/getuid.2.html).)
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getuid) {
+                 *   console.log(`Current uid: ${process.getuid()}`);
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * @since v0.1.28
+                 */
+                getuid(): number;
+                /**
+                 * The `process.setuid(id)` method sets the user identity of the process. (See [`setuid(2)`](http://man7.org/linux/man-pages/man2/setuid.2.html).) The `id` can be passed as either a
+                 * numeric ID or a username string.
+                 * If a username is specified, the method blocks while resolving the associated
+                 * numeric ID.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getuid &#x26;&#x26; process.setuid) {
+                 *   console.log(`Current uid: ${process.getuid()}`);
+                 *   try {
+                 *     process.setuid(501);
+                 *     console.log(`New uid: ${process.getuid()}`);
+                 *   } catch (err) {
+                 *     console.log(`Failed to set uid: ${err}`);
+                 *   }
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * This feature is not available in `Worker` threads.
+                 * @since v0.1.28
+                 */
+                setuid(id: number | string): void;
+                /**
+                 * The `process.geteuid()` method returns the numerical effective user identity of
+                 * the process. (See [`geteuid(2)`](http://man7.org/linux/man-pages/man2/geteuid.2.html).)
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.geteuid) {
+                 *   console.log(`Current uid: ${process.geteuid()}`);
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * @since v2.0.0
+                 */
+                geteuid(): number;
+                /**
+                 * The `process.seteuid()` method sets the effective user identity of the process.
+                 * (See [`seteuid(2)`](http://man7.org/linux/man-pages/man2/seteuid.2.html).) The `id` can be passed as either a numeric ID or a username
+                 * string. If a username is specified, the method blocks while resolving the
+                 * associated numeric ID.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.geteuid &#x26;&#x26; process.seteuid) {
+                 *   console.log(`Current uid: ${process.geteuid()}`);
+                 *   try {
+                 *     process.seteuid(501);
+                 *     console.log(`New uid: ${process.geteuid()}`);
+                 *   } catch (err) {
+                 *     console.log(`Failed to set uid: ${err}`);
+                 *   }
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * This feature is not available in `Worker` threads.
+                 * @since v2.0.0
+                 * @param id A user name or ID
+                 */
+                seteuid(id: number | string): void;
+                /**
+                 * The `process.getegid()` method returns the numerical effective group identity
+                 * of the Node.js process. (See [`getegid(2)`](http://man7.org/linux/man-pages/man2/getegid.2.html).)
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getegid) {
+                 *   console.log(`Current gid: ${process.getegid()}`);
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * @since v2.0.0
+                 */
+                getegid(): number;
+                /**
+                 * The `process.setegid()` method sets the effective group identity of the process.
+                 * (See [`setegid(2)`](http://man7.org/linux/man-pages/man2/setegid.2.html).) The `id` can be passed as either a numeric ID or a group
+                 * name string. If a group name is specified, this method blocks while resolving
+                 * the associated a numeric ID.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getegid &#x26;&#x26; process.setegid) {
+                 *   console.log(`Current gid: ${process.getegid()}`);
+                 *   try {
+                 *     process.setegid(501);
+                 *     console.log(`New gid: ${process.getegid()}`);
+                 *   } catch (err) {
+                 *     console.log(`Failed to set gid: ${err}`);
+                 *   }
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * This feature is not available in `Worker` threads.
+                 * @since v2.0.0
+                 * @param id A group name or ID
+                 */
+                setegid(id: number | string): void;
+                /**
+                 * The `process.getgroups()` method returns an array with the supplementary group
+                 * IDs. POSIX leaves it unspecified if the effective group ID is included but
+                 * Node.js ensures it always is.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getgroups) {
+                 *   console.log(process.getgroups()); // [ 16, 21, 297 ]
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * @since v0.9.4
+                 */
+                getgroups(): number[];
+                /**
+                 * The `process.setgroups()` method sets the supplementary group IDs for the
+                 * Node.js process. This is a privileged operation that requires the Node.js
+                 * process to have `root` or the `CAP_SETGID` capability.
+                 *
+                 * The `groups` array can contain numeric group IDs, group names, or both.
+                 *
+                 * ```js
+                 * import process from 'process';
+                 *
+                 * if (process.getgroups &#x26;&#x26; process.setgroups) {
+                 *   try {
+                 *     process.setgroups([501]);
+                 *     console.log(process.getgroups()); // new groups
+                 *   } catch (err) {
+                 *     console.log(`Failed to set groups: ${err}`);
+                 *   }
+                 * }
+                 * ```
+                 *
+                 * This function is only available on POSIX platforms (i.e. not Windows or
+                 * Android).
+                 * This feature is not available in `Worker` threads.
+                 * @since v0.9.4
+                 */
+                setgroups(groups: ReadonlyArray<string | number>): void;
+                /**
+                 * The `process.setUncaughtExceptionCaptureCallback()` function sets a function
+                 * that will be invoked when an uncaught exception occurs, which will receive the
+                 * exception value itself as its first argument.
+                 *
+                 * If such a function is set, the `'uncaughtException'` event will
+                 * not be emitted. If `--abort-on-uncaught-exception` was passed from the
+                 * command line or set through `v8.setFlagsFromString()`, the process will
+                 * not abort. Actions configured to take place on exceptions such as report
+                 * generations will be affected too
+                 *
+                 * To unset the capture function,`process.setUncaughtExceptionCaptureCallback(null)` may be used. Calling this
+                 * method with a non-`null` argument while another capture function is set will
+                 * throw an error.
+                 *
+                 * Using this function is mutually exclusive with using the deprecated `domain` built-in module.
+                 * @since v9.3.0
+                 */
+                setUncaughtExceptionCaptureCallback(cb: ((err: Error) => void) | null): void;
+                /**
+                 * Indicates whether a callback has been set using {@link setUncaughtExceptionCaptureCallback}.
+                 * @since v9.3.0
+                 */
+                hasUncaughtExceptionCaptureCallback(): boolean;
+                /**
+                 * The `process.version` property contains the Node.js version string.
+                 *
+                 * ```js
+                 * import { version } from 'process';
+                 *
+                 * console.log(`Version: ${version}`);
+                 * // Version: v14.8.0
+                 * ```
+                 *
+                 * To get the version string without the prepended _v_, use`process.versions.node`.
+                 * @since v0.1.3
+                 */
+                readonly version: string;
+                /**
+                 * The `process.versions` property returns an object listing the version strings of
+                 * Node.js and its dependencies. `process.versions.modules` indicates the current
+                 * ABI version, which is increased whenever a C++ API changes. Node.js will refuse
+                 * to load modules that were compiled against a different module ABI version.
+                 *
+                 * ```js
+                 * import { versions } from 'process';
+                 *
+                 * console.log(versions);
+                 * ```
+                 *
+                 * Will generate an object similar to:
+                 *
+                 * ```console
+                 * { node: '11.13.0',
+                 *   v8: '7.0.276.38-node.18',
+                 *   uv: '1.27.0',
+                 *   zlib: '1.2.11',
+                 *   brotli: '1.0.7',
+                 *   ares: '1.15.0',
+                 *   modules: '67',
+                 *   nghttp2: '1.34.0',
+                 *   napi: '4',
+                 *   llhttp: '1.1.1',
+                 *   openssl: '1.1.1b',
+                 *   cldr: '34.0',
+                 *   icu: '63.1',
+                 *   tz: '2018e',
+                 *   unicode: '11.0' }
+                 * ```
+                 * @since v0.2.0
+                 */
+                readonly versions: ProcessVersions;
+                /**
+                 * The `process.config` property returns an `Object` containing the JavaScript
+                 * representation of the configure options used to compile the current Node.js
+                 * executable. This is the same as the `config.gypi` file that was produced when
+                 * running the `./configure` script.
+                 *
+                 * An example of the possible output looks like:
+                 *
+                 * ```js
+                 * {
+                 *   target_defaults:
+                 *    { cflags: [],
+                 *      default_configuration: 'Release',
+                 *      defines: [],
+                 *      include_dirs: [],
+                 *      libraries: [] },
+                 *   variables:
+                 *    {
+                 *      host_arch: 'x64',
+                 *      napi_build_version: 5,
+                 *      node_install_npm: 'true',
+                 *      node_prefix: '',
+                 *      node_shared_cares: 'false',
+                 *      node_shared_http_parser: 'false',
+                 *      node_shared_libuv: 'false',
+                 *      node_shared_zlib: 'false',
+                 *      node_use_dtrace: 'false',
+                 *      node_use_openssl: 'true',
+                 *      node_shared_openssl: 'false',
+                 *      strict_aliasing: 'true',
+                 *      target_arch: 'x64',
+                 *      v8_use_snapshot: 1
+                 *    }
+                 * }
+                 * ```
+                 *
+                 * The `process.config` property is **not** read-only and there are existing
+                 * modules in the ecosystem that are known to extend, modify, or entirely replace
+                 * the value of `process.config`.
+                 *
+                 * Modifying the `process.config` property, or any child-property of the`process.config` object has been deprecated. The `process.config` will be made
+                 * read-only in a future release.
+                 * @since v0.7.7
+                 */
+                readonly config: ProcessConfig;
+                /**
+                 * The `process.kill()` method sends the `signal` to the process identified by`pid`.
+                 *
+                 * Signal names are strings such as `'SIGINT'` or `'SIGHUP'`. See `Signal Events` and [`kill(2)`](http://man7.org/linux/man-pages/man2/kill.2.html) for more information.
+                 *
+                 * This method will throw an error if the target `pid` does not exist. As a special
+                 * case, a signal of `0` can be used to test for the existence of a process.
+                 * Windows platforms will throw an error if the `pid` is used to kill a process
+                 * group.
+                 *
+                 * Even though the name of this function is `process.kill()`, it is really just a
+                 * signal sender, like the `kill` system call. The signal sent may do something
+                 * other than kill the target process.
+                 *
+                 * ```js
+                 * import process, { kill } from 'process';
+                 *
+                 * process.on('SIGHUP', () => {
+                 *   console.log('Got SIGHUP signal.');
+                 * });
+                 *
+                 * setTimeout(() => {
+                 *   console.log('Exiting.');
+                 *   process.exit(0);
+                 * }, 100);
+                 *
+                 * kill(process.pid, 'SIGHUP');
+                 * ```
+                 *
+                 * When `SIGUSR1` is received by a Node.js process, Node.js will start the
+                 * debugger. See `Signal Events`.
+                 * @since v0.0.6
+                 * @param pid A process ID
+                 * @param [signal='SIGTERM'] The signal to send, either as a string or number.
+                 */
+                kill(pid: number, signal?: string | number): true;
+                /**
+                 * The `process.pid` property returns the PID of the process.
+                 *
+                 * ```js
+                 * import { pid } from 'process';
+                 *
+                 * console.log(`This process is pid ${pid}`);
+                 * ```
+                 * @since v0.1.15
+                 */
+                readonly pid: number;
+                /**
+                 * The `process.ppid` property returns the PID of the parent of the
+                 * current process.
+                 *
+                 * ```js
+                 * import { ppid } from 'process';
+                 *
+                 * console.log(`The parent process is pid ${ppid}`);
+                 * ```
+                 * @since v9.2.0, v8.10.0, v6.13.0
+                 */
+                readonly ppid: number;
+                /**
+                 * The `process.title` property returns the current process title (i.e. returns
+                 * the current value of `ps`). Assigning a new value to `process.title` modifies
+                 * the current value of `ps`.
+                 *
+                 * When a new value is assigned, different platforms will impose different maximum
+                 * length restrictions on the title. Usually such restrictions are quite limited.
+                 * For instance, on Linux and macOS, `process.title` is limited to the size of the
+                 * binary name plus the length of the command-line arguments because setting the`process.title` overwrites the `argv` memory of the process. Node.js v0.8
+                 * allowed for longer process title strings by also overwriting the `environ`memory but that was potentially insecure and confusing in some (rather obscure)
+                 * cases.
+                 *
+                 * Assigning a value to `process.title` might not result in an accurate label
+                 * within process manager applications such as macOS Activity Monitor or Windows
+                 * Services Manager.
+                 * @since v0.1.104
+                 */
+                title: string;
+                /**
+                 * The operating system CPU architecture for which the Node.js binary was compiled.
+                 * Possible values are: `'arm'`, `'arm64'`, `'ia32'`, `'mips'`,`'mipsel'`, `'ppc'`,`'ppc64'`, `'s390'`, `'s390x'`, `'x32'`, and `'x64'`.
+                 *
+                 * ```js
+                 * import { arch } from 'process';
+                 *
+                 * console.log(`This processor architecture is ${arch}`);
+                 * ```
+                 * @since v0.5.0
+                 */
+                readonly arch: string;
+                /**
+                 * The `process.platform` property returns a string identifying the operating
+                 * system platform on which the Node.js process is running.
+                 *
+                 * Currently possible values are:
+                 *
+                 * * `'aix'`
+                 * * `'darwin'`
+                 * * `'freebsd'`
+                 * * `'linux'`
+                 * * `'openbsd'`
+                 * * `'sunos'`
+                 * * `'win32'`
+                 *
+                 * ```js
+                 * import { platform } from 'process';
+                 *
+                 * console.log(`This platform is ${platform}`);
+                 * ```
+                 *
+                 * The value `'android'` may also be returned if the Node.js is built on the
+                 * Android operating system. However, Android support in Node.js [is experimental](https://github.com/nodejs/node/blob/HEAD/BUILDING.md#androidandroid-based-devices-eg-firefox-os).
+                 * @since v0.1.16
+                 */
+                readonly platform: Platform;
+                /**
+                 * The `process.mainModule` property provides an alternative way of retrieving `require.main`. The difference is that if the main module changes at
+                 * runtime, `require.main` may still refer to the original main module in
+                 * modules that were required before the change occurred. Generally, it's
+                 * safe to assume that the two refer to the same module.
+                 *
+                 * As with `require.main`, `process.mainModule` will be `undefined` if there
+                 * is no entry script.
+                 * @since v0.1.17
+                 * @deprecated Since v14.0.0 - Use `main` instead.
+                 */
+                mainModule?: Module | undefined;
+                memoryUsage: MemoryUsageFn;
+                /**
+                 * The `process.cpuUsage()` method returns the user and system CPU time usage of
+                 * the current process, in an object with properties `user` and `system`, whose
+                 * values are microsecond values (millionth of a second). These values measure time
+                 * spent in user and system code respectively, and may end up being greater than
+                 * actual elapsed time if multiple CPU cores are performing work for this process.
+                 *
+                 * The result of a previous call to `process.cpuUsage()` can be passed as the
+                 * argument to the function, to get a diff reading.
+                 *
+                 * ```js
+                 * import { cpuUsage } from 'process';
+                 *
+                 * const startUsage = cpuUsage();
+                 * // { user: 38579, system: 6986 }
+                 *
+                 * // spin the CPU for 500 milliseconds
+                 * const now = Date.now();
+                 * while (Date.now() - now < 500);
+                 *
+                 * console.log(cpuUsage(startUsage));
+                 * // { user: 514883, system: 11226 }
+                 * ```
+                 * @since v6.1.0
+                 * @param previousValue A previous return value from calling `process.cpuUsage()`
+                 */
+                cpuUsage(previousValue?: CpuUsage): CpuUsage;
+                /**
+                 * `process.nextTick()` adds `callback` to the ""next tick queue"". This queue is
+                 * fully drained after the current operation on the JavaScript stack runs to
+                 * completion and before the event loop is allowed to continue. It's possible to
+                 * create an infinite loop if one were to recursively call `process.nextTick()`.
+                 * See the [Event Loop](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/#process-nexttick) guide for more background.
+                 *
+                 * ```js
+                 * import { nextTick } from 'process';
+                 *
+                 * console.log('start');
+                 * nextTick(() => {
+                 *   console.log('nextTick callback');
+                 * });
+                 * console.log('scheduled');
+                 * // Output:
+                 * // start
+                 * // scheduled
+                 * // nextTick callback
+                 * ```
+                 *
+                 * This is important when developing APIs in order to give users the opportunity
+                 * to assign event handlers _after_ an object has been constructed but before any
+                 * I/O has occurred:
+                 *
+                 * ```js
+                 * import { nextTick } from 'process';
+                 *
+                 * function MyThing(options) {
+                 *   this.setupOptions(options);
+                 *
+                 *   nextTick(() => {
+                 *     this.startDoingStuff();
+                 *   });
+                 * }
+                 *
+                 * const thing = new MyThing();
+                 * thing.getReadyForStuff();
+                 *
+                 * // thing.startDoingStuff() gets called now, not before.
+                 * ```
+                 *
+                 * It is very important for APIs to be either 100% synchronous or 100%
+                 * asynchronous. Consider this example:
+                 *
+                 * ```js
+                 * // WARNING!  DO NOT USE!  BAD UNSAFE HAZARD!
+                 * function maybeSync(arg, cb) {
+                 *   if (arg) {
+                 *     cb();
+                 *     return;
+                 *   }
+                 *
+                 *   fs.stat('file', cb);
+                 * }
+                 * ```
+                 *
+                 * This API is hazardous because in the following case:
+                 *
+                 * ```js
+                 * const maybeTrue = Math.random() > 0.5;
+                 *
+                 * maybeSync(maybeTrue, () => {
+                 *   foo();
+                 * });
+                 *
+                 * bar();
+                 * ```
+                 *
+                 * It is not clear whether `foo()` or `bar()` will be called first.
+                 *
+                 * The following approach is much better:
+                 *
+                 * ```js
+                 * import { nextTick } from 'process';
+                 *
+                 * function definitelyAsync(arg, cb) {
+                 *   if (arg) {
+                 *     nextTick(cb);
+                 *     return;
+                 *   }
+                 *
+                 *   fs.stat('file', cb);
+                 * }
+                 * ```
+                 * @since v0.1.26
+                 * @param args Additional arguments to pass when invoking the `callback`
+                 */
+                nextTick(callback: Function, ...args: any[]): void;
+                /**
+                 * The `process.release` property returns an `Object` containing metadata related
+                 * to the current release, including URLs for the source tarball and headers-only
+                 * tarball.
+                 *
+                 * `process.release` contains the following properties:
+                 *
+                 * ```js
+                 * {
+                 *   name: 'node',
+                 *   lts: 'Erbium',
+                 *   sourceUrl: 'https://nodejs.org/download/release/v12.18.1/node-v12.18.1.tar.gz',
+                 *   headersUrl: 'https://nodejs.org/download/release/v12.18.1/node-v12.18.1-headers.tar.gz',
+                 *   libUrl: 'https://nodejs.org/download/release/v12.18.1/win-x64/node.lib'
+                 * }
+                 * ```
+                 *
+                 * In custom builds from non-release versions of the source tree, only the`name` property may be present. The additional properties should not be
+                 * relied upon to exist.
+                 * @since v3.0.0
+                 */
+                readonly release: ProcessRelease;
+                features: {
+                    inspector: boolean;
+                    debug: boolean;
+                    uv: boolean;
+                    ipv6: boolean;
+                    tls_alpn: boolean;
+                    tls_sni: boolean;
+                    tls_ocsp: boolean;
+                    tls: boolean;
+                };
+                /**
+                 * `process.umask()` returns the Node.js process's file mode creation mask. Child
+                 * processes inherit the mask from the parent process.
+                 * @since v0.1.19
+                 * @deprecated Calling `process.umask()` with no argument causes the process-wide umask to be written twice. This introduces a race condition between threads, and is a potential   *
+                 * security vulnerability. There is no safe, cross-platform alternative API.
+                 */
+                umask(): number;
+                /**
+                 * Can only be set if not in worker thread.
+                 */
+                umask(mask: string | number): number;
+                /**
+                 * The `process.uptime()` method returns the number of seconds the current Node.js
+                 * process has been running.
+                 *
+                 * The return value includes fractions of a second. Use `Math.floor()` to get whole
+                 * seconds.
+                 * @since v0.5.0
+                 */
+                uptime(): number;
+                hrtime: HRTime;
+                /**
+                 * If Node.js is spawned with an IPC channel, the `process.send()` method can be
+                 * used to send messages to the parent process. Messages will be received as a `'message'` event on the parent's `ChildProcess` object.
+                 *
+                 * If Node.js was not spawned with an IPC channel, `process.send` will be`undefined`.
+                 *
+                 * The message goes through serialization and parsing. The resulting message might
+                 * not be the same as what is originally sent.
+                 * @since v0.5.9
+                 * @param options used to parameterize the sending of certain types of handles.`options` supports the following properties:
+                 */
+                send?(
+                    message: any,
+                    sendHandle?: any,
+                    options?: {
+                        swallowErrors?: boolean | undefined;
+                    },
+                    callback?: (error: Error | null) => void
+                ): boolean;
+                /**
+                 * If the Node.js process is spawned with an IPC channel (see the `Child Process` and `Cluster` documentation), the `process.disconnect()` method will close the
+                 * IPC channel to the parent process, allowing the child process to exit gracefully
+                 * once there are no other connections keeping it alive.
+                 *
+                 * The effect of calling `process.disconnect()` is the same as calling `ChildProcess.disconnect()` from the parent process.
+                 *
+                 * If the Node.js process was not spawned with an IPC channel,`process.disconnect()` will be `undefined`.
+                 * @since v0.7.2
+                 */
+                disconnect(): void;
+                /**
+                 * If the Node.js process is spawned with an IPC channel (see the `Child Process` and `Cluster` documentation), the `process.connected` property will return`true` so long as the IPC
+                 * channel is connected and will return `false` after`process.disconnect()` is called.
+                 *
+                 * Once `process.connected` is `false`, it is no longer possible to send messages
+                 * over the IPC channel using `process.send()`.
+                 * @since v0.7.2
+                 */
+                connected: boolean;
+                /**
+                 * The `process.allowedNodeEnvironmentFlags` property is a special,
+                 * read-only `Set` of flags allowable within the `NODE_OPTIONS` environment variable.
+                 *
+                 * `process.allowedNodeEnvironmentFlags` extends `Set`, but overrides`Set.prototype.has` to recognize several different possible flag
+                 * representations. `process.allowedNodeEnvironmentFlags.has()` will
+                 * return `true` in the following cases:
+                 *
+                 * * Flags may omit leading single (`-`) or double (`--`) dashes; e.g.,`inspect-brk` for `--inspect-brk`, or `r` for `-r`.
+                 * * Flags passed through to V8 (as listed in `--v8-options`) may replace
+                 * one or more _non-leading_ dashes for an underscore, or vice-versa;
+                 * e.g., `--perf_basic_prof`, `--perf-basic-prof`, `--perf_basic-prof`,
+                 * etc.
+                 * * Flags may contain one or more equals (`=`) characters; all
+                 * characters after and including the first equals will be ignored;
+                 * e.g., `--stack-trace-limit=100`.
+                 * * Flags _must_ be allowable within `NODE_OPTIONS`.
+                 *
+                 * When iterating over `process.allowedNodeEnvironmentFlags`, flags will
+                 * appear only _once_; each will begin with one or more dashes. Flags
+                 * passed through to V8 will contain underscores instead of non-leading
+                 * dashes:
+                 *
+                 * ```js
+                 * import { allowedNodeEnvironmentFlags } from 'process';
+                 *
+                 * allowedNodeEnvironmentFlags.forEach((flag) => {
+                 *   // -r
+                 *   // --inspect-brk
+                 *   // --abort_on_uncaught_exception
+                 *   // ...
+                 * });
+                 * ```
+                 *
+                 * The methods `add()`, `clear()`, and `delete()` of`process.allowedNodeEnvironmentFlags` do nothing, and will fail
+                 * silently.
+                 *
+                 * If Node.js was compiled _without_ `NODE_OPTIONS` support (shown in {@link config}), `process.allowedNodeEnvironmentFlags` will
+                 * contain what _would have_ been allowable.
+                 * @since v10.10.0
+                 */
+                allowedNodeEnvironmentFlags: ReadonlySet<string>;
+                /**
+                 * `process.report` is an object whose methods are used to generate diagnostic
+                 * reports for the current process. Additional documentation is available in the `report documentation`.
+                 * @since v11.8.0
+                 */
+                report?: ProcessReport | undefined;
+                /**
+                 * ```js
+                 * import { resourceUsage } from 'process';
+                 *
+                 * console.log(resourceUsage());
+                 * /*
+                 *   Will output:
+                 *   {
+                 *     userCPUTime: 82872,
+                 *     systemCPUTime: 4143,
+                 *     maxRSS: 33164,
+                 *     sharedMemorySize: 0,
+                 *     unsharedDataSize: 0,
+                 *     unsharedStackSize: 0,
+                 *     minorPageFault: 2469,
+                 *     majorPageFault: 0,
+                 *     swappedOut: 0,
+                 *     fsRead: 0,
+                 *     fsWrite: 8,
+                 *     ipcSent: 0,
+                 *     ipcReceived: 0,
+                 *     signalsCount: 0,
+                 *     voluntaryContextSwitches: 79,
+                 *     involuntaryContextSwitches: 1
+                 *   }
+                 *
+                 * ```
+                 * @since v12.6.0
+                 * @return the resource usage for the current process. All of these values come from the `uv_getrusage` call which returns a [`uv_rusage_t` struct][uv_rusage_t].
+                 */
+                resourceUsage(): ResourceUsage;
+                /**
+                 * The `process.traceDeprecation` property indicates whether the`--trace-deprecation` flag is set on the current Node.js process. See the
+                 * documentation for the `'warning' event` and the `emitWarning() method` for more information about this
+                 * flag's behavior.
+                 * @since v0.8.0
+                 */
+                traceDeprecation: boolean;
+                /* EventEmitter */
+                addListener(event: 'beforeExit', listener: BeforeExitListener): this;
+                addListener(event: 'disconnect', listener: DisconnectListener): this;
+                addListener(event: 'exit', listener: ExitListener): this;
+                addListener(event: 'rejectionHandled', listener: RejectionHandledListener): this;
+                addListener(event: 'uncaughtException', listener: UncaughtExceptionListener): this;
+                addListener(event: 'uncaughtExceptionMonitor', listener: UncaughtExceptionListener): this;
+                addListener(event: 'unhandledRejection', listener: UnhandledRejectionListener): this;
+                addListener(event: 'warning', listener: WarningListener): this;
+                addListener(event: 'message', listener: MessageListener): this;
+                addListener(event: Signals, listener: SignalsListener): this;
+                addListener(event: 'multipleResolves', listener: MultipleResolveListener): this;
+                addListener(event: 'worker', listener: WorkerListener): this;
+                emit(event: 'beforeExit', code: number): boolean;
+                emit(event: 'disconnect'): boolean;
+                emit(event: 'exit', code: number): boolean;
+                emit(event: 'rejectionHandled', promise: Promise<unknown>): boolean;
+                emit(event: 'uncaughtException', error: Error): boolean;
+                emit(event: 'uncaughtExceptionMonitor', error: Error): boolean;
+                emit(event: 'unhandledRejection', reason: unknown, promise: Promise<unknown>): boolean;
+                emit(event: 'warning', warning: Error): boolean;
+                emit(event: 'message', message: unknown, sendHandle: unknown): this;
+                emit(event: Signals, signal?: Signals): boolean;
+                emit(event: 'multipleResolves', type: MultipleResolveType, promise: Promise<unknown>, value: unknown): this;
+                emit(event: 'worker', listener: WorkerListener): this;
+                on(event: 'beforeExit', listener: BeforeExitListener): this;
+                on(event: 'disconnect', listener: DisconnectListener): this;
+                on(event: 'exit', listener: ExitListener): this;
+                on(event: 'rejectionHandled', listener: RejectionHandledListener): this;
+                on(event: 'uncaughtException', listener: UncaughtExceptionListener): this;
+                on(event: 'uncaughtExceptionMonitor', listener: UncaughtExceptionListener): this;
+                on(event: 'unhandledRejection', listener: UnhandledRejectionListener): this;
+                on(event: 'warning', listener: WarningListener): this;
+                on(event: 'message', listener: MessageListener): this;
+                on(event: Signals, listener: SignalsListener): this;
+                on(event: 'multipleResolves', listener: MultipleResolveListener): this;
+                on(event: 'worker', listener: WorkerListener): this;
+                on(event: string | symbol, listener: (...args: any[]) => void): this;
+                once(event: 'beforeExit', listener: BeforeExitListener): this;
+                once(event: 'disconnect', listener: DisconnectListener): this;
+                once(event: 'exit', listener: ExitListener): this;
+                once(event: 'rejectionHandled', listener: RejectionHandledListener): this;
+                once(event: 'uncaughtException', listener: UncaughtExceptionListener): this;
+                once(event: 'uncaughtExceptionMonitor', listener: UncaughtExceptionListener): this;
+                once(event: 'unhandledRejection', listener: UnhandledRejectionListener): this;
+                once(event: 'warning', listener: WarningListener): this;
+                once(event: 'message', listener: MessageListener): this;
+                once(event: Signals, listener: SignalsListener): this;
+                once(event: 'multipleResolves', listener: MultipleResolveListener): this;
+                once(event: 'worker', listener: WorkerListener): this;
+                once(event: string | symbol, listener: (...args: any[]) => void): this;
+                prependListener(event: 'beforeExit', listener: BeforeExitListener): this;
+                prependListener(event: 'disconnect', listener: DisconnectListener): this;
+                prependListener(event: 'exit', listener: ExitListener): this;
+                prependListener(event: 'rejectionHandled', listener: RejectionHandledListener): this;
+                prependListener(event: 'uncaughtException', listener: UncaughtExceptionListener): this;
+                prependListener(event: 'uncaughtExceptionMonitor', listener: UncaughtExceptionListener): this;
+                prependListener(event: 'unhandledRejection', listener: UnhandledRejectionListener): this;
+                prependListener(event: 'warning', listener: WarningListener): this;
+                prependListener(event: 'message', listener: MessageListener): this;
+                prependListener(event: Signals, listener: SignalsListener): this;
+                prependListener(event: 'multipleResolves', listener: MultipleResolveListener): this;
+                prependListener(event: 'worker', listener: WorkerListener): this;
+                prependOnceListener(event: 'beforeExit', listener: BeforeExitListener): this;
+                prependOnceListener(event: 'disconnect', listener: DisconnectListener): this;
+                prependOnceListener(event: 'exit', listener: ExitListener): this;
+                prependOnceListener(event: 'rejectionHandled', listener: RejectionHandledListener): this;
+                prependOnceListener(event: 'uncaughtException', listener: UncaughtExceptionListener): this;
+                prependOnceListener(event: 'uncaughtExceptionMonitor', listener: UncaughtExceptionListener): this;
+                prependOnceListener(event: 'unhandledRejection', listener: UnhandledRejectionListener): this;
+                prependOnceListener(event: 'warning', listener: WarningListener): this;
+                prependOnceListener(event: 'message', listener: MessageListener): this;
+                prependOnceListener(event: Signals, listener: SignalsListener): this;
+                prependOnceListener(event: 'multipleResolves', listener: MultipleResolveListener): this;
+                prependOnceListener(event: 'worker', listener: WorkerListener): this;
+                listeners(event: 'beforeExit'): BeforeExitListener[];
+                listeners(event: 'disconnect'): DisconnectListener[];
+                listeners(event: 'exit'): ExitListener[];
+                listeners(event: 'rejectionHandled'): RejectionHandledListener[];
+                listeners(event: 'uncaughtException'): UncaughtExceptionListener[];
+                listeners(event: 'uncaughtExceptionMonitor'): UncaughtExceptionListener[];
+                listeners(event: 'unhandledRejection'): UnhandledRejectionListener[];
+                listeners(event: 'warning'): WarningListener[];
+                listeners(event: 'message'): MessageListener[];
+                listeners(event: Signals): SignalsListener[];
+                listeners(event: 'multipleResolves'): MultipleResolveListener[];
+                listeners(event: 'worker'): WorkerListener[];
+            }
+        }
+    }
+    export = process;
+}
+declare module 'node:process' {
+    import process = require('process');
+    export = process;
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * **The version of the punycode module bundled in Node.js is being deprecated.**In a future major version of Node.js this module will be removed. Users
+ * currently depending on the `punycode` module should switch to using the
+ * userland-provided [Punycode.js](https://github.com/bestiejs/punycode.js) module instead. For punycode-based URL
+ * encoding, see `url.domainToASCII` or, more generally, the `WHATWG URL API`.
+ *
+ * The `punycode` module is a bundled version of the [Punycode.js](https://github.com/bestiejs/punycode.js) module. It
+ * can be accessed using:
+ *
+ * ```js
+ * const punycode = require('punycode');
+ * ```
+ *
+ * [Punycode](https://tools.ietf.org/html/rfc3492) is a character encoding scheme defined by RFC 3492 that is
+ * primarily intended for use in Internationalized Domain Names. Because host
+ * names in URLs are limited to ASCII characters only, Domain Names that contain
+ * non-ASCII characters must be converted into ASCII using the Punycode scheme.
+ * For instance, the Japanese character that translates into the English word,`'example'` is `'例'`. The Internationalized Domain Name, `'例.com'` (equivalent
+ * to `'example.com'`) is represented by Punycode as the ASCII string`'xn--fsq.com'`.
+ *
+ * The `punycode` module provides a simple implementation of the Punycode standard.
+ *
+ * The `punycode` module is a third-party dependency used by Node.js and
+ * made available to developers as a convenience. Fixes or other modifications to
+ * the module must be directed to the [Punycode.js](https://github.com/bestiejs/punycode.js) project.
+ * @deprecated Since v7.0.0 - Deprecated
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/punycode.js)
+ */
+declare module 'punycode' {
+    /**
+     * The `punycode.decode()` method converts a [Punycode](https://tools.ietf.org/html/rfc3492) string of ASCII-only
+     * characters to the equivalent string of Unicode codepoints.
+     *
+     * ```js
+     * punycode.decode('maana-pta'); // 'mañana'
+     * punycode.decode('--dqo34k'); // '☃-⌘'
+     * ```
+     * @since v0.5.1
+     */
+    function decode(string: string): string;
+    /**
+     * The `punycode.encode()` method converts a string of Unicode codepoints to a [Punycode](https://tools.ietf.org/html/rfc3492) string of ASCII-only characters.
+     *
+     * ```js
+     * punycode.encode('mañana'); // 'maana-pta'
+     * punycode.encode('☃-⌘'); // '--dqo34k'
+     * ```
+     * @since v0.5.1
+     */
+    function encode(string: string): string;
+    /**
+     * The `punycode.toUnicode()` method converts a string representing a domain name
+     * containing [Punycode](https://tools.ietf.org/html/rfc3492) encoded characters into Unicode. Only the [Punycode](https://tools.ietf.org/html/rfc3492) encoded parts of the domain name are be
+     * converted.
+     *
+     * ```js
+     * // decode domain names
+     * punycode.toUnicode('xn--maana-pta.com'); // 'mañana.com'
+     * punycode.toUnicode('xn----dqo34k.com');  // '☃-⌘.com'
+     * punycode.toUnicode('example.com');       // 'example.com'
+     * ```
+     * @since v0.6.1
+     */
+    function toUnicode(domain: string): string;
+    /**
+     * The `punycode.toASCII()` method converts a Unicode string representing an
+     * Internationalized Domain Name to [Punycode](https://tools.ietf.org/html/rfc3492). Only the non-ASCII parts of the
+     * domain name will be converted. Calling `punycode.toASCII()` on a string that
+     * already only contains ASCII characters will have no effect.
+     *
+     * ```js
+     * // encode domain names
+     * punycode.toASCII('mañana.com');  // 'xn--maana-pta.com'
+     * punycode.toASCII('☃-⌘.com');   // 'xn----dqo34k.com'
+     * punycode.toASCII('example.com'); // 'example.com'
+     * ```
+     * @since v0.6.1
+     */
+    function toASCII(domain: string): string;
+    /**
+     * @deprecated since v7.0.0
+     * The version of the punycode module bundled in Node.js is being deprecated.
+     * In a future major version of Node.js this module will be removed.
+     * Users currently depending on the punycode module should switch to using
+     * the userland-provided Punycode.js module instead.
+     */
+    const ucs2: ucs2;
+    interface ucs2 {
+        /**
+         * @deprecated since v7.0.0
+         * The version of the punycode module bundled in Node.js is being deprecated.
+         * In a future major version of Node.js this module will be removed.
+         * Users currently depending on the punycode module should switch to using
+         * the userland-provided Punycode.js module instead.
+         */
+        decode(string: string): number[];
+        /**
+         * @deprecated since v7.0.0
+         * The version of the punycode module bundled in Node.js is being deprecated.
+         * In a future major version of Node.js this module will be removed.
+         * Users currently depending on the punycode module should switch to using
+         * the userland-provided Punycode.js module instead.
+         */
+        encode(codePoints: ReadonlyArray<number>): string;
+    }
+    /**
+     * @deprecated since v7.0.0
+     * The version of the punycode module bundled in Node.js is being deprecated.
+     * In a future major version of Node.js this module will be removed.
+     * Users currently depending on the punycode module should switch to using
+     * the userland-provided Punycode.js module instead.
+     */
+    const version: string;
+}
+declare module 'node:punycode' {
+    export * from 'punycode';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `querystring` module provides utilities for parsing and formatting URL
+ * query strings. It can be accessed using:
+ *
+ * ```js
+ * const querystring = require('querystring');
+ * ```
+ *
+ * The `querystring` API is considered Legacy. While it is still maintained,
+ * new code should use the `URLSearchParams` API instead.
+ * @deprecated Legacy
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/querystring.js)
+ */
+declare module 'querystring' {
+    interface StringifyOptions {
+        encodeURIComponent?: ((str: string) => string) | undefined;
+    }
+    interface ParseOptions {
+        maxKeys?: number | undefined;
+        decodeURIComponent?: ((str: string) => string) | undefined;
+    }
+    interface ParsedUrlQuery extends NodeJS.Dict<string | string[]> {}
+    interface ParsedUrlQueryInput extends NodeJS.Dict<string | number | boolean | ReadonlyArray<string> | ReadonlyArray<number> | ReadonlyArray<boolean> | null> {}
+    /**
+     * The `querystring.stringify()` method produces a URL query string from a
+     * given `obj` by iterating through the object's ""own properties"".
+     *
+     * It serializes the following types of values passed in `obj`:[string](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#String_type) |
+     * [number](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#Number_type) |
+     * [bigint](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt) |
+     * [boolean](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#Boolean_type) |
+     * [string\[\]](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#String_type) |
+     * [number\[\]](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#Number_type) |
+     * [bigint\[\]](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt) |
+     * [boolean\[\]](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#Boolean_type) The numeric values must be finite. Any other input values will be coerced to
+     * empty strings.
+     *
+     * ```js
+     * querystring.stringify({ foo: 'bar', baz: ['qux', 'quux'], corge: '' });
+     * // Returns 'foo=bar&#x26;baz=qux&#x26;baz=quux&#x26;corge='
+     *
+     * querystring.stringify({ foo: 'bar', baz: 'qux' }, ';', ':');
+     * // Returns 'foo:bar;baz:qux'
+     * ```
+     *
+     * By default, characters requiring percent-encoding within the query string will
+     * be encoded as UTF-8\. If an alternative encoding is required, then an alternative`encodeURIComponent` option will need to be specified:
+     *
+     * ```js
+     * // Assuming gbkEncodeURIComponent function already exists,
+     *
+     * querystring.stringify({ w: '中文', foo: 'bar' }, null, null,
+     *                       { encodeURIComponent: gbkEncodeURIComponent });
+     * ```
+     * @since v0.1.25
+     * @param obj The object to serialize into a URL query string
+     * @param [sep='&'] The substring used to delimit key and value pairs in the query string.
+     * @param [eq='='] . The substring used to delimit keys and values in the query string.
+     */
+    function stringify(obj?: ParsedUrlQueryInput, sep?: string, eq?: string, options?: StringifyOptions): string;
+    /**
+     * The `querystring.parse()` method parses a URL query string (`str`) into a
+     * collection of key and value pairs.
+     *
+     * For example, the query string `'foo=bar&#x26;abc=xyz&#x26;abc=123'` is parsed into:
+     *
+     * ```js
+     * {
+     *   foo: 'bar',
+     *   abc: ['xyz', '123']
+     * }
+     * ```
+     *
+     * The object returned by the `querystring.parse()` method _does not_prototypically inherit from the JavaScript `Object`. This means that typical`Object` methods such as `obj.toString()`,
+     * `obj.hasOwnProperty()`, and others
+     * are not defined and _will not work_.
+     *
+     * By default, percent-encoded characters within the query string will be assumed
+     * to use UTF-8 encoding. If an alternative character encoding is used, then an
+     * alternative `decodeURIComponent` option will need to be specified:
+     *
+     * ```js
+     * // Assuming gbkDecodeURIComponent function already exists...
+     *
+     * querystring.parse('w=%D6%D0%CE%C4&#x26;foo=bar', null, null,
+     *                   { decodeURIComponent: gbkDecodeURIComponent });
+     * ```
+     * @since v0.1.25
+     * @param str The URL query string to parse
+     * @param [sep='&'] The substring used to delimit key and value pairs in the query string.
+     * @param [eq='='] . The substring used to delimit keys and values in the query string.
+     */
+    function parse(str: string, sep?: string, eq?: string, options?: ParseOptions): ParsedUrlQuery;
+    /**
+     * The querystring.encode() function is an alias for querystring.stringify().
+     */
+    const encode: typeof stringify;
+    /**
+     * The querystring.decode() function is an alias for querystring.parse().
+     */
+    const decode: typeof parse;
+    /**
+     * The `querystring.escape()` method performs URL percent-encoding on the given`str` in a manner that is optimized for the specific requirements of URL
+     * query strings.
+     *
+     * The `querystring.escape()` method is used by `querystring.stringify()` and is
+     * generally not expected to be used directly. It is exported primarily to allow
+     * application code to provide a replacement percent-encoding implementation if
+     * necessary by assigning `querystring.escape` to an alternative function.
+     * @since v0.1.25
+     */
+    function escape(str: string): string;
+    /**
+     * The `querystring.unescape()` method performs decoding of URL percent-encoded
+     * characters on the given `str`.
+     *
+     * The `querystring.unescape()` method is used by `querystring.parse()` and is
+     * generally not expected to be used directly. It is exported primarily to allow
+     * application code to provide a replacement decoding implementation if
+     * necessary by assigning `querystring.unescape` to an alternative function.
+     *
+     * By default, the `querystring.unescape()` method will attempt to use the
+     * JavaScript built-in `decodeURIComponent()` method to decode. If that fails,
+     * a safer equivalent that does not throw on malformed URLs will be used.
+     * @since v0.1.25
+     */
+    function unescape(str: string): string;
+}
+declare module 'node:querystring' {
+    export * from 'querystring';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `readline` module provides an interface for reading data from a `Readable` stream (such as `process.stdin`) one line at a time.
+ *
+ * To use the promise-based APIs:
+ *
+ * ```js
+ * import * as readline from 'node:readline/promises';
+ * ```
+ *
+ * To use the callback and sync APIs:
+ *
+ * ```js
+ * import * as readline from 'node:readline';
+ * ```
+ *
+ * The following simple example illustrates the basic use of the `readline` module.
+ *
+ * ```js
+ * import * as readline from 'node:readline/promises';
+ * import { stdin as input, stdout as output } from 'process';
+ *
+ * const rl = readline.createInterface({ input, output });
+ *
+ * const answer = await rl.question('What do you think of Node.js? ');
+ *
+ * console.log(`Thank you for your valuable feedback: ${answer}`);
+ *
+ * rl.close();
+ * ```
+ *
+ * Once this code is invoked, the Node.js application will not terminate until the`readline.Interface` is closed because the interface waits for data to be
+ * received on the `input` stream.
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/readline.js)
+ */
+declare module 'readline' {
+    import { Abortable, EventEmitter } from 'node:events';
+    interface Key {
+        sequence?: string | undefined;
+        name?: string | undefined;
+        ctrl?: boolean | undefined;
+        meta?: boolean | undefined;
+        shift?: boolean | undefined;
+    }
+    /**
+     * Instances of the `readline.Interface` class are constructed using the`readline.createInterface()` method. Every instance is associated with a
+     * single `input` `Readable` stream and a single `output` `Writable` stream.
+     * The `output` stream is used to print prompts for user input that arrives on,
+     * and is read from, the `input` stream.
+     * @since v0.1.104
+     */
+    class Interface extends EventEmitter {
+        readonly terminal: boolean;
+        /**
+         * The current input data being processed by node.
+         *
+         * This can be used when collecting input from a TTY stream to retrieve the
+         * current value that has been processed thus far, prior to the `line` event
+         * being emitted. Once the `line` event has been emitted, this property will
+         * be an empty string.
+         *
+         * Be aware that modifying the value during the instance runtime may have
+         * unintended consequences if `rl.cursor` is not also controlled.
+         *
+         * **If not using a TTY stream for input, use the `'line'` event.**
+         *
+         * One possible use case would be as follows:
+         *
+         * ```js
+         * const values = ['lorem ipsum', 'dolor sit amet'];
+         * const rl = readline.createInterface(process.stdin);
+         * const showResults = debounce(() => {
+         *   console.log(
+         *     '\n',
+         *     values.filter((val) => val.startsWith(rl.line)).join(' ')
+         *   );
+         * }, 300);
+         * process.stdin.on('keypress', (c, k) => {
+         *   showResults();
+         * });
+         * ```
+         * @since v0.1.98
+         */
+        readonly line: string;
+        /**
+         * The cursor position relative to `rl.line`.
+         *
+         * This will track where the current cursor lands in the input string, when
+         * reading input from a TTY stream. The position of cursor determines the
+         * portion of the input string that will be modified as input is processed,
+         * as well as the column where the terminal caret will be rendered.
+         * @since v0.1.98
+         */
+        readonly cursor: number;
+        /**
+         * NOTE: According to the documentation:
+         *
+         * > Instances of the `readline.Interface` class are constructed using the
+         * > `readline.createInterface()` method.
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/readline.html#readline_class_interface
+         */
+        protected constructor(input: NodeJS.ReadableStream, output?: NodeJS.WritableStream, completer?: Completer | AsyncCompleter, terminal?: boolean);
+        /**
+         * NOTE: According to the documentation:
+         *
+         * > Instances of the `readline.Interface` class are constructed using the
+         * > `readline.createInterface()` method.
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/readline.html#readline_class_interface
+         */
+        protected constructor(options: ReadLineOptions);
+        /**
+         * The `rl.getPrompt()` method returns the current prompt used by `rl.prompt()`.
+         * @since v15.3.0
+         * @return the current prompt string
+         */
+        getPrompt(): string;
+        /**
+         * The `rl.setPrompt()` method sets the prompt that will be written to `output`whenever `rl.prompt()` is called.
+         * @since v0.1.98
+         */
+        setPrompt(prompt: string): void;
+        /**
+         * The `rl.prompt()` method writes the `readline.Interface` instances configured`prompt` to a new line in `output` in order to provide a user with a new
+         * location at which to provide input.
+         *
+         * When called, `rl.prompt()` will resume the `input` stream if it has been
+         * paused.
+         *
+         * If the `readline.Interface` was created with `output` set to `null` or`undefined` the prompt is not written.
+         * @since v0.1.98
+         * @param preserveCursor If `true`, prevents the cursor placement from being reset to `0`.
+         */
+        prompt(preserveCursor?: boolean): void;
+        /**
+         * The `rl.question()` method displays the `query` by writing it to the `output`,
+         * waits for user input to be provided on `input`, then invokes the `callback`function passing the provided input as the first argument.
+         *
+         * When called, `rl.question()` will resume the `input` stream if it has been
+         * paused.
+         *
+         * If the `readline.Interface` was created with `output` set to `null` or`undefined` the `query` is not written.
+         *
+         * The `callback` function passed to `rl.question()` does not follow the typical
+         * pattern of accepting an `Error` object or `null` as the first argument.
+         * The `callback` is called with the provided answer as the only argument.
+         *
+         * Example usage:
+         *
+         * ```js
+         * rl.question('What is your favorite food? ', (answer) => {
+         *   console.log(`Oh, so your favorite food is ${answer}`);
+         * });
+         * ```
+         *
+         * Using an `AbortController` to cancel a question.
+         *
+         * ```js
+         * const ac = new AbortController();
+         * const signal = ac.signal;
+         *
+         * rl.question('What is your favorite food? ', { signal }, (answer) => {
+         *   console.log(`Oh, so your favorite food is ${answer}`);
+         * });
+         *
+         * signal.addEventListener('abort', () => {
+         *   console.log('The food question timed out');
+         * }, { once: true });
+         *
+         * setTimeout(() => ac.abort(), 10000);
+         * ```
+         *
+         * If this method is invoked as it's util.promisify()ed version, it returns a
+         * Promise that fulfills with the answer. If the question is canceled using
+         * an `AbortController` it will reject with an `AbortError`.
+         *
+         * ```js
+         * const util = require('util');
+         * const question = util.promisify(rl.question).bind(rl);
+         *
+         * async function questionExample() {
+         *   try {
+         *     const answer = await question('What is you favorite food? ');
+         *     console.log(`Oh, so your favorite food is ${answer}`);
+         *   } catch (err) {
+         *     console.error('Question rejected', err);
+         *   }
+         * }
+         * questionExample();
+         * ```
+         * @since v0.3.3
+         * @param query A statement or query to write to `output`, prepended to the prompt.
+         * @param callback A callback function that is invoked with the user's input in response to the `query`.
+         */
+        question(query: string, callback: (answer: string) => void): void;
+        question(query: string, options: Abortable, callback: (answer: string) => void): void;
+        /**
+         * The `rl.pause()` method pauses the `input` stream, allowing it to be resumed
+         * later if necessary.
+         *
+         * Calling `rl.pause()` does not immediately pause other events (including`'line'`) from being emitted by the `readline.Interface` instance.
+         * @since v0.3.4
+         */
+        pause(): this;
+        /**
+         * The `rl.resume()` method resumes the `input` stream if it has been paused.
+         * @since v0.3.4
+         */
+        resume(): this;
+        /**
+         * The `rl.close()` method closes the `readline.Interface` instance and
+         * relinquishes control over the `input` and `output` streams. When called,
+         * the `'close'` event will be emitted.
+         *
+         * Calling `rl.close()` does not immediately stop other events (including `'line'`)
+         * from being emitted by the `readline.Interface` instance.
+         * @since v0.1.98
+         */
+        close(): void;
+        /**
+         * The `rl.write()` method will write either `data` or a key sequence identified
+         * by `key` to the `output`. The `key` argument is supported only if `output` is
+         * a `TTY` text terminal. See `TTY keybindings` for a list of key
+         * combinations.
+         *
+         * If `key` is specified, `data` is ignored.
+         *
+         * When called, `rl.write()` will resume the `input` stream if it has been
+         * paused.
+         *
+         * If the `readline.Interface` was created with `output` set to `null` or`undefined` the `data` and `key` are not written.
+         *
+         * ```js
+         * rl.write('Delete this!');
+         * // Simulate Ctrl+U to delete the line written previously
+         * rl.write(null, { ctrl: true, name: 'u' });
+         * ```
+         *
+         * The `rl.write()` method will write the data to the `readline` `Interface`'s`input`_as if it were provided by the user_.
+         * @since v0.1.98
+         */
+        write(data: string | Buffer, key?: Key): void;
+        write(data: undefined | null | string | Buffer, key: Key): void;
+        /**
+         * Returns the real position of the cursor in relation to the input
+         * prompt + string. Long input (wrapping) strings, as well as multiple
+         * line prompts are included in the calculations.
+         * @since v13.5.0, v12.16.0
+         */
+        getCursorPos(): CursorPos;
+        /**
+         * events.EventEmitter
+         * 1. close
+         * 2. line
+         * 3. pause
+         * 4. resume
+         * 5. SIGCONT
+         * 6. SIGINT
+         * 7. SIGTSTP
+         * 8. history
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'line', listener: (input: string) => void): this;
+        addListener(event: 'pause', listener: () => void): this;
+        addListener(event: 'resume', listener: () => void): this;
+        addListener(event: 'SIGCONT', listener: () => void): this;
+        addListener(event: 'SIGINT', listener: () => void): this;
+        addListener(event: 'SIGTSTP', listener: () => void): this;
+        addListener(event: 'history', listener: (history: string[]) => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'close'): boolean;
+        emit(event: 'line', input: string): boolean;
+        emit(event: 'pause'): boolean;
+        emit(event: 'resume'): boolean;
+        emit(event: 'SIGCONT'): boolean;
+        emit(event: 'SIGINT'): boolean;
+        emit(event: 'SIGTSTP'): boolean;
+        emit(event: 'history', history: string[]): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'line', listener: (input: string) => void): this;
+        on(event: 'pause', listener: () => void): this;
+        on(event: 'resume', listener: () => void): this;
+        on(event: 'SIGCONT', listener: () => void): this;
+        on(event: 'SIGINT', listener: () => void): this;
+        on(event: 'SIGTSTP', listener: () => void): this;
+        on(event: 'history', listener: (history: string[]) => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'line', listener: (input: string) => void): this;
+        once(event: 'pause', listener: () => void): this;
+        once(event: 'resume', listener: () => void): this;
+        once(event: 'SIGCONT', listener: () => void): this;
+        once(event: 'SIGINT', listener: () => void): this;
+        once(event: 'SIGTSTP', listener: () => void): this;
+        once(event: 'history', listener: (history: string[]) => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'line', listener: (input: string) => void): this;
+        prependListener(event: 'pause', listener: () => void): this;
+        prependListener(event: 'resume', listener: () => void): this;
+        prependListener(event: 'SIGCONT', listener: () => void): this;
+        prependListener(event: 'SIGINT', listener: () => void): this;
+        prependListener(event: 'SIGTSTP', listener: () => void): this;
+        prependListener(event: 'history', listener: (history: string[]) => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'line', listener: (input: string) => void): this;
+        prependOnceListener(event: 'pause', listener: () => void): this;
+        prependOnceListener(event: 'resume', listener: () => void): this;
+        prependOnceListener(event: 'SIGCONT', listener: () => void): this;
+        prependOnceListener(event: 'SIGINT', listener: () => void): this;
+        prependOnceListener(event: 'SIGTSTP', listener: () => void): this;
+        prependOnceListener(event: 'history', listener: (history: string[]) => void): this;
+        [Symbol.asyncIterator](): AsyncIterableIterator<string>;
+    }
+    type ReadLine = Interface; // type forwarded for backwards compatibility
+    type Completer = (line: string) => CompleterResult;
+    type AsyncCompleter = (line: string, callback: (err?: null | Error, result?: CompleterResult) => void) => void;
+    type CompleterResult = [string[], string];
+    interface ReadLineOptions {
+        input: NodeJS.ReadableStream;
+        output?: NodeJS.WritableStream | undefined;
+        completer?: Completer | AsyncCompleter | undefined;
+        terminal?: boolean | undefined;
+        /**
+         *  Initial list of history lines. This option makes sense
+         * only if `terminal` is set to `true` by the user or by an internal `output`
+         * check, otherwise the history caching mechanism is not initialized at all.
+         * @default []
+         */
+        history?: string[] | undefined;
+        historySize?: number | undefined;
+        prompt?: string | undefined;
+        crlfDelay?: number | undefined;
+        /**
+         * If `true`, when a new input line added
+         * to the history list duplicates an older one, this removes the older line
+         * from the list.
+         * @default false
+         */
+        removeHistoryDuplicates?: boolean | undefined;
+        escapeCodeTimeout?: number | undefined;
+        tabSize?: number | undefined;
+    }
+    /**
+     * The `readline.createInterface()` method creates a new `readline.Interface`instance.
+     *
+     * ```js
+     * const readline = require('readline');
+     * const rl = readline.createInterface({
+     *   input: process.stdin,
+     *   output: process.stdout
+     * });
+     * ```
+     *
+     * Once the `readline.Interface` instance is created, the most common case is to
+     * listen for the `'line'` event:
+     *
+     * ```js
+     * rl.on('line', (line) => {
+     *   console.log(`Received: ${line}`);
+     * });
+     * ```
+     *
+     * If `terminal` is `true` for this instance then the `output` stream will get
+     * the best compatibility if it defines an `output.columns` property and emits
+     * a `'resize'` event on the `output` if or when the columns ever change
+     * (`process.stdout` does this automatically when it is a TTY).
+     *
+     * When creating a `readline.Interface` using `stdin` as input, the program
+     * will not terminate until it receives `EOF` (Ctrl+D on
+     * Linux/macOS, Ctrl+Z followed by Return on
+     * Windows).
+     * If you want your application to exit without waiting for user input, you can `unref()` the standard input stream:
+     *
+     * ```js
+     * process.stdin.unref();
+     * ```
+     * @since v0.1.98
+     */
+    function createInterface(input: NodeJS.ReadableStream, output?: NodeJS.WritableStream, completer?: Completer | AsyncCompleter, terminal?: boolean): Interface;
+    function createInterface(options: ReadLineOptions): Interface;
+    /**
+     * The `readline.emitKeypressEvents()` method causes the given `Readable` stream to begin emitting `'keypress'` events corresponding to received input.
+     *
+     * Optionally, `interface` specifies a `readline.Interface` instance for which
+     * autocompletion is disabled when copy-pasted input is detected.
+     *
+     * If the `stream` is a `TTY`, then it must be in raw mode.
+     *
+     * This is automatically called by any readline instance on its `input` if the`input` is a terminal. Closing the `readline` instance does not stop
+     * the `input` from emitting `'keypress'` events.
+     *
+     * ```js
+     * readline.emitKeypressEvents(process.stdin);
+     * if (process.stdin.isTTY)
+     *   process.stdin.setRawMode(true);
+     * ```
+     *
+     * ## Example: Tiny CLI
+     *
+     * The following example illustrates the use of `readline.Interface` class to
+     * implement a small command-line interface:
+     *
+     * ```js
+     * const readline = require('readline');
+     * const rl = readline.createInterface({
+     *   input: process.stdin,
+     *   output: process.stdout,
+     *   prompt: 'OHAI> '
+     * });
+     *
+     * rl.prompt();
+     *
+     * rl.on('line', (line) => {
+     *   switch (line.trim()) {
+     *     case 'hello':
+     *       console.log('world!');
+     *       break;
+     *     default:
+     *       console.log(`Say what? I might have heard '${line.trim()}'`);
+     *       break;
+     *   }
+     *   rl.prompt();
+     * }).on('close', () => {
+     *   console.log('Have a great day!');
+     *   process.exit(0);
+     * });
+     * ```
+     *
+     * ## Example: Read file stream line-by-Line
+     *
+     * A common use case for `readline` is to consume an input file one line at a
+     * time. The easiest way to do so is leveraging the `fs.ReadStream` API as
+     * well as a `for await...of` loop:
+     *
+     * ```js
+     * const fs = require('fs');
+     * const readline = require('readline');
+     *
+     * async function processLineByLine() {
+     *   const fileStream = fs.createReadStream('input.txt');
+     *
+     *   const rl = readline.createInterface({
+     *     input: fileStream,
+     *     crlfDelay: Infinity
+     *   });
+     *   // Note: we use the crlfDelay option to recognize all instances of CR LF
+     *   // ('\r\n') in input.txt as a single line break.
+     *
+     *   for await (const line of rl) {
+     *     // Each line in input.txt will be successively available here as `line`.
+     *     console.log(`Line from file: ${line}`);
+     *   }
+     * }
+     *
+     * processLineByLine();
+     * ```
+     *
+     * Alternatively, one could use the `'line'` event:
+     *
+     * ```js
+     * const fs = require('fs');
+     * const readline = require('readline');
+     *
+     * const rl = readline.createInterface({
+     *   input: fs.createReadStream('sample.txt'),
+     *   crlfDelay: Infinity
+     * });
+     *
+     * rl.on('line', (line) => {
+     *   console.log(`Line from file: ${line}`);
+     * });
+     * ```
+     *
+     * Currently, `for await...of` loop can be a bit slower. If `async` / `await`flow and speed are both essential, a mixed approach can be applied:
+     *
+     * ```js
+     * const { once } = require('events');
+     * const { createReadStream } = require('fs');
+     * const { createInterface } = require('readline');
+     *
+     * (async function processLineByLine() {
+     *   try {
+     *     const rl = createInterface({
+     *       input: createReadStream('big-file.txt'),
+     *       crlfDelay: Infinity
+     *     });
+     *
+     *     rl.on('line', (line) => {
+     *       // Process the line.
+     *     });
+     *
+     *     await once(rl, 'close');
+     *
+     *     console.log('File processed.');
+     *   } catch (err) {
+     *     console.error(err);
+     *   }
+     * })();
+     * ```
+     * @since v0.7.7
+     */
+    function emitKeypressEvents(stream: NodeJS.ReadableStream, readlineInterface?: Interface): void;
+    type Direction = -1 | 0 | 1;
+    interface CursorPos {
+        rows: number;
+        cols: number;
+    }
+    /**
+     * The `readline.clearLine()` method clears current line of given `TTY` stream
+     * in a specified direction identified by `dir`.
+     * @since v0.7.7
+     * @param callback Invoked once the operation completes.
+     * @return `false` if `stream` wishes for the calling code to wait for the `'drain'` event to be emitted before continuing to write additional data; otherwise `true`.
+     */
+    function clearLine(stream: NodeJS.WritableStream, dir: Direction, callback?: () => void): boolean;
+    /**
+     * The `readline.clearScreenDown()` method clears the given `TTY` stream from
+     * the current position of the cursor down.
+     * @since v0.7.7
+     * @param callback Invoked once the operation completes.
+     * @return `false` if `stream` wishes for the calling code to wait for the `'drain'` event to be emitted before continuing to write additional data; otherwise `true`.
+     */
+    function clearScreenDown(stream: NodeJS.WritableStream, callback?: () => void): boolean;
+    /**
+     * The `readline.cursorTo()` method moves cursor to the specified position in a
+     * given `TTY` `stream`.
+     * @since v0.7.7
+     * @param callback Invoked once the operation completes.
+     * @return `false` if `stream` wishes for the calling code to wait for the `'drain'` event to be emitted before continuing to write additional data; otherwise `true`.
+     */
+    function cursorTo(stream: NodeJS.WritableStream, x: number, y?: number, callback?: () => void): boolean;
+    /**
+     * The `readline.moveCursor()` method moves the cursor _relative_ to its current
+     * position in a given `TTY` `stream`.
+     *
+     * ## Example: Tiny CLI
+     *
+     * The following example illustrates the use of `readline.Interface` class to
+     * implement a small command-line interface:
+     *
+     * ```js
+     * const readline = require('readline');
+     * const rl = readline.createInterface({
+     *   input: process.stdin,
+     *   output: process.stdout,
+     *   prompt: 'OHAI> '
+     * });
+     *
+     * rl.prompt();
+     *
+     * rl.on('line', (line) => {
+     *   switch (line.trim()) {
+     *     case 'hello':
+     *       console.log('world!');
+     *       break;
+     *     default:
+     *       console.log(`Say what? I might have heard '${line.trim()}'`);
+     *       break;
+     *   }
+     *   rl.prompt();
+     * }).on('close', () => {
+     *   console.log('Have a great day!');
+     *   process.exit(0);
+     * });
+     * ```
+     *
+     * ## Example: Read file stream line-by-Line
+     *
+     * A common use case for `readline` is to consume an input file one line at a
+     * time. The easiest way to do so is leveraging the `fs.ReadStream` API as
+     * well as a `for await...of` loop:
+     *
+     * ```js
+     * const fs = require('fs');
+     * const readline = require('readline');
+     *
+     * async function processLineByLine() {
+     *   const fileStream = fs.createReadStream('input.txt');
+     *
+     *   const rl = readline.createInterface({
+     *     input: fileStream,
+     *     crlfDelay: Infinity
+     *   });
+     *   // Note: we use the crlfDelay option to recognize all instances of CR LF
+     *   // ('\r\n') in input.txt as a single line break.
+     *
+     *   for await (const line of rl) {
+     *     // Each line in input.txt will be successively available here as `line`.
+     *     console.log(`Line from file: ${line}`);
+     *   }
+     * }
+     *
+     * processLineByLine();
+     * ```
+     *
+     * Alternatively, one could use the `'line'` event:
+     *
+     * ```js
+     * const fs = require('fs');
+     * const readline = require('readline');
+     *
+     * const rl = readline.createInterface({
+     *   input: fs.createReadStream('sample.txt'),
+     *   crlfDelay: Infinity
+     * });
+     *
+     * rl.on('line', (line) => {
+     *   console.log(`Line from file: ${line}`);
+     * });
+     * ```
+     *
+     * Currently, `for await...of` loop can be a bit slower. If `async` / `await`flow and speed are both essential, a mixed approach can be applied:
+     *
+     * ```js
+     * const { once } = require('events');
+     * const { createReadStream } = require('fs');
+     * const { createInterface } = require('readline');
+     *
+     * (async function processLineByLine() {
+     *   try {
+     *     const rl = createInterface({
+     *       input: createReadStream('big-file.txt'),
+     *       crlfDelay: Infinity
+     *     });
+     *
+     *     rl.on('line', (line) => {
+     *       // Process the line.
+     *     });
+     *
+     *     await once(rl, 'close');
+     *
+     *     console.log('File processed.');
+     *   } catch (err) {
+     *     console.error(err);
+     *   }
+     * })();
+     * ```
+     * @since v0.7.7
+     * @param callback Invoked once the operation completes.
+     * @return `false` if `stream` wishes for the calling code to wait for the `'drain'` event to be emitted before continuing to write additional data; otherwise `true`.
+     */
+    function moveCursor(stream: NodeJS.WritableStream, dx: number, dy: number, callback?: () => void): boolean;
+}
+declare module 'node:readline' {
+    export * from 'readline';
+}"
KO;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;
OK;20;trung501;translateWordInPDF;03e9da091bb56568ed6074ab52df5f64490fc59e;update google translate, bing translate , my memory translate;"+/**
+ * The `repl` module provides a Read-Eval-Print-Loop (REPL) implementation that
+ * is available both as a standalone program or includible in other applications.
+ * It can be accessed using:
+ *
+ * ```js
+ * const repl = require('repl');
+ * ```
+ * @see [source](https://github.com/nodejs/node/blob/v17.0.0/lib/repl.js)
+ */
+declare module 'repl' {
+    import { Interface, Completer, AsyncCompleter } from 'node:readline';
+    import { Context } from 'node:vm';
+    import { InspectOptions } from 'node:util';
+    interface ReplOptions {
+        /**
+         * The input prompt to display.
+         * @default ""> ""
+         */
+        prompt?: string | undefined;
+        /**
+         * The `Readable` stream from which REPL input will be read.
+         * @default process.stdin
+         */
+        input?: NodeJS.ReadableStream | undefined;
+        /**
+         * The `Writable` stream to which REPL output will be written.
+         * @default process.stdout
+         */
+        output?: NodeJS.WritableStream | undefined;
+        /**
+         * If `true`, specifies that the output should be treated as a TTY terminal, and have
+         * ANSI/VT100 escape codes written to it.
+         * Default: checking the value of the `isTTY` property on the output stream upon
+         * instantiation.
+         */
+        terminal?: boolean | undefined;
+        /**
+         * The function to be used when evaluating each given line of input.
+         * Default: an async wrapper for the JavaScript `eval()` function. An `eval` function can
+         * error with `repl.Recoverable` to indicate the input was incomplete and prompt for
+         * additional lines.
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_default_evaluation
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_custom_evaluation_functions
+         */
+        eval?: REPLEval | undefined;
+        /**
+         * Defines if the repl prints output previews or not.
+         * @default `true` Always `false` in case `terminal` is falsy.
+         */
+        preview?: boolean | undefined;
+        /**
+         * If `true`, specifies that the default `writer` function should include ANSI color
+         * styling to REPL output. If a custom `writer` function is provided then this has no
+         * effect.
+         * Default: the REPL instance's `terminal` value.
+         */
+        useColors?: boolean | undefined;
+        /**
+         * If `true`, specifies that the default evaluation function will use the JavaScript
+         * `global` as the context as opposed to creating a new separate context for the REPL
+         * instance. The node CLI REPL sets this value to `true`.
+         * Default: `false`.
+         */
+        useGlobal?: boolean | undefined;
+        /**
+         * If `true`, specifies that the default writer will not output the return value of a
+         * command if it evaluates to `undefined`.
+         * Default: `false`.
+         */
+        ignoreUndefined?: boolean | undefined;
+        /**
+         * The function to invoke to format the output of each command before writing to `output`.
+         * Default: a wrapper for `util.inspect`.
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_customizing_repl_output
+         */
+        writer?: REPLWriter | undefined;
+        /**
+         * An optional function used for custom Tab auto completion.
+         *
+         * @see https://nodejs.org/dist/latest-v11.x/docs/api/readline.html#readline_use_of_the_completer_function
+         */
+        completer?: Completer | AsyncCompleter | undefined;
+        /**
+         * A flag that specifies whether the default evaluator executes all JavaScript commands in
+         * strict mode or default (sloppy) mode.
+         * Accepted values are:
+         * - `repl.REPL_MODE_SLOPPY` - evaluates expressions in sloppy mode.
+         * - `repl.REPL_MODE_STRICT` - evaluates expressions in strict mode. This is equivalent to
+         *   prefacing every repl statement with `'use strict'`.
+         */
+        replMode?: typeof REPL_MODE_SLOPPY | typeof REPL_MODE_STRICT | undefined;
+        /**
+         * Stop evaluating the current piece of code when `SIGINT` is received, i.e. `Ctrl+C` is
+         * pressed. This cannot be used together with a custom `eval` function.
+         * Default: `false`.
+         */
+        breakEvalOnSigint?: boolean | undefined;
+    }
+    type REPLEval = (this: REPLServer, evalCmd: string, context: Context, file: string, cb: (err: Error | null, result: any) => void) => void;
+    type REPLWriter = (this: REPLServer, obj: any) => string;
+    /**
+     * This is the default ""writer"" value, if none is passed in the REPL options,
+     * and it can be overridden by custom print functions.
+     */
+    const writer: REPLWriter & {
+        options: InspectOptions;
+    };
+    type REPLCommandAction = (this: REPLServer, text: string) => void;
+    interface REPLCommand {
+        /**
+         * Help text to be displayed when `.help` is entered.
+         */
+        help?: string | undefined;
+        /**
+         * The function to execute, optionally accepting a single string argument.
+         */
+        action: REPLCommandAction;
+    }
+    /**
+     * Instances of `repl.REPLServer` are created using the {@link start} method
+     * or directly using the JavaScript `new` keyword.
+     *
+     * ```js
+     * const repl = require('repl');
+     *
+     * const options = { useColors: true };
+     *
+     * const firstInstance = repl.start(options);
+     * const secondInstance = new repl.REPLServer(options);
+     * ```
+     * @since v0.1.91
+     */
+    class REPLServer extends Interface {
+        /**
+         * The `vm.Context` provided to the `eval` function to be used for JavaScript
+         * evaluation.
+         */
+        readonly context: Context;
+        /**
+         * @deprecated since v14.3.0 - Use `input` instead.
+         */
+        readonly inputStream: NodeJS.ReadableStream;
+        /**
+         * @deprecated since v14.3.0 - Use `output` instead.
+         */
+        readonly outputStream: NodeJS.WritableStream;
+        /**
+         * The `Readable` stream from which REPL input will be read.
+         */
+        readonly input: NodeJS.ReadableStream;
+        /**
+         * The `Writable` stream to which REPL output will be written.
+         */
+        readonly output: NodeJS.WritableStream;
+        /**
+         * The commands registered via `replServer.defineCommand()`.
+         */
+        readonly commands: NodeJS.ReadOnlyDict<REPLCommand>;
+        /**
+         * A value indicating whether the REPL is currently in ""editor mode"".
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_commands_and_special_keys
+         */
+        readonly editorMode: boolean;
+        /**
+         * A value indicating whether the `_` variable has been assigned.
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_assignment_of_the_underscore_variable
+         */
+        readonly underscoreAssigned: boolean;
+        /**
+         * The last evaluation result from the REPL (assigned to the `_` variable inside of the REPL).
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_assignment_of_the_underscore_variable
+         */
+        readonly last: any;
+        /**
+         * A value indicating whether the `_error` variable has been assigned.
+         *
+         * @since v9.8.0
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_assignment_of_the_underscore_variable
+         */
+        readonly underscoreErrAssigned: boolean;
+        /**
+         * The last error raised inside the REPL (assigned to the `_error` variable inside of the REPL).
+         *
+         * @since v9.8.0
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_assignment_of_the_underscore_variable
+         */
+        readonly lastError: any;
+        /**
+         * Specified in the REPL options, this is the function to be used when evaluating each
+         * given line of input. If not specified in the REPL options, this is an async wrapper
+         * for the JavaScript `eval()` function.
+         */
+        readonly eval: REPLEval;
+        /**
+         * Specified in the REPL options, this is a value indicating whether the default
+         * `writer` function should include ANSI color styling to REPL output.
+         */
+        readonly useColors: boolean;
+        /**
+         * Specified in the REPL options, this is a value indicating whether the default `eval`
+         * function will use the JavaScript `global` as the context as opposed to creating a new
+         * separate context for the REPL instance.
+         */
+        readonly useGlobal: boolean;
+        /**
+         * Specified in the REPL options, this is a value indicating whether the default `writer`
+         * function should output the result of a command if it evaluates to `undefined`.
+         */
+        readonly ignoreUndefined: boolean;
+        /**
+         * Specified in the REPL options, this is the function to invoke to format the output of
+         * each command before writing to `outputStream`. If not specified in the REPL options,
+         * this will be a wrapper for `util.inspect`.
+         */
+        readonly writer: REPLWriter;
+        /**
+         * Specified in the REPL options, this is the function to use for custom Tab auto-completion.
+         */
+        readonly completer: Completer | AsyncCompleter;
+        /**
+         * Specified in the REPL options, this is a flag that specifies whether the default `eval`
+         * function should execute all JavaScript commands in strict mode or default (sloppy) mode.
+         * Possible values are:
+         * - `repl.REPL_MODE_SLOPPY` - evaluates expressions in sloppy mode.
+         * - `repl.REPL_MODE_STRICT` - evaluates expressions in strict mode. This is equivalent to
+         *    prefacing every repl statement with `'use strict'`.
+         */
+        readonly replMode: typeof REPL_MODE_SLOPPY | typeof REPL_MODE_STRICT;
+        /**
+         * NOTE: According to the documentation:
+         *
+         * > Instances of `repl.REPLServer` are created using the `repl.start()` method and
+         * > _should not_ be created directly using the JavaScript `new` keyword.
+         *
+         * `REPLServer` cannot be subclassed due to implementation specifics in NodeJS.
+         *
+         * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_class_replserver
+         */
+        private constructor();
+        /**
+         * The `replServer.defineCommand()` method is used to add new `.`\-prefixed commands
+         * to the REPL instance. Such commands are invoked by typing a `.` followed by the`keyword`. The `cmd` is either a `Function` or an `Object` with the following
+         * properties:
+         *
+         * The following example shows two new commands added to the REPL instance:
+         *
+         * ```js
+         * const repl = require('repl');
+         *
+         * const replServer = repl.start({ prompt: '> ' });
+         * replServer.defineCommand('sayhello', {
+         *   help: 'Say hello',
+         *   action(name) {
+         *     this.clearBufferedCommand();
+         *     console.log(`Hello, ${name}!`);
+         *     this.displayPrompt();
+         *   }
+         * });
+         * replServer.defineCommand('saybye', function saybye() {
+         *   console.log('Goodbye!');
+         *   this.close();
+         * });
+         * ```
+         *
+         * The new commands can then be used from within the REPL instance:
+         *
+         * ```console
+         * > .sayhello Node.js User
+         * Hello, Node.js User!
+         * > .saybye
+         * Goodbye!
+         * ```
+         * @since v0.3.0
+         * @param keyword The command keyword (*without* a leading `.` character).
+         * @param cmd The function to invoke when the command is processed.
+         */
+        defineCommand(keyword: string, cmd: REPLCommandAction | REPLCommand): void;
+        /**
+         * The `replServer.displayPrompt()` method readies the REPL instance for input
+         * from the user, printing the configured `prompt` to a new line in the `output`and resuming the `input` to accept new input.
+         *
+         * When multi-line input is being entered, an ellipsis is printed rather than the
+         * 'prompt'.
+         *
+         * When `preserveCursor` is `true`, the cursor placement will not be reset to `0`.
+         *
+         * The `replServer.displayPrompt` method is primarily intended to be called from
+         * within the action function for commands registered using the`replServer.defineCommand()` method.
+         * @since v0.1.91
+         */
+        displayPrompt(preserveCursor?: boolean): void;
+        /**
+         * The `replServer.clearBufferedCommand()` method clears any command that has been
+         * buffered but not yet executed. This method is primarily intended to be
+         * called from within the action function for commands registered using the`replServer.defineCommand()` method.
+         * @since v9.0.0
+         */
+        clearBufferedCommand(): void;
+        /**
+         * Initializes a history log file for the REPL instance. When executing the
+         * Node.js binary and using the command-line REPL, a history file is initialized
+         * by default. However, this is not the case when creating a REPL
+         * programmatically. Use this method to initialize a history log file when working
+         * with REPL instances programmatically.
+         * @since v11.10.0
+         * @param historyPath the path to the history file
+         * @param callback called when history writes are ready or upon error
+         */
+        setupHistory(path: string, callback: (err: Error | null, repl: this) => void): void;
+        /**
+         * events.EventEmitter
+         * 1. close - inherited from `readline.Interface`
+         * 2. line - inherited from `readline.Interface`
+         * 3. pause - inherited from `readline.Interface`
+         * 4. resume - inherited from `readline.Interface`
+         * 5. SIGCONT - inherited from `readline.Interface`
+         * 6. SIGINT - inherited from `readline.Interface`
+         * 7. SIGTSTP - inherited from `readline.Interface`
+         * 8. exit
+         * 9. reset
+         */
+        addListener(event: string, listener: (...args: any[]) => void): this;
+        addListener(event: 'close', listener: () => void): this;
+        addListener(event: 'line', listener: (input: string) => void): this;
+        addListener(event: 'pause', listener: () => void): this;
+        addListener(event: 'resume', listener: () => void): this;
+        addListener(event: 'SIGCONT', listener: () => void): this;
+        addListener(event: 'SIGINT', listener: () => void): this;
+        addListener(event: 'SIGTSTP', listener: () => void): this;
+        addListener(event: 'exit', listener: () => void): this;
+        addListener(event: 'reset', listener: (context: Context) => void): this;
+        emit(event: string | symbol, ...args: any[]): boolean;
+        emit(event: 'close'): boolean;
+        emit(event: 'line', input: string): boolean;
+        emit(event: 'pause'): boolean;
+        emit(event: 'resume'): boolean;
+        emit(event: 'SIGCONT'): boolean;
+        emit(event: 'SIGINT'): boolean;
+        emit(event: 'SIGTSTP'): boolean;
+        emit(event: 'exit'): boolean;
+        emit(event: 'reset', context: Context): boolean;
+        on(event: string, listener: (...args: any[]) => void): this;
+        on(event: 'close', listener: () => void): this;
+        on(event: 'line', listener: (input: string) => void): this;
+        on(event: 'pause', listener: () => void): this;
+        on(event: 'resume', listener: () => void): this;
+        on(event: 'SIGCONT', listener: () => void): this;
+        on(event: 'SIGINT', listener: () => void): this;
+        on(event: 'SIGTSTP', listener: () => void): this;
+        on(event: 'exit', listener: () => void): this;
+        on(event: 'reset', listener: (context: Context) => void): this;
+        once(event: string, listener: (...args: any[]) => void): this;
+        once(event: 'close', listener: () => void): this;
+        once(event: 'line', listener: (input: string) => void): this;
+        once(event: 'pause', listener: () => void): this;
+        once(event: 'resume', listener: () => void): this;
+        once(event: 'SIGCONT', listener: () => void): this;
+        once(event: 'SIGINT', listener: () => void): this;
+        once(event: 'SIGTSTP', listener: () => void): this;
+        once(event: 'exit', listener: () => void): this;
+        once(event: 'reset', listener: (context: Context) => void): this;
+        prependListener(event: string, listener: (...args: any[]) => void): this;
+        prependListener(event: 'close', listener: () => void): this;
+        prependListener(event: 'line', listener: (input: string) => void): this;
+        prependListener(event: 'pause', listener: () => void): this;
+        prependListener(event: 'resume', listener: () => void): this;
+        prependListener(event: 'SIGCONT', listener: () => void): this;
+        prependListener(event: 'SIGINT', listener: () => void): this;
+        prependListener(event: 'SIGTSTP', listener: () => void): this;
+        prependListener(event: 'exit', listener: () => void): this;
+        prependListener(event: 'reset', listener: (context: Context) => void): this;
+        prependOnceListener(event: string, listener: (...args: any[]) => void): this;
+        prependOnceListener(event: 'close', listener: () => void): this;
+        prependOnceListener(event: 'line', listener: (input: string) => void): this;
+        prependOnceListener(event: 'pause', listener: () => void): this;
+        prependOnceListener(event: 'resume', listener: () => void): this;
+        prependOnceListener(event: 'SIGCONT', listener: () => void): this;
+        prependOnceListener(event: 'SIGINT', listener: () => void): this;
+        prependOnceListener(event: 'SIGTSTP', listener: () => void): this;
+        prependOnceListener(event: 'exit', listener: () => void): this;
+        prependOnceListener(event: 'reset', listener: (context: Context) => void): this;
+    }
+    /**
+     * A flag passed in the REPL options. Evaluates expressions in sloppy mode.
+     */
+    const REPL_MODE_SLOPPY: unique symbol;
+    /**
+     * A flag passed in the REPL options. Evaluates expressions in strict mode.
+     * This is equivalent to prefacing every repl statement with `'use strict'`.
+     */
+    const REPL_MODE_STRICT: unique symbol;
+    /**
+     * The `repl.start()` method creates and starts a {@link REPLServer} instance.
+     *
+     * If `options` is a string, then it specifies the input prompt:
+     *
+     * ```js
+     * const repl = require('repl');
+     *
+     * // a Unix style prompt
+     * repl.start('$ ');
+     * ```
+     * @since v0.1.91
+     */
+    function start(options?: string | ReplOptions): REPLServer;
+    /**
+     * Indicates a recoverable error that a `REPLServer` can use to support multi-line input.
+     *
+     * @see https://nodejs.org/dist/latest-v10.x/docs/api/repl.html#repl_recoverable_errors
+     */
+    class Recoverable extends SyntaxError {
+        err: Error;
+        constructor(err: Error);
+    }
+}
+declare module 'node:repl' {
+    export * from 'repl';
+}"
KO;20;salustiana;spot;88c52b8c01d4b5387fc0817824f5d67faf86e0df;using memory cache now;" 
 import spotipy
 from spotipy.oauth2 import SpotifyOAuth
 
 class Manager:
     def __init__(
@@ -30,6 +31,7 @@ def __init__(
                 client_secret=client_secret,
                 redirect_uri=""http://127.0.0.1:5907/callback"",
                 scope=""ugc-image-upload playlist-modify-public"",
             ),
             retries=5,
         )"
OK;20;salustiana;spot;88c52b8c01d4b5387fc0817824f5d67faf86e0df;using memory cache now;" 
 import spotipy
 from spotipy.oauth2 import SpotifyOAuth
+from spotipy.cache_handler import MemoryCacheHandler
 
 class Manager:
     def __init__(
@@ -30,6 +31,7 @@ def __init__(
                 client_secret=client_secret,
                 redirect_uri=""http://127.0.0.1:5907/callback"",
                 scope=""ugc-image-upload playlist-modify-public"",
+                cache_handler=MemoryCacheHandler(),
             ),
             retries=5,
         )"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
+from .netserialize import *
 from .dockernet import *
 from .dockershm import *
+from .dockernetserialize import *"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+import os
+
+def get_data():
+   testfile = ""airbnb.csv""
+   datadir = ""/data/""
+   currdir = os.path.dirname(os.path.abspath(__file__)) 
+   f = open(currdir + datadir + testfile, ""r"")
+   data = f.read()
+   return data
+
+if __name__ == ""__main__"":
+    get_data()"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
-print(f""recv reading from {__file__}..."")
-print(f""recv data: {contents}"")
 f.close()"
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
+print(f""recv: read from {__file__}"")
 f.close()"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
-print(f""send writing to shared mem from {__file__}..."")
-f.write(""a very simple message for recv"")
 f.close()"
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+from data import get_data
+
 f = open(""test.txt"", ""w+"")
+print(f""send: writing to shared mem from {__file__}..."")
+f.write(get_data())
 f.close()"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
+def copy_data_to_volume(volume): 
+    data_script = ""data.py""
+    curr_dir = os.path.dirname(os.path.abspath(__file__))
+    data_dir = curr_dir + ""/data""
+    dst_dir = volume[""Mountpoint""]+f""/data""
+    if os.path.exists(dst_dir):
+        shutil.rmtree(dst_dir)
+    shutil.copytree(data_dir, dst_dir)
+    shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
+
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
+    copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
-def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
-    data = sendsock.recv(1024)
-    print(f""send {data}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
-    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
+from data import get_data
+
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
+def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
+
     sendsock.sendall(b""my simple message"")
+    
+    # wait for confirmation
+    recvdata = sendsock.recv(1024)
+    print(f""send: {recvdata}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
+    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
-def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
-    print(""send writing to shared mem..."")
-    f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
-    print(f""recv data: {contents}"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
-    sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;21;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
+from data import get_data
+
+def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
+    print(""send: writing to shared mem..."")
+    f.write(data)
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
+    print(f""recv: received data"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
+    data = get_data()
+    sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
-    FLAG_STRATEGY_FILE_PATH
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_timestep,
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_freqstep,
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
-                    ""check_function"": self._check_compress,
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
-                    ""check_function"": self._check_flag_strategy,
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
-                    step_dict[key_lower][""value""] = value
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
+    FLAG_STRATEGY_FILE_PATH,
+    DEFAULT_FLAG_RFI,
+    DEFAULT_FLAG_MEMORYPERC
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_timestep
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_freqstep
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
+                    ""check_function"": self._check_compress
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
+                    ""check_function"": self._check_flag_strategy
+                },
+                ""flag_rfi"": {
+                    ""value"": None,
+                    ""default"": DEFAULT_FLAG_RFI,
+                    ""parsing_function"": (lambda x: bool(x)),
+                    ""check_function"": self._check_flag_rfi
+                },
+                ""flag_memoryperc"": {
+                    ""value"": DEFAULT_FLAG_MEMORYPERC, # This prevents any update from the parameter field
+                    ""default"": DEFAULT_FLAG_MEMORYPERC,
+                    ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_flag_memoryperc
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
+                    if step_dict[key_lower][""value""] is None:
+                        step_dict[key_lower][""value""] = value
+                    else:
+                        # The parameter has a fixed value that cannot be set
+                        break
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
+                # If the loop has not broken, it means the parameter is not expected
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
+    @staticmethod
+    def _check_flag_rfi(flag_rfi: bool) -> bool:
+        """""" """"""
+        return isinstance(flag_rfi, bool)
+
+
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
+    @staticmethod
+    def _check_flag_memoryperc(flag_memoryperc: int) -> bool:
+        """""" """"""
+        is_int = isinstance(flag_memoryperc, int)
+        is_percent = 0 <= flag_memoryperc <= 100
+        return is_int & is_percent
+
+
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
KO;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
OK;22;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
KO;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";" 
 #!/usr/bin/python3
 
 from threading import *
 import json
 
-# Modules to import
 import processmodule
 import statusmodule
 
@@ -16,19 +18,22 @@ def processModule():
         with open('server-proc.json', 'w', encoding='utf-8') as f:
             json.dump(processmodule.getProcess(), f, ensure_ascii=False, indent=4)
     except:
-        print(""Process module failed"")
 
 def serverStatusModule():
     try:
         with open(""server-status.json"", ""w"", encoding=""utf-8"") as f:
             json.dump(statusmodule.getLinuxStatus(), f, ensure_ascii=False, indent=4)
     except:
-        print(""Status module failed"")
 
 if __name__ == ""__main__"":
-    # Create threads to run modules
-    pmThread = Thread(target=processModule)
-    pmThread.start()
 
-    ssmThread = Thread(target=serverStatusModule)
-    ssmThread.start()"
OK;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";" 
 #!/usr/bin/python3
 
+# Imports
 from threading import *
 import json
+import time
 
+# Program modules to import
 import processmodule
 import statusmodule
 
@@ -16,19 +18,22 @@ def processModule():
         with open('server-proc.json', 'w', encoding='utf-8') as f:
             json.dump(processmodule.getProcess(), f, ensure_ascii=False, indent=4)
     except:
+        print(""\n\t\t[!] Process module failed\n"")
 
 def serverStatusModule():
     try:
         with open(""server-status.json"", ""w"", encoding=""utf-8"") as f:
             json.dump(statusmodule.getLinuxStatus(), f, ensure_ascii=False, indent=4)
     except:
+        print(""\n\t\t[!] Status module failed\n"")
 
 if __name__ == ""__main__"":
+    while True:
+        # Create threads to run modules
+        pmThread = Thread(target=processModule)
+        pmThread.start()
 
+        ssmThread = Thread(target=serverStatusModule)
+        ssmThread.start()
+
+        time.sleep(60)"
KO;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";" [
     {
-        ""pid"": 20296,
-        ""name"": ""Discord.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 420.77734375
     },
     {
-        ""pid"": 8440,
         ""name"": ""MsMpEng.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 338.98828125
     },
     {
-        ""pid"": 21684,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 318.40234375
     },
     {
-        ""pid"": 23592,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 281.82421875
     },
     {
-        ""pid"": 22080,
         ""name"": ""Code.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 217.32421875
     },
     {
-        ""pid"": 16008,
-        ""name"": ""EpicGamesLauncher.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 216.80078125
     },
     {
-        ""pid"": 2676,
-        ""name"": ""explorer.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 215.5078125
     },
     {
-        ""pid"": 4916,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 207.44140625
     },
     {
-        ""pid"": 6228,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 190.95703125
     },
     {
-        ""pid"": 7196,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 185.953125
     },
     {
-        ""pid"": 11184,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 180.52734375
     },
     {
-        ""pid"": 7132,
-        ""name"": ""SearchHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 167.234375
     },
     {
-        ""pid"": 2008,
         ""name"": ""dwm.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 165.79296875
     },
     {
-        ""pid"": 20188,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 163.6015625
     },
     {
-        ""pid"": 20436,
-        ""name"": ""Grammarly.Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 156.34765625
     },
     {
-        ""pid"": 5512,
-        ""name"": ""Discord.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 151.9765625
     },
     {
-        ""pid"": 28588,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 144.8515625
     },
     {
-        ""pid"": 21328,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 144.61328125
     },
     {
-        ""pid"": 26688,
-        ""name"": ""CefSharp.BrowserSubprocess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 132.2421875
     },
     {
-        ""pid"": 23276,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 130.58203125
     },
     {
-        ""pid"": 17928,
-        ""name"": ""QMxNetworkSync.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 117.45703125
     },
     {
-        ""pid"": 13140,
         ""name"": ""Razer Synapse Service.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 114.421875
     },
     {
-        ""pid"": 4552,
-        ""name"": ""StreamDeck.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 114.30078125
     },
     {
-        ""pid"": 20308,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 113.3203125
     },
     {
-        ""pid"": 23392,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 109.15234375
     },
     {
-        ""pid"": 10816,
-        ""name"": ""Razer Synapse 3.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 109.02734375
     },
     {
-        ""pid"": 25556,
-        ""name"": ""RazerCortex.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 105.23046875
     },
     {
-        ""pid"": 20164,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 102.51171875
     },
     {
-        ""pid"": 2076,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 95.81640625
     },
     {
-        ""pid"": 2296,
-        ""name"": ""GitHubDesktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 89.94140625
     },
     {
-        ""pid"": 24708,
-        ""name"": ""Spotify.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 89.6796875
     },
     {
-        ""pid"": 21924,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 87.1484375
     },
     {
-        ""pid"": 31376,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 79.9375
     },
     {
-        ""pid"": 23128,
         ""name"": ""Spotify.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 78.40625
     },
     {
-        ""pid"": 23172,
-        ""name"": ""Razer Central.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 73.09375
     },
     {
-        ""pid"": 18500,
-        ""name"": ""Battle.net.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 70.43359375
     },
     {
-        ""pid"": 27480,
-        ""name"": ""CefSharp.BrowserSubprocess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 68.96875
     },
     {
-        ""pid"": 22868,
-        ""name"": ""Microsoft.Photos.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 68.59765625
     },
     {
-        ""pid"": 3108,
-        ""name"": ""GitHubDesktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 68.24609375
     },
     {
-        ""pid"": 12772,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 64.0703125
     },
     {
-        ""pid"": 15684,
-        ""name"": ""wallpaper32.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 62.48828125
     },
     {
-        ""pid"": 13672,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 61.9609375
     },
     {
-        ""pid"": 8316,
-        ""name"": ""GameManagerService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 59.70703125
     },
     {
-        ""pid"": 16000,
-        ""name"": ""Discord.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 58.98046875
     },
     {
-        ""pid"": 1692,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 58.50390625
     },
     {
-        ""pid"": 9704,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 56.6953125
     },
     {
-        ""pid"": 10456,
-        ""name"": ""SearchIndexer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 56.5546875
     },
     {
-        ""pid"": 9392,
-        ""name"": ""powershell.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 56.45703125
     },
     {
-        ""pid"": 20016,
-        ""name"": ""NVIDIA Share.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 55.4375
     },
     {
-        ""pid"": 11968,
-        ""name"": ""nvcontainer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 54.85546875
     },
     {
-        ""pid"": 15356,
-        ""name"": ""powershell.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 53.828125
     },
     {
-        ""pid"": 5932,
-        ""name"": ""WindowsTerminal.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 53.65234375
     },
     {
-        ""pid"": 18196,
-        ""name"": ""ShellExperienceHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 52.42578125
     },
     {
-        ""pid"": 26288,
-        ""name"": ""Agent.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 50.3125
     },
     {
-        ""pid"": 1960,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 50.09375
     },
     {
-        ""pid"": 18000,
-        ""name"": ""EpicWebHelper.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 49.55859375
     },
     {
-        ""pid"": 7160,
-        ""name"": ""StartMenuExperienceHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 49.02734375
     },
     {
-        ""pid"": 21320,
-        ""name"": ""YourPhone.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 48.578125
     },
     {
-        ""pid"": 20256,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 48.30078125
     },
     {
-        ""pid"": 21820,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 48.2109375
     },
     {
-        ""pid"": 30728,
-        ""name"": ""GitHubDesktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 46.09375
     },
     {
-        ""pid"": 8212,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 45.7421875
     },
     {
-        ""pid"": 31016,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 45.13671875
     },
     {
-        ""pid"": 20056,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 44.796875
     },
     {
-        ""pid"": 11588,
-        ""name"": ""NVIDIA Share.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 44.19921875
     },
     {
-        ""pid"": 8344,
-        ""name"": ""RazerCentralService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 42.89453125
     },
     {
-        ""pid"": 6132,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 37.96875
     },
     {
-        ""pid"": 3396,
-        ""name"": ""NVDisplay.Container.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 37.44921875
     },
     {
-        ""pid"": 30104,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 37.3671875
     },
     {
-        ""pid"": 11172,
-        ""name"": ""Razer Synapse Service Process.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 36.3828125
     },
     {
-        ""pid"": 2892,
-        ""name"": ""GameBar.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 36.1953125
     },
     {
-        ""pid"": 25252,
-        ""name"": ""CefSharp.BrowserSubprocess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 35.99609375
     },
     {
-        ""pid"": 6656,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 34.25390625
     },
     {
-        ""pid"": 23620,
-        ""name"": ""Spotify.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 33.72265625
     },
     {
-        ""pid"": 18272,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 33.57421875
     },
     {
-        ""pid"": 29648,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 33.53515625
     },
     {
-        ""pid"": 8256,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 33.3515625
     },
     {
-        ""pid"": 25356,
-        ""name"": ""Battle.net.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 32.21484375
     },
     {
-        ""pid"": 11656,
-        ""name"": ""LockApp.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 31.95703125
     },
     {
-        ""pid"": 17088,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 31.53125
     },
     {
-        ""pid"": 10144,
-        ""name"": ""ArmouryCrate.UserSessionHelper.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 30.83984375
     },
     {
-        ""pid"": 15304,
-        ""name"": ""NVIDIA Web Helper.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 30.31640625
     },
     {
-        ""pid"": 27556,
-        ""name"": ""asus_framework.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 29.3359375
     },
     {
-        ""pid"": 8756,
-        ""name"": ""OfficeClickToRun.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 29.33203125
     },
     {
-        ""pid"": 29192,
-        ""name"": ""asus_framework.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 28.05078125
     },
     {
-        ""pid"": 21448,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 28.0390625
     },
     {
-        ""pid"": 19484,
-        ""name"": ""QtWebEngineProcess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 27.67578125
     },
     {
-        ""pid"": 4772,
-        ""name"": ""asus_framework.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 27.53125
     },
     {
-        ""pid"": 30132,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 27.44921875
     },
     {
-        ""pid"": 21556,
-        ""name"": ""chrome.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 27.42578125
     },
     {
-        ""pid"": 4696,
-        ""name"": ""AcPowerNotification.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 26.78125
     },
     {
-        ""pid"": 21004,
-        ""name"": ""EpicWebHelper.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 26.05859375
     },
     {
-        ""pid"": 29236,
-        ""name"": ""asus_framework.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 26.0234375
     },
     {
-        ""pid"": 10032,
-        ""name"": ""wireguard.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 25.57421875
     },
     {
-        ""pid"": 18564,
-        ""name"": ""com.barraider.shadowplay.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 25.56640625
     },
     {
-        ""pid"": 18572,
-        ""name"": ""com.barraider.spotify.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 24.62890625
     },
     {
-        ""pid"": 4960,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 24.4296875
     },
     {
-        ""pid"": 25044,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 23.38671875
     },
     {
-        ""pid"": 26324,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 22.87890625
     },
     {
-        ""pid"": 6816,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 22.19921875
     },
     {
-        ""pid"": 27508,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 22.0625
     },
     {
-        ""pid"": 8396,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 22.00390625
     },
     {
-        ""pid"": 18648,
-        ""name"": ""QtWebEngineProcess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 21.9140625
     },
     {
-        ""pid"": 15080,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 21.75
     },
     {
-        ""pid"": 13984,
-        ""name"": ""NVIDIA Share.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 21.55859375
     },
     {
-        ""pid"": 4112,
-        ""name"": ""WmiPrvSE.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 20.703125
     },
     {
-        ""pid"": 16876,
-        ""name"": ""QtWebEngineProcess.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 20.4765625
     },
     {
-        ""pid"": 23708,
-        ""name"": ""Spotify.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 20.23828125
     },
     {
-        ""pid"": 26004,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 19.40625
     },
     {
-        ""pid"": 30832,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 19.1328125
     },
     {
-        ""pid"": 8580,
-        ""name"": ""wireguard.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 18.97265625
     },
     {
-        ""pid"": 3464,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 18.67578125
     },
     {
-        ""pid"": 26608,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 18.27734375
     },
     {
-        ""pid"": 5940,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 18.2421875
     },
     {
-        ""pid"": 18680,
-        ""name"": ""QtWebEngineProcess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 17.80859375
     },
     {
-        ""pid"": 22620,
-        ""name"": ""Mobalytics Desktop.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 17.62890625
     },
     {
-        ""pid"": 2964,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 17.5234375
     },
     {
-        ""pid"": 6644,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 17.33984375
     },
     {
-        ""pid"": 21428,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 17.27734375
     },
     {
-        ""pid"": 11788,
-        ""name"": ""OneDrive.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 16.88671875
     },
     {
-        ""pid"": 16548,
-        ""name"": ""Discord.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 16.83203125
     },
     {
-        ""pid"": 2240,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 16.76953125
     },
     {
-        ""pid"": 19416,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.9609375
     },
     {
-        ""pid"": 28848,
-        ""name"": ""ArmourySwAgent.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.90234375
     },
     {
-        ""pid"": 3380,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.40625
     },
     {
-        ""pid"": 23124,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.40625
     },
     {
-        ""pid"": 19524,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.1875
     },
     {
-        ""pid"": 32584,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.14453125
     },
     {
-        ""pid"": 8204,
-        ""name"": ""ArmouryCrate.Service.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 14.078125
     },
     {
-        ""pid"": 8564,
-        ""name"": ""vgc.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 13.77734375
     },
     {
-        ""pid"": 5964,
-        ""name"": ""Discord.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 13.0078125
     },
     {
-        ""pid"": 19272,
-        ""name"": ""GitHubDesktop.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 12.8203125
     },
     {
-        ""pid"": 5260,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 12.80078125
     },
     {
-        ""pid"": 13092,
-        ""name"": ""Discord.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 12.390625
     },
     {
-        ""pid"": 8556,
-        ""name"": ""nvcontainer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 12.26953125
     },
     {
-        ""pid"": 204,
-        ""name"": ""Registry"",
         ""cpu_percent"": 0.0,
-        ""vms"": 12.23828125
     },
     {
-        ""pid"": 23700,
-        ""name"": ""Spotify.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 12.125
     },
     {
-        ""pid"": 7968,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 11.80078125
     },
     {
-        ""pid"": 8408,
-        ""name"": ""ROGLiveService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 11.7578125
     },
     {
-        ""pid"": 22712,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 11.74609375
     },
     {
-        ""pid"": 2044,
-        ""name"": ""Spotify.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 11.69140625
     },
     {
-        ""pid"": 17328,
-        ""name"": ""QtWebEngineProcess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 11.5546875
     },
     {
-        ""pid"": 1600,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 11.5078125
     },
     {
-        ""pid"": 26996,
-        ""name"": ""Code.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 10.79296875
     },
     {
-        ""pid"": 21956,
-        ""name"": ""Teams.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 10.70703125
     },
     {
-        ""pid"": 3084,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 10.375
     },
     {
-        ""pid"": 1492,
-        ""name"": ""lsass.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 10.17578125
     },
     {
-        ""pid"": 4476,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 9.875
     },
     {
-        ""pid"": 1732,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 9.5859375
     },
     {
-        ""pid"": 19284,
-        ""name"": ""msteams.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 9.58203125
     },
     {
-        ""pid"": 29668,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 9.56640625
     },
     {
-        ""pid"": 12608,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 9.5546875
     },
     {
-        ""pid"": 30384,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 9.15625
-    },
-    {
-        ""pid"": 4340,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 9.078125
     },
     {
-        ""pid"": 15488,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.9453125
     },
     {
-        ""pid"": 4440,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.92578125
     },
     {
-        ""pid"": 26344,
-        ""name"": ""python3.10.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.8984375
     },
     {
-        ""pid"": 11928,
-        ""name"": ""nvcontainer.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 8.76953125
     },
     {
-        ""pid"": 8572,
-        ""name"": ""LightingService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.70703125
     },
     {
-        ""pid"": 23144,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.59375
     },
     {
-        ""pid"": 31560,
-        ""name"": ""SgrmBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.46484375
     },
     {
-        ""pid"": 8796,
-        ""name"": ""gamingservices.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 8.37890625
     },
     {
-        ""pid"": 12332,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.98046875
     },
     {
-        ""pid"": 4532,
-        ""name"": ""taskhostw.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.97265625
     },
     {
-        ""pid"": 8284,
-        ""name"": ""RzSDKService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.671875
     },
     {
-        ""pid"": 1904,
-        ""name"": ""fontdrvhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.64453125
     },
     {
-        ""pid"": 13596,
-        ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 7.55859375
     },
     {
-        ""pid"": 8916,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.35546875
     },
     {
-        ""pid"": 3964,
-        ""name"": ""audiodg.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.32421875
     },
     {
-        ""pid"": 11248,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.234375
     },
     {
-        ""pid"": 2904,
-        ""name"": ""NVDisplay.Container.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.16796875
     },
     {
-        ""pid"": 27756,
-        ""name"": ""Battle.net.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.12109375
     },
     {
-        ""pid"": 11064,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.08203125
     },
     {
-        ""pid"": 7952,
-        ""name"": ""spoolsv.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.05859375
-    },
-    {
-        ""pid"": 24404,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 7.03125
     },
     {
-        ""pid"": 1468,
-        ""name"": ""services.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 7.0078125
     },
     {
-        ""pid"": 1284,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.95703125
-    },
-    {
-        ""pid"": 1516,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 6.94140625
     },
     {
-        ""pid"": 2120,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.88671875
     },
     {
-        ""pid"": 30148,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.8046875
     },
     {
-        ""pid"": 4288,
-        ""name"": ""sihost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.796875
-    },
-    {
-        ""pid"": 24688,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 6.7421875
     },
     {
-        ""pid"": 7724,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.32421875
     },
     {
-        ""pid"": 3784,
-        ""name"": ""atkexComSvc.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.23828125
     },
     {
-        ""pid"": 19324,
-        ""name"": ""browser_assistant.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.17578125
     },
     {
-        ""pid"": 19372,
-        ""name"": ""ApplicationFrameHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.15234375
-    },
-    {
-        ""pid"": 14492,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 6.03515625
     },
     {
-        ""pid"": 8932,
-        ""name"": ""jucheck.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 6.03125
     },
     {
-        ""pid"": 7636,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.92578125
     },
     {
-        ""pid"": 29652,
-        ""name"": ""PMRunner32.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.8046875
     },
     {
-        ""pid"": 29680,
-        ""name"": ""PMRunner64.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.70703125
     },
     {
-        ""pid"": 26352,
-        ""name"": ""conhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.6796875
     },
     {
-        ""pid"": 4416,
-        ""name"": ""dasHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.27734375
     },
     {
-        ""pid"": 15196,
-        ""name"": ""conhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.26171875
     },
     {
-        ""pid"": 10168,
-        ""name"": ""conhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.25
     },
     {
-        ""pid"": 18600,
-        ""name"": ""conhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.2421875
     },
     {
-        ""pid"": 18620,
-        ""name"": ""conhost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 5.23046875
     },
     {
-        ""pid"": 18968,
-        ""name"": ""conhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.23046875
     },
     {
-        ""pid"": 13776,
-        ""name"": ""NisSrv.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 5.12890625
-    },
-    {
         ""pid"": 4632,
-        ""name"": ""ArmourySocketServer.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 5.07421875
     },
     {
-        ""pid"": 23412,
-        ""name"": ""Rainmeter.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.81640625
     },
     {
-        ""pid"": 8196,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.7890625
     },
     {
-        ""pid"": 4812,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.71875
     },
     {
-        ""pid"": 3760,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.6171875
-    },
-    {
-        ""pid"": 6100,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 4.55078125
     },
     {
-        ""pid"": 2576,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.51171875
-    },
-    {
-        ""pid"": 5876,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 4.4921875
     },
     {
-        ""pid"": 12892,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.421875
     },
     {
-        ""pid"": 20744,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.3828125
     },
     {
-        ""pid"": 11472,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.30859375
     },
     {
-        ""pid"": 10884,
-        ""name"": ""dllhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.20703125
     },
     {
-        ""pid"": 8332,
-        ""name"": ""RzUpdateEngineService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.125
     },
     {
-        ""pid"": 12356,
-        ""name"": ""ctfmon.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.109375
     },
     {
-        ""pid"": 8448,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 4.06640625
     },
     {
-        ""pid"": 13524,
-        ""name"": ""SecurityHealthService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.9140625
-    },
-    {
-        ""pid"": 12424,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 3.8984375
     },
     {
-        ""pid"": 17504,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.78515625
     },
     {
-        ""pid"": 2972,
-        ""name"": ""AsusCertService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.76953125
     },
     {
-        ""pid"": 18952,
-        ""name"": ""twitchstudiostreamdeck.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.72265625
     },
     {
-        ""pid"": 9520,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.7109375
-    },
-    {
-        ""pid"": 25032,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 3.65625
     },
     {
-        ""pid"": 1416,
-        ""name"": ""csrss.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.6484375
-    },
-    {
-        ""pid"": 2648,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 3.59765625
     },
     {
-        ""pid"": 4792,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.56640625
     },
     {
-        ""pid"": 9456,
-        ""name"": ""FPSRunner32.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.484375
     },
     {
-        ""pid"": 5088,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.46484375
-    },
-    {
-        ""pid"": 9076,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 3.4453125
     },
     {
-        ""pid"": 20548,
-        ""name"": ""browser_assistant.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.390625
     },
     {
-        ""pid"": 29860,
-        ""name"": ""ArmouryWebBrowserEdge.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.390625
     },
     {
-        ""pid"": 1784,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.37890625
     },
     {
-        ""pid"": 20968,
-        ""name"": ""AppleMobileDeviceProcess.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.1796875
     },
     {
-        ""pid"": 8120,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.140625
     },
     {
-        ""pid"": 3612,
-        ""name"": ""WmiPrvSE.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.1015625
     },
     {
-        ""pid"": 8488,
-        ""name"": ""chrome.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.09765625
-    },
-    {
-        ""pid"": 12556,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 3.06640625
     },
     {
-        ""pid"": 8228,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.0234375
     },
     {
-        ""pid"": 12384,
-        ""name"": ""nvsphelper64.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 3.015625
     },
     {
-        ""pid"": 30564,
-        ""name"": ""Aac3572MbHal_x86.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.99609375
-    },
-    {
-        ""pid"": 1212,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.98828125
     },
     {
-        ""pid"": 8668,
-        ""name"": ""RzSDKServer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.9609375
-    },
-    {
-        ""pid"": 4480,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.953125
     },
     {
-        ""pid"": 6216,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.953125
     },
     {
-        ""pid"": 7480,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.9375
     },
     {
-        ""pid"": 8276,
-        ""name"": ""AsusFanControlService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.921875
-    },
-    {
-        ""pid"": 5868,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.91015625
     },
     {
-        ""pid"": 2384,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.90625
-    },
-    {
-        ""pid"": 19624,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.90234375
     },
     {
-        ""pid"": 4368,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.890625
     },
     {
-        ""pid"": 30204,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.8828125
     },
     {
-        ""pid"": 4600,
-        ""name"": ""ArmouryAIOFanServer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.84765625
     },
     {
-        ""pid"": 9048,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.79296875
     },
     {
-        ""pid"": 30712,
-        ""name"": ""FPSRunner64.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.73046875
     },
     {
-        ""pid"": 22040,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.6953125
     },
     {
-        ""pid"": 5292,
-        ""name"": ""rundll32.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.68359375
     },
     {
-        ""pid"": 1304,
-        ""name"": ""csrss.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.65625
     },
     {
-        ""pid"": 5308,
-        ""name"": ""GameBarFTServer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.59375
     },
     {
-        ""pid"": 3156,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.5703125
     },
     {
-        ""pid"": 5908,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.53515625
-    },
-    {
-        ""pid"": 14076,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.50390625
     },
     {
-        ""pid"": 1848,
-        ""name"": ""winlogon.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.4921875
     },
     {
-        ""pid"": 17048,
-        ""name"": ""openvpn-gui.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.4609375
     },
     {
-        ""pid"": 4064,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.33984375
-    },
-    {
-        ""pid"": 6496,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.33203125
     },
     {
-        ""pid"": 31936,
-        ""name"": ""conhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.28125
-    },
-    {
-        ""pid"": 23244,
         ""name"": ""MpCopyAccelerator.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.24609375
     },
     {
-        ""pid"": 22600,
-        ""name"": ""OpenConsole.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.2421875
     },
     {
-        ""pid"": 12364,
-        ""name"": ""AggregatorHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.22265625
-    },
-    {
-        ""pid"": 1900,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.21875
     },
     {
-        ""pid"": 13052,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.20703125
     },
     {
-        ""pid"": 23856,
-        ""name"": ""jusched.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.17578125
-    },
-    {
-        ""pid"": 3348,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.1640625
     },
     {
-        ""pid"": 14388,
-        ""name"": ""Aac3572MbHal_x86.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.1640625
-    },
-    {
-        ""pid"": 2280,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.1484375
     },
     {
-        ""pid"": 1628,
-        ""name"": ""fontdrvhost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.1328125
     },
     {
-        ""pid"": 1436,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.1171875
     },
     {
-        ""pid"": 8548,
-        ""name"": ""RzChromaStreamServer.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.1015625
     },
     {
-        ""pid"": 10740,
-        ""name"": ""RuntimeBroker.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.0859375
-    },
-    {
-        ""pid"": 18936,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.05078125
     },
     {
-        ""pid"": 1136,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 2.0390625
-    },
-    {
-        ""pid"": 13040,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 2.01171875
     },
     {
-        ""pid"": 4608,
-        ""name"": ""msedgewebview2.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.98046875
     },
     {
-        ""pid"": 3256,
-        ""name"": ""MemCompression"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.9765625
     },
     {
-        ""pid"": 12604,
-        ""name"": ""SecurityHealthSystray.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.96484375
-    },
-    {
-        ""pid"": 8092,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.90625
     },
     {
-        ""pid"": 18548,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.90625
     },
     {
-        ""pid"": 3168,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.90234375
-    },
-    {
-        ""pid"": 3248,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.8984375
     },
     {
-        ""pid"": 8324,
-        ""name"": ""sqlwriter.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.8828125
     },
     {
-        ""pid"": 2500,
-        ""name"": ""WUDFHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.86328125
     },
     {
-        ""pid"": 3356,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.82421875
-    },
-    {
-        ""pid"": 7488,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.78515625
     },
     {
-        ""pid"": 14420,
-        ""name"": ""vgtray.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.76171875
-    },
-    {
-        ""pid"": 4280,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.75
     },
     {
-        ""pid"": 14404,
-        ""name"": ""GoogleCrashHandler.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.73046875
     },
     {
-        ""pid"": 9244,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.6796875
     },
     {
-        ""pid"": 14472,
-        ""name"": ""GoogleCrashHandler64.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.67578125
     },
     {
-        ""pid"": 8308,
-        ""name"": ""armsvc.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.66015625
-    },
-    {
-        ""pid"": 2548,
         ""name"": ""svchost.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.62890625
     },
     {
-        ""pid"": 8220,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.62109375
-    },
-    {
-        ""pid"": 8184,
         ""name"": ""unsecapp.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.6171875
     },
     {
-        ""pid"": 15332,
-        ""name"": ""AacKingstonDramHal_x86.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.609375
     },
     {
-        ""pid"": 15020,
-        ""name"": ""extensionCardHal_x86.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.56640625
     },
     {
-        ""pid"": 6992,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.5625
     },
     {
-        ""pid"": 19840,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.5625
     },
     {
-        ""pid"": 14728,
-        ""name"": ""AacKingstonDramHal_x64.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.55078125
     },
     {
-        ""pid"": 1396,
-        ""name"": ""wininit.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.53125
     },
     {
-        ""pid"": 1668,
-        ""name"": ""WUDFHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.515625
     },
     {
-        ""pid"": 19448,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.5078125
     },
     {
-        ""pid"": 10960,
-        ""name"": ""rundll32.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.49609375
     },
     {
-        ""pid"": 10692,
-        ""name"": ""Aac3572DramHal_x86.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.48828125
     },
     {
-        ""pid"": 4188,
-        ""name"": ""unsecapp.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.4765625
-    },
-    {
-        ""pid"": 21244,
         ""name"": ""unsecapp.exe"",
-        ""cpu_percent"": 0.0,
-        ""vms"": 1.40234375
     },
     {
-        ""pid"": 3152,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.3125
     },
     {
-        ""pid"": 8236,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.296875
     },
     {
-        ""pid"": 3012,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.27734375
     },
     {
-        ""pid"": 19308,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
         ""vms"": 1.23828125
     },
     {
-        ""pid"": 8128,
-        ""name"": ""gamingservicesnet.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.2265625
     },
     {
-        ""pid"": 2212,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.1796875
     },
     {
-        ""pid"": 5756,
-        ""name"": ""dasHost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.16796875
     },
     {
-        ""pid"": 8292,
-        ""name"": ""openvpnserv.exe"",
         ""cpu_percent"": 0.0,
         ""vms"": 1.1328125
     },
     {
-        ""pid"": 740,
-        ""name"": ""smss.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.09765625
     },
     {
-        ""pid"": 8420,
-        ""name"": ""RzKLService.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 1.09375
     },
     {
-        ""pid"": 11864,
-        ""name"": ""svchost.exe"",
         ""cpu_percent"": 0.0,
-        ""vms"": 0.96875
     },
     {
-        ""pid"": 4,
         ""name"": ""System"",
-        ""cpu_percent"": 0.0,
         ""vms"": 0.0625
     },
     {
-        ""pid"": 0,
         ""name"": ""System Idle Process"",
-        ""cpu_percent"": 0.0,
         ""vms"": 0.05859375
     }
 ]
\ No newline at end of file"
OK;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";" [
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 21656,
+        ""vms"": 275.32421875
     },
     {
+        ""cpu_percent"": 0.5,
         ""name"": ""MsMpEng.exe"",
+        ""pid"": 8376,
+        ""vms"": 267.625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""EpicGamesLauncher.exe"",
+        ""pid"": 22640,
+        ""vms"": 219.23828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 4948,
+        ""vms"": 212.08203125
     },
     {
+        ""cpu_percent"": 0.3,
         ""name"": ""Code.exe"",
+        ""pid"": 18512,
+        ""vms"": 185.3359375
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""Code.exe"",
+        ""pid"": 20260,
+        ""vms"": 179.328125
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""Discord.exe"",
+        ""pid"": 20244,
+        ""vms"": 178.85546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""explorer.exe"",
+        ""pid"": 5104,
+        ""vms"": 164.09765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 19692,
+        ""vms"": 147.65625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""CefSharp.BrowserSubprocess.exe"",
+        ""pid"": 6188,
+        ""vms"": 132.11328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 20512,
+        ""vms"": 130.09765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 22704,
+        ""vms"": 128.95703125
     },
     {
+        ""cpu_percent"": 1.8,
         ""name"": ""dwm.exe"",
+        ""pid"": 2016,
+        ""vms"": 128.58984375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 21288,
+        ""vms"": 124.81640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Razer Synapse 3.exe"",
+        ""pid"": 15804,
+        ""vms"": 121.42578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Grammarly.Desktop.exe"",
+        ""pid"": 20532,
+        ""vms"": 120.8671875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""SearchHost.exe"",
+        ""pid"": 6996,
+        ""vms"": 119.69140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""QMxNetworkSync.exe"",
+        ""pid"": 16320,
+        ""vms"": 116.8046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 19252,
+        ""vms"": 115.8515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 14896,
+        ""vms"": 112.9296875
     },
     {
+        ""cpu_percent"": 1.7,
+        ""name"": ""StreamDeck.exe"",
+        ""pid"": 14952,
+        ""vms"": 112.27734375
     },
     {
+        ""cpu_percent"": 0.1,
         ""name"": ""Razer Synapse Service.exe"",
+        ""pid"": 13316,
+        ""vms"": 112.015625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RazerCortex.exe"",
+        ""pid"": 26152,
+        ""vms"": 107.6796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 18852,
+        ""vms"": 96.81640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 17552,
+        ""vms"": 92.46875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 28476,
+        ""vms"": 90.72265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 25296,
+        ""vms"": 84.51953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""SearchIndexer.exe"",
+        ""pid"": 11232,
+        ""vms"": 83.82421875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Razer Central.exe"",
+        ""pid"": 19612,
+        ""vms"": 81.84375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GameManagerService.exe"",
+        ""pid"": 8348,
+        ""vms"": 74.4296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GitHubDesktop.exe"",
+        ""pid"": 3836,
+        ""vms"": 74.16796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 3904,
+        ""vms"": 72.6328125
+    },
+    {
+        ""cpu_percent"": 0.2,
+        ""name"": ""wallpaper32.exe"",
+        ""pid"": 15948,
+        ""vms"": 70.9296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Discord.exe"",
+        ""pid"": 16932,
+        ""vms"": 68.84375
     },
     {
+        ""cpu_percent"": 0.1,
         ""name"": ""Spotify.exe"",
+        ""pid"": 23132,
+        ""vms"": 68.31640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""CefSharp.BrowserSubprocess.exe"",
+        ""pid"": 27032,
+        ""vms"": 67.54296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Battle.net.exe"",
+        ""pid"": 19460,
+        ""vms"": 67.234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Spotify.exe"",
+        ""pid"": 24620,
+        ""vms"": 62.9140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""powershell.exe"",
+        ""pid"": 2536,
+        ""vms"": 56.4375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 15548,
+        ""vms"": 55.4609375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 15420,
+        ""vms"": 54.39453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 24688,
+        ""vms"": 53.9765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NVIDIA Share.exe"",
+        ""pid"": 32324,
+        ""vms"": 50.47265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Spotify.exe"",
+        ""pid"": 23632,
+        ""vms"": 50.4609375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Agent.exe"",
+        ""pid"": 25824,
+        ""vms"": 50.05078125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""EpicWebHelper.exe"",
+        ""pid"": 11600,
+        ""vms"": 49.37890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""YourPhone.exe"",
+        ""pid"": 20956,
+        ""vms"": 48.96484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""StartMenuExperienceHost.exe"",
+        ""pid"": 7024,
+        ""vms"": 48.69921875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 1412,
+        ""vms"": 45.90234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RazerCentralService.exe"",
+        ""pid"": 8228,
+        ""vms"": 45.4375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GitHubDesktop.exe"",
+        ""pid"": 17956,
+        ""vms"": 42.9453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NVIDIA Share.exe"",
+        ""pid"": 12316,
+        ""vms"": 42.47265625
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""nvcontainer.exe"",
+        ""pid"": 12272,
+        ""vms"": 42.21484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Razer Synapse Service Process.exe"",
+        ""pid"": 7492,
+        ""vms"": 40.81640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Discord.exe"",
+        ""pid"": 16260,
+        ""vms"": 40.43359375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 24960,
+        ""vms"": 37.953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 20352,
+        ""vms"": 37.5546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 8048,
+        ""vms"": 37.5390625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NVDisplay.Container.exe"",
+        ""pid"": 3340,
+        ""vms"": 37.36328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GameBar.exe"",
+        ""pid"": 8408,
+        ""vms"": 36.26953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NVIDIA Web Helper.exe"",
+        ""pid"": 14340,
+        ""vms"": 35.78125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""CefSharp.BrowserSubprocess.exe"",
+        ""pid"": 23156,
+        ""vms"": 35.27734375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ShellExperienceHost.exe"",
+        ""pid"": 30104,
+        ""vms"": 34.63671875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GitHubDesktop.exe"",
+        ""pid"": 26008,
+        ""vms"": 34.37890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Battle.net.exe"",
+        ""pid"": 28644,
+        ""vms"": 32.5
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""LockApp.exe"",
+        ""pid"": 12836,
+        ""vms"": 32.3359375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 24104,
+        ""vms"": 31.38671875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""OfficeClickToRun.exe"",
+        ""pid"": 8512,
+        ""vms"": 30.625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 8124,
+        ""vms"": 30.546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""asus_framework.exe"",
+        ""pid"": 29464,
+        ""vms"": 30.23828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""asus_framework.exe"",
+        ""pid"": 31296,
+        ""vms"": 28.265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""asus_framework.exe"",
+        ""pid"": 4708,
+        ""vms"": 27.4140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 10852,
+        ""vms"": 27.265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 32524,
+        ""vms"": 27.12890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ArmouryCrate.UserSessionHelper.exe"",
+        ""pid"": 10500,
+        ""vms"": 27.10546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""AcPowerNotification.exe"",
+        ""pid"": 4616,
+        ""vms"": 26.796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""QtWebEngineProcess.exe"",
+        ""pid"": 17860,
+        ""vms"": 26.45703125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""asus_framework.exe"",
+        ""pid"": 31244,
+        ""vms"": 25.640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""com.barraider.shadowplay.exe"",
+        ""pid"": 17016,
+        ""vms"": 25.48046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""EpicWebHelper.exe"",
+        ""pid"": 4408,
+        ""vms"": 25.39453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 21296,
+        ""vms"": 24.6953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""com.barraider.spotify.exe"",
+        ""pid"": 17028,
+        ""vms"": 24.47265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""wireguard.exe"",
+        ""pid"": 9588,
+        ""vms"": 24.34765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 8148,
+        ""vms"": 24.234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NVIDIA Share.exe"",
+        ""pid"": 15348,
+        ""vms"": 23.578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 9024,
+        ""vms"": 22.44921875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 23208,
+        ""vms"": 22.39453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""QtWebEngineProcess.exe"",
+        ""pid"": 17092,
+        ""vms"": 22.19921875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 33436,
+        ""vms"": 21.91015625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""QtWebEngineProcess.exe"",
+        ""pid"": 16364,
+        ""vms"": 20.6953125
     },
     {
+        ""cpu_percent"": 8.5,
+        ""name"": ""WmiPrvSE.exe"",
+        ""pid"": 3336,
+        ""vms"": 19.578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 26112,
+        ""vms"": 19.36328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 7716,
+        ""vms"": 19.21875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 31284,
+        ""vms"": 19.13671875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Spotify.exe"",
+        ""pid"": 23816,
+        ""vms"": 18.5390625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 28480,
+        ""vms"": 18.484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""wireguard.exe"",
+        ""pid"": 8384,
+        ""vms"": 18.20703125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 5980,
+        ""vms"": 18.06640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""QtWebEngineProcess.exe"",
+        ""pid"": 17132,
+        ""vms"": 18.04296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""OneDrive.exe"",
+        ""pid"": 14380,
+        ""vms"": 17.69140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2908,
+        ""vms"": 17.5
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 6536,
+        ""vms"": 17.23046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Mobalytics Desktop.exe"",
+        ""pid"": 21624,
+        ""vms"": 16.4609375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Discord.exe"",
+        ""pid"": 17184,
+        ""vms"": 16.05078125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 20616,
+        ""vms"": 15.890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ArmourySwAgent.exe"",
+        ""pid"": 30156,
+        ""vms"": 15.21875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Registry"",
+        ""pid"": 204,
+        ""vms"": 14.96875
     },
     {
+        ""cpu_percent"": 0.3,
+        ""name"": ""svchost.exe"",
+        ""pid"": 3404,
+        ""vms"": 14.84375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""vgc.exe"",
+        ""pid"": 8444,
+        ""vms"": 13.79296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ArmouryCrate.Service.exe"",
+        ""pid"": 8164,
+        ""vms"": 13.31640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Discord.exe"",
+        ""pid"": 26088,
+        ""vms"": 12.98046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GitHubDesktop.exe"",
+        ""pid"": 22220,
+        ""vms"": 12.84765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 5520,
+        ""vms"": 12.76953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2512,
+        ""vms"": 12.6328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Discord.exe"",
+        ""pid"": 16772,
+        ""vms"": 12.203125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""nvcontainer.exe"",
+        ""pid"": 8276,
+        ""vms"": 12.11328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Spotify.exe"",
+        ""pid"": 23804,
+        ""vms"": 12.109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 19296,
+        ""vms"": 11.93359375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Spotify.exe"",
+        ""pid"": 23004,
+        ""vms"": 11.6796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""QtWebEngineProcess.exe"",
+        ""pid"": 15884,
+        ""vms"": 11.515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ROGLiveService.exe"",
+        ""pid"": 8332,
+        ""vms"": 10.828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Code.exe"",
+        ""pid"": 28828,
+        ""vms"": 10.7421875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Teams.exe"",
+        ""pid"": 22272,
+        ""vms"": 10.71484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msteams.exe"",
+        ""pid"": 20768,
+        ""vms"": 9.8828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 1604,
+        ""vms"": 9.875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""lsass.exe"",
+        ""pid"": 1496,
+        ""vms"": 9.8203125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 24984,
+        ""vms"": 9.59375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""SgrmBroker.exe"",
+        ""pid"": 4816,
+        ""vms"": 9.5078125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4376,
+        ""vms"": 9.234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 32220,
+        ""vms"": 9.21484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 13268,
+        ""vms"": 9.015625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""LightingService.exe"",
+        ""pid"": 8492,
+        ""vms"": 8.765625
     },
     {
+        ""cpu_percent"": 1.4,
+        ""name"": ""python3.10.exe"",
+        ""pid"": 12732,
+        ""vms"": 8.7109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 1736,
+        ""vms"": 8.69921875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4260,
+        ""vms"": 8.58984375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""nvcontainer.exe"",
+        ""pid"": 12200,
+        ""vms"": 8.1484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""fontdrvhost.exe"",
+        ""pid"": 1908,
+        ""vms"": 8.12109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""gamingservices.exe"",
+        ""pid"": 8864,
+        ""vms"": 8.0546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 6472,
+        ""vms"": 7.90625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 11660,
+        ""vms"": 7.6796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RzSDKService.exe"",
+        ""pid"": 8216,
+        ""vms"": 7.41796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 25020,
+        ""vms"": 7.28515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NVDisplay.Container.exe"",
+        ""pid"": 2856,
+        ""vms"": 7.17578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""spoolsv.exe"",
+        ""pid"": 7664,
+        ""vms"": 7.02734375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 4572,
+        ""vms"": 6.96484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""taskhostw.exe"",
+        ""pid"": 4440,
+        ""vms"": 6.953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 1524,
+        ""vms"": 6.90234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Battle.net.exe"",
+        ""pid"": 28852,
+        ""vms"": 6.875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 10824,
+        ""vms"": 6.86328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""services.exe"",
+        ""pid"": 1472,
+        ""vms"": 6.81640625
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""svchost.exe"",
+        ""pid"": 8052,
+        ""vms"": 6.78125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 24912,
+        ""vms"": 6.7734375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 1244,
+        ""vms"": 6.75390625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 2096,
+        ""vms"": 6.6484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""sihost.exe"",
+        ""pid"": 4208,
+        ""vms"": 6.62890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 7824,
+        ""vms"": 6.44140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 33160,
+        ""vms"": 6.3515625
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""PMRunner32.exe"",
+        ""pid"": 28472,
+        ""vms"": 6.29296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""browser_assistant.exe"",
+        ""pid"": 20668,
+        ""vms"": 6.28125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 7624,
+        ""vms"": 6.234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""atkexComSvc.exe"",
+        ""pid"": 3636,
+        ""vms"": 6.21484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 12520,
+        ""vms"": 5.921875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""dasHost.exe"",
+        ""pid"": 4336,
+        ""vms"": 5.84765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 7568,
+        ""vms"": 5.66796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 25820,
+        ""vms"": 5.5703125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""NisSrv.exe"",
+        ""pid"": 3956,
+        ""vms"": 5.56640625
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""PMRunner64.exe"",
+        ""pid"": 28040,
+        ""vms"": 5.53125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 14440,
+        ""vms"": 5.234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 10520,
+        ""vms"": 5.23046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 17080,
+        ""vms"": 5.22265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 17588,
+        ""vms"": 5.2109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 17036,
+        ""vms"": 5.203125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ArmourySocketServer.exe"",
+        ""pid"": 4580,
+        ""vms"": 5.1328125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 2644,
+        ""vms"": 5.03515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Rainmeter.exe"",
+        ""pid"": 24516,
+        ""vms"": 4.859375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 22232,
+        ""vms"": 4.83203125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2584,
+        ""vms"": 4.48828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 5900,
+        ""vms"": 4.3828125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 8140,
+        ""vms"": 4.3828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 11088,
+        ""vms"": 4.375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""dllhost.exe"",
+        ""pid"": 11364,
+        ""vms"": 4.328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""jucheck.exe"",
+        ""pid"": 3948,
+        ""vms"": 4.09765625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 8180,
+        ""vms"": 4.09375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 3612,
+        ""vms"": 4.06640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""SecurityHealthService.exe"",
+        ""pid"": 2728,
+        ""vms"": 3.98828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 5972,
+        ""vms"": 3.97265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ctfmon.exe"",
+        ""pid"": 12700,
+        ""vms"": 3.7578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""twitchstudiostreamdeck.exe"",
+        ""pid"": 17560,
+        ""vms"": 3.73046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4748,
+        ""vms"": 3.64453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 10188,
+        ""vms"": 3.640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""csrss.exe"",
+        ""pid"": 1420,
+        ""vms"": 3.62890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""jusched.exe"",
+        ""pid"": 25524,
+        ""vms"": 3.6171875
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""AsusCertService.exe"",
+        ""pid"": 2916,
+        ""vms"": 3.59375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2564,
+        ""vms"": 3.5546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
         ""pid"": 4632,
+        ""vms"": 3.46484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""browser_assistant.exe"",
+        ""pid"": 22292,
+        ""vms"": 3.44140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""FPSRunner32.exe"",
+        ""pid"": 28724,
+        ""vms"": 3.4375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ArmouryWebBrowserEdge.exe"",
+        ""pid"": 32164,
+        ""vms"": 3.41015625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 7232,
+        ""vms"": 3.40625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 11580,
+        ""vms"": 3.2734375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 31088,
+        ""vms"": 3.1875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""AppleMobileDeviceProcess.exe"",
+        ""pid"": 21940,
+        ""vms"": 3.140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""chrome.exe"",
+        ""pid"": 9688,
+        ""vms"": 3.1015625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""nvsphelper64.exe"",
+        ""pid"": 14444,
+        ""vms"": 3.06640625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 1788,
+        ""vms"": 3.0390625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RzSDKServer.exe"",
+        ""pid"": 8416,
+        ""vms"": 2.99609375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Aac3572MbHal_x86.exe"",
+        ""pid"": 32060,
+        ""vms"": 2.9921875
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 1268,
+        ""vms"": 2.95703125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4684,
+        ""vms"": 2.9453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 32548,
+        ""vms"": 2.89453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RzUpdateEngineService.exe"",
+        ""pid"": 8288,
+        ""vms"": 2.88671875
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 5888,
+        ""vms"": 2.86328125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 7408,
+        ""vms"": 2.8515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 8132,
+        ""vms"": 2.8359375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""ArmouryAIOFanServer.exe"",
+        ""pid"": 4516,
+        ""vms"": 2.828125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 19880,
+        ""vms"": 2.796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4672,
+        ""vms"": 2.78125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 4180,
+        ""vms"": 2.765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""AsusFanControlService.exe"",
+        ""pid"": 5124,
+        ""vms"": 2.7421875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""FPSRunner64.exe"",
+        ""pid"": 28764,
+        ""vms"": 2.7265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 12448,
+        ""vms"": 2.66796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 6148,
+        ""vms"": 2.64453125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 3796,
+        ""vms"": 2.62109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GameBarFTServer.exe"",
+        ""pid"": 24780,
+        ""vms"": 2.5546875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""WmiPrvSE.exe"",
+        ""pid"": 3568,
+        ""vms"": 2.51953125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 2404,
+        ""vms"": 2.51171875
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 5780,
+        ""vms"": 2.484375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""winlogon.exe"",
+        ""pid"": 1852,
+        ""vms"": 2.39453125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""csrss.exe"",
+        ""pid"": 1308,
+        ""vms"": 2.38671875
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 3096,
+        ""vms"": 2.31640625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 6380,
+        ""vms"": 2.28515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""conhost.exe"",
+        ""pid"": 3716,
+        ""vms"": 2.28125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 22080,
+        ""vms"": 2.27734375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""openvpn-gui.exe"",
+        ""pid"": 17544,
+        ""vms"": 2.25390625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 29360,
+        ""vms"": 2.22265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""rundll32.exe"",
+        ""pid"": 5204,
+        ""vms"": 2.21875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2068,
+        ""vms"": 2.203125
     },
     {
+        ""cpu_percent"": 0.1,
+        ""name"": ""Aac3572MbHal_x86.exe"",
+        ""pid"": 4660,
+        ""vms"": 2.17578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 3280,
+        ""vms"": 2.16796875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""AggregatorHost.exe"",
+        ""pid"": 10020,
+        ""vms"": 2.13671875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""fontdrvhost.exe"",
+        ""pid"": 1632,
+        ""vms"": 2.1328125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 2292,
+        ""vms"": 2.12890625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RuntimeBroker.exe"",
+        ""pid"": 23792,
+        ""vms"": 2.12109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RzChromaStreamServer.exe"",
+        ""pid"": 8356,
+        ""vms"": 2.0859375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 1040,
+        ""vms"": 2.07421875
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""MpCopyAccelerator.exe"",
+        ""pid"": 21152,
+        ""vms"": 2.0234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 7952,
+        ""vms"": 1.9609375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 19800,
+        ""vms"": 1.94140625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""msedgewebview2.exe"",
+        ""pid"": 24452,
+        ""vms"": 1.9375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 3276,
+        ""vms"": 1.91796875
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 3112,
+        ""vms"": 1.8828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""sqlwriter.exe"",
+        ""pid"": 8400,
+        ""vms"": 1.87109375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 3196,
+        ""vms"": 1.83984375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""SecurityHealthSystray.exe"",
+        ""pid"": 14928,
+        ""vms"": 1.828125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 3292,
+        ""vms"": 1.8125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 4200,
+        ""vms"": 1.8046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4796,
+        ""vms"": 1.79296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""vgtray.exe"",
+        ""pid"": 14560,
+        ""vms"": 1.77734375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 7416,
+        ""vms"": 1.765625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GoogleCrashHandler.exe"",
+        ""pid"": 14912,
+        ""vms"": 1.70703125
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 8708,
+        ""vms"": 1.6953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""GoogleCrashHandler64.exe"",
+        ""pid"": 15044,
+        ""vms"": 1.67578125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""armsvc.exe"",
+        ""pid"": 8188,
+        ""vms"": 1.62890625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 8156,
+        ""vms"": 1.625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 19840,
+        ""vms"": 1.55859375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""wininit.exe"",
+        ""pid"": 1400,
+        ""vms"": 1.54296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""WUDFHost.exe"",
+        ""pid"": 1672,
+        ""vms"": 1.5390625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""extensionCardHal_x86.exe"",
+        ""pid"": 13812,
+        ""vms"": 1.5390625
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""svchost.exe"",
+        ""pid"": 19796,
+        ""vms"": 1.52734375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""unsecapp.exe"",
+        ""pid"": 4080,
+        ""vms"": 1.5234375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2572,
+        ""vms"": 1.515625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""AacKingstonDramHal_x86.exe"",
+        ""pid"": 13768,
+        ""vms"": 1.50390625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""AacKingstonDramHal_x64.exe"",
+        ""pid"": 14660,
+        ""vms"": 1.5
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 22928,
+        ""vms"": 1.5
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 4320,
+        ""vms"": 1.48828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""rundll32.exe"",
+        ""pid"": 10948,
+        ""vms"": 1.47265625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""unsecapp.exe"",
+        ""pid"": 13620,
+        ""vms"": 1.46875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 6872,
+        ""vms"": 1.4375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""MemCompression"",
+        ""pid"": 3216,
+        ""vms"": 1.375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""Aac3572DramHal_x86.exe"",
+        ""pid"": 10696,
+        ""vms"": 1.375
     },
     {
         ""cpu_percent"": 0.0,
         ""name"": ""unsecapp.exe"",
+        ""pid"": 22772,
+        ""vms"": 1.3671875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 3104,
+        ""vms"": 1.29296875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 8172,
+        ""vms"": 1.2734375
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 2956,
+        ""vms"": 1.26953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 18912,
         ""vms"": 1.23828125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""gamingservicesnet.exe"",
+        ""pid"": 8872,
+        ""vms"": 1.1953125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""dasHost.exe"",
+        ""pid"": 5688,
+        ""vms"": 1.16015625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 33360,
+        ""vms"": 1.15625
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""openvpnserv.exe"",
+        ""pid"": 8236,
         ""vms"": 1.1328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""smss.exe"",
+        ""pid"": 756,
+        ""vms"": 1.11328125
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""RzKLService.exe"",
+        ""pid"": 8336,
+        ""vms"": 1.046875
     },
     {
         ""cpu_percent"": 0.0,
+        ""name"": ""svchost.exe"",
+        ""pid"": 11696,
+        ""vms"": 0.97265625
     },
     {
+        ""cpu_percent"": 0.4,
         ""name"": ""System"",
+        ""pid"": 4,
         ""vms"": 0.0625
     },
     {
+        ""cpu_percent"": 1583.3,
         ""name"": ""System Idle Process"",
+        ""pid"": 0,
         ""vms"": 0.05859375
     }
 ]
\ No newline at end of file"
KO;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";\ No newline at end of file
OK;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";"+{
+    ""memTotal"": 16178.4609375,
+    ""memFree"": 8434.1484375,
+    ""memUsed"": 7744.3125,
+    ""cpu"": 0.01,
+    ""diskTotal"": 953189.3515625,
+    ""diskUsed"": 421016.609375,
+    ""diskFree"": 532172.7421875,
+    ""netSent"": 13.381887435913086,
+    ""netRecv"": 165.38859844207764
+}
\ No newline at end of file"
KO;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";" 
 #!/usr/bin/python3
 
 def getLinuxStatus():
     statusData = {}
 
-    # Computer upload and download speed
-    with open('/proc/net/dev', 'r') as f:
-        for line in f:
-            if 'eth0' in line:
-                download = line.split()[1]
-                upload = line.split()[9]
-
-                download = int(download) / 1024 / 1024
-                upload = int(upload) / 1024 / 1024
 
-                statusData['download'] = download
-                statusData['upload'] = upload
-    
-    # Computer uptime
-    with open('/proc/uptime', 'r') as f:
-        uptime = f.readline().split()[0]
 
-        days = int(uptime) // 86400
-        hours = int(uptime) // 3600 % 24
-        minutes = int(uptime) // 60 % 60
 
-        statusData['uptime'] = days, hours, minutes
-    
-    # Computer memory usage
-    with open('/proc/meminfo', 'r') as f:
-        for line in f:
-            if 'MemTotal' in line:
-                memTotal = line.split()[1]
-            if 'MemFree' in line:
-                memFree = line.split()[1]
-            if 'Buffers' in line:
-                buffers = line.split()[1]
-            if 'Cached' in line:
-                cached = line.split()[1]
 
-        memTotal = int(memTotal) / 1024
-        memFree = int(memFree) / 1024
-        buffers = int(buffers) / 1024
-        cached = int(cached) / 1024
 
-        statusData['memTotal'] = memTotal
-        statusData['memFree'] = memFree
-        statusData['buffers'] = buffers
-        statusData['cached'] = cached
-    
-    # Computer CPU usage
-    with open('/proc/stat', 'r') as f:
-        for line in f:
-            if 'cpu' in line:
-                cpu = line.split()[1]
 
-        cpu = int(cpu) / 100
 
-        statusData['cpu'] = cpu
 
     return statusData"
OK;26;IvoryCoding;SEAM-Project;743eba43980d9263fbe67d81de82ba6ebbc3215c;"rewrite

rewrote the status module which now gets memory, cpu, and disk space. Added a timer to repeat the cycle every 60 seconds.";" 
 #!/usr/bin/python3
 
+import psutil
+
 def getLinuxStatus():
     statusData = {}
 
+    # Memory usage
+    memTotal = psutil.virtual_memory().total / 1024 / 1024
+    memUsed = psutil.virtual_memory().used / 1024 / 1024
+    memFree = psutil.virtual_memory().free / 1024 / 1024
 
+    statusData['memTotal'] = memTotal
+    statusData['memFree'] = memFree
+    statusData['memUsed'] = memUsed
 
+    # CPU usage
+    cpu = psutil.cpu_percent()
+    cpu = int(cpu) / 100
 
+    statusData['cpu'] = cpu
 
+    # Disk usage
+    disk = psutil.disk_usage('/')
+    diskTotal = disk.total / 1024 / 1024
+    diskUsed = disk.used / 1024 / 1024
+    diskFree = disk.free / 1024 / 1024
 
+    statusData['diskTotal'] = diskTotal
+    statusData['diskUsed'] = diskUsed
+    statusData['diskFree'] = diskFree
 
+    # Network usage
+    net = psutil.net_io_counters()
+    netSent = net.bytes_sent / 1024 / 1024
+    netRecv = net.bytes_recv / 1024 / 1024
 
+    statusData['netSent'] = netSent
+    statusData['netRecv'] = netRecv
 
     return statusData"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
+from .netserialize import *
 from .dockernet import *
 from .dockershm import *
+from .dockernetserialize import *"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+import os
+
+def get_data():
+   testfile = ""airbnb.csv""
+   datadir = ""/data/""
+   currdir = os.path.dirname(os.path.abspath(__file__)) 
+   f = open(currdir + datadir + testfile, ""r"")
+   data = f.read()
+   return data
+
+if __name__ == ""__main__"":
+    get_data()"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
-print(f""recv reading from {__file__}..."")
-print(f""recv data: {contents}"")
 f.close()"
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
+print(f""recv: read from {__file__}"")
 f.close()"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
-print(f""send writing to shared mem from {__file__}..."")
-f.write(""a very simple message for recv"")
 f.close()"
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;"+from data import get_data
+
 f = open(""test.txt"", ""w+"")
+print(f""send: writing to shared mem from {__file__}..."")
+f.write(get_data())
 f.close()"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
+def copy_data_to_volume(volume): 
+    data_script = ""data.py""
+    curr_dir = os.path.dirname(os.path.abspath(__file__))
+    data_dir = curr_dir + ""/data""
+    dst_dir = volume[""Mountpoint""]+f""/data""
+    if os.path.exists(dst_dir):
+        shutil.rmtree(dst_dir)
+    shutil.copytree(data_dir, dst_dir)
+    shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
+
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
@@ -95,6 +105,7 @@ def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
+    copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
-def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
-    data = sendsock.recv(1024)
-    print(f""send {data}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
-    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
+from data import get_data
+
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
+def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
@@ -22,9 +24,12 @@ def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
+
     sendsock.sendall(b""my simple message"")
+    
+    # wait for confirmation
+    recvdata = sendsock.recv(1024)
+    print(f""send: {recvdata}"")
     
     sendsock.close()
 
@@ -69,7 +74,7 @@ def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
+    sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
-def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
-    print(""send writing to shared mem..."")
-    f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
-    print(f""recv data: {contents}"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
-    sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;26;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
+from data import get_data
+
+def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
+    print(""send: writing to shared mem..."")
+    f.write(data)
     f.close()
 
     # wait until file is written
@@ -26,7 +28,7 @@ def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
+    print(f""recv: received data"")
     f.close()
 
 def main():
@@ -39,7 +41,8 @@ def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
+    data = get_data()
+    sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;27;loongtop;Django_Mall;17f752abf00ebcd92b2b1afc7fa0c19ffe77dc12;using yield to decrease the usage of memory in file head_data_list.py;" 
 
 def get_head(data_pack):
-    head_list = []
     display_list = data_pack.display
 
     if display_list:
@@ -15,38 +14,35 @@ def get_head(data_pack):
                 verbose_name = key_or_func(param, obj=None, is_header=True)
             else:
                 verbose_name = data_pack.model_class._meta.get_field(key_or_func).verbose_name
-            head_list.append(verbose_name)
     else:
-        head_list.append(data_pack._model_class._meta.model_name)
-    return head_list
 
 
 def get_data(data_pack):
-    body_list = []
-    display_list = data_pack.display
     for row in data_pack.data_list:
         tr_list = []
-        if display_list:
-            for key_or_func in display_list:
-                if isinstance(key_or_func, FunctionType):
-                    name = key_or_func.__name__
-                    url_name = data_pack.get_app_model_name(name)
-                    param = {'request': data_pack.request, 'namespace': data_pack.name_dict['namespace'], 'url_name': url_name}
-                    tr_list.append(key_or_func(param, row, is_header=False))
-                else:
-                    tr_list.append(getattr(row, key_or_func))  # obj.gender
-        else:
-            tr_list.append(row)
-        body_list.append(tr_list)
-
-    return body_list
 
 
 @register.inclusion_tag('crud/head_data_list.html')
 def head_data_list(data_pack):
 
-    head_list = get_head(data_pack)
-    data_list = get_data(data_pack)
-
-    return {'head_list': head_list, 'data_list': data_list}
 "
OK;27;loongtop;Django_Mall;17f752abf00ebcd92b2b1afc7fa0c19ffe77dc12;using yield to decrease the usage of memory in file head_data_list.py;" 
 
 def get_head(data_pack):
     display_list = data_pack.display
 
     if display_list:
@@ -15,38 +14,35 @@ def get_head(data_pack):
                 verbose_name = key_or_func(param, obj=None, is_header=True)
             else:
                 verbose_name = data_pack.model_class._meta.get_field(key_or_func).verbose_name
+            yield verbose_name
     else:
+        yield data_pack.model_class._meta.model_name
 
 
 def get_data(data_pack):
     for row in data_pack.data_list:
+        if not data_pack.display:
+            yield row
+            continue
+
         tr_list = []
+        for key_or_func in data_pack.display:
+            if isinstance(key_or_func, FunctionType):
+                name = key_or_func.__name__
+                url_name = data_pack.get_app_model_name(name)
+                param = {'request': data_pack.request,
+                         'namespace': data_pack.name_dict['namespace'],
+                         'url_name': url_name}
+                val = key_or_func(param, row, is_header=False)
+            else:
+                val = getattr(row, key_or_func)
+
+            tr_list.append(val)
+        yield tr_list
 
 
 @register.inclusion_tag('crud/head_data_list.html')
 def head_data_list(data_pack):
 
+    return {'head_list': get_head(data_pack), 'data_list': get_data(data_pack)}
 "
KO;28;Redninya;EXAM;da020b2ed76633822159002b48a2cc38a291622a;Create memory game.py;\ No newline at end of file
OK;28;Redninya;EXAM;da020b2ed76633822159002b48a2cc38a291622a;Create memory game.py;"+from multiprocessing.connection import answer_challenge
+from tkinter import*
+from tkinter import messagebox
+import random
+
+
+myObj = Tk()
+myObj.title(""Memory game"")
+count = 0
+correctAnsw = 0
+answ = list()
+answ_dict = dict()
+numList = [1,1,2,2,3,3,4,4,5,5,6,6]
+random.shuffle(numList)
+
+
+
+def newGame():
+    global correctAnsw, count, answ_dict, answ
+    count = 0
+    answ = []
+    answ_dict = {}
+    random.shuffle(numList)
+
+    for key in [btn0, btn1, btn2, btn3, btn4, btn5, btn6, btn7, btn8, btn9, btn10, btn11]:
+      key['state'] = NORMAL
+      key['bg'] = '#12CA00'
+      key['text'] = """"
+    
+    return None
+
+def about():
+    newWin = Toplevel()
+    newWin.title(""Par autoru"")
+    newWin.geometry(""200x200"")
+    lblAbout = Label(newWin, text = ""Dairis Mivreniks 11B"")
+    lblAbout.grid(row=0, column=0)
+    return None
+
+sljapa = Menu(myObj)
+myObj.config(menu = sljapa)
+sljapa.add_command(label = ""Jauna spēle"", command=newGame)
+sljapa.add_command(label = ""Par autoru"", command=about)
+sljapa.add_command(label = ""Iziet"", command=myObj.quit)
+
+def btnClick(btn, number):
+    global correctAnsw, count, answ_dict, answ
+    if btn['bg'] == '#12CA00' and count < 2:
+        btn['bg'] = '#A0FF96'
+        btn['text'] = numList[number]
+        answ.append(number)
+        answ_dict[btn] = numList[number]
+        count += 1
+
+    if len(answ) == 2:
+        if numList[answ[0]] == numList[answ[1]]:
+            for key in answ_dict:
+                key['state'] = DISABLED
+            correctAnsw += 1
+            if correctAnsw == 6:
+                messagebox.showinfo(""Memory Game"", ""YOU WON!"")
+
+        else:
+            messagebox.showinfo(""Memory Game"", ""Nope"")
+            for key in answ_dict:
+                key['bg'] = '#12CA00'
+                key['text'] = """"
+        
+        count = 0
+        answ = []
+        answ_dict = {}
+    return None
+
+btn0 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn0, 0))
+btn1 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn1, 1))
+btn2 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn2, 2))
+btn3 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn3, 3))
+btn4 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn4, 4))
+btn5 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn5, 5))
+btn6 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn6, 6))
+btn7 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn7, 7))
+btn8 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn8, 8))
+btn9 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn9, 9))
+btn10 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn10, 10))
+btn11 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn11, 11))  
+
+btn0.grid(row=0, column=0)
+btn1.grid(row=0, column=1)
+btn2.grid(row=0, column=2)
+
+btn3.grid(row=1, column=0)
+btn4.grid(row=1, column=1)
+btn5.grid(row=1, column=2)
+
+btn6.grid(row=2, column=0)
+btn7.grid(row=2, column=1)
+btn8.grid(row=2, column=2)
+
+btn9.grid(row=3, column=0)
+btn10.grid(row=3, column=1)
+btn11.grid(row=3, column=2)
+
+
+
+myObj.mainloop()
\ No newline at end of file"
KO;32;Redninya;EXAM;da020b2ed76633822159002b48a2cc38a291622a;Create memory game.py;\ No newline at end of file
OK;32;Redninya;EXAM;da020b2ed76633822159002b48a2cc38a291622a;Create memory game.py;"+from multiprocessing.connection import answer_challenge
+from tkinter import*
+from tkinter import messagebox
+import random
+
+
+myObj = Tk()
+myObj.title(""Memory game"")
+count = 0
+correctAnsw = 0
+answ = list()
+answ_dict = dict()
+numList = [1,1,2,2,3,3,4,4,5,5,6,6]
+random.shuffle(numList)
+
+
+
+def newGame():
+    global correctAnsw, count, answ_dict, answ
+    count = 0
+    answ = []
+    answ_dict = {}
+    random.shuffle(numList)
+
+    for key in [btn0, btn1, btn2, btn3, btn4, btn5, btn6, btn7, btn8, btn9, btn10, btn11]:
+      key['state'] = NORMAL
+      key['bg'] = '#12CA00'
+      key['text'] = """"
+    
+    return None
+
+def about():
+    newWin = Toplevel()
+    newWin.title(""Par autoru"")
+    newWin.geometry(""200x200"")
+    lblAbout = Label(newWin, text = ""Dairis Mivreniks 11B"")
+    lblAbout.grid(row=0, column=0)
+    return None
+
+sljapa = Menu(myObj)
+myObj.config(menu = sljapa)
+sljapa.add_command(label = ""Jauna spēle"", command=newGame)
+sljapa.add_command(label = ""Par autoru"", command=about)
+sljapa.add_command(label = ""Iziet"", command=myObj.quit)
+
+def btnClick(btn, number):
+    global correctAnsw, count, answ_dict, answ
+    if btn['bg'] == '#12CA00' and count < 2:
+        btn['bg'] = '#A0FF96'
+        btn['text'] = numList[number]
+        answ.append(number)
+        answ_dict[btn] = numList[number]
+        count += 1
+
+    if len(answ) == 2:
+        if numList[answ[0]] == numList[answ[1]]:
+            for key in answ_dict:
+                key['state'] = DISABLED
+            correctAnsw += 1
+            if correctAnsw == 6:
+                messagebox.showinfo(""Memory Game"", ""YOU WON!"")
+
+        else:
+            messagebox.showinfo(""Memory Game"", ""Nope"")
+            for key in answ_dict:
+                key['bg'] = '#12CA00'
+                key['text'] = """"
+        
+        count = 0
+        answ = []
+        answ_dict = {}
+    return None
+
+btn0 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn0, 0))
+btn1 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn1, 1))
+btn2 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn2, 2))
+btn3 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn3, 3))
+btn4 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn4, 4))
+btn5 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn5, 5))
+btn6 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn6, 6))
+btn7 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn7, 7))
+btn8 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn8, 8))
+btn9 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn9, 9))
+btn10 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn10, 10))
+btn11 = Button(myObj, bg = ""#12CA00"", width=10, height=5, font=(""Helvetiva"", 13), command = lambda:btnClick(btn11, 11))  
+
+btn0.grid(row=0, column=0)
+btn1.grid(row=0, column=1)
+btn2.grid(row=0, column=2)
+
+btn3.grid(row=1, column=0)
+btn4.grid(row=1, column=1)
+btn5.grid(row=1, column=2)
+
+btn6.grid(row=2, column=0)
+btn7.grid(row=2, column=1)
+btn8.grid(row=2, column=2)
+
+btn9.grid(row=3, column=0)
+btn10.grid(row=3, column=1)
+btn11.grid(row=3, column=2)
+
+
+
+myObj.mainloop()
\ No newline at end of file"
KO;34;loongtop;Django_Mall;17f752abf00ebcd92b2b1afc7fa0c19ffe77dc12;using yield to decrease the usage of memory in file head_data_list.py;" 
 
 def get_head(data_pack):
-    head_list = []
     display_list = data_pack.display
 
     if display_list:
@@ -15,38 +14,35 @@ def get_head(data_pack):
                 verbose_name = key_or_func(param, obj=None, is_header=True)
             else:
                 verbose_name = data_pack.model_class._meta.get_field(key_or_func).verbose_name
-            head_list.append(verbose_name)
     else:
-        head_list.append(data_pack._model_class._meta.model_name)
-    return head_list
 
 
 def get_data(data_pack):
-    body_list = []
-    display_list = data_pack.display
     for row in data_pack.data_list:
         tr_list = []
-        if display_list:
-            for key_or_func in display_list:
-                if isinstance(key_or_func, FunctionType):
-                    name = key_or_func.__name__
-                    url_name = data_pack.get_app_model_name(name)
-                    param = {'request': data_pack.request, 'namespace': data_pack.name_dict['namespace'], 'url_name': url_name}
-                    tr_list.append(key_or_func(param, row, is_header=False))
-                else:
-                    tr_list.append(getattr(row, key_or_func))  # obj.gender
-        else:
-            tr_list.append(row)
-        body_list.append(tr_list)
-
-    return body_list
 
 
 @register.inclusion_tag('crud/head_data_list.html')
 def head_data_list(data_pack):
 
-    head_list = get_head(data_pack)
-    data_list = get_data(data_pack)
-
-    return {'head_list': head_list, 'data_list': data_list}
 "
OK;34;loongtop;Django_Mall;17f752abf00ebcd92b2b1afc7fa0c19ffe77dc12;using yield to decrease the usage of memory in file head_data_list.py;" 
 
 def get_head(data_pack):
     display_list = data_pack.display
 
     if display_list:
@@ -15,38 +14,35 @@ def get_head(data_pack):
                 verbose_name = key_or_func(param, obj=None, is_header=True)
             else:
                 verbose_name = data_pack.model_class._meta.get_field(key_or_func).verbose_name
+            yield verbose_name
     else:
+        yield data_pack.model_class._meta.model_name
 
 
 def get_data(data_pack):
     for row in data_pack.data_list:
+        if not data_pack.display:
+            yield row
+            continue
+
         tr_list = []
+        for key_or_func in data_pack.display:
+            if isinstance(key_or_func, FunctionType):
+                name = key_or_func.__name__
+                url_name = data_pack.get_app_model_name(name)
+                param = {'request': data_pack.request,
+                         'namespace': data_pack.name_dict['namespace'],
+                         'url_name': url_name}
+                val = key_or_func(param, row, is_header=False)
+            else:
+                val = getattr(row, key_or_func)
+
+            tr_list.append(val)
+        yield tr_list
 
 
 @register.inclusion_tag('crud/head_data_list.html')
 def head_data_list(data_pack):
 
+    return {'head_list': get_head(data_pack), 'data_list': get_data(data_pack)}
 "
KO;35;salustiana;spot;88c52b8c01d4b5387fc0817824f5d67faf86e0df;using memory cache now;" 
 import spotipy
 from spotipy.oauth2 import SpotifyOAuth
 
 class Manager:
     def __init__(
@@ -30,6 +31,7 @@ def __init__(
                 client_secret=client_secret,
                 redirect_uri=""http://127.0.0.1:5907/callback"",
                 scope=""ugc-image-upload playlist-modify-public"",
             ),
             retries=5,
         )"
OK;35;salustiana;spot;88c52b8c01d4b5387fc0817824f5d67faf86e0df;using memory cache now;" 
 import spotipy
 from spotipy.oauth2 import SpotifyOAuth
+from spotipy.cache_handler import MemoryCacheHandler
 
 class Manager:
     def __init__(
@@ -30,6 +31,7 @@ def __init__(
                 client_secret=client_secret,
                 redirect_uri=""http://127.0.0.1:5907/callback"",
                 scope=""ugc-image-upload playlist-modify-public"",
+                cache_handler=MemoryCacheHandler(),
             ),
             retries=5,
         )"
KO;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
OK;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
+arrMatOperands = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
KO;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
-# Avance: Se arreglaron prioridades, ejecucion de maquina virtual para estatutos lineales. 
\ No newline at end of file
\ No newline at end of file"
OK;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
\ No newline at end of file
+# Avance: Ejecucion de estatutos condicionales y generacion de codigo de arreglos/tipos estructurados (falta que haga operaciones con arreglos)
\ No newline at end of file"
KO;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"-from cuadruplos import Quadruples
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
@@ -50,16 +50,15 @@ def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
-    #Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
-        # quad.print()
         newIndex = executeInstruction(quad)
-        if quad.operator != ""+ADD"":
-            if newIndex:
-                index = newIndex
-            else:
-                index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
@@ -106,6 +105,12 @@ def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
@@ -210,32 +215,8 @@ def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
-        if lOp == 12:
-            localMem.insertInt(getValueFromAddress(getValueFromAddress(quad.left_operand)), getValueFromAddress(quad.result))
-        elif lOp == 11:
-            localMem.insertChar(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 10:
-            localMem.insertFloat(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 9:
-            localMem.insertInt(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 8:
-            tempMem.insertChar(tempMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 7:
-            tempMem.insertFloat(tempMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 6:
-            tempMem.insertInt(tempMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 5:
-            tempMem.insertChar(localMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 4:
-            tempMem.insertFloat(localMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 3:
-            tempMem.insertInt(localMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 2:
-            tempMem.insertChar(globalMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 1:
-            tempMem.insertFloat(globalMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 0:
-            tempMem.insertInt(globalMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
         
 def add(quad):
     res_address = quad.result // 1000
@@ -255,7 +236,7 @@ def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
-        pointerMemStack.append(lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
@@ -513,4 +494,13 @@ def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
-        localMem.adjustCharArrSize(quad.result)
\ No newline at end of file
\ No newline at end of file"
OK;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"+from cuadruplos import Quadruples, Quadruple
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
@@ -50,16 +50,15 @@ def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
+    Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
+        #quad.print()
         newIndex = executeInstruction(quad)
+        if newIndex:
+            index = newIndex
+        else:
+            index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
@@ -106,6 +105,12 @@ def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
+    elif quad.operator == ""ARR="":
+        return arrAssign(quad)
+    elif quad.operator == ""ARR+"":
+        return arrAdd(quad)
+    elif quad.operator == ""ARR-"":
+        return arrSubtract(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
@@ -210,32 +215,8 @@ def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
+        add_type = getValueFromAddress(quad.result)
+        assign(Quadruple(quad.operator, quad.left_operand, ""_"", add_type))
         
 def add(quad):
     res_address = quad.result // 1000
@@ -255,7 +236,7 @@ def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
+        pointerMemStack.insert(quad.result % 1000, lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
@@ -513,4 +494,13 @@ def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
\ No newline at end of file
+        localMem.adjustCharArrSize(quad.result)
+
+def arrAssign(quad):
+    pass
+
+def arrAdd(quad):
+    pass
+
+def arrSubtract(quad):
+    pass 
\ No newline at end of file"
KO;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.ints) <= realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.floats) <= realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.chars) <= realSup:
             self.chars.append("""")"
OK;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.ints) < realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.floats) < realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.chars) < realSup:
             self.chars.append("""")"
KO;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
@@ -76,7 +77,13 @@ def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
-	if t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
@@ -102,6 +109,7 @@ def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
@@ -218,17 +226,23 @@ def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
-	if t[1] in variableTable[currentScope]:
-		address = variableTable[currentScope][t[1]][""address""]
-		temp_quad = Quadruple(""="", cstAddress, '_', address)
-		Quadruples.push_quad(temp_quad)
-	#Checar si el id existe en global scope y asignar su valor
-	elif t[1] in variableTable[""global""]:
-		address = variableTable[""global""][t[1]][""address""]
-		temp_quad = Quadruple(""="", t[3], '_', address)
-		Quadruples.push_quad(temp_quad)
 	else:
-		Error.undefined_variable(t[1], t.lexer.lineno)
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
@@ -613,6 +627,65 @@ def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
@@ -625,6 +698,12 @@ def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
@@ -766,6 +845,9 @@ def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
@@ -831,6 +913,9 @@ def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
@@ -874,10 +959,12 @@ def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
 
 def p_addTypeId(t):
 	'addTypeId : '
-	# Push types to types stack
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
@@ -890,6 +977,7 @@ def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
OK;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
+	# arrMatOperands.print()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
@@ -76,7 +77,13 @@ def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
+	if arrMatOperands.size() > 0:
+		types.pop()
+		assign = arrMatOperands.pop()
+		address = arrMatOperands.pop()
+		temp_quad = Quadruple(""ARR="", assign, ""_"", address)
+		Quadruples.push_quad(temp_quad)
+	elif t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
@@ -102,6 +109,7 @@ def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
+			Quadruples.push_quad(temp_quad)
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
@@ -218,17 +226,23 @@ def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
+	if ""rows"" not in variableTable[currentScope][t[1]]:
+		#Checar si el id existe en currentScope y asignar su valor
+		if t[1] in variableTable[currentScope]:
+			address = variableTable[currentScope][t[1]][""address""]
+			temp_quad = Quadruple(""="", cstAddress, '_', address)
+			Quadruples.push_quad(temp_quad)
+		#Checar si el id existe en global scope y asignar su valor
+		elif t[1] in variableTable[""global""]:
+			address = variableTable[""global""][t[1]][""address""]
+			temp_quad = Quadruple(""="", t[3], '_', address)
+			Quadruples.push_quad(temp_quad)
+		else:
+			Error.undefined_variable(t[1], t.lexer.lineno)
 	else:
+		print(""Error: invalid assignment to non-atomic variable in line %d."" % (t.lexer.lineno))
+		exit(0)
+		# Actualizar con clase error
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
@@ -613,6 +627,65 @@ def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
+			# Checar y validar operandos y tamanos del arreglo.
+			if arrMatOperands.size() > 1:
+				rId = arrMatOperands.pop()
+				lId = arrMatOperands.pop()
+				# rDimRow = 0
+				# rDimCol = 0
+				# lDimRow = 0
+				# lDimCol = 0
+				# if rOp >= 0 and rOp < 3000:
+				# 	rOpAdd = variableTable[""global""][rId][""address""]
+				# 	if ""rows"" in variableTable[""global""][rId]:
+				# 		rDimRow = variableTable[""global""][rId][""rows""]
+				# 	if ""cols"" in variableTable[""global""][rId]:
+				# 		rDimCol = variableTable[""global""][rId][""cols""]
+				# elif rOp >= 3000 and rOp < 6000:
+				# 	rOpAdd = variableTable[currentScope][rId][""address""]
+				# 	if ""rows"" in variableTable[currentScope][rId]:
+				# 		rDimRow = variableTable[currentScope][rId][""rows""]
+				# 	if ""cols"" in variableTable[currentScope][rId]:
+				# 		rDimCol = variableTable[currentScope][rId][""cols""]
+				# if lOp >= 0 and lOp < 3000:
+				# 	lOpAdd = variableTable[""global""][lId][""address""]
+				# 	if ""rows"" in variableTable[""global""][lId]:
+				# 		lDimRow = variableTable[""global""][lId][""rows""]
+				# 	if ""cols"" in variableTable[""global""][lId]:
+				# 		lDimCol = variableTable[""global""][lId][""cols""]
+				# elif lOp >= 3000 and lOp < 6000:
+				# 	lOpAdd = variableTable[currentScope][lId][""address""]
+				# 	if ""rows"" in variableTable[currentScope][lId]:
+				# 		lDimRow = variableTable[currentScope][lId][""rows""]
+				# 	if ""cols"" in variableTable[currentScope][lId]:
+				# 		lDimCol = variableTable[currentScope][lId][""cols""]
+				# Validate equal dimensions
+				if ""cols"" not in lId and ""cols"" not in rId:
+					lId[""cols""] = 0
+					rId[""cols""] = 0
+				if lId[""rows""] == rId[""rows""] and lId[""cols""] == rId[""cols""]:
+					if oper == ""+"":
+						oper = ""ARR+""
+					else:
+						oper = ""ARR-""
+					lOp = {
+						""address"": lId[""address""],
+						""rows"": lId[""rows""],
+						""cols"": lId[""cols""]
+					}
+					rOp = {
+						""address"": rId[""address""],
+						""rows"": rId[""rows""],
+						""cols"": rId[""cols""]
+					}
+				else:
+					print(""Error: operation between variables with dimensions that don't match in line %d."" % (t.lexer.lineno))
+					exit(0)
+					# Error class call
+			elif arrMatOperands.size() == 1:
+				print(""Error: invalid operation in line %d."" % (t.lexer.lineno))
+				exit(0)
+				# Error class call
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
@@ -625,6 +698,12 @@ def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
+				if oper == ""ARR+"" or oper == ""ARR-"":
+					arrMatOperands.push({
+						""address"": addresses[address_type],
+						""rows"": lOp[""rows""],
+						""cols"": lOp[""cols""]
+					})
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
@@ -766,6 +845,9 @@ def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
+	if arrMatOperands.size() > 0:
+		print(""Error: print invalido en variable de array en la linea  %d."" % (t.lexer.lineno))
+		exit(0)
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
@@ -831,6 +913,9 @@ def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
+	if arrMatOperands.size() > 0:
+		print(""Error: array parameter in module call in in line %d."" % (t.lexer.lineno))
+		exit(0)
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
@@ -874,10 +959,12 @@ def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
+	if ""rows"" in variableTable[arrMatScope.peek()][t[-1]]:
+		arrMatOperands.push(variableTable[arrMatScope.peek()][t[-1]])
 
 def p_addTypeId(t):
 	'addTypeId : '
+	# Push a tipos a la pila de tipos
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
@@ -890,6 +977,7 @@ def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
+	arrMatOperands.pop()
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
KO;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" 
-program myprog;
-
-function int uno(int c, int d) {
     var char x, y;
     return(999);
 }
 
-function int dos(int a, int b) {
     var char x, y;
     return(1000);
 }
@@ -17,9 +16,9 @@ main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
-    j[2][1] = dos(2,2);
     print(j[2][z[0]]);
-    print(dos(1,2) + uno(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
OK;39;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"+program myprogram;
 
+function int test1(int c, int d) {
     var char x, y;
     return(999);
 }
 
+function int test2(int a, int b) {
     var char x, y;
     return(1000);
 }
@@ -17,9 +16,9 @@ main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
+    j[2][1] = test2(2,2);
     print(j[2][z[0]]);
+    print(test2(1,2) + test1(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
KO;39;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
-        # localMem.printInts()
-        # print(getValueFromAddress(quad.left_operand))
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
@@ -389,14 +388,12 @@ def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
-    # localMem.printInts()
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
-    global localMemStack
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
@@ -415,28 +412,29 @@ def gotofor(quad):
     return quad.result
 
 def gosub(quad):
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
-    global localMem
     localMemStack.append(localMem)
     currentFunctionStack.append(quad.left_operand)
-    localMem = Memory()
 
 def param(quad):
-    global localMem
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
-        localMem.insertInt(lOp, quad.result)
     if address == 4:
-        localMem.insertFloat(lOp, quad.result)
     if address == 5:
-        localMem.insertChar(lOp, quad.result)
 
 def rtn(quad):
-    global tempMem
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
@@ -449,9 +447,13 @@ def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
 
 def verify(quad):
-    global tempMem
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
OK;39;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
+
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
@@ -389,14 +388,12 @@ def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
@@ -415,28 +412,29 @@ def gotofor(quad):
     return quad.result
 
 def gosub(quad):
+    global newMem
+    global localMem
+    localMem = newMem
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
     localMemStack.append(localMem)
+    global newMem
+    newMem = Memory()
     currentFunctionStack.append(quad.left_operand)
 
 def param(quad):
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
+        newMem.insertInt(lOp, quad.result)
     if address == 4:
+        newMem.insertFloat(lOp, quad.result)
     if address == 5:
+        newMem.insertChar(lOp, quad.result)    
 
 def rtn(quad):
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
@@ -449,9 +447,13 @@ def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
+    newIndex = quad.id + 1
+    if Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
+        while Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
+            newIndex += 1
+        return newIndex
 
 def verify(quad):
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
KO;39;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
-
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
@@ -822,6 +822,7 @@ def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
 	operators.pop()
 	types.pop()
 "
OK;39;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
+arrMatId = Stack()
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
@@ -822,6 +822,7 @@ def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
+		types.push(variableTable[""global""][funcName][""type""])
 	operators.pop()
 	types.pop()
 "
KO;39;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
-var int i[1][1], j[1], k;
 
 function int uno(int c, int d) {
     var char x, y;
-    x = ""a"";
-    y = ""b"";
-    return(8);
 }
 
 function int dos(int a, int b) {
     var char x, y;
-    x = ""g"";
-    y = ""h"";
 }
 
 main() {
-    var int c;
         float k;
-    c = 4;
-    k = 1 + 2 - (3 * 4) / c;
-    if (1 > 2) then {
-        read(c);
-    } else {
-        read(k);
-    }
-    while (1 < 3) {
-        read(k);
-    }
-    for c = 1 to c < 10 {
-        read(j);
-    }
-    c = 2 < 1;
-    c = 1 | 0;
-    read(i);
-    print(i);
-    uno(1, 2);
-    dos(3, 4);
 }
\ No newline at end of file"
OK;39;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
 
 function int uno(int c, int d) {
     var char x, y;
+    return(999);
 }
 
 function int dos(int a, int b) {
     var char x, y;
+    return(1000);
 }
 
 main() {
+    var int c, z[5], j[3][3];
         float k;
+    z[1+2] = 2;
+    z[0] = 1;
+    j[2][1] = 3;
+    print(dos(1,2) + uno(1,2) * j[2][1] * j[2][1] / z[3]);
+    print(j[z[3]][z[0]]);
+    print(""END OF MAIN"");
 }
\ No newline at end of file"
KO;40;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
-  # sage.interfaces.gap.gap_cmd = 'gap -r -o 24G '
-  sage.interfaces.gap.gap_cmd = 'gap -r -o 50G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
-    set_gap_memory_pool_size(50* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
OK;40;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
+  sage.interfaces.gap.gap_cmd = 'gap -r -o 256G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
+    set_gap_memory_pool_size(256* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
KO;40;nednoodlehead;punge;a8d96a5395cfdf42cb30a0956fc3d4079b444e82;"can now pause and close app properly.
-after pausing and closing, the .exe persists in memory until the .sleep() called exits. It takes up ~100mb of memory while doing this. No known way to terminate the sleep... yet
-also need to add pausing on app close. eventually ill get der";"class music_player:
     current_playing2 = 2
     playback = ''
     sleeptimer = 0
 
     def query_list(self):
         big_ol_list = []
@@ -695,18 +696,17 @@ def initialized_song(self, song_link2):
         return songpt1, songpt2, songpt3
 
     def pydub_playsong(self, song_to_play):
-        print(song_to_play.duration_seconds) #if was paused do self.playback = sleeptimer - pause_adjusttimer (paused now = true). else normal
-        self.sleeptimer = song_to_play.duration_seconds - .25 #TODO change to less i think
-        self.playback = pydub.playback._play_with_simpleaudio(song_to_play)
-        time.sleep(self.sleeptimer) #TODO check if not playing then close thread? Threads linger after app closes..
-
-        print(""done playing rn"")
 
 
     def main_music_loop(self, song_list, thread_one, song_one):
-        global inner_playing_loop
-        inner_playing_loop = True
-        while inner_playing_loop is True:
             try:
                 print(""Thread begins"")
                 thread_one.start()
@@ -741,25 +741,28 @@ def main_music_loop(self, song_list, thread_one, song_one):
                 continue
          #- should crossfade into song 3...?
     def main_music_loop_entry(self, song_list):
-        entry_song = self.initialized_song(song_list[0])
-        begin_thr = threading.Thread(target=self.pydub_playsong, args=(entry_song[0],))
-        begin_thr.start()
-        print(""initialize song 2"")
-        entry_song_MT = threading.Thread(target=self.pydub_playsong, args=(entry_song[1],))
-        begin_thr.join()
-        entry_song_MT.start()
-        song_one = self.initialized_song(song_list[1])
-        entry_crossfade = entry_song[2].append(song_one[0], crossfade=crossfade_ms)
-        thread_one = threading.Thread(target=self.pydub_playsong, args=(song_one[1],))
-        entry_song_MT.join()
-        ent_crsfade = threading.Thread(target=self.pydub_playsong, args=(entry_crossfade,))
-        ent_crsfade.start()
-        print(""entering loop"")
-        ent_crsfade.join()
-        self.main_music_loop(song_list, thread_one, song_one)
 
     def pause(self): #TODO this should also begin to kill the threads before they keep going. Essentially, music loop needs to have a check ccondition for if/when
         print(""clicked pause"")
         self.playback.stop()
 
 "
OK;40;nednoodlehead;punge;a8d96a5395cfdf42cb30a0956fc3d4079b444e82;"can now pause and close app properly.
-after pausing and closing, the .exe persists in memory until the .sleep() called exits. It takes up ~100mb of memory while doing this. No known way to terminate the sleep... yet
-also need to add pausing on app close. eventually ill get der";"class music_player:
     current_playing2 = 2
     playback = ''
     sleeptimer = 0
+    inner_playing_loop = True
 
     def query_list(self):
         big_ol_list = []
@@ -695,18 +696,17 @@ def initialized_song(self, song_link2):
         return songpt1, songpt2, songpt3
 
     def pydub_playsong(self, song_to_play):
+        if self.inner_playing_loop is True:
+            print(song_to_play.duration_seconds) #if was paused do self.playback = sleeptimer - pause_adjusttimer (paused now = true). else normal
+            self.sleeptimer = song_to_play.duration_seconds - .05 #TODO change to less i think
+            self.playback = pydub.playback._play_with_simpleaudio(song_to_play)
+            time.sleep(self.sleeptimer) #TODO check if not playing then close thread? Threads linger after app closes..
+            print(""done playing rn"")
+            print(self.inner_playing_loop)
 
 
     def main_music_loop(self, song_list, thread_one, song_one):
+        while self.inner_playing_loop is True:
             try:
                 print(""Thread begins"")
                 thread_one.start()
@@ -741,25 +741,28 @@ def main_music_loop(self, song_list, thread_one, song_one):
                 continue
          #- should crossfade into song 3...?
     def main_music_loop_entry(self, song_list):
+        self.inner_playing_loop = True
+        while self.inner_playing_loop is True:
+            entry_song = self.initialized_song(song_list[0])
+            begin_thr = threading.Thread(target=self.pydub_playsong, args=(entry_song[0],))
+            begin_thr.start()
+            print(""initialize song 2"")
+            entry_song_MT = threading.Thread(target=self.pydub_playsong, args=(entry_song[1],))
+            begin_thr.join()
+            entry_song_MT.start()
+            song_one = self.initialized_song(song_list[1])
+            entry_crossfade = entry_song[2].append(song_one[0], crossfade=crossfade_ms)
+            thread_one = threading.Thread(target=self.pydub_playsong, args=(song_one[1],))
+            entry_song_MT.join()
+            ent_crsfade = threading.Thread(target=self.pydub_playsong, args=(entry_crossfade,))
+            ent_crsfade.start()
+            print(""entering loop"")
+            ent_crsfade.join()
+            self.main_music_loop(song_list, thread_one, song_one)
 
     def pause(self): #TODO this should also begin to kill the threads before they keep going. Essentially, music loop needs to have a check ccondition for if/when
         print(""clicked pause"")
+        self.inner_playing_loop = False
         self.playback.stop()
 
 "
KO;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
\ No newline at end of file"
OK;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
+Computer Architecture project to simulate a CPU, including memory and cache 
+
+1. What does the program do? 
+   Program will prompt user to point the program to two input files, the cpu instructions file as well as data input 
+
+2. What data do you need?
+   Program needs CPU instructions as well as data input to initialize memory bus 
+
+3. What aspects of a CPU can you simulate using Python?
+   The program can simulate 
+
+4. How will your CPU handle incoming instructions?
+   The cpu will receive the insructions from the cpu instructions file and run the appropriate ISA instructions
+
+5. How will your CPU output instructions?
+   CPU will output instructions to the terminal
+ 
+6. How will your CPU store data?
+   CPU will store the data in memory using the Memory class. 
\ No newline at end of file"
KO;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;
OK;40;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;"+MEMORY_BUS_SIZE = 128
+
+# This class implements a memory bus
+class Memory:
+    def __init__(self):
+        self.memory_bus = {}
+        self.initialize_memory_bus()
+
+    # Method to initialize memory bus with size based on MEMORY_BUS_SIZE variable
+    # Memory bus is setup as a dict with binary address being key, and value being value 
+    def initialize_memory_bus(self):
+        for i in range(MEMORY_BUS_SIZE):
+            self.memory_bus[f'{i:08b}'] = 0
+    
+    # Method to search for address in memory bus and return value if available 
+    def search_memory_bus(self, address):
+        if self.memory_bus.get(address) != None:
+            return self.memory_bus.get(address)
+        return None 
+    
+    # Method to write in memory bus only if address is within the bus size 
+    def write_memory_bus(self, address, value):
+        if self.memory_bus.get(address) != None:
+            self.memory_bus[address] = value"
KO;42;SkullzZz666;Memory;5f602b55c2d5b0054c60de68f4be9181d64e4ace;memory;\ No newline at end of file
OK;42;SkullzZz666;Memory;5f602b55c2d5b0054c60de68f4be9181d64e4ace;memory;"+class Carte:
+    def __init__(self,valeur):
+        self._valeur = valeur
+        self._estVisible = False
+    def __eq__(self,carteComparee):
+        return self._valeur == carteComparee._valeur
+    def estVisible(self):
+        return self._estVisible
+    def affiche(self):
+        if(self._estVisible):
+            print(self._valeur, end="" "")
+        else:
+            print(""X"", end="" "")
+    def retourne(self):
+        self._estVisible = True if not self._estVisible else False
+        
+class Memory:
+    def __init__(self):
+        self._cartes = [Carte(""A""), Carte(""B""), Carte(""B""), Carte(""A"")]
+        self._carteRetenue = None
+        self._score = 0
+        self._tour = 0
+    def affichePartie(self):
+        for carte in self._cartes:
+            carte.affiche()
+        print(""\n"",end="""")
+    def partieGagnee(self):
+        win = True 
+        for carte in self._cartes:
+            win = win and carte.estVisible()
+        return win
+    def compareCartes(self,secondeCarte):
+        self._tour+=1 
+        if(self._carteRetenue==secondeCarte):
+            self._score+=1 
+            input(""Paire correcte ! Validez pour continuer."")
+        else:
+            input(""Cartes différentes ! Validez pour continuer."")
+            self._carteRetenue.retourne()
+            secondeCarte.retourne()
+    def retourneCarte(self,numero):
+        if(not self._cartes[numero].estVisible()):
+            self._cartes[numero].retourne()
+            if(self._carteRetenue is None):
+                self._carteRetenue = self._cartes[numero]
+                self.affichePartie()
+            else:
+                self.compareCartes(self._cartes[numero])
+                self._carteRetenue = None
+        else:
+            nouveauNumero = int(input(""Carte déjà retournée, veuillez en choisir une autre !""))
+            self.retourneCarte(nouveauNumero)
+    def play(self):
+        while(self._tour<3 and not self.partieGagnee()):
+            self.affichePartie()
+            numCarte = int(input(""Quelle carte voulez-vous retourner ?""))
+            self.retourneCarte(numCarte)
+            numCarte = int(input(""Quelle seconde carte voulez-vous retourner ?""))
+            self.retourneCarte(numCarte)
+        print(""Vous avez gagné en"",self._tour,""comparaisons !"")
+
+myGame = Memory()
+myGame.play()
\ No newline at end of file"
KO;42;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
-  # sage.interfaces.gap.gap_cmd = 'gap -r -o 24G '
-  sage.interfaces.gap.gap_cmd = 'gap -r -o 50G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
-    set_gap_memory_pool_size(50* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
OK;42;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
+  sage.interfaces.gap.gap_cmd = 'gap -r -o 256G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
+    set_gap_memory_pool_size(256* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
KO;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
OK;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
KO;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
OK;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
+DEFAULT_FLAG_RFI = True
+DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
KO;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
-    FLAG_STRATEGY_FILE_PATH
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_timestep,
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
-                    ""check_function"": self._check_avg_freqstep,
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
-                    ""check_function"": self._check_compress,
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
-                    ""check_function"": self._check_flag_strategy,
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
-                    step_dict[key_lower][""value""] = value
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
OK;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
+    FLAG_STRATEGY_FILE_PATH,
+    DEFAULT_FLAG_RFI,
+    DEFAULT_FLAG_MEMORYPERC
 )
 
 
@@ -183,13 +185,13 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_timestep
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_avg_freqstep
                 },
                 ""startchan"": {
                     ""value"": None,
@@ -207,13 +209,25 @@ def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
+                    ""check_function"": self._check_compress
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
+                    ""check_function"": self._check_flag_strategy
+                },
+                ""flag_rfi"": {
+                    ""value"": None,
+                    ""default"": DEFAULT_FLAG_RFI,
+                    ""parsing_function"": (lambda x: bool(x)),
+                    ""check_function"": self._check_flag_rfi
+                },
+                ""flag_memoryperc"": {
+                    ""value"": DEFAULT_FLAG_MEMORYPERC, # This prevents any update from the parameter field
+                    ""default"": DEFAULT_FLAG_MEMORYPERC,
+                    ""parsing_function"": (lambda x: int(x)),
+                    ""check_function"": self._check_flag_memoryperc
                 }
             },
             ""quality"": {
@@ -339,10 +353,15 @@ def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
+                    if step_dict[key_lower][""value""] is None:
+                        step_dict[key_lower][""value""] = value
+                    else:
+                        # The parameter has a fixed value that cannot be set
+                        break
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
+                # If the loop has not broken, it means the parameter is not expected
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
@@ -430,6 +449,12 @@ def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
+    @staticmethod
+    def _check_flag_rfi(flag_rfi: bool) -> bool:
+        """""" """"""
+        return isinstance(flag_rfi, bool)
+
+
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
@@ -444,6 +469,14 @@ def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
+    @staticmethod
+    def _check_flag_memoryperc(flag_memoryperc: int) -> bool:
+        """""" """"""
+        is_int = isinstance(flag_memoryperc, int)
+        is_percent = 0 <= flag_memoryperc <= 100
+        return is_int & is_percent
+
+
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
KO;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
OK;42;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
@@ -97,6 +99,8 @@ def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
+        'flag_rfi = True\n'
+        'flag_memoryperc = 30\n'
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.
+
+# dependencies
+/node_modules
+/.pnp
+.pnp.js
+
+# testing
+/coverage
+
+# production
+/build
+
+# misc
+.DS_Store
+.env.local
+.env.development.local
+.env.test.local
+.env.production.local
+
+npm-debug.log*
+yarn-debug.log*
+yarn-error.log*"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+# Getting Started with Create React App
+
+This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).
+
+## Available Scripts
+
+In the project directory, you can run:
+
+### `npm start`
+
+Runs the app in the development mode.\
+Open [http://localhost:3000](http://localhost:3000) to view it in your browser.
+
+The page will reload when you make changes.\
+You may also see any lint errors in the console.
+
+### `npm test`
+
+Launches the test runner in the interactive watch mode.\
+See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.
+
+### `npm run build`
+
+Builds the app for production to the `build` folder.\
+It correctly bundles React in production mode and optimizes the build for the best performance.
+
+The build is minified and the filenames include the hashes.\
+Your app is ready to be deployed!
+
+See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.
+
+### `npm run eject`
+
+**Note: this is a one-way operation. Once you `eject`, you can't go back!**
+
+If you aren't satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.
+
+Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you're on your own.
+
+You don't have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn't feel obligated to use this feature. However we understand that this tool wouldn't be useful if you couldn't customize it when you are ready for it.
+
+## Learn More
+
+You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).
+
+To learn React, check out the [React documentation](https://reactjs.org/).
+
+### Code Splitting
+
+This section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)
+
+### Analyzing the Bundle Size
+
+This section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)
+
+### Making a Progressive Web App
+
+This section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)
+
+### Advanced Configuration
+
+This section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)
+
+### Deployment
+
+This section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)
+
+### `npm run build` fails to minify
+
+This section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+{
+  ""name"": ""front-memory-test"",
+  ""version"": ""0.1.0"",
+  ""private"": true,
+  ""dependencies"": {
+    ""@testing-library/jest-dom"": ""^5.16.4"",
+    ""@testing-library/react"": ""^13.1.1"",
+    ""@testing-library/user-event"": ""^13.5.0"",
+    ""react"": ""^18.0.0"",
+    ""react-dom"": ""^18.0.0"",
+    ""react-scripts"": ""5.0.1"",
+    ""web-vitals"": ""^2.1.4""
+  },
+  ""scripts"": {
+    ""start"": ""react-scripts start"",
+    ""build"": ""react-scripts build"",
+    ""test"": ""react-scripts test"",
+    ""eject"": ""react-scripts eject""
+  },
+  ""eslintConfig"": {
+    ""extends"": [
+      ""react-app"",
+      ""react-app/jest""
+    ]
+  },
+  ""browserslist"": {
+    ""production"": [
+      "">0.2%"",
+      ""not dead"",
+      ""not op_mini all""
+    ],
+    ""development"": [
+      ""last 1 chrome version"",
+      ""last 1 firefox version"",
+      ""last 1 safari version""
+    ]
+  }
+}"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+<!DOCTYPE html>
+<html lang=""en"">
+  <head>
+    <meta charset=""utf-8"" />
+    <link rel=""icon"" href=""%PUBLIC_URL%/favicon.ico"" />
+    <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
+    <meta name=""theme-color"" content=""#000000"" />
+    <meta
+      name=""description""
+      content=""Web site created using create-react-app""
+    />
+    <link rel=""apple-touch-icon"" href=""%PUBLIC_URL%/logo192.png"" />
+    <!--
+      manifest.json provides metadata used when your web app is installed on a
+      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
+    -->
+    <link rel=""manifest"" href=""%PUBLIC_URL%/manifest.json"" />
+    <!--
+      Notice the use of %PUBLIC_URL% in the tags above.
+      It will be replaced with the URL of the `public` folder during the build.
+      Only files inside the `public` folder can be referenced from the HTML.
+
+      Unlike ""/favicon.ico"" or ""favicon.ico"", ""%PUBLIC_URL%/favicon.ico"" will
+      work correctly both with client-side routing and a non-root public URL.
+      Learn how to configure a non-root public URL by running `npm run build`.
+    -->
+    <title>React App</title>
+  </head>
+  <body>
+    <noscript>You need to enable JavaScript to run this app.</noscript>
+    <div id=""root""></div>
+    <!--
+      This HTML file is a template.
+      If you open it directly in the browser, you will see an empty page.
+
+      You can add webfonts, meta tags, or analytics to this file.
+      The build step will place the bundled scripts into the <body> tag.
+
+      To begin the development, run `npm start` or `yarn start`.
+      To create a production bundle, use `npm run build` or `yarn build`.
+    -->
+  </body>
+</html>"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+{
+  ""short_name"": ""React App"",
+  ""name"": ""Create React App Sample"",
+  ""icons"": [
+    {
+      ""src"": ""favicon.ico"",
+      ""sizes"": ""64x64 32x32 24x24 16x16"",
+      ""type"": ""image/x-icon""
+    },
+    {
+      ""src"": ""logo192.png"",
+      ""type"": ""image/png"",
+      ""sizes"": ""192x192""
+    },
+    {
+      ""src"": ""logo512.png"",
+      ""type"": ""image/png"",
+      ""sizes"": ""512x512""
+    }
+  ],
+  ""start_url"": ""."",
+  ""display"": ""standalone"",
+  ""theme_color"": ""#000000"",
+  ""background_color"": ""#ffffff""
+}"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+# https://www.robotstxt.org/robotstxt.html
+User-agent: *
+Disallow:"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+.App {
+  text-align: center;
+}
+
+.App-logo {
+  height: 40vmin;
+  pointer-events: none;
+}
+
+@media (prefers-reduced-motion: no-preference) {
+  .App-logo {
+    animation: App-logo-spin infinite 20s linear;
+  }
+}
+
+.App-header {
+  background-color: #282c34;
+  min-height: 100vh;
+  display: flex;
+  flex-direction: column;
+  align-items: center;
+  justify-content: center;
+  font-size: calc(10px + 2vmin);
+  color: white;
+}
+
+.App-link {
+  color: #61dafb;
+}
+
+@keyframes App-logo-spin {
+  from {
+    transform: rotate(0deg);
+  }
+  to {
+    transform: rotate(360deg);
+  }
+}"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+import logo from './logo.svg';
+import './App.css';
+
+function App() {
+  return (
+    <div className=""App"">
+      <header className=""App-header"">
+        <img src={logo} className=""App-logo"" alt=""logo"" />
+        <p>
+          Edit <code>src/App.js</code> and save to reload.
+        </p>
+        <a
+          className=""App-link""
+          href=""https://reactjs.org""
+          target=""_blank""
+          rel=""noopener noreferrer""
+        >
+          Learn React
+        </a>
+      </header>
+    </div>
+  );
+}
+
+export default App;"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+import { render, screen } from '@testing-library/react';
+import App from './App';
+
+test('renders learn react link', () => {
+  render(<App />);
+  const linkElement = screen.getByText(/learn react/i);
+  expect(linkElement).toBeInTheDocument();
+});"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+body {
+  margin: 0;
+  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
+    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
+    sans-serif;
+  -webkit-font-smoothing: antialiased;
+  -moz-osx-font-smoothing: grayscale;
+}
+
+code {
+  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
+    monospace;
+}"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+import React from 'react';
+import ReactDOM from 'react-dom/client';
+import './index.css';
+import App from './App';
+import reportWebVitals from './reportWebVitals';
+
+const root = ReactDOM.createRoot(document.getElementById('root'));
+root.render(
+  <React.StrictMode>
+    <App />
+  </React.StrictMode>
+);
+
+// If you want to start measuring performance in your app, pass a function
+// to log results (for example: reportWebVitals(console.log))
+// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
+reportWebVitals();"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;\ No newline at end of file
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+<svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 841.9 595.3""><g fill=""#61DAFB""><path d=""M666.3 296.5c0-32.5-40.7-63.3-103.1-82.4 14.4-63.6 8-114.2-20.2-130.4-6.5-3.8-14.1-5.6-22.4-5.6v22.3c4.6 0 8.3.9 11.4 2.6 13.6 7.8 19.5 37.5 14.9 75.7-1.1 9.4-2.9 19.3-5.1 29.4-19.6-4.8-41-8.5-63.5-10.9-13.5-18.5-27.5-35.3-41.6-50 32.6-30.3 63.2-46.9 84-46.9V78c-27.5 0-63.5 19.6-99.9 53.6-36.4-33.8-72.4-53.2-99.9-53.2v22.3c20.7 0 51.4 16.5 84 46.6-14 14.7-28 31.4-41.3 49.9-22.6 2.4-44 6.1-63.6 11-2.3-10-4-19.7-5.2-29-4.7-38.2 1.1-67.9 14.6-75.8 3-1.8 6.9-2.6 11.5-2.6V78.5c-8.4 0-16 1.8-22.6 5.6-28.1 16.2-34.4 66.7-19.9 130.1-62.2 19.2-102.7 49.9-102.7 82.3 0 32.5 40.7 63.3 103.1 82.4-14.4 63.6-8 114.2 20.2 130.4 6.5 3.8 14.1 5.6 22.5 5.6 27.5 0 63.5-19.6 99.9-53.6 36.4 33.8 72.4 53.2 99.9 53.2 8.4 0 16-1.8 22.6-5.6 28.1-16.2 34.4-66.7 19.9-130.1 62-19.1 102.5-49.9 102.5-82.3zm-130.2-66.7c-3.7 12.9-8.3 26.2-13.5 39.5-4.1-8-8.4-16-13.1-24-4.6-8-9.5-15.8-14.4-23.4 14.2 2.1 27.9 4.7 41 7.9zm-45.8 106.5c-7.8 13.5-15.8 26.3-24.1 38.2-14.9 1.3-30 2-45.2 2-15.1 0-30.2-.7-45-1.9-8.3-11.9-16.4-24.6-24.2-38-7.6-13.1-14.5-26.4-20.8-39.8 6.2-13.4 13.2-26.8 20.7-39.9 7.8-13.5 15.8-26.3 24.1-38.2 14.9-1.3 30-2 45.2-2 15.1 0 30.2.7 45 1.9 8.3 11.9 16.4 24.6 24.2 38 7.6 13.1 14.5 26.4 20.8 39.8-6.3 13.4-13.2 26.8-20.7 39.9zm32.3-13c5.4 13.4 10 26.8 13.8 39.8-13.1 3.2-26.9 5.9-41.2 8 4.9-7.7 9.8-15.6 14.4-23.7 4.6-8 8.9-16.1 13-24.1zM421.2 430c-9.3-9.6-18.6-20.3-27.8-32 9 .4 18.2.7 27.5.7 9.4 0 18.7-.2 27.8-.7-9 11.7-18.3 22.4-27.5 32zm-74.4-58.9c-14.2-2.1-27.9-4.7-41-7.9 3.7-12.9 8.3-26.2 13.5-39.5 4.1 8 8.4 16 13.1 24 4.7 8 9.5 15.8 14.4 23.4zM420.7 163c9.3 9.6 18.6 20.3 27.8 32-9-.4-18.2-.7-27.5-.7-9.4 0-18.7.2-27.8.7 9-11.7 18.3-22.4 27.5-32zm-74 58.9c-4.9 7.7-9.8 15.6-14.4 23.7-4.6 8-8.9 16-13 24-5.4-13.4-10-26.8-13.8-39.8 13.1-3.1 26.9-5.8 41.2-7.9zm-90.5 125.2c-35.4-15.1-58.3-34.9-58.3-50.6 0-15.7 22.9-35.6 58.3-50.6 8.6-3.7 18-7 27.7-10.1 5.7 19.6 13.2 40 22.5 60.9-9.2 20.8-16.6 41.1-22.2 60.6-9.9-3.1-19.3-6.5-28-10.2zM310 490c-13.6-7.8-19.5-37.5-14.9-75.7 1.1-9.4 2.9-19.3 5.1-29.4 19.6 4.8 41 8.5 63.5 10.9 13.5 18.5 27.5 35.3 41.6 50-32.6 30.3-63.2 46.9-84 46.9-4.5-.1-8.3-1-11.3-2.7zm237.2-76.2c4.7 38.2-1.1 67.9-14.6 75.8-3 1.8-6.9 2.6-11.5 2.6-20.7 0-51.4-16.5-84-46.6 14-14.7 28-31.4 41.3-49.9 22.6-2.4 44-6.1 63.6-11 2.3 10.1 4.1 19.8 5.2 29.1zm38.5-66.7c-8.6 3.7-18 7-27.7 10.1-5.7-19.6-13.2-40-22.5-60.9 9.2-20.8 16.6-41.1 22.2-60.6 9.9 3.1 19.3 6.5 28.1 10.2 35.4 15.1 58.3 34.9 58.3 50.6-.1 15.7-23 35.6-58.4 50.6zM320.8 78.4z""/><circle cx=""420.9"" cy=""296.5"" r=""45.7""/><path d=""M520.5 78.1z""/></g></svg>
\ No newline at end of file"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+const reportWebVitals = onPerfEntry => {
+  if (onPerfEntry && onPerfEntry instanceof Function) {
+    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
+      getCLS(onPerfEntry);
+      getFID(onPerfEntry);
+      getFCP(onPerfEntry);
+      getLCP(onPerfEntry);
+      getTTFB(onPerfEntry);
+    });
+  }
+};
+
+export default reportWebVitals;"
KO;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;
OK;44;SebaViana;verbal-memory-test;c0b7188761a121c3c4aa14d1aa64e7d697b5d958;front-memory-test;"+// jest-dom adds custom jest matchers for asserting on DOM nodes.
+// allows you to do things like:
+// expect(element).toHaveTextContent(/react/i)
+// learn more: https://github.com/testing-library/jest-dom
+import '@testing-library/jest-dom';"
KO;45;guyzam12;AnomalyDetection;0ba878e517b276c45d1195eb29f6f5f72659a05b;fix cuda memory, super res settings, fp16 training (#5);"def setup_dist():
     """"""
     if dist.is_initialized():
         return
 
     comm = MPI.COMM_WORLD
     backend = ""gloo"" if not th.cuda.is_available() else ""nccl""
@@ -46,7 +47,7 @@ def dev():
     Get the device to use for torch.distributed.
     """"""
     if th.cuda.is_available():
-        return th.device(f""cuda:{MPI.COMM_WORLD.Get_rank() % GPUS_PER_NODE}"")
     return th.device(""cpu"")
 
 "
OK;45;guyzam12;AnomalyDetection;0ba878e517b276c45d1195eb29f6f5f72659a05b;fix cuda memory, super res settings, fp16 training (#5);"def setup_dist():
     """"""
     if dist.is_initialized():
         return
+    os.environ[""CUDA_VISIBLE_DEVICES""] = f""{MPI.COMM_WORLD.Get_rank() % GPUS_PER_NODE}""
 
     comm = MPI.COMM_WORLD
     backend = ""gloo"" if not th.cuda.is_available() else ""nccl""
@@ -46,7 +47,7 @@ def dev():
     Get the device to use for torch.distributed.
     """"""
     if th.cuda.is_available():
+        return th.device(f""cuda"")
     return th.device(""cpu"")
 
 "
KO;45;guyzam12;AnomalyDetection;0ba878e517b276c45d1195eb29f6f5f72659a05b;fix cuda memory, super res settings, fp16 training (#5);"def _optimize_fp16(self, opt: th.optim.Optimizer):
         logger.logkv_mean(""grad_norm"", grad_norm)
         logger.logkv_mean(""param_norm"", param_norm)
 
-        opt.step(grad_scale=2.0 ** self.lg_loss_scale)
         zero_master_grads(self.master_params)
         master_params_to_model_params(self.param_groups_and_shapes, self.master_params)
         self.lg_loss_scale += self.fp16_scale_growth"
OK;45;guyzam12;AnomalyDetection;0ba878e517b276c45d1195eb29f6f5f72659a05b;fix cuda memory, super res settings, fp16 training (#5);"def _optimize_fp16(self, opt: th.optim.Optimizer):
         logger.logkv_mean(""grad_norm"", grad_norm)
         logger.logkv_mean(""param_norm"", param_norm)
 
+        self.master_params[0].grad.mul_(1.0 / (2 ** self.lg_loss_scale))
+        opt.step()
         zero_master_grads(self.master_params)
         master_params_to_model_params(self.param_groups_and_shapes, self.master_params)
         self.lg_loss_scale += self.fp16_scale_growth"
KO;45;guyzam12;AnomalyDetection;0ba878e517b276c45d1195eb29f6f5f72659a05b;fix cuda memory, super res settings, fp16 training (#5);"def sr_create_model_and_diffusion(
     num_channels,
     num_res_blocks,
     num_heads,
     num_heads_upsample,
     attention_resolutions,
     dropout,
@@ -295,6 +296,8 @@ def sr_create_model_and_diffusion(
     rescale_learned_sigmas,
     use_checkpoint,
     use_scale_shift_norm,
 ):
     model = sr_create_model(
         large_size,
@@ -306,9 +309,12 @@ def sr_create_model_and_diffusion(
         use_checkpoint=use_checkpoint,
         attention_resolutions=attention_resolutions,
         num_heads=num_heads,
         num_heads_upsample=num_heads_upsample,
         use_scale_shift_norm=use_scale_shift_norm,
         dropout=dropout,
     )
     diffusion = create_gaussian_diffusion(
         steps=diffusion_steps,
@@ -333,13 +339,18 @@ def sr_create_model(
     use_checkpoint,
     attention_resolutions,
     num_heads,
     num_heads_upsample,
     use_scale_shift_norm,
     dropout,
 ):
     _ = small_size  # hack to prevent unused variable
 
-    if large_size == 256:
         channel_mult = (1, 1, 2, 2, 4, 4)
     elif large_size == 64:
         channel_mult = (1, 2, 3, 4)
@@ -362,8 +373,11 @@ def sr_create_model(
         num_classes=(NUM_CLASSES if class_cond else None),
         use_checkpoint=use_checkpoint,
         num_heads=num_heads,
         num_heads_upsample=num_heads_upsample,
         use_scale_shift_norm=use_scale_shift_norm,
     )
 
 "
OK;45;guyzam12;AnomalyDetection;0ba878e517b276c45d1195eb29f6f5f72659a05b;fix cuda memory, super res settings, fp16 training (#5);"def sr_create_model_and_diffusion(
     num_channels,
     num_res_blocks,
     num_heads,
+    num_head_channels,
     num_heads_upsample,
     attention_resolutions,
     dropout,
@@ -295,6 +296,8 @@ def sr_create_model_and_diffusion(
     rescale_learned_sigmas,
     use_checkpoint,
     use_scale_shift_norm,
+    resblock_updown,
+    use_fp16,
 ):
     model = sr_create_model(
         large_size,
@@ -306,9 +309,12 @@ def sr_create_model_and_diffusion(
         use_checkpoint=use_checkpoint,
         attention_resolutions=attention_resolutions,
         num_heads=num_heads,
+        num_head_channels=num_head_channels,
         num_heads_upsample=num_heads_upsample,
         use_scale_shift_norm=use_scale_shift_norm,
         dropout=dropout,
+        resblock_updown=resblock_updown,
+        use_fp16=use_fp16,
     )
     diffusion = create_gaussian_diffusion(
         steps=diffusion_steps,
@@ -333,13 +339,18 @@ def sr_create_model(
     use_checkpoint,
     attention_resolutions,
     num_heads,
+    num_head_channels,
     num_heads_upsample,
     use_scale_shift_norm,
     dropout,
+    resblock_updown,
+    use_fp16,
 ):
     _ = small_size  # hack to prevent unused variable
 
+    if large_size == 512:
+        channel_mult = (1, 1, 2, 2, 4, 4)
+    elif large_size == 256:
         channel_mult = (1, 1, 2, 2, 4, 4)
     elif large_size == 64:
         channel_mult = (1, 2, 3, 4)
@@ -362,8 +373,11 @@ def sr_create_model(
         num_classes=(NUM_CLASSES if class_cond else None),
         use_checkpoint=use_checkpoint,
         num_heads=num_heads,
+        num_head_channels=num_head_channels,
         num_heads_upsample=num_heads_upsample,
         use_scale_shift_norm=use_scale_shift_norm,
+        resblock_updown=resblock_updown,
+        use_fp16=use_fp16,
     )
 
 "
