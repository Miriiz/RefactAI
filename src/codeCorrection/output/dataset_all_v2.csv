Label;Page;Username;Repo;Commit;Bug;Code
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";"will
   creating packages and building Docker images
 - scale from a single function to thousands of parallel tasks
 
 For more information see Meadowrun's
 [documentation](https://docs.meadowrun.io/en/latest/), or the [project
 homepage](https://meadowrun.io).
 
 ## When would I use Meadowrun?
 "
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";"will
   creating packages and building Docker images
 - scale from a single function to thousands of parallel tasks
 
 For more information see Meadowrun's [documentation](https://docs.meadowrun.io), the
 [project homepage](https://meadowrun.io), or [join the chat on
 Gitter](https://gitter.im/meadowdata/meadowrun)
 
 ## When would I use Meadowrun?
 "
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" # 1. Update this file to be in sync with pyproject.toml, see the comments in this file
 #    for what needs to be synced from pyproject.toml
 # 2. Build the pip package using `poetry build`
 # 3. Unpack the resulting meadowrun-0.1.7.tar.gz file into /dist/meadowrun-0.1.7. (E.g.
 #    setup.py should be at /dist/meadowrun-0.1.7/setup.py.)
 # 4. Now run `conda build -c defaults -c conda-forge --python 3.9 .` from this directory
 
 # 5a. To test this, you can install this package into a conda environment by running
 #     `conda install meadowrun -c c:\bin\Miniconda\conda-bld -c defaults -c conda-forge`

 
 package:
   name: meadowrun
   version: 0.1.7  # from pyproject.toml
 source:
   path: ../../dist/meadowrun-0.1.7  # from pyproject.toml
 build:
   entry_points:  # from pyproject.toml
     - meadowrun-local = meadowrun.run_job_local_main:command_line_main"
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" # 1. Update this file to be in sync with pyproject.toml, see the comments in this file
 #    for what needs to be synced from pyproject.toml
 # 2. Build the pip package using `poetry build`
 # 3. Unpack the resulting meadowrun-0.1.7.tar.gz file into /dist/meadowrun-0.1.7 with
 #    `tar xzvf dist\meadowrun-0.1.8a1.tar.gz -C dist` (E.g. setup.py should be at
 #    /dist/meadowrun-0.1.7/setup.py)
 # 4. Now run `conda build -c defaults -c conda-forge --python 3.9
 #    build_scripts\conda-recipe`
 
 # 5a. To test this, you can install this package into a conda environment by running
 #     `conda install meadowrun -c c:\bin\Miniconda\conda-bld -c defaults -c conda-forge`

 
 package:
   name: meadowrun
   version: 0.1.8a1  # from pyproject.toml
 source:
   path: ../../dist/meadowrun-0.1.8a1  # from pyproject.toml
 build:
   entry_points:  # from pyproject.toml
     - meadowrun-local = meadowrun.run_job_local_main:command_line_main"
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" [tool.poetry]
 name = ""meadowrun""
 version = ""0.1.7""
 description = ""The easiest way to run python code on one or more remote machines""
 authors = [""Richard Lee <hrichardlee@gmail.com>""]
 homepage = ""https://github.com/meadowdata/meadowrun"""
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" [tool.poetry]
 name = ""meadowrun""
 version = ""0.1.8a1""
 description = ""The easiest way to run python code on one or more remote machines""
 authors = [""Richard Lee <hrichardlee@gmail.com>""]
 homepage = ""https://github.com/meadowdata/meadowrun"""
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" 
 import boto3
 
 import meadowrun.aws_integration
 from meadowrun.aws_integration.aws_core import (
     _get_account_number,
     _get_default_region_name,
 )
 from meadowrun.aws_integration.aws_permissions import (
     _CLEAN_UP_LAMBDA_NAME,
     _CLEAN_UP_LAMBDA_SCHEDULE_RULE,
     _EC2_ALLOC_LAMBDA_NAME,
     _EC2_ALLOC_LAMBDA_SCHEDULE_RULE,
     _MANAGEMENT_LAMBDA_ROLE,
     _ensure_management_lambda_role,
 )

 
 _T = TypeVar(""_T"")
 
 
 def _get_zipped_lambda_code() -> bytes:
     """""""
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" 
 import boto3
 
 import meadowrun.aws_integration.management_lambdas.adjust_ec2_instances
 import meadowrun.aws_integration.management_lambdas.clean_up
 from meadowrun.aws_integration.aws_core import (
     _get_account_number,
     _get_default_region_name,
 )
 from meadowrun.aws_integration.aws_permissions import (
     _MANAGEMENT_LAMBDA_ROLE,
     _ensure_management_lambda_role,
 )

 
 _T = TypeVar(""_T"")
 
 # the name of the lambda that runs adjust_ec2_instances.py
 _EC2_ALLOC_LAMBDA_NAME = ""meadowrun_ec2_alloc_lambda""
 # the EventBridge rule that triggers the lambda
 _EC2_ALLOC_LAMBDA_SCHEDULE_RULE = ""meadowrun_ec2_alloc_lambda_schedule_rule""
 # the name of the lambda that runs clean_up.py
 _CLEAN_UP_LAMBDA_NAME = ""meadowrun_clean_up""
 _CLEAN_UP_LAMBDA_SCHEDULE_RULE = ""meadowrun_clean_up_lambda_schedule_rule""
 
 
 def _get_zipped_lambda_code() -> bytes:
     """""""
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" }""""""
 
 
 # the name of the lambda that runs adjust_ec2_instances.py
 _EC2_ALLOC_LAMBDA_NAME = ""meadowrun_ec2_alloc_lambda""
 # the EventBridge rule that triggers the lambda
 _EC2_ALLOC_LAMBDA_SCHEDULE_RULE = ""meadowrun_ec2_alloc_lambda_schedule_rule""
 
 # the name of the lambda that runs clean_up.py
 _CLEAN_UP_LAMBDA_NAME = ""meadowrun_clean_up""
 _CLEAN_UP_LAMBDA_SCHEDULE_RULE = ""meadowrun_clean_up_lambda_schedule_rule""
 
 # the role that these lambdas run as
 _MANAGEMENT_LAMBDA_ROLE = ""meadowrun_management_lambda_role""
 "
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" }""""""
 
 
 # the role that these lambdas run as
 _MANAGEMENT_LAMBDA_ROLE = ""meadowrun_management_lambda_role""
 "
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" 
 import meadowrun.aws_integration.management_lambdas.adjust_ec2_instances
 import meadowrun.aws_integration.management_lambdas.clean_up
 from meadowrun.aws_integration.ec2 import _MEADOWRUN_SSH_SECURITY_GROUP
 from meadowrun.aws_integration.aws_permissions import (
     _CLEAN_UP_LAMBDA_NAME,
     _CLEAN_UP_LAMBDA_SCHEDULE_RULE,
     _EC2_ALLOC_LAMBDA_NAME,
     _EC2_ALLOC_LAMBDA_SCHEDULE_RULE,
     _EC2_ALLOC_ROLE,
     _EC2_ALLOC_ROLE_INSTANCE_PROFILE,
     _MANAGEMENT_LAMBDA_ROLE,

     _ensure_meadowrun_sqs_access_policy,
     _ensure_s3_access_policy,
 )
 from meadowrun.aws_integration.ec2_ssh_keys import (
     MEADOWRUN_KEY_PAIR_NAME,
     _MEADOWRUN_KEY_PAIR_SECRET_NAME,

     _MEADOWRUN_GENERATED_DOCKER_REPO,
     ignore_boto3_error_code,
 )
 import meadowrun.aws_integration.s3
 
 
 def _delete_iam_role(iam: Any, role_name: str) -> None:"
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" 
 import meadowrun.aws_integration.management_lambdas.adjust_ec2_instances
 import meadowrun.aws_integration.management_lambdas.clean_up
 import meadowrun.aws_integration.s3
 from meadowrun.aws_integration.aws_mgmt_lambda_setup import (
     _EC2_ALLOC_LAMBDA_NAME,
     _EC2_ALLOC_LAMBDA_SCHEDULE_RULE,
     _CLEAN_UP_LAMBDA_NAME,
     _CLEAN_UP_LAMBDA_SCHEDULE_RULE,
 )
 from meadowrun.aws_integration.aws_permissions import (
     _EC2_ALLOC_ROLE,
     _EC2_ALLOC_ROLE_INSTANCE_PROFILE,
     _MANAGEMENT_LAMBDA_ROLE,

     _ensure_meadowrun_sqs_access_policy,
     _ensure_s3_access_policy,
 )
 from meadowrun.aws_integration.ec2 import _MEADOWRUN_SSH_SECURITY_GROUP
 from meadowrun.aws_integration.ec2_ssh_keys import (
     MEADOWRUN_KEY_PAIR_NAME,
     _MEADOWRUN_KEY_PAIR_SECRET_NAME,

     _MEADOWRUN_GENERATED_DOCKER_REPO,
     ignore_boto3_error_code,
 )
 
 
 def _delete_iam_role(iam: Any, role_name: str) -> None:"
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";"def delete_old_task_queues(region_name: str) -> None:
     now = datetime.datetime.utcnow()
     client = boto3.client(""sqs"", region_name=region_name)
     for queue_url in _get_meadowrun_task_queue_urls(client):
         response = client.get_queue_attributes(
             QueueUrl=queue_url,
             AttributeNames=[
                 ""ApproximateNumberOfMessages"",
                 ""ApproximateNumberOfMessagesNotVisible"",
                 ""CreatedTimestamp"",
                 ""LastModifiedTimestamp"",
             ],
         )
         attributes = response[""Attributes""]
         # we should only need to check LastModifiedTimestamp, but we include
         # CreatedTimestamp just in case
def delete_old_task_queues(region_name: str) -> None:
             datetime.datetime.fromtimestamp(int(attributes[""LastModifiedTimestamp""])),
         )
         if (
             attributes[""ApproximateNumberOfMessages""] == ""0""
             and attributes[""ApproximateNumberOfMessagesNotVisible""] == ""0""
             and now - last_modified > _QUEUE_DELETION_TIMEOUT
         ):
             print(f""Deleting queue {queue_url}, was last modified at {last_modified}"")"
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";"def delete_old_task_queues(region_name: str) -> None:
     now = datetime.datetime.utcnow()
     client = boto3.client(""sqs"", region_name=region_name)
     for queue_url in _get_meadowrun_task_queue_urls(client):
         success, response = ignore_boto3_error_code(
             lambda: client.get_queue_attributes(
                 QueueUrl=queue_url,
                 AttributeNames=[
                     ""ApproximateNumberOfMessages"",
                     ""ApproximateNumberOfMessagesNotVisible"",
                     ""CreatedTimestamp"",
                     ""LastModifiedTimestamp"",
                 ],
             ),
             ""AWS.SimpleQueueService.NonExistentQueue"",
         )
         if not success:
             continue
         assert response is not None  # just for mypy
         attributes = response[""Attributes""]
         # we should only need to check LastModifiedTimestamp, but we include
         # CreatedTimestamp just in case
def delete_old_task_queues(region_name: str) -> None:
             datetime.datetime.fromtimestamp(int(attributes[""LastModifiedTimestamp""])),
         )
         if (
             attributes[""ApproximateNumberOfMessagesNotVisible""] == ""0""
             and now - last_modified > _QUEUE_DELETION_TIMEOUT
         ):
             print(f""Deleting queue {queue_url}, was last modified at {last_modified}"")"
KO;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" import asyncio
 import hashlib
 import io
 import tarfile
 import urllib.parse
 import urllib.request
async def build_image(
                     if ""stream"" in line:
                         print(line[""stream""], end="""")
                     if ""errorDetail"" in line:
                         raise ValueError(
                             f""Error building docker image: f{line['errorDetail']}""
                         )
 
 "
OK;1;meadowdata;meadowrun;06ed4c03a061b1f9e8169b2bda5b4acdbc2ca1c3;"Minor changes:
- Changing the docs link to point to the stable version, and adding a link to Gitter
- Increment version to 0.1.8a1
- Add some more instructions to meta.yaml for building the conda package
- Don't throw an exception when we try to delete a queue that is already in the process of being deleted
- Delete old queues even if they still have messages in them
- Add a richer error message when building a docker container runs out of memory
- Moving lambda names from aws_permissions.py to aws_mgmt_lambda_setup.py which makes more sense
- Fixing a semi-cosmetic import issue in aws_mgmt_lambda_setup";" import asyncio
 import hashlib
 import io
 import json
 import tarfile
 import urllib.parse
 import urllib.request
async def build_image(
                     if ""stream"" in line:
                         print(line[""stream""], end="""")
                     if ""errorDetail"" in line:
                         error_detail = line[""errorDetail""]
                         try:
                             error_detail_json = json.loads(error_detail)
                             code = error_detail_json.get(""code"")
                             if code == 137:
                                 raise ValueError(
                                     ""Error building docker image: ran out of memory, ""
                                     ""please provision an instance that has more memory""
                                 )
                         except Exception:
                             pass
                         raise ValueError(
                             f""Error building docker image: f{error_detail}""
                         )
 
 "
KO;2;Edgecortix-Inc;mera-tvm-public;1bae425628e015d50c7d9806375d58c7e8e42336;"Update TVM VTA (VTA Chisel Wide memory interface) (#8973)

* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI

* Refactor Verialtor check conditions

* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.

* remove build/libvta_tsim.so from non tsim targeting builds

* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings

* Update to latest VTA"; Subproject commit dfe9f572a43d41e0c1ecdf036cea97042a0febfe
OK;2;Edgecortix-Inc;mera-tvm-public;1bae425628e015d50c7d9806375d58c7e8e42336;"Update TVM VTA (VTA Chisel Wide memory interface) (#8973)

* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI

* Refactor Verialtor check conditions

* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.

* remove build/libvta_tsim.so from non tsim targeting builds

* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings

* Update to latest VTA"; Subproject commit 36a91576edf633479c78649e050f18dd2ddc8103
KO;9;Tencent;BlazerML-tvm;2dad4d18f96c17cc4e035e1a9c80d8ac3476d47b;"[BugFix] Fix divide by zero error in TIR pass lower_warp_memory (#9485)

* fix factor divide by zero

* add a test case in test_tir_transform_lower_warp_memory.py

* fix type

* make it more elegant";"class WarpAccessRewriter : protected StmtExprMutator {
     // Align the local memory size. The number of elements may not
     // be a multiple of width_ * warp_coeff_; round it up.
     int factor = width_ * warp_coeff_;
     warp_group_ = (alloc_size + (factor - 1)) / factor;
     alloc_size = warp_group_ * factor;
 "
OK;9;Tencent;BlazerML-tvm;2dad4d18f96c17cc4e035e1a9c80d8ac3476d47b;"[BugFix] Fix divide by zero error in TIR pass lower_warp_memory (#9485)

* fix factor divide by zero

* add a test case in test_tir_transform_lower_warp_memory.py

* fix type

* make it more elegant";"class WarpAccessRewriter : protected StmtExprMutator {
     // Align the local memory size. The number of elements may not
     // be a multiple of width_ * warp_coeff_; round it up.
     int factor = width_ * warp_coeff_;
     ICHECK_NE(factor, 0) << ""Divide by zero"";
     warp_group_ = (alloc_size + (factor - 1)) / factor;
     alloc_size = warp_group_ * factor;
 "
KO;9;Tencent;BlazerML-tvm;2dad4d18f96c17cc4e035e1a9c80d8ac3476d47b;"[BugFix] Fix divide by zero error in TIR pass lower_warp_memory (#9485)

* fix factor divide by zero

* add a test case in test_tir_transform_lower_warp_memory.py

* fix type

* make it more elegant";" 
 import numpy as np
 import tvm.testing
 
 
 @tvm.testing.requires_cuda
def test_lower_warp_memory_same_thread():
     assert ""tvm_warp_shuffle"" not in fdevice.astext()
 
 
 if __name__ == ""__main__"":
     test_lower_warp_memory_local_scope()
     test_lower_warp_memory_correct_indices()
     test_lower_warp_memory_cuda_end_to_end()
     test_lower_warp_memory_cuda_half_a_warp()
     test_lower_warp_memory_cuda_2_buffers()
     test_lower_warp_memory_roundup()
     test_lower_warp_memory_same_thread()"
OK;9;Tencent;BlazerML-tvm;2dad4d18f96c17cc4e035e1a9c80d8ac3476d47b;"[BugFix] Fix divide by zero error in TIR pass lower_warp_memory (#9485)

* fix factor divide by zero

* add a test case in test_tir_transform_lower_warp_memory.py

* fix type

* make it more elegant";" 
 import numpy as np
 import tvm.testing
 import pytest
 
 
 @tvm.testing.requires_cuda
def test_lower_warp_memory_same_thread():
     assert ""tvm_warp_shuffle"" not in fdevice.astext()
 
 
 @tvm.testing.requires_cuda
 def test_lower_warp_memory_divide_by_factor():
     ib = tvm.tir.ir_builder.IRBuilder()
     bx = te.thread_axis(""blockIdx.x"")
     tx = te.thread_axis(""threadIdx.x"")
 
     with ib.new_scope():
         ib.scope_attr(bx, ""thread_extent"", 32)
         ib.scope_attr(tx, ""thread_extent"", 32)
         t = ib.allocate(""float32"", 16, name=""t"", scope=""warp"")
         n = ib.allocate(""float32"", 16, name=""n"", scope=""local"")
         n[0] = t[0]
 
     stmt = ib.get()
     func = tvm.tir.PrimFunc([], stmt)
     func = func.with_attr(""from_legacy_te_schedule"", True)
     cuda_target = tvm.target.Target(""cuda"")
     mod = tvm.lower(func, name=""f"")
     mod = tvm.tir.transform.Apply(lambda f: f.with_attr(""target"", cuda_target))(mod)
     with pytest.raises(tvm.error.TVMError, match=""Divide by zero"") as cm:
         tvm.tir.transform.LowerWarpMemory()(mod)[""f_kernel0""]
 
 
 if __name__ == ""__main__"":
     pytest.main([__file__])"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from sqlalchemy import Column, String, BigInteger, JSON
 from sqlalchemy import create_engine
 from sqlalchemy.orm import sessionmaker, declarative_base
 
 
 Base = declarative_base()
 
 
 class PqlEntity(Base):
     __tablename__ = ""entitys""
     id = Column(String(36), primary_key=True, unique=True)
     start = Column(BigInteger)
     end = Column(BigInteger)
     value = Column(JSON)
 
     def __repr__(self):
         return ""<Entity(id='%s', start='%s', end='%s', value='%s')>"" % (
             self.id,
             self.start,
             self.end,
             self.value,
         )
 
 
 def connect_to_db():
     engine = create_engine(""sqlite:///sqlite.db"", echo=True)
     connection = engine.connect()
     return connection, engine
 
 
 def get_session():
     connection, engine = connect_to_db()
     Session = sessionmaker(bind=engine)
     session = Session()
     return session, connection, engine
 
 
 def migrate():
     session, connection, engine = get_session()
     Base.metadata.create_all(engine)
     session.close()
 
 
 def fetch_data_from_query(sql_query: str):
     session, connection, engine = get_session()
     with engine.connect().execution_options(autocommit=True) as conn:
         fetched_data = conn.execute(sql_query)
     session.close()
     return fetched_data
 
 
 def delete_all_rows():
     fetch_data_from_query(""delete  from entitys where TRUE;"")
 
 
 def insert_entity(insert_entity: PqlEntity):
     session, connection, engine = get_session()
     session.add(insert_entity)
     session.commit()
     session.close()
 
 
 def update_end_of_entity_with_id(id, end: int):
     session, connection, engine = get_session()
     session.query(PqlEntity).filter(PqlEntity.id == id).update(
         {PqlEntity.end: end}, synchronize_session=False
     )
     session.commit()
     session.close()
 
 
 def update_end_of_entity_and_in_json_with_id(id, end: int, updated_dict):
     session, connection, engine = get_session()
     session.query(PqlEntity).filter(PqlEntity.id == id).update(
         {PqlEntity.end: end}, synchronize_session=False
     )
     session.query(PqlEntity).filter(PqlEntity.id == id).update(
         {PqlEntity.value: updated_dict}, synchronize_session=False
     )
     session.commit()
     session.close()
 
 
 def get_all_entitys():
     session, connection, engine = get_session()
     list_enttiys: [PqlEntity] = session.query(PqlEntity).all()
     session.close()
     return list_enttiys"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from json import loads
 
 from sqlalchemy import Column, String, BigInteger, JSON, and_
 from sqlalchemy import create_engine
 from sqlalchemy.engine import row, Engine, Connection
 from sqlalchemy.orm import sessionmaker, declarative_base
 
 Base = declarative_base()
 
 
 class PqlEntity(Base):
     __tablename__ = ""entitys""
     id = Column(String(36), primary_key=True, unique=True)
     name = Column(String(256))
     start = Column(BigInteger)
     end = Column(BigInteger)
     value = Column(JSON)
 
     def __repr__(self):
         return ""<Entity(id='%s', name='%s', start='%s', end='%s', value='%s')>"" % (
             self.id,
             self.name,
             self.start,
             self.end,
             self.value,
         )
 
 
 class ExtractCounter(Base):
     __tablename__ = ""extract_counter""
     name = Column(String(36), primary_key=True, unique=True)
     counter_value = Column(BigInteger)
 
 
 def select_counter_from_name(name: str):
     sql_str: str = f""SELECT counter_value FROM extract_counter WHERE name ='{name}';""
     session, connection, engine = connect_to_db()
     result = session.execute(sql_str)
     names: [dict] = [row[0] for row in result]
     # if nix drin dann leeres array names: [dict] = [row[0] for row in result]
     return names
 
 
 def update_counter_by_name(name: str, counter: int):
     from sqlalchemy import update
 
     upd = update(ExtractCounter)
     val = upd.values({""counter_value"": counter})
     cond = val.where(ExtractCounter.name == name)
 
     session, connection, engine = connect_to_db()
     engine.execute(cond)
     session.commit()
     session.close()
     connection.close()
 
 
 def insert_save_extract(entity: ExtractCounter):
     session, connection, engine = connect_to_db()
     try:
         session.add(entity)
         session.commit()
         session.close()
         connection.close()
     except KeyboardInterrupt:
         session.close()
         connection.close()
 
 
 def connect_to_db():
     engine: Engine = create_engine(""sqlite:///sqlite.db"", echo=False)
     connection: Connection = engine.connect()
     Session = sessionmaker(bind=engine)
     session: Session = Session()
     return session, connection, engine
 
 
 def migrate():
     session, connection, engine = connect_to_db()
     Base.metadata.create_all(engine)
     session.close()
     connection.close()
 
 
 def fetch_data_from_query(sql_query: str):
     session, connection, engine = connect_to_db()
     with engine.connect().execution_options(autocommit=True) as conn:
         fetched_data = conn.execute(sql_query)
     session.close()
     return fetched_data
 
 
 def delete_all_rows_from_pql_entity():
     fetch_data_from_query(""DELETE FROM entitys WHERE TRUE;"")
     # delete_all_rows_from_counter_saving()
 
 
 def delete_all_rows_from_counter_saving():
     fetch_data_from_query(""DELETE FROM extract_counter WHERE TRUE;"")
 
 
 def insert_entity(insert_entity: PqlEntity):
     session, connection, engine = connect_to_db()
     try:
         session.add(insert_entity)
         session.commit()
         session.close()
         connection.close()
     except KeyboardInterrupt:
         session.close()
         connection.close()
 
 
 def update_end_of_entity_with_id(id, end: int):
     session, connection, engine = connect_to_db()
     session.query(PqlEntity).filter(PqlEntity.id == id).update(
         {PqlEntity.end: end}, synchronize_session=False
     )
     session.commit()
     session.close()
     connection.close()
 
 
 def update_end_of_entity_and_in_json_with_id(id: str, updated_dict: dict):
     session, connection, engine = connect_to_db()
     session.query(PqlEntity).filter(PqlEntity.id == id).update(
         {PqlEntity.end: updated_dict.get(""end"")}, synchronize_session=False
     )
     session.query(PqlEntity).filter(PqlEntity.id == id).update(
         {PqlEntity.value: updated_dict}, synchronize_session=False
     )
     session.commit()
     session.close()
     connection.close()
 
 
 def get_entity_with_name(name: str):
     session, connection, engine = connect_to_db()
     list_entitys: [PqlEntity] = session.query(PqlEntity).filter_by(name=name).all()
     session.close()
     connection.close()
     return list_entitys
 
 
 def get_entity_with_name_and_predicate(value_filter: dict):
     ### property geht nicht. es muss ein value sein. hier dann switchcase oder so
     session, connection, engine = connect_to_db()
     list_entitys: [row] = (
         session.query(PqlEntity.value).filter_by(and_(value_filter)).all()
     )
     session.close()
     connection.close()
     return list_entitys
 
 
 def get_all_entitys():
     session, connection, engine = connect_to_db()
     list_entitys: [PqlEntity] = session.query(PqlEntity).all()
     session.close()
     connection.close()
     return list_entitys
 
 
 def get_all_names():
     session, connection, engine = connect_to_db()
     list_entitys: [row] = session.query(PqlEntity.name).distinct().all()
     session.close()
     connection.close()
     return list_entitys
 
 
 def get_values_from_name(name: str):
     session, connection, engine = connect_to_db()
     list_entitys: [row] = session.query(PqlEntity.value).filter_by(name=name).all()
     session.close()
     connection.close()
     return list_entitys
 
 
 def get_values_from_all_entitys_as_dict(name: str):
     dict_array: [dict] = []
     el: row
     for el in get_values_from_name(name):
         dict_array.append(el._mapping.get(""value""))
     return dict_array
 
 
 def execute_raw_sql_query(name: str, value_json, groupBy: str):
     session, connection, engine = connect_to_db()
     # result = session.execute(f""SELECT value FROM entitys WHERE name = '{name}' AND {value}"")
     # result=session.execute(f""SELECT * FROM entitys WHERE name = 'Cycle' AND json_extract(value, '$.start') = 0 {groupBy}"")
     result = session.execute(
         f""SELECT json_extract(value, '$')  FROM entitys WHERE entitys.name ='{name}' AND {value_json} {groupBy}""
     )
     # session.query(PqlEntity.value).filter(PqlEntity.value['start'] == '129', name == 'Cycle').all()
     # session.query(PqlEntity.value).filter(and_(PqlEntity.name == ""Cycle"", PqlEntity.start == 129)).all()
     # session.query(PqlEntity.value).filter(and_(PqlEntity.name == ""Cycle"", PqlEntity.value[""start""] == ""993"")).all()
     # ({'id': 114, 'uuid': '404e8909-1d5e-45d0-b0a8-734387bb1956', 'start': 993, 'end': 999, 'material_equipped': 11},)
     # session.query(PqlEntity).filter_by(PqlEntity.name == ""Cycle"", PqlEntity.value['id'] == ""5"").all()
     names: [dict] = [loads(row[0]) for row in result]
     session.close()
     connection.close()
     return names"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import random
 from typing import Any
 
 from faker import Faker

     migrate,
     insert_entity,
     PqlEntity,
 )
 
 faker1: Faker = Faker()
def reset(self):
     def reset_all(self):
         self.counter = 0
 
     def __str__(self) -> str:
         return f""AutoGenerated({self.counter + 1})""
 
def is_foreignkey(self) -> bool:
         return False
 
     def apply(self, current_state: dict):
         self.uuid = str(faker1.uuid4())
         # self.uuid = str(uuid.uuid4())
         return self.uuid
 
     def reset(self):
         self.uuid = 0
 
     def reset_all(self):
         self.uuid = 0
 
     def __str__(self) -> str:
         return f""AutoGenerated({self.uuid})""
def apply(self, parent_entity: str, key, end_result: dict):
         }
         if contstraint_entry in self.constraints:
             raise RuntimeError(f""The following: {contstraint_entry} is duplicated"")
             return
         for element in self.constraints:
             if parent_entity_id in element.values():
                 raise RuntimeError(
                     f""The following ID: {parent_entity_id} from {parent_entity} is duplicated!""
                 )
                 return
         self.constraints.append(contstraint_entry)
         return_dict: dict = {f""{key}"": foreignkey_id}
         return return_dict
class SyncDatabase:
     def __init__(self, end_result) -> None:
         super().__init__()
         self.end_result = end_result
 
     def synchronize_database(self):
         migrate()
def synchronize_database(self):
                 insert_entity(
                     PqlEntity(
                         id=en.get(""uuid""),
                         start=en.get(""start""),
                         end=en.get(""end""),
                         value=en,
                     )
                 )
 
 
 class StateProcessor(object):
     def __init__(self, config) -> None:
         super().__init__()
         self.config = config
         self.context = {}
         self.end_result = {}
         self.init_ids_in_foreignkeys()
 
     def __exctract_all_items__(self, entity, state):
         all_items: dict = {
             k: v.apply(state)
             for k, v in self.config.get(entity).get(""fields"").items()
             if not v.is_foreignkey()
         }
         all_items.update(
             {""start"": state.get(""timestamp""), ""end"": state.get(""timestamp"")}
         )
def init_context(self, first_element_of_stream):
         The dict gets its Entitys here and get empty Arrays for appending each entry in the array later
         The first Element of the Stream will be append here, so we check if the acctual element is diffrent form this init  one""""""
         self.changed = set()
         for entity in self.config.keys():
             x = {
                 k: v.apply(first_element_of_stream)
                 for k, v in self.config.get(entity).get(""fields"").items()
                 if not v.is_foreignkey()
             }
             x.update(
                 {
                     ""start"": first_element_of_stream.get(""timestamp""),
def process_state(self, state):
             self.context[entity] = current_context
 
         self.process_foreingkeys()
 
     def process_foreingkeys(self):
         """"""First loop is for all Entitys which changed
def process_foreingkeys(self):
         After that the check is if this Entity is the same which has the foreinkey in the Config file
         If so it is applyed
         If not the other Entity has to be the Entity which has the Foreignky so it apllys
         If BothcChange the Entity it self or the Related Entity one em will be removed so per change the apply method will be call one time""""""
         removed = set()
         for element in self.changed:
             if not element in removed:
def process_foreingkeys(self):
     def get_result(self):
         return self.end_result
 
 
 if __name__ == ""__main__"":
     """"""
def get_config() -> dict:
         }
 
     # print(states)
     processor = StateProcessor(get_config())
     processor.init_context(states[0])
     for state in states:
         processor.process_state(state)
def get_config() -> dict:
     #     print(f""Entity: {entity}"")
     #     print(f""Result Dict 2: {result_dict_2}"")
 
     db_sync = SyncDatabase(end_result)
     db_sync.synchronize_database()
 
     result = {"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import random
 import uuid
 from typing import Any
 
 from faker import Faker

     migrate,
     insert_entity,
     PqlEntity,
     update_end_of_entity_and_in_json_with_id,
     select_counter_from_name,
     update_counter_by_name,
     insert_save_extract,
     ExtractCounter,
 )
 
 faker1: Faker = Faker()
def reset(self):
     def reset_all(self):
         self.counter = 0
 
     def set_counter(self, counter: int):
         self.counter = counter
 
     def __str__(self) -> str:
         return f""AutoGenerated({self.counter + 1})""
 
def is_foreignkey(self) -> bool:
         return False
 
     def apply(self, current_state: dict):
         self.uuid = str(uuid.uuid4())
         return self.uuid
 
     def reset(self):
         self.uuid = """"
 
     def reset_all(self):
         self.uuid = """"
 
     def __str__(self) -> str:
         return f""AutoGenerated({self.uuid})""
def apply(self, parent_entity: str, key, end_result: dict):
         }
         if contstraint_entry in self.constraints:
             raise RuntimeError(f""The following: {contstraint_entry} is duplicated"")
 
         for element in self.constraints:
             if parent_entity_id in element.values():
                 raise RuntimeError(
                     f""The following ID: {parent_entity_id} from {parent_entity} is duplicated!""
                 )
 
         self.constraints.append(contstraint_entry)
         return_dict: dict = {f""{key}"": foreignkey_id}
         return return_dict
class SyncDatabase:
     def __init__(self, end_result) -> None:
         super().__init__()
         self.end_result = end_result
         self.current_state = end_result
         self.sync_index = {}
 
     def synchronize_database(self):
         migrate()
def synchronize_database(self):
                 insert_entity(
                     PqlEntity(
                         id=en.get(""uuid""),
                         name=el,
                         start=en.get(""start""),
                         end=en.get(""end""),
                         value=en,
                     )
                 )
 
     def sync_database_with_index(self):
         migrate()
         en: dict
         self.init_sync_index()
         for el in self.end_result:
             dict_element_new: dict = self.end_result.get(el)[-1]
             # neues element dazu gekommen
             if self.sync_index[el] != dict_element_new.get(""uuid""):
                 insert_entity(
                     PqlEntity(
                         id=dict_element_new.get(""uuid""),
                         name=el,
                         start=dict_element_new.get(""start""),
                         end=dict_element_new.get(""end""),
                         value=dict_element_new,
                     )
                 )
                 if len(self.end_result.get(el)) >= 2:
                     dict_element_old = self.end_result.get(el)[-2]
                     if self.sync_index[el] == dict_element_old.get(""uuid""):
                         update_end_of_entity_and_in_json_with_id(
                             dict_element_old.get(""uuid""), dict_element_old
                         )
                         self.sync_index[el] = dict_element_new.get(""uuid"")
                 else:
                     self.sync_index[el] = dict_element_new.get(""uuid"")
                 # kein neues element nur das alte updaten.
             else:
                 update_end_of_entity_and_in_json_with_id(
                     dict_element_new.get(""uuid""), dict_element_new
                 )
 
     def init_sync_index(self):
         if self.end_result:
             if not self.sync_index:
                 for el in self.end_result:
                     self.sync_index[el] = """"
 
     def insert_last_dicts_from_each_entity(self):
         for el in self.end_result:
             new_element: dict = self.end_result.get(el)[-1]
             if new_element.get(""uuid"") != self.current_state.get(el)[-1].get(""uuid""):
                 insert_entity(
                     PqlEntity(
                         id=new_element.get(""uuid""),
                         name=el,
                         start=new_element.get(""start""),
                         end=new_element.get(""end""),
                         value=new_element,
                     )
                 )
                 self.current_state[el] = new_element
 
     def set_end_result(self, end_result):
         self.end_result = end_result
 
     def append_end_result(self, end_result):
         self.end_result.append(end_result)
 
 
 class StateProcessor(object):
     def __init__(self, config) -> None:
         super().__init__()
         migrate()
         self.config = config
         self.context = {}
         self.end_result = {}
         self.init_ids_in_foreignkeys()
 
     def __exctract_all_items__(self, entity, state):
         all_items: dict = {}
         counter: int = 0
         for k, v in self.config.get(entity).get(""fields"").items():
             if not v.is_foreignkey():
                 if type(v) is AutoGeneratedField:
                     counter = v.apply(state)
                     update_counter_by_name(entity, counter)
                     all_items.update({k: counter})
                 else:
                     all_items.update({k: v.apply(state)})
         all_items.update(
             {""start"": state.get(""timestamp""), ""end"": state.get(""timestamp"")}
         )
def init_context(self, first_element_of_stream):
         The dict gets its Entitys here and get empty Arrays for appending each entry in the array later
         The first Element of the Stream will be append here, so we check if the acctual element is diffrent form this init  one""""""
         self.changed = set()
         counter: int = 0
         for entity in self.config.keys():
             x: dict = {}
             for k, v in self.config.get(entity).get(""fields"").items():
                 if not v.is_foreignkey():
                     if type(v) is AutoGeneratedField:
                         saved_counter = select_counter_from_name(entity)
                         if saved_counter:
                             v.set_counter(saved_counter[0])
                             counter = v.apply(first_element_of_stream)
                             update_counter_by_name(entity, counter)
                             x.update({k: counter})
                         else:
                             counter = v.apply(first_element_of_stream)
                             insert_save_extract(
                                 ExtractCounter(name=entity, counter_value=counter)
                             )
                             x.update({k: counter})
                     else:
                         x.update({k: v.apply(first_element_of_stream)})
             x.update(
                 {
                     ""start"": first_element_of_stream.get(""timestamp""),
def process_state(self, state):
             self.context[entity] = current_context
 
         self.process_foreingkeys()
         return self.end_result
 
     def process_foreingkeys(self):
         """"""First loop is for all Entitys which changed
def process_foreingkeys(self):
         After that the check is if this Entity is the same which has the foreinkey in the Config file
         If so it is applyed
         If not the other Entity has to be the Entity which has the Foreignky so it apllys
         # If BothcChange the Entity it self or the Related Entity one em will be removed so per change the apply method will be call one time""""""
         removed = set()
         for element in self.changed:
             if not element in removed:
def process_foreingkeys(self):
     def get_result(self):
         return self.end_result
 
     def get_last_result(self):
         last_values: dict = {}
         for el in self.end_result:
             last_values.update({el: self.end_result.get(el)[-1]})
         return last_values
 
 
 if __name__ == ""__main__"":
     """"""
def get_config() -> dict:
         }
 
     # print(states)
     processor: StateProcessor = StateProcessor(get_config())
     processor.init_context(states[0])
     for state in states:
         processor.process_state(state)
def get_config() -> dict:
     #     print(f""Entity: {entity}"")
     #     print(f""Result Dict 2: {result_dict_2}"")
 
     db_sync: SyncDatabase = SyncDatabase(end_result)
     db_sync.synchronize_database()
 
     result = {"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import uuid
 from datetime import datetime, timedelta
 from random import uniform
 from typing import Iterable, List, Dict
 
 from pql import (
     Query,
     Projection,
def create_cycles(n):
     for i in range(0, n):
         cycle_duration: timedelta = timedelta(seconds=uniform(20.0, 30.0))
         pause: timedelta = timedelta(seconds=uniform(1.0, 60.0))
         cycles.append(
             {
                 ""id"": i,
                 ""start"": timestamp,
                 ""end"": timestamp + cycle_duration,
                 ""machine"": ""LHL 01"",
             }
         )
 
         timestamp = timestamp + cycle_duration + pause
 
     return cycles
def create_tools(n):
     for i in range(0, n):
         duration: timedelta = timedelta(minutes=uniform(20.0, 40.0))
         pause: timedelta = timedelta(minutes=uniform(5.0, 15.0))
         tools.append(
             {
                 ""id"": uuid.uuid4(),
                 ""name"": f""Tool {i}"",
                 ""start"": timestamp,
                 ""end"": timestamp + duration,
                 ""machine"": ""LHL 01"",
             }
         )
 
         timestamp = timestamp + duration + pause
 
     return tools
def create_materials(n):
     for i in range(0, n):
         duration: timedelta = timedelta(minutes=uniform(10.0, 20.0))
         pause: timedelta = timedelta(minutes=uniform(0.0, 5.0))
         materials.append(
             {
                 ""id"": uuid.uuid4(),
                 ""material"": f""Material {i % 2}"",
                 ""start"": timestamp,
                 ""end"": timestamp + duration,
                 ""machine"": ""LHL 01"",
             }
         )
 
         timestamp = timestamp + duration + pause
 
     return materials
 
 
 def print_index_of_array(array: []):
     element: dict
     for element in array:
         print(element)
 
 
 generated_tools = create_tools(5)
 generated_cycles = create_cycles(100)
 generated_materials = create_materials(10)
 
 
 # print_index_of_array(generated_tools)
 # print_index_of_array(generated_cycles)
 # print_index_of_array(generated_materials)
 
 
 def get_all_assets(name: str) -> Iterable[dict]:
     if name == ""Tools"":
         return generated_tools
     elif name == ""Cycles"":
         return generated_cycles
     elif name == ""Materials"":
         return generated_materials
     else:
         raise Exception("""")
     # Root Context is what defines the boundaries and where to read the entities from
 
 
 class InMemoryAssetRetriever:
def get_group_for_object(o):
         return assets
 
 
 if __name__ == ""__main__"":
     # SELECT t.name, COUNT(SELECT c FROM Cycles [WHERE t.start <= c.start AND c.start < t.end]),
     # LIST(SELECT m.material FROM Materials [WHERE t.start <= m.start AND m.start < t.end])
     # FROM Tools
     asset_retriever: InMemoryAssetRetriever = InMemoryAssetRetriever()
     # context = RootContext(asset_retriever.get_assets, lambda s: agg_functions.get(s))
 
     context: RootContext = RootContext(
         asset_retriever.get_assets, lambda s: agg_functions.get(s)
     )
     # Concrete Example:
     # SELECT t.name, COUNT(SELECT c FROM Cycles) AS ""cycles"" FROM Tools
     print(context)
     query: Query = Query(
         [
             Projection(""id""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycles""), name=""cycles""),
         ],
         ""Tools"",
     )
     results: List[Dict] = query.execute(context)
 
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycles""), name=""cycles""),
         ],
         ""Tools"",
         EqPredicate(""name"", ""Tool 0""),
     )
     results: List[Dict] = query.execute(context)
     print(results)
def get_group_for_object(o):
     # FROM Tools AS t
     query: Query = Query(
         [
             Projection(""name""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycles""), name=""cycles""),
             Aggregation(
                 ""flatten"", Query([Projection(""material"")], ""Materials""), name=""products""
             ),
             SubQuery(
                 Query(
                     [
                         Projection(""material""),
                         Aggregation(
                             ""count"", Query([Projection(""id"")], ""Cycles""), name=""cycles""
                         ),
                     ],
                     ""Materials"",
                 ),
                 name=""material_and_count"",
             ),
         ],
         ""Tools"",
     )
 
     results: List[Dict] = query.execute(context)
 
     # The result is a list of dicts representing a (probably nested) table
def get_group_for_object(o):
     # FROM Materials
     query: Query = Query(
         [
             Projection(""material""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycles""), name=""cycles""),
         ],
         ""Materials"",
         group_by_clause=[Projection(""material"")],
     )
 
     results: List[Dict] = query.execute(context)
 
     # The result is a list of dicts representing a (probably nested) table
     print(results)
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""material""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Materials"",
     )
     results_all_materials: List[Dict] = query.execute(context)
 
     assert results_all_materials == generated_materials
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""name""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Tools"",
     )
     results_all_tools: List[Dict] = query.execute(context)
 
     assert results_all_tools == generated_tools
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Cycles"",
     )
     results_all_cycles: List[Dict] = query.execute(context)
 
     assert results_all_cycles == generated_cycles
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Cycles"",
         EqPredicate(""id"", generated_cycles[0].get(""id"")),
     )
     results_eq_cycles: List[Dict] = query.execute(context)
 
     assert results_eq_cycles[0] == generated_cycles[0]
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Cycles"",
         LowerPredicate(""id"", 5),
     )
     results_lower_cycle: List[Dict] = query.execute(context)
 
     first_four = generated_cycles[0:5]
     assert results_lower_cycle == first_four
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Cycles"",
         LowerEqPredicate(""id"", 5),
     )
     results_lower_eq_cycles: List[Dict] = query.execute(context)
 
     first_five = generated_cycles[0:6]
     assert results_lower_eq_cycles == first_five
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Cycles"",
         GreaterPredicate(""id"", 5),
     )
     results_greater_cycles: List[Dict] = query.execute(context)
 
     first_four = generated_cycles[6:]
     assert results_greater_cycles == first_four
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Cycles"",
         GreaterEqPredicate(""id"", 5),
     )
     results_greater_eq_cycles: List[Dict] = query.execute(context)
 
     first_five = generated_cycles[5:]
     assert results_greater_eq_cycles == first_five
 
     # Materials
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""material""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Materials"",
         EqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""material""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Materials"",
         LowerPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""material""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Materials"",
         LowerEqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""material""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Materials"",
         GreaterPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""material""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Materials"",
         GreaterEqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
     # Tools
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""name""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Tools"",
         EqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""name""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Tools"",
         LowerPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""name""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Tools"",
         LowerEqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""name""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Tools"",
         GreaterPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""id""),
             Projection(""name""),
             Projection(""start""),
             Projection(""end""),
             Projection(""machine""),
         ],
         ""Tools"",
         GreaterEqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import uuid
 from datetime import datetime, timedelta
 from random import uniform
 from typing import Iterable, List, Dict, Union, Callable
 
 from sqlalchemy.engine import row
 
 from database_methods import (
     PqlEntity,
     get_entity_with_name,
     get_all_names,
     get_values_from_all_entitys_as_dict,
     execute_raw_sql_query,
     get_entity_with_name_and_predicate,
     migrate,
     insert_entity,
     delete_all_rows_from_pql_entity,
     delete_all_rows_from_counter_saving,
 )
 from pql import (
     Query,
     Projection,
def create_cycles(n):
     for i in range(0, n):
         cycle_duration: timedelta = timedelta(seconds=uniform(20.0, 30.0))
         pause: timedelta = timedelta(seconds=uniform(1.0, 60.0))
         element_dict: dict = {
             ""id"": i,
             ""start"": f""{timestamp}"",
             ""end"": f""{timestamp + cycle_duration}"",
             ""machine"": ""LHL 01"",
         }
         cycles.append(element_dict)
         insert_entity(
             PqlEntity(
                 id=element_dict.get(""id""),
                 name=""Cycle"",
                 start=element_dict.get(""start""),
                 end=element_dict.get(""end""),
                 value=element_dict,
             )
         )
         timestamp = timestamp + cycle_duration + pause
 
     return cycles
def create_tools(n):
     for i in range(0, n):
         duration: timedelta = timedelta(minutes=uniform(20.0, 40.0))
         pause: timedelta = timedelta(minutes=uniform(5.0, 15.0))
         element_dict: dict = {
             ""id"": str(uuid.uuid4()),
             ""tool_name"": f""Tool {i}"",
             ""start"": f""{timestamp}"",
             ""end"": f""{timestamp + duration}"",
             ""machine"": ""LHL 01"",
         }
         tools.append(element_dict)
         insert_entity(
             PqlEntity(
                 id=element_dict.get(""id""),
                 name=""ToolEquipped"",
                 start=element_dict.get(""start""),
                 end=element_dict.get(""end""),
                 value=element_dict,
             )
         )
         timestamp = timestamp + duration + pause
 
     return tools
def create_materials(n):
     for i in range(0, n):
         duration: timedelta = timedelta(minutes=uniform(10.0, 20.0))
         pause: timedelta = timedelta(minutes=uniform(0.0, 5.0))
         element_dict: dict = {
             ""id"": str(uuid.uuid4()),
             ""material_name"": f""Material {i % 2}"",
             ""start"": f""{timestamp}"",
             ""end"": f""{timestamp + duration}"",
             ""machine"": ""LHL 01"",
         }
         materials.append(element_dict)
         insert_entity(
             PqlEntity(
                 id=element_dict.get(""id""),
                 name=""MaterialEquipped"",
                 start=element_dict.get(""start""),
                 end=element_dict.get(""end""),
                 value=element_dict,
             )
         )
         timestamp = timestamp + duration + pause
 
     return materials
 
 
 def get_distinct_names_from_db():
     all_names = get_all_names()
     el: row
     names: [str] = []
     for el in all_names:
         names.append(el._mapping.get(""name""))
     return names
 
 
 def get_sorted_entitys():
     all_names: [str] = get_distinct_names_from_db()
     end_dict: dict = {}
     for el in all_names:
         mid_array = []
         entity: PqlEntity
         for entity in get_entity_with_name(el):
             mid_array.append(entity.value)
         end_dict.update({el: mid_array})
     return end_dict
 
 
 def print_index_of_array(array: []):
     element: dict
     for element in array:
         print(element)
 
 
 migrate()
 delete_all_rows_from_pql_entity()
 delete_all_rows_from_counter_saving()
 create_tools(5)
 create_cycles(100)
 create_materials(10)
 
 all_assets = get_sorted_entitys()
 generated_tools = all_assets.get(""ToolEquipped"")
 generated_cycles = all_assets.get(""Cycle"")
 generated_materials = all_assets.get(""MaterialEquipped"")
 
 
 def get_all_assets(name: str) -> Iterable[dict]:
     try:
         return all_assets.get(name)
     except Exception as ex:
         print(ex)
         raise Exception("""")
 
 
 class InMemoryAssetRetriever:
def get_group_for_object(o):
         return assets
 
 
 class DbMemoryAssetRetriever:
     def get_assest(
         self, asset_type, where_clause: Union[Predicate, Callable], group_by_clause
     ):
         assets = get_values_from_all_entitys_as_dict(asset_type)
         group_by_sql: str = """"
         if group_by_clause:
             print(group_by_clause)
             group_by_sql = ""GROUP BY ""
             for el in group_by_clause:
                 group_by_sql += el.field
         if where_clause:
             if isinstance(where_clause, Predicate):
                 where_clause_type = type(where_clause)
                 assets = execute_raw_sql_query(
                     asset_type,
                     self.wrap_pql_predicate_to_sqll(
                         where_clause_type,
                         where_clause.property,
                         where_clause.value,
                         asset_type,
                     ),
                     group_by_sql,
                 )
                 # for later usage if where_clause is an list of Where clauses.
                 # where_clauses = [element for element in where_clause self.wrap_pql_predicate_to_sql(type(element))]
                 # assets = [element for element in assets if where_clause.check(element)]
             else:
                 assets = [o for o in assets if where_clause(o)]
         return assets
 
     def wrap_name_to_pql_attribute(self, property: str):
         if property == ""id"":
             return PqlEntity.id
         elif property == ""name"":
             return PqlEntity.name
         elif property == ""start"":
             return PqlEntity.start
         elif property == ""end"":
             return PqlEntity.end
         elif property == ""value"":
             return PqlEntity.value
         else:
             return PqlEntity
 
     def wrap_pql_predicate_to_sql(self, test, property, value, name):
         property_sql = self.wrap_name_to_pql_attribute(property)
         if test == EqPredicate:
             return ""PqlEntity.name == name,property_sql == value""
         elif test == GreaterPredicate:
             return ""PqlEntity.name == name,property_sql > value""
         elif test == GreaterEqPredicate:
             return ""PqlEntity.name == name ,property_sql >= value""
         elif test == LowerPredicate:
             return ""PqlEntity.name == name, property_sql < value""
         elif test == LowerEqPredicate:
             return ""PqlEntity.name==name, property_sql <= value""
         else:
             return """"
 
     def wrap_pql_predicate_to_sqll(self, test, property: str, value, name):
         if test == EqPredicate:
             if type(value) == str:
                 return f""json_extract(value, '$.{property}') = '{value}' ""
             else:
                 return f""json_extract(value, '$.{property}') = {value} ""
         elif test == GreaterPredicate:
             if type(value) == str:
                 return f""json_extract(value, '$.{property}') > '{value}' ""
             else:
                 return f""json_extract(value, '$.{property}') > {value} ""
         elif test == GreaterEqPredicate:
             if type(value) == str:
                 return f""json_extract(value, '$.{property}') >= '{value}' ""
             else:
                 return f""json_extract(value, '$.{property}') >= {value} ""
         elif test == LowerPredicate:
             if type(value) == str:
                 return f""json_extract(value, '$.{property}') < '{value}' ""
             else:
                 return f""json_extract(value, '$.{property}') < {value} ""
         elif test == LowerEqPredicate:
             if type(value) == str:
                 return f""json_extract(value, '$.{property}') <= '{value}' ""
             else:
                 return f""json_extract(value, '$.{property}') <= {value} ""
         else:
             return """"
 
 
 if __name__ == ""__main__"":
     # SELECT t.name, COUNT(SELECT c FROM Cycles [WHERE t.start <= c.start AND c.start < t.end]),
     # LIST(SELECT m.material FROM Materials [WHERE t.start <= m.start AND m.start < t.end])
     # FROM Tools
     # * Cycle
     # * MaterialEquipped
     # * ToolEquipped
 
     dbassetRetriever: DbMemoryAssetRetriever = DbMemoryAssetRetriever()
     context: RootContext = RootContext(
         dbassetRetriever.get_assest, lambda s: agg_functions.get(s)
     )
     # Concrete Example:
     # SELECT t.name, COUNT(SELECT c FROM Cycles) AS ""cycles"" FROM Tool
     # a = ctypes.cast(context, ctypes.py_object).value
     # print(a)
     query: Query = Query(
         [
             Projection(""tool_name""),
             Projection(""id""),
             Projection(""uuid""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycle""), name=""cycles""),
         ],
         ""ToolEquipped"",
     )
     results: List[Dict] = query.execute(context)
 
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""tool_name""),
             Projection(""id""),
             Projection(""uuid""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycle""), name=""cycles""),
         ],
         ""ToolEquipped"",
         EqPredicate(""tool_name"", ""Tool 0""),
     )
     results: List[Dict] = query.execute(context)
     print(results)
def get_group_for_object(o):
     # FROM Tools AS t
     query: Query = Query(
         [
             Projection(""tool_name""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycle""), name=""cycles""),
             Aggregation(
                 ""flatten"",
                 Query([Projection(""material_name"")], ""MaterialEquipped""),
                 name=""products"",
             ),
             SubQuery(
                 Query(
                     [
                         Projection(""material_name""),
                         Aggregation(
                             ""count"", Query([Projection(""id"")], ""Cycle""), name=""cycles""
                         ),
                     ],
                     ""MaterialEquipped"",
                 ),
                 name=""material_and_count"",
             ),
         ],
         ""ToolEquipped"",
     )
     #
     results: List[Dict] = query.execute(context)
 
     # The result is a list of dicts representing a (probably nested) table
def get_group_for_object(o):
     # FROM Materials
     query: Query = Query(
         [
             Projection(""material_name""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycle""), name=""cycles""),
         ],
         ""MaterialEquipped"",
         group_by_clause=[Projection(""material_name"")],
     )
     #
     results: List[Dict] = query.execute(context)
 
     # The result is a list of dicts representing a (probably nested) table
     print(results)
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
     )
     results_all_materials: List[Dict] = query.execute(context)
 
     assert results_all_materials == generated_materials
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
     )
     results_all_tools: List[Dict] = query.execute(context)
 
     assert results_all_tools == generated_tools
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
     )
     results_all_cycles: List[Dict] = query.execute(context)
 
     assert results_all_cycles == generated_cycles
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         EqPredicate(""start"", generated_cycles[0].get(""start"")),
     )
     results_eq_cycles: List[Dict] = query.execute(context)
 
     assert results_eq_cycles[0] == generated_cycles[0]
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         LowerPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_lower_cycle: List[Dict] = query.execute(context)
 
     first_four = generated_cycles[0 : int(len(generated_cycles) / 2)]
     assert results_lower_cycle == first_four
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         LowerEqPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_lower_eq_cycles: List[Dict] = query.execute(context)
 
     first_five = generated_cycles[0 : int(len(generated_cycles) / 2) + 1]
     assert results_lower_eq_cycles == first_five
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         GreaterPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_greater_cycles: List[Dict] = query.execute(context)
 
     first_four = generated_cycles[int(len(generated_cycles) / 2) + 1 :]
     assert results_greater_cycles == first_four
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         GreaterEqPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_greater_eq_cycles: List[Dict] = query.execute(context)
 
     first_five = generated_cycles[int(len(generated_cycles) / 2) :]
     assert results_greater_eq_cycles == first_five
 
     # Materials
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         EqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         LowerPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         LowerEqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         GreaterPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         GreaterEqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
def get_group_for_object(o):
     # Tools
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         EqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         LowerPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         LowerEqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         GreaterPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
def get_group_for_object(o):
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         GreaterEqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" # PLC Server Settings
 from plc_server_client import ApiClient, Configuration
 from plc_server_client.api.plc_rest_controller_api import PlcRestControllerApi
 from plc_server_client.model.plc_read_request import PlcReadRequest
 from plc_server_client.model.plc_read_response import PlcReadResponse
 
 from extract import (
     AutoGeneratedUUID,
     Parameter,
def read_response(controller_api):
 
 def make_dict_from_response(response: PlcReadResponse):
     element_dict: dict = {}
     for i in range(0, len(response.fields)):
         element_dict.update(
             {response.fields[i].get(""field""): response.fields[i].get(""value"")}
         )
     element_dict.update({""timestamp"": n})
     return element_dict
 
 
 if __name__ == ""__main__"":
     # thread der alle x sec ne anfrage macht.
     api_client = get_api_client()
     controller_api = get_controller_api(api_client)
 
     # threading.Timer(1.0,read_response,[controller_api]).start()
     end_result = []
     for n in range(0, 100):
         end_result.append(make_dict_from_response(read_response(controller_api)))
 
     def get_config() -> dict:
         return {
             ""Cycle"": {
                 ""fields"": {
                     ""uuid"": AutoGeneratedUUID(),
                     ""id"": AutoGeneratedField(),
                     ""druck"": Parameter(""%DB444:4.0:REAL""),
                     ""cycle"": Parameter(""%DB4560:12.0:UDINT""),
                 },
                 ""primary_key"": ""id"",
             },
             # CREATE TABLE Cycle (id BIGINT SERIAL PRIMARY KEY, material_equipped BIGINT NON NULL)
             # CREATE FOREIGN KEY ...
             ""ToolEquipped"": {
                 ""fields"": {
                     ""id"": AutoGeneratedField(),
def get_config() -> dict:
             },
         }
 
     # print(states)
     processor = StateProcessor(get_config())
     processor.init_context(end_result[0])
     for state in end_result:
         processor.process_state(state)
     result_end = processor.get_result()
     db_sync = SyncDatabase(result_end)
     db_sync.synchronize_database()"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" # PLC Server Settings
 from datetime import datetime
 from time import sleep
 
 from plc_server_client import ApiClient, Configuration
 from plc_server_client.api.plc_rest_controller_api import PlcRestControllerApi
 from plc_server_client.model.plc_read_request import PlcReadRequest
 from plc_server_client.model.plc_read_response import PlcReadResponse
 
 from database_methods import get_all_entitys
 from extract import (
     AutoGeneratedUUID,
     Parameter,
def read_response(controller_api):
 
 def make_dict_from_response(response: PlcReadResponse):
     element_dict: dict = {}
 
     for i in range(0, len(response.fields)):
         element_dict.update(
             {response.fields[i].get(""field""): response.fields[i].get(""value"")}
         )
     element_dict.update({""timestamp"": f""{datetime.now()}""})
     return element_dict
 
 
 if __name__ == ""__main__"":
     # thread der alle x sec ne anfrage macht.
     api_client: ApiClient = get_api_client()
     controller_api: PlcRestControllerApi = get_controller_api(api_client)
 
     def get_config() -> dict:
         return {
             ""Cycle"": {
                 ""fields"": {
                     ""uuid"": AutoGeneratedUUID(),
                     ""id"": AutoGeneratedField(),
                     # ""druck"": Parameter(""%DB444:4.0:REAL""),
                     ""cycle"": Parameter(""%DB4560:12.0:UDINT""),
                 },
                 ""primary_key"": ""id"",
             },
             ""ToolEquipped"": {
                 ""fields"": {
                     ""id"": AutoGeneratedField(),
def get_config() -> dict:
             },
         }
 
     # threading.Timer(1.0,read_response,[controller_api]).start()
     end_result = []
     end_result.append(make_dict_from_response(read_response(controller_api)))
     processor: StateProcessor = StateProcessor(get_config())
     processor.init_context(end_result[0])
     db_sync: SyncDatabase = SyncDatabase(processor.get_result())
     db_sync.sync_database_with_index()
     sleep(1)
     while True:
         end_result.append(make_dict_from_response(read_response(controller_api)))
         processor.process_state(end_result[-1])
         print(end_result[-1])
         db_sync.set_end_result(processor.get_result())
         db_sync.sync_database_with_index()
         sleep(0.1)
     arrie = get_all_entitys()"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";"def get_aggregate_function(self, name: str):
         pass
 
 
 class RootContext(Context):
     def __init__(self, entity_list_resolver, aggregate_functions):
         super().__init__()
def __init__(
         self.group_by_clause = group_by_clause
 
     def execute(self, context: Context) -> List[Dict]:
         objects = context.list_entity(
             self.entity, self.where_clause, self.group_by_clause
         )
 
         results = []
         for o in objects:
             ctx = context.create_query_context(o)
             single_result: dict = dict([(s.name, s.execute(ctx)) for s in self.selects])
             results.append(single_result)
 
         return results
 
 "
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";"def get_aggregate_function(self, name: str):
         pass
 
 
 #
 class RootContext(Context):
     def __init__(self, entity_list_resolver, aggregate_functions):
         super().__init__()
def __init__(
         self.group_by_clause = group_by_clause
 
     def execute(self, context: Context) -> List[Dict]:
         ##wieso machen wir die Logic von InMemoryAssetRetriever nicht hier rein???
         ##wei lhier würde es ja Sinn machen und hingehören wenn ich das Query excute soll es mit ja die Entitys zurück geben
         objects = context.list_entity(
             self.entity, self.where_clause, self.group_by_clause
         )
         results = []
         for o in objects:
             ctx = context.create_query_context(o)
             for s in self.selects:
                 if s.name == ""*"":
                     results.append(s.execute(ctx))
                 else:
                     results.append(dict([(s.name, s.execute(ctx))]))
         return results
 
 "
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import random
 from time import sleep
 from typing import List, Dict
 
 from database_methods import (
     delete_all_rows_from_pql_entity,
     migrate,
     delete_all_rows_from_counter_saving,
 )
 from extract import (
     StateProcessor,
     SyncDatabase,
     Parameter,
     AutoGeneratedUUID,
     AutoGeneratedManyToOne,
     AutoGeneratedField,
 )
 from main import get_sorted_entitys, DbMemoryAssetRetriever
 from pql import (
     RootContext,
     agg_functions,
     Query,
     Projection,
     EqPredicate,
     GreaterEqPredicate,
     GreaterPredicate,
     LowerEqPredicate,
     LowerPredicate,
 )
 
 
 def intiStates():
     material_id = 1
     tool_id = 1
     cycle_id = 1
     old_cycle_id = 1
 
     states = []
 
     random.seed(1)
 
     for t in range(0, 1000):
 
         if random.uniform(0.0, 100.0) < 10.0:
             cycle_id += 1
         current_cycle_number = cycle_id
 
         if random.uniform(0.0, 50.0) < 1.0:
             if old_cycle_id != current_cycle_number:
                 tool_id += 1
         current_tool = f""Tool {tool_id}""
         if random.uniform(0.0, 100.0) < 1.0:
             if old_cycle_id != current_cycle_number:
                 material_id = (material_id + 1) % 3
         current_material = f""Material {material_id}""
 
         old_cycle_id = current_cycle_number
         state = {
             ""timestamp"": t,
             ""material_name"": current_material,
             ""material_type"": current_material,
             ""tool_name"": current_tool,
             ""cycle_number"": current_cycle_number,
         }
 
         # print(f""Aktueller Zustand: {state}"")
 
         states.append(state)
     return states
 
 
 def get_config() -> dict:
     return {
         ""Cycle"": {
             ""fields"": {
                 # ""id"": AutoGeneratedField(),
                 ""id"": Parameter(""cycle_number""),
                 # ""cycle"": Parameter(""cycle_number""),
                 ""uuid"": AutoGeneratedUUID(),
                 ""material_equipped"": AutoGeneratedManyToOne(""MaterialEquipped"", ""id""),
             },
             ""primary_key"": ""id"",
         },
         # CREATE TABLE Cycle (id BIGINT SERIAL PRIMARY KEY, material_equipped BIGINT NON NULL)
         # CREATE FOREIGN KEY ...
         ""ToolEquipped"": {
             ""fields"": {
                 ""id"": AutoGeneratedField(),
                 ""uuid"": AutoGeneratedUUID(),
                 ""tool_name"": Parameter(""tool_name""),
             },
             ""primary_key"": ""id"",
         },
         ""MaterialEquipped"": {
             ""fields"": {
                 ""id"": AutoGeneratedField(),
                 ""uuid"": AutoGeneratedUUID(),
                 ""material_name"": Parameter(""material_name""),
                 ""material_typ"": Parameter(""material_type""),
             },
             ""primary_key"": ""id"",
         },
     }
 
 
 def test_one():
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     states = intiStates()
     processor: StateProcessor = StateProcessor(get_config())
     processor.init_context(states[0])
     db_sync: SyncDatabase = SyncDatabase(processor.get_result())
     db_sync.sync_database_with_index()
     for state in states:
         processor.process_state(state)
         db_sync.set_end_result(processor.get_result())
         db_sync.sync_database_with_index()
     end_result = processor.get_result()
     end_result_db_values = get_sorted_entitys()
 
     assert end_result_db_values == end_result
 
     generated_tools = end_result.get(""ToolEquipped"")
     generated_cycles = end_result.get(""Cycle"")
     generated_materials = end_result.get(""MaterialEquipped"")
 
     dbassetRetriever: DbMemoryAssetRetriever = DbMemoryAssetRetriever()
     context: RootContext = RootContext(
         dbassetRetriever.get_assest, lambda s: agg_functions.get(s)
     )
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
     )
     results_all_materials: List[Dict] = query.execute(context)
 
     assert results_all_materials == generated_materials
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
     )
     results_all_tools: List[Dict] = query.execute(context)
 
     assert results_all_tools == generated_tools
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
     )
     results_all_cycles: List[Dict] = query.execute(context)
 
     assert results_all_cycles == generated_cycles
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         EqPredicate(""start"", generated_cycles[0].get(""start"")),
     )
     results_eq_cycles: List[Dict] = query.execute(context)
 
     assert results_eq_cycles[0] == generated_cycles[0]
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         LowerPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_lower_cycle: List[Dict] = query.execute(context)
 
     first_four = generated_cycles[0 : int(len(generated_cycles) / 2)]
     assert results_lower_cycle == first_four
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         LowerEqPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_lower_eq_cycles: List[Dict] = query.execute(context)
 
     first_five = generated_cycles[0 : int(len(generated_cycles) / 2) + 1]
     assert results_lower_eq_cycles == first_five
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         GreaterPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_greater_cycles: List[Dict] = query.execute(context)
 
     first_four = generated_cycles[int(len(generated_cycles) / 2) + 1 :]
     assert results_greater_cycles == first_four
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""Cycle"",
         GreaterEqPredicate(
             ""start"", generated_cycles[int(len(generated_cycles) / 2)].get(""start"")
         ),
     )
     results_greater_eq_cycles: List[Dict] = query.execute(context)
 
     first_five = generated_cycles[int(len(generated_cycles) / 2) :]
     assert results_greater_eq_cycles == first_five
 
     # Materials
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         EqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
     )
     results_eq: List[Dict] = query.execute(context)
     assert results_eq[0] == generated_materials[int(len(generated_materials) / 2)]
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         LowerPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
     )
     results_lower_materials: List[Dict] = query.execute(context)
 
     last_half_materials = generated_materials[0 : int(len(generated_materials) / 2)]
     assert results_lower_materials == last_half_materials
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         LowerEqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
     )
     results_lower_eq_materials: List[Dict] = query.execute(context)
 
     last_half_plus_one_materials = generated_materials[
         0 : int(len(generated_materials) / 2) + 1
     ]
     assert results_lower_eq_materials == last_half_plus_one_materials
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         GreaterPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
     )
     results_greater_materials: List[Dict] = query.execute(context)
 
     first_half_materials = generated_materials[int(len(generated_materials) / 2) + 1 :]
     assert results_greater_materials == first_half_materials
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""MaterialEquipped"",
         GreaterEqPredicate(
             ""start"", generated_materials[int(len(generated_materials) / 2)].get(""start"")
         ),
     )
     results_greater_eq_materials: List[Dict] = query.execute(context)
 
     first_half_minus_one_materials = generated_materials[
         int(len(generated_materials) / 2) :
     ]
     assert results_greater_eq_materials == first_half_minus_one_materials
 
     # Tools
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         EqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
     )
     results_eq_tool = query.execute(context)
 
     assert results_eq_tool[0] == generated_tools[int(len(generated_tools) / 2)]
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         LowerPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
     )
     results_tool_lower: List[Dict] = query.execute(context)
 
     first_half_tools = generated_tools[0 : int(len(generated_tools) / 2)]
     assert results_tool_lower == first_half_tools
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         LowerEqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
     )
     results_lower_eq_tools: List[Dict] = query.execute(context)
 
     first_half_plus_one_tools = generated_tools[0 : int(len(generated_tools) / 2) + 1]
     assert results_lower_eq_tools == first_half_plus_one_tools
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         GreaterPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
     )
     results_greater_tools: List[Dict] = query.execute(context)
 
     greater_tools = generated_tools[int(len(generated_tools) / 2) + 1 :]
     assert results_greater_tools == greater_tools
 
     query: Query = Query(
         [
             Projection(""*""),
         ],
         ""ToolEquipped"",
         GreaterEqPredicate(
             ""start"", generated_tools[int(len(generated_tools) / 2)].get(""start"")
         ),
     )
     results_greater_eq_tools: List[Dict] = query.execute(context)
 
     greater_eq_tools = generated_tools[int(len(generated_tools) / 2) :]
     assert results_greater_eq_tools == greater_eq_tools"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from time import sleep
 
 from plc_server_client import ApiClient
 from plc_server_client.api.plc_rest_controller_api import PlcRestControllerApi
 
 from database_methods import (
     migrate,
     delete_all_rows_from_pql_entity,
     delete_all_rows_from_counter_saving,
 )
 from extract import (
     AutoGeneratedUUID,
     AutoGeneratedField,
     Parameter,
     StateProcessor,
     SyncDatabase,
     FieldType,
 )
 from main import get_sorted_entitys
 from plc import (
     get_api_client,
     get_controller_api,
     make_dict_from_response,
     read_response,
 )
 
 api_client: ApiClient = get_api_client()
 controller_api: PlcRestControllerApi = get_controller_api(api_client)
 
 
 def get_config() -> dict:
     return {
         ""Cycle"": {
             ""fields"": {
                 ""uuid"": AutoGeneratedUUID(),
                 ""id"": AutoGeneratedField(),
                 ""cycle"": Parameter(""%DB4560:12.0:UDINT""),
             },
             ""primary_key"": ""id"",
         },
         ""ToolEquipped"": {
             ""fields"": {
                 ""id"": AutoGeneratedField(),
                 ""uuid"": AutoGeneratedUUID(),
                 ""tool_name"": Parameter(""%DB444:6.0:REAL""),
             },
             ""primary_key"": ""id"",
         },
         ""MaterialEquipped"": {
             ""fields"": {
                 ""id"": AutoGeneratedField(),
                 ""uuid"": AutoGeneratedUUID(),
                 ""material_name"": Parameter(""%DB444.DBD22:DINT""),
                 ""material_typ"": Parameter(""%DB444.DBW20:INT""),
             },
             ""primary_key"": ""id"",
         },
     }
 
 
 def test_one():
     # print(get_sorted_entitys())
     # threading.Timer(1.0,read_response,[controller_api]).start()
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     end_result = []
     end_result.append(make_dict_from_response(read_response(controller_api)))
     processor: StateProcessor = StateProcessor(get_config())
     processor.init_context(end_result[0])
     db_sync: SyncDatabase = SyncDatabase(processor.get_result())
     db_sync.sync_database_with_index()
     sleep(1)
     for n in range(1, 12):
         end_result.append(make_dict_from_response(read_response(controller_api)))
         processor.process_state(end_result[-1])
         db_sync.set_end_result(processor.get_result())
         db_sync.sync_database_with_index()
         sleep(1)
     end_result_processor = processor.get_result()
     end_result_db = get_sorted_entitys()
 
     assert end_result_processor == end_result_db
 
     first_timestamp: str = end_result[0].get(""timestamp"")
     last_timestamp: str = end_result[-1].get(""timestamp"")
 
     for el in end_result_db.keys():
         first_ts_db: str = end_result_db.get(el)[0].get(""start"")
         last_ts_db: str = end_result_db.get(el)[-1].get(""start"")
 
         first_ts_processor: str = end_result_processor.get(el)[0].get(""start"")
         last_ts_processor: str = end_result_processor.get(el)[-1].get(""start"")
 
         if len(end_result_db.get(el)) == 1 and len(end_result_processor.get(el)) == 1:
             # spezial Fall start gleich ende sonst nicht
             assert (
                 first_timestamp == first_ts_db == first_ts_processor
             ), f""Assertion Error first Timestamp from PLC Server: {first_timestamp} got first Timestamp from {el} as Database {first_ts_db} and from Processor {first_ts_processor}""
             assert (
                 first_timestamp == last_ts_db == last_ts_processor
             ), f""Assertion Error last Timestamp from PLC Server: {first_timestamp} got first Timestamp from {el} as Database {last_ts_db} and from Processor {last_ts_processor}""
         else:
             assert (
                 first_timestamp == first_ts_db == first_ts_processor
             ), f""Assertion Error first Timestamp from PLC Server: {first_timestamp} got first Timestamp from {el} as Database {first_ts_db} and from Processor {first_ts_processor}""
             assert (
                 last_timestamp == last_ts_db == last_ts_processor
             ), f""Assertion Error last Timestamp from PLC Server: {last_timestamp} got first Timestamp from {el} as Database {last_ts_db} and from Processor {last_ts_processor}""
             assert get_config().get(el).get(""fields"")
 
         for parameter in get_config().get(el).get(""fields""):
             element: FieldType = get_config().get(el).get(""fields"").get(parameter)
             if type(element) == Parameter:
                 first_value_end_result = end_result[0].get(element.field_name)
                 first_value_db_result = end_result_db.get(el)[0].get(parameter)
                 first_value_processor = end_result_processor.get(el)[0].get(parameter)
 
                 last_value_end_result = end_result[-1].get(element.field_name)
                 last_value_db_result = end_result_db.get(el)[-1].get(parameter)
                 last_value_processor = end_result_processor.get(el)[-1].get(parameter)
 
                 if (
                     len(end_result_db.get(el)) == 1
                     and len(end_result_processor.get(el)) == 1
                 ):
                     assert (
                         first_value_end_result
                         == first_value_db_result
                         == first_value_processor
                     ), f""Assertion Error {first_value_end_result} didnt match to result of db: {first_value_db_result} and processor:{first_value_processor}""
 
                     assert (
                         last_value_end_result
                         == first_value_db_result
                         == first_value_processor
                     ), f""Assertion Error {first_value_end_result} didnt match to result of db: {first_value_db_result} and processor:{first_value_processor}""
                 else:
                     assert (
                         first_value_end_result
                         == first_value_db_result
                         == first_value_processor
                     ), f""Assertion Error {first_value_end_result} didnt match to result of db: {first_value_db_result} and processor:{first_value_processor}""
 
                     assert (
                         last_value_end_result
                         == last_value_db_result
                         == last_value_processor
                     ), f""Assertion Error {last_value_end_result} didnt match to result of db: {last_value_db_result} and processor:{last_value_processor}"""
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import pytest
 
 from database_methods import migrate, delete_all_rows, get_all_entitys, PqlEntity
 from extract import SyncDatabase
 
 
 def run_database_processor(end_result):
     migrate()
     delete_all_rows()
     db_sync: SyncDatabase = SyncDatabase(end_result)
     db_sync.synchronize_database()
     return get_all_entitys()"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from database_methods import (
     migrate,
     delete_all_rows_from_pql_entity,
     get_all_entitys,
     PqlEntity,
     delete_all_rows_from_counter_saving,
 )
 from extract import SyncDatabase
 
 
 def run_database_processor(end_result):
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     db_sync: SyncDatabase = SyncDatabase(end_result)
     db_sync.synchronize_database()
     return get_all_entitys()"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from typing import List, Dict
 
 from database_methods import (
     PqlEntity,
     insert_entity,
     delete_all_rows_from_pql_entity,
     migrate,
     delete_all_rows_from_counter_saving,
 )
 from main import DbMemoryAssetRetriever
 from pql import (
     Query,
     Projection,
     Aggregation,
     RootContext,
     agg_functions,
     EqPredicate,
 )
 
 
 def test_one():
     states: dict = {
         ""Cycle"": [
             {
                 ""id"": 21,
                 ""uuid"": ""b1cb1816-7d85-457c-9f32-2a1422d00e17"",
                 ""start"": 1,
                 ""end"": 2,
                 ""material_equipped"": 1,
             },
             {
                 ""id"": 22,
                 ""uuid"": ""ce9c02e4-aafb-4062-97ff-bbaaeca05990"",
                 ""start"": 3,
                 ""end"": 4,
                 ""material_equipped"": 1,
             },
         ],
         ""MaterialEquipped"": [
             {
                 ""id"": 1,
                 ""uuid"": ""5bb7fb0f-d8d9-415b-8725-460b8ce504db"",
                 ""material_name"": ""Material 1"",
                 ""start"": 1,
                 ""end"": 4,
             }
         ],
     }
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     for el in states:
         for en in states.get(el):
             insert_entity(
                 PqlEntity(
                     id=en.get(""uuid""),
                     name=el,
                     start=en.get(""start""),
                     end=en.get(""end""),
                     value=en,
                 )
             )
 
     # Concrete Example:
     # SELECT COUNT(*) FROM Cycles, Material AS m WHERE m.material_name = ""Material 1""
     dbassetRetriever: DbMemoryAssetRetriever = DbMemoryAssetRetriever()
     context: RootContext = RootContext(
         dbassetRetriever.get_assest, lambda s: agg_functions.get(s)
     )
     query: Query = Query(
         [
             Projection(""material_name""),
             Projection(""id""),
             Projection(""uuid""),
             Aggregation(""count"", Query([Projection(""id"")], ""Cycle""), name=""cycles""),
         ],
         ""MaterialEquipped"",
         EqPredicate(""material_name"", ""Material 1""),
     )
     results: List[Dict] = query.execute(context)
     assert results == [
         {""material_name"": ""Material 1""},
         {""id"": 1},
         {""uuid"": ""5bb7fb0f-d8d9-415b-8725-460b8ce504db""},
         {""cycles"": 2},
     ]
 
 
 def test_two():
     states: dict = {
         ""Cycle"": [
             {
                 ""id"": 21,
                 ""uuid"": ""b1cb1816-7d85-457c-9f32-2a1422d00e17"",
                 ""start"": 1,
                 ""end"": 2,
                 ""material_equipped"": 1,
             },
             {
                 ""id"": 22,
                 ""uuid"": ""ce9c02e4-aafb-4062-97ff-bbaaeca05990"",
                 ""start"": 3,
                 ""end"": 4,
                 ""material_equipped"": 1,
             },
         ],
         ""MaterialEquipped"": [
             {
                 ""id"": 1,
                 ""uuid"": ""5bb7fb0f-d8d9-415b-8725-460b8ce504db"",
                 ""material_name"": ""Material 1"",
                 ""start"": 1,
                 ""end"": 4,
             }
         ],
     }
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     for el in states:
         for en in states.get(el):
             insert_entity(
                 PqlEntity(
                     id=en.get(""uuid""),
                     name=el,
                     start=en.get(""start""),
                     end=en.get(""end""),
                     value=en,
                 )
             )
 
     # Concrete Example:
     # SELECT COUNT(*) FROM Cycles, Material AS m WHERE m.material_name = ""Material 1""
     dbassetRetriever: DbMemoryAssetRetriever = DbMemoryAssetRetriever()
     context: RootContext = RootContext(
         dbassetRetriever.get_assest, lambda s: agg_functions.get(s)
     )
     query: Query = Query(
         [
             Aggregation(""count"", Query([Projection(""*"")], ""Cycle""), name=""cycles""),
         ],
         ""Cycle"",
         group_by_clause=[Projection(""Cycle"")],
     )
     results: List[Dict] = query.execute(context)
     assert results == [{""cycles"": 1}, {""cycles"": 1}]"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import json
 from typing import overload
 
 import pytest
 
 import extract
 from database_methods import migrate, delete_all_rows, PqlEntity, get_all_entitys
 from extract import (
     Parameter,
     AutoGeneratedField,

     AutoGeneratedManyToMany,
     AutoGeneratedUUID,
 )
 from faker import Faker
 
 extract.faker1 = Faker()
 Faker.seed(4711)
def run_processor(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
     processor = StateProcessor(get_config())
def test_one():
     ]
 
     results = run_processor(states)
     print(results)
     assert len(results.get(""Cycle"")) == 2
     assert len(results.get(""MaterialEquipped"")) == 1
     assert results.get(""Cycle"")[0].get(""start"") == 1
def test_two():
     ]
 
     results = run_processor(states)
     print(results)
 
     assert len(results.get(""Cycle"")) == 2
     assert len(results.get(""MaterialEquipped"")) == 1
def test_cycle_material_switch():
     ]
 
     results = run_processor(states)
     print(results)
 
     material_dict: dict = {
         ""id"": 1,
def run_processor1(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_tool_and_others_not():
     ]
 
     results = run_processor1(states)
     print(results)
 
     tool_dict: dict = {
         ""ToolEquipped"": [
def run_processor2(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_tool_change_and_relation_with_cycle():
     ]
 
     results = run_processor2(states)
     print(results)
     tool_dict: dict = {
         ""ToolEquipped"": [
             {
def run_processor3(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_tool_change_and_relation_befor_entity():
     ]
 
     results = run_processor3(states)
     print(results)
     assert results == {
         ""Cycle"": [
             {
def run_processor4(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_relation_one_to_one_true():
     ]
 
     results = run_processor4(states)
     print(results)
     assert results == {
         ""Cycle"": [
             {
def test_relation_one_to_one_false():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor4(states)
         print(end_result)
 
 
 def test_relation_one_to_one_false_two():
def test_relation_one_to_one_false_two():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor4(states)
         print(end_result)
 
 
 def test_relation_one_to_one_false_three():
def test_relation_one_to_one_false_three():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor4(states)
         print(end_result)
 
 
 def get_config5() -> dict:
def run_processor5(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_relation_one_to_many_true():
     ]
 
     end_result = run_processor5(states)
     print(end_result)
     assert end_result == {
         ""Cycle"": [
             {
def test_relation_one_to_many_false():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor5(states)
         print(end_result)
 
 
 def get_config6() -> dict:
def run_processor6(states):
     :param states:
     :return:
     """"""
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_relation_many_to_many_true():
     ]
 
     end_result = run_processor6(states)
     print(end_result)
     assert end_result == {
         ""Cycle"": [
             {"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" import pytest
 from faker import Faker
 
 import extract
 from database_methods import (
     migrate,
     delete_all_rows_from_pql_entity,
     delete_all_rows_from_counter_saving,
 )
 from extract import (
     Parameter,
     AutoGeneratedField,

     AutoGeneratedManyToMany,
     AutoGeneratedUUID,
 )
 
 extract.faker1 = Faker()
 Faker.seed(4711)
def run_processor(states):
     :param states:
     :return:
     """"""
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
     processor = StateProcessor(get_config())
def test_one():
     ]
 
     results = run_processor(states)
     assert len(results.get(""Cycle"")) == 2
     assert len(results.get(""MaterialEquipped"")) == 1
     assert results.get(""Cycle"")[0].get(""start"") == 1
def test_two():
     ]
 
     results = run_processor(states)
 
     assert len(results.get(""Cycle"")) == 2
     assert len(results.get(""MaterialEquipped"")) == 1
def test_cycle_material_switch():
     ]
 
     results = run_processor(states)
 
     material_dict: dict = {
         ""id"": 1,
def run_processor1(states):
     :param states:
     :return:
     """"""
 
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_tool_and_others_not():
     ]
 
     results = run_processor1(states)
 
     tool_dict: dict = {
         ""ToolEquipped"": [
def run_processor2(states):
     :param states:
     :return:
     """"""
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_tool_change_and_relation_with_cycle():
     ]
 
     results = run_processor2(states)
     tool_dict: dict = {
         ""ToolEquipped"": [
             {
def run_processor3(states):
     :param states:
     :return:
     """"""
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_tool_change_and_relation_befor_entity():
     ]
 
     results = run_processor3(states)
     assert results == {
         ""Cycle"": [
             {
def run_processor4(states):
     :param states:
     :return:
     """"""
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_relation_one_to_one_true():
     ]
 
     results = run_processor4(states)
     assert results == {
         ""Cycle"": [
             {
def test_relation_one_to_one_false():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor4(states)
 
 
 def test_relation_one_to_one_false_two():
def test_relation_one_to_one_false_two():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor4(states)
 
 
 def test_relation_one_to_one_false_three():
def test_relation_one_to_one_false_three():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor4(states)
 
 
 def get_config5() -> dict:
def run_processor5(states):
     :param states:
     :return:
     """"""
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_relation_one_to_many_true():
     ]
 
     end_result = run_processor5(states)
     assert end_result == {
         ""Cycle"": [
             {
def test_relation_one_to_many_false():
     ]
     with pytest.raises(RuntimeError):
         end_result = run_processor5(states)
 
 
 def get_config6() -> dict:
def run_processor6(states):
     :param states:
     :return:
     """"""
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     extract.faker1 = Faker()
     Faker.seed(4711)
 
def test_relation_many_to_many_true():
     ]
 
     end_result = run_processor6(states)
     assert end_result == {
         ""Cycle"": [
             {"
KO;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from typing import List, Dict
 
 from main import create_tools, create_cycles, create_materials, InMemoryAssetRetriever
 from pql import (
     agg_functions,
     RootContext,

     SubQuery,
 )
 
 generated_tools = create_tools(5)
 generated_cycles = create_cycles(100)
 generated_materials = create_materials(10)
 
 asset_retriever = InMemoryAssetRetriever()
 context = RootContext(asset_retriever.get_assets, lambda s: agg_functions.get(s))
 
 
 def test_query():
     query: Query = Query(
         [
             Projection(""name""),
             Aggregation(""count"", Query([Projection(""*"")], ""Cycles""), name=""cycles""),
             Aggregation(
                 ""flatten"", Query([Projection(""material"")], ""Materials""), name=""products""
             ),
             SubQuery(
                 Query(
                     [
                         Projection(""material""),
                         Aggregation(
                             ""count"", Query([Projection(""*"")], ""Cycles""), name=""cycles""
                         ),
                     ],
                     ""Materials"",
                 ),
                 name=""material_and_count"",
             ),
         ],
         ""Tools"",
     )
 
     results: List[Dict] = query.execute(context)
     assert results[0] == {
         ""name"": ""Tool 0"",
         ""cycles"": 26,
         ""products"": [""Material 0"", ""Material 1""],
         ""material_and_count"": [
             {""material"": ""Material 0"", ""cycles"": 14},
             {""material"": ""Material 1"", ""cycles"": 14},
         ],
     }
 
 
 # def test_part_query():
 #     query = Query(
 #                 [
 #                     Projection(""material""),
 #                     Aggregation(
 #                         ""count"", Query([Projection(""*"")], ""Cycles""), name=""cycles""
 #                     ),
 #                 ],
 #                 ""Materials"",
 #             )
 #     resulst = query.execute(context)
 #     print(resulst)
 #
 # def test_part_qury2():
 #     query = Query(
 #     [
 #         Projection(""name""),
 #         Aggregation(""count"", Query([Projection(""*"")], ""Cycles""), name=""cycles""),
 #         Aggregation(
 #             ""flatten"", Query([Projection(""material"")], ""Materials""), name=""products""
 #         )],
 #     ""Materials"",
 # )
 #     result = query.execute(context)
 #     print(result)
 #
 # def test_simple_group_by():
 #     # print(generated_materials)
 #     query =   Query(
 #         [Projection(""id""),Projection(""machine""),
 #          Aggregation(""count"",
 #                      Query(
 #             [Projection(""machine"")],
 #             ""Materials""),name=""Anzahl"")],
 #         ""Materials"")
 #     result = query.execute(context)
 #     print(result)
 #
 # def test_count_material_on_machine():
 #     pass
 # def test_sub_query():
 #     query = SubQuery(Query([Projection(""id""), Projection(""start""), Projection(""end""), Projection(""machine"")], ""Cycles""),""Cycles"")
 #     query.execute(context)
 # def test_sub_query2():
 #     query = Query(
 #     [
 #         SubQuery(
 #             Query(
 #                 [
 #                     Projection(""material""),
 #                     Aggregation(
 #                         ""count"", Query([Projection(""*"")], ""Cycles""), name=""cycles""
 #                     ),
 #                 ],
 #                 ""Materials"",
 #             ),
 #             name=""material_and_count"",
 #         ),
 #     ],
 #     ""Tools"",
 # )
 #     results= query.execute(context)
 #     print(results)"
OK;11;pragmaticindustries;pql-demo;661dd80e6a208ea196ac60692d9578bac4a49bfb;"Added example (#25)

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

cleaned up Code

introdiced DbmemoryAssetRetriever

ran black and set other echo level

ran black and set other echo level

ran black and set other echo level

added Tests and added that the AssetReriever now working from DB.

ran black

added test for PLC Server

made complete test with plc server but got timestamp problems,tryed to fix them

fixed Timestamp Error

zwischenstand

more tests

ran black

optimized imports

zwischenstand

added config for id in DB

cleaned up code

cleaned up code

fixed json deserizable errors

ran black

tryed to fix errors

added plc to requirements.txt

added secret for plc-server-openapi-client

old version

commented in

commented in

fixed tests

fixed tests

removed print

ran black

made valid test Data

removed prints

* Added example

added plc server

ran black

optimized imports

tryed to figure out what the AssesRetreiver does

tryed to figure out what the AssesRetreiver does

cleaned up code

made more methods

made some Comments

made some Comments

cleaned up Code

* cleaned up Code

* Removed faker uuid

Co-authored-by: julian <j.feinauer@pragmaticminds.de>";" from typing import List, Dict
 
 from database_methods import (
     migrate,
     delete_all_rows_from_pql_entity,
     delete_all_rows_from_counter_saving,
 )
 from main import DbMemoryAssetRetriever, create_tools, create_cycles, create_materials
 from pql import (
     agg_functions,
     RootContext,

     SubQuery,
 )
 
 
 def test_query():
     migrate()
     delete_all_rows_from_pql_entity()
     delete_all_rows_from_counter_saving()
     create_tools(5)
     create_cycles(100)
     create_materials(10)
 
     dbassetRetriever: DbMemoryAssetRetriever = DbMemoryAssetRetriever()
     context: RootContext = RootContext(
         dbassetRetriever.get_assest, lambda s: agg_functions.get(s)
     )
 
     query: Query = Query(
         [
             Projection(""tool_name""),
             Aggregation(""count"", Query([Projection(""*"")], ""Cycle""), name=""cycles""),
             Aggregation(
                 ""flatten"",
                 Query([Projection(""material_name"")], ""MaterialEquipped""),
                 name=""products"",
             ),
             SubQuery(
                 Query(
                     [
                         Projection(""material_name""),
                         Aggregation(
                             ""count"", Query([Projection(""*"")], ""Cycle""), name=""cycles""
                         ),
                     ],
                     ""MaterialEquipped"",
                 ),
                 name=""material_and_count"",
             ),
         ],
         ""ToolEquipped"",
     )
 
     results: List[Dict] = query.execute(context)
     assert results == [
         {""tool_name"": ""Tool 0""},
         {""cycles"": 26},
         {""products"": [""Material 0"", ""Material 1""]},
         {
             ""material_and_count"": [
                 {""material_name"": ""Material 0""},
                 {""cycles"": 14},
                 {""material_name"": ""Material 1""},
                 {""cycles"": 14},
             ]
         },
         {""tool_name"": ""Tool 1""},
         {""cycles"": 31},
         {""products"": [""Material 1""]},
         {""material_and_count"": [{""material_name"": ""Material 1""}, {""cycles"": 16}]},
         {""tool_name"": ""Tool 2""},
         {""cycles"": 28},
         {""products"": [""Material 0"", ""Material 1"", ""Material 0""]},
         {
             ""material_and_count"": [
                 {""material_name"": ""Material 0""},
                 {""cycles"": 20},
                 {""material_name"": ""Material 1""},
                 {""cycles"": 7},
                 {""material_name"": ""Material 0""},
                 {""cycles"": 0},
             ]
         },
         {""tool_name"": ""Tool 3""},
         {""cycles"": 0},
         {""products"": [""Material 1"", ""Material 0""]},
         {
             ""material_and_count"": [
                 {""material_name"": ""Material 1""},
                 {""cycles"": 0},
                 {""material_name"": ""Material 0""},
                 {""cycles"": 0},
             ]
         },
         {""tool_name"": ""Tool 4""},
         {""cycles"": 0},
         {""products"": []},
         {""material_and_count"": []},
     ]"
KO;4;unsigned-maki;cherubim-core;886bf1b14083b4727f8d00c5c1d49d61e4e2d0cf;feat(memory): add temporary errors;"def __init__(self, chain, arg_list):
 
     def call(self, args):
         if len(args) == len(arg_list):
             #  TODO: implement resolving of chain
         else:
             #  TODO: implement error
 
 "
OK;4;unsigned-maki;cherubim-core;886bf1b14083b4727f8d00c5c1d49d61e4e2d0cf;feat(memory): add temporary errors;"def __init__(self, chain, arg_list):
 
     def call(self, args):
         if len(args) == len(arg_list):
             raise ValueError(""Error"")
             #  TODO: implement resolving of chain
         else:
             raise ValueError(""Error"")
             #  TODO: implement error
 
 "
KO;8;kmartinez;dgpstracker2;ff9406e6b646fb36c70565c7e489c32d2a551950;Added helper function to write binary data to given file, changed incoming long-range transmission to go into a single file, datalog.bin. Fixed file reading to stop memory errors when loading large files.;" import pyb
 import Formats
 
 DEVICE_ID = 0
 PRECISION = 0 # 0=day,1=hour
 
 class RawLog:
     data = bytearray()
def getLogString(self):
         return self.data
 
     def writeLog(self):
         logfile = open(getdtstring() + ""log.bin"", ""ab"")
         logfile.write(self.getLogString())
         logfile.flush()
         logfile.close()
 
 class DataLog:
     payload = bytearray()
def getLogString(self):
         return pack
 
     def writeLog(self):
         logfile = open(getdtstring() + ""log.bin"", ""ab"")
         logfile.write(self.getLogString())
         logfile.flush()
         logfile.close()
 
 
 class ECEFLog(DataLog):
def __init__(self, ecefMsg, smoothType, satMsg):
 class EventLog:
     class_id = bytearray()
     payload = bytearray()
     acceptable_ids = [b'\x00', b'\x01', b'\x02', b'\x1e', b'\x1f', b'\x20', b'\x21', b'\xe2', b'\xf1', b'\xf2', b'\xf3',
                       b'\xf4', b'\xf5', b'\xfe', b'\xff']
 
def getLogString(self):
     def writeLog(self):
         if self.class_id not in self.acceptable_ids:
             return bytearray()
         file = open(getdtstring() + ""eventLog.bin"", ""ab"")
         file.write(self.getLogString())
         file.flush()
         file.close()
         pass
 
 
 class StartupEvent(EventLog):
class TimeWakeupSyncEvent(EventLog):
 class LocationEvent(EventLog):
 
     def __init__(self, eventType):
         self.class_id = bwAnd(eventType, b'\x1F')
 
 
 class LCDEvent(EventLog):
def __init__(self, parseClass, parseID, byteLength, parseLength):
 class UnacceptableLengthError(EventLog):
     class_id = b'\xF1'
 
 
 class NoMessageError(EventLog):
     class_id = b'\xE2'
def bwOr(b1, b2):
 
 
 def getTime(bytes):
     year = Formats.U2(bytes[0:2])
     month = Formats.U1(bytes[2])
     day = Formats.U1(bytes[3])
def curTimeInBytes():
     return Formats.u2toBytes(year) + Formats.u1toBytes(month) + Formats.u1toBytes(day) + Formats.u1toBytes(hours) + \
            Formats.u1toBytes(minutes) + Formats.u1toBytes(seconds)
 
 def getdtstring():
     time = pyb.RTC().datetime()
     return ""{0}-{1}-{2}-"".format(time[2], time[1], time[0])
 
 
 waiting_logs = {}
 def unparseLogs():
     try:
         dtstring = getdtstring()
         unparseLog(dtstring+""log.bin"")
         unparseLog(dtstring+""eventlog.bin"")
         # list files that haven't been erased yet
         if dtstring not in waiting_logs:
             waiting_logs[dtstring] = 0
     except Exception as e:
         print(""Error while decoding logs? Might not exist"", e)
 
 
 def unparseLog(filename):
     inf = open(filename, ""rb"")
     lines = inf.read()
     inf.close()
     if len(lines) == 0:
         return
     # SHOULD only be on line, since no \n written... but just in case
     logs = lines.split(b'\xb5b')  # uses same preamble as ubx logs for simplicity
     f = open(""readablelogs.txt"", ""w"")
     for line in logs:
         print(line)
         # skip empty lines
         if len(line) == 0:
             continue
         year, month, day, hour, minute, second = list(map(str, getTime(line[:7])))
         did = line[7]
         type = Formats.U1(line[8:9])
         length = Formats.U2(line[9:11])
         logdata = line[11:]
         readable = ""[{0}] - {1}/{2}/{3} {4}:{5}:{6} - "".format(did, day, month, year, hour, minute, second)
         # print(type, length, readable, logdata)
         if type == 0x00:
             readable += ""Device startup""
         elif type == 0x02:
             readable += ""RTC time updated""
         elif type == 0x02:
             readable += ""Wakeup events synced to RTC""
         elif readable == 0x03:
             readable += ""Calibration succeeded""
         elif type == 0x10:
             readable += ""ECEF Location logged""
         elif type == 0x11:
             # raw location log
             x = str(Formats.I4(logdata[0:4]) + .1 * Formats.I1(logdata[12:13]))
             y = str(Formats.I4(logdata[4:8]) + .1 * Formats.I1(logdata[13:14]))
             z = str(Formats.I4(logdata[8:12]) + .1 * Formats.I1(logdata[14:15]))
             pacc = str(Formats.I4(logdata[15:19]) * .01)
             svs = str(Formats.U1(logdata[19:20]))
             readable += ""Raw location: ["" + x + "", "" + y + "", "" + z + ""]: "" + pacc + ""cm, svs="" + svs
         elif type == 0x12:
             x = str(Formats.I4(logdata[0:4]) + .1 * Formats.I1(logdata[12:13]))
             y = str(Formats.I4(logdata[4:8]) + .1 * Formats.I1(logdata[13:14]))
             z = str(Formats.I4(logdata[8:12]) + .1 * Formats.I1(logdata[14:15]))
             pacc = str(Formats.I4(logdata[15:19]) * .01)
             svs = str(Formats.U1(logdata[19:20]))
             readable += ""Median-smoothed location: ["" + x + "", "" + y + "", "" + z + ""]: "" + pacc + ""cm, svs="" + svs
         elif type == 0x13:
             x = str(Formats.I4(logdata[0:4]) + .1 * Formats.I1(logdata[12:13]))
             y = str(Formats.I4(logdata[4:8]) + .1 * Formats.I1(logdata[13:14]))
             z = str(Formats.I4(logdata[8:12]) + .1 * Formats.I1(logdata[14:15]))
             pacc = str(Formats.I4(logdata[15:19]) * .01)
             svs = str(Formats.U1(logdata[19:20]))
             readable += ""Best-accuracy location: ["" + x + "", "" + y + "", "" + z + ""]: "" + pacc + ""cm, svs="" + svs
         elif type == 0x1E:
             readable += ""Location logs transmitted""
         elif type == 0x1F:
             readable += ""Location logs cleared""
         elif type == 0x21:
             readable += ""LCD on""
         elif type == 0x22:
             readable += ""LCD off""
         elif type == 0x23:
             readable += ""LCD locked""
         elif type == 0x24:
             readable += ""LCD unlocked""
         elif type == 0xE0:
             # len forcibly changed by code
             ubxClass = Formats.U1(logdata[:1])
             ubxID = Formats.U1(logdata[1:2])
             byteLength = Formats.U2(logdata[2:4])
             newLength = Formats.U2(logdata[4:6])
             readable += ""Length forcibly changed for "" + str([ubxClass, ubxID]) + "": "" + str(
                 byteLength) + "" -> "" + str(
                 newLength)
         elif type == 0xE1:
def unparseLog(filename):
             # no parse data for incoming message
             ubxClass = Formats.U1(logdata[:1])
             ubxID = Formats.U1(logdata[1:2])
             readable += ""No programmed message for class="" + str(ubxClass) + "", id="" + str(ubxID)
         elif type == 0xF0:
             readable += ""UART port uncalibrated""
         elif type == 0xF1:
             # unacceptable packet length on uart stream (>100 and not sat)
             badLength = Formats.U2(logdata[:2])
             readable += ""Unacceptable packet length read from UART stream: "" + str(badLength)
         elif type == 0xF2:
             # number ENcoding error
             badNumber = Formats.I4(logdata[:4])
             format = Formats.U2(logdata[4:6])
             readable += ""Number encoding error: "" + str(badNumber) + "": "" + str(format)
         elif type == 0xF3:
             # number DEcoding error
             badNumber = Formats.I4(logdata[:4])
             format = Formats.U2(logdata[4:6])
             readable += ""Number decoding error: "" + str(badNumber) + "": "" + str(format)
         elif type == 0xF4:
             readable += ""Calibration failed due to repeated timeouts""
         elif type == 0xF5:
             readable += ""Reading failed due to repeated timeouts""
         elif type == 0xFE:
             readable += ""Not enough space in storage to keep logs""
         elif type == 0xFF:
             readable += ""Unknown error"" + str(logdata)
         # write to file - no check for file space :/
         f.write(readable + ""\n"")
         print(""------"")
     f.flush()
     f.close()"
OK;8;kmartinez;dgpstracker2;ff9406e6b646fb36c70565c7e489c32d2a551950;Added helper function to write binary data to given file, changed incoming long-range transmission to go into a single file, datalog.bin. Fixed file reading to stop memory errors when loading large files.;" import pyb
 import Formats
 import os
 
 DEVICE_ID = 0
 PRECISION = 0  # 0=day,1=hour
 
 
 class RawLog:
     data = bytearray()
def getLogString(self):
         return self.data
 
     def writeLog(self):
         fn = ""datalog.bin""
         writeDataToFile(fn, self.getLogString())
 
 
 class DataLog:
     payload = bytearray()
def getLogString(self):
         return pack
 
     def writeLog(self):
         fn = getdtstring() + ""log.bin""
         writeDataToFile(fn, self.getLogString())
 
 
 class ECEFLog(DataLog):
def __init__(self, ecefMsg, smoothType, satMsg):
 class EventLog:
     class_id = bytearray()
     payload = bytearray()
     # used to filter out latency issues caused by over-using I/O with minor, unimportant events
     acceptable_ids = [b'\x00', b'\x01', b'\x02', b'\x1e', b'\x1f', b'\x20', b'\x21', b'\xe2', b'\xf1', b'\xf2', b'\xf3',
                       b'\xf4', b'\xf5', b'\xfe', b'\xff']
 
def getLogString(self):
     def writeLog(self):
         if self.class_id not in self.acceptable_ids:
             return bytearray()
         fn = getdtstring() + ""eventLog.bin""
         writeDataToFile(fn, self.getLogString())
 
 
 waiting_logs = {}
 
 
 def writeDataToFile(filename, data):
     if filename not in waiting_logs:
         waiting_logs[filename] = 0
     file = open(filename, ""ab"")
     file.write(data)
     file.flush()
     file.close()
 
 
 class StartupEvent(EventLog):
class TimeWakeupSyncEvent(EventLog):
 class LocationEvent(EventLog):
 
     def __init__(self, eventType):
         self.class_id = b'\x10'
         self.payload = eventType
 
 
 class LCDEvent(EventLog):
def __init__(self, parseClass, parseID, byteLength, parseLength):
 class UnacceptableLengthError(EventLog):
     class_id = b'\xF1'
 
     def __init__(self, lengthbytes):
         self.payload.extend(lengthbytes)
 
 
 class NoMessageError(EventLog):
     class_id = b'\xE2'
def bwOr(b1, b2):
 
 
 def getTime(bytes):
     if bytes is None or bytes == b'':
         return 0, 0, 0, 0, 0, 0
 
     year = Formats.U2(bytes[0:2])
     month = Formats.U1(bytes[2])
     day = Formats.U1(bytes[3])
def curTimeInBytes():
     return Formats.u2toBytes(year) + Formats.u1toBytes(month) + Formats.u1toBytes(day) + Formats.u1toBytes(hours) + \
            Formats.u1toBytes(minutes) + Formats.u1toBytes(seconds)
 
 
 def getdtstring():
     time = pyb.RTC().datetime()
     return ""{0}-{1}-{2}-"".format(time[2], time[1], time[0])
 
 
 def unparseLogs():
     # try:
     filesToDecode = list(filter(lambda i: "".bin"" in i, os.listdir()))
     for fn in filesToDecode:
         # check the extension of files
         print(fn)
         unparseLog(fn)
     # except Exception as e:
     #     print(""Error while decoding logs?"", e)
 
 
 def calibrateFile(file):
     start_bit_one = False
     start_bit_two = False
     curr = b'\x00'
     while not (start_bit_one and start_bit_two) and curr is not None and curr != b'':
         curr = file.read(1)
         if curr == b'\xb5':
             start_bit_one = True
         elif curr == b'\x62':
             start_bit_two = True
         else:
             start_bit_one = False
             start_bit_two = False
 
 
 def getLine(file):
     print(""------"")
     calibrateFile(file)
     date = file.read(7)
     did = file.read(1)
     type = file.read(1)
     lbytes = file.read(2)
     print(date, did, type, lbytes)
     if lbytes == b'':
         # eof reached
         return None
 
     length = Formats.U2(lbytes)
     if length > 50:
         return b''
 
     data = file.read(length)
     print(data)
     crc = file.read(2)
     if crc is None or crc == b'':
         return None
 
     return date, did, type, data
 
 
 def unparseLog(filename):
     inf = open(filename, ""rb"")
     line = b''
     fev = open(filename.split(""."")[0] + ""_parsed_events.txt"", ""w"")
     fcsv = open(filename.split(""."")[0] + ""_parsed_data.csv"", ""w"")
     dups = set()
     while line is not None:
         line = getLine(inf)
         if line is None or len(line) == 0:
             continue
         # SHOULD only be on line, since no \n written... but just in case
         # print(line)
         # skip empty lines
         if len(line) == 0:
             continue
         year, month, day, hour, minute, second = list(map(str, getTime(line[0])))
         did = Formats.U1(line[1])
         type = Formats.U1(line[2])
         logdata = line[3]
         readable = ""[{0}] - {1}/{2}/{3} {4}:{5}:{6} - "".format(did, day, month, year, hour, minute, second)
         csv = ""{0},{1}/{2}/{3} {4}:{5}:{6},"".format(did, day, month, year, hour, minute, second)
         print(type, readable, logdata)
         if type == 0x00:
             readable += ""Device startup""
         elif type == 0x01:
             readable += ""RTC synchronised""
         elif type == 0x02:
             readable += ""RTC time updated""
         elif type == 0x02:
             readable += ""Wakeup events synced to RTC""
         elif readable == 0x03:
             readable += ""Calibration succeeded""
         elif type == 0x10:
             eType = logdata[0]
             if eType == 0x11:
                 eType = ""unfiltered""
             elif eType == 0x12:
                 eType = ""median""
             elif eType == 0x13:
                 eType = ""best-acc""
             else:
                 eType = ""unknown""
             readable += ""ECEF Location logged [{0}]"".format(eType)
         elif type == 0x11:
             # raw location log
             x = Formats.I4(logdata[0:4]) + 1e-2 * Formats.I1(logdata[12:13])
             y = Formats.I4(logdata[4:8]) + 1e-2 * Formats.I1(logdata[13:14])
             z = Formats.I4(logdata[8:12]) + 1e-2 * Formats.I1(logdata[14:15])
             pacc = Formats.I4(logdata[15:19]) * .01
             svs = Formats.U1(logdata[19:20])
             readable += ""Raw location, accuracy: "" + str(pacc)
             csv += ""raw,{0:.2f},{1:.2f},{2:.2f},{3:.2f},{4:.2f}"".format(x, y, z, pacc, svs)
             print(readable)
         elif type == 0x12:
             x = Formats.I4(logdata[0:4]) + 1e-2 * Formats.I1(logdata[12:13])
             y = Formats.I4(logdata[4:8]) + 1e-2 * Formats.I1(logdata[13:14])
             z = Formats.I4(logdata[8:12]) + 1e-2 * Formats.I1(logdata[14:15])
             pacc = Formats.I4(logdata[15:19]) * .01
             svs = Formats.U1(logdata[19:20])
             readable += ""Median-filtered location, accuracy: "" + str(pacc)+""cm""
             csv += ""med,{0:.2f},{1:.2f},{2:.2f},{3:.2f},{4:.2f}"".format(x, y, z, pacc, svs)
             print(readable)
         elif type == 0x13:
             x = Formats.I4(logdata[0:4]) + 1e-2 * Formats.I1(logdata[12:13])
             y = Formats.I4(logdata[4:8]) + 1e-2 * Formats.I1(logdata[13:14])
             z = Formats.I4(logdata[8:12]) + 1e-2 * Formats.I1(logdata[14:15])
             pacc = Formats.I4(logdata[15:19]) * .01
             svs = Formats.U1(logdata[19:20])
             readable += ""Best-accuracy-filtered location, accuracy: "" + str(pacc)+""cm""
             csv += ""ba,{0:.2f},{1:.2f},{2:.2f},{3:.2f},{4:.2f}"".format(x, y, z, pacc, svs)
             print(readable)
         elif type == 0x1E:
             readable += ""Location logs transmitted""
         elif type == 0x1F:
             readable += ""Location logs cleared""
         elif type == 0x20:
             readable += ""LCD on""
         elif type == 0x21:
             readable += ""LCD off""
         elif type == 0x22:
             readable += ""LCD locked""
         elif type == 0x23:
             readable += ""LCD unlocked""
         elif type == 0xE0:
             # len forcibly changed by code
             ubxClass = Formats.U1(logdata[:1])
             ubxID = Formats.U1(logdata[1:2])
             byteLength = Formats.U2(logdata[2:4])
             newLength = Formats.U2(logdata[4:6])
             readable += ""Length force for "" + str([ubxClass, ubxID]) + "": "" + str(
                 byteLength) + "" -> "" + str(
                 newLength)
         elif type == 0xE1:
def unparseLog(filename):
             # no parse data for incoming message
             ubxClass = Formats.U1(logdata[:1])
             ubxID = Formats.U1(logdata[1:2])
             readable += ""No class="" + str(ubxClass) + "", id="" + str(ubxID)
         elif type == 0xF0:
             readable += ""UART port uncalibrated""
         elif type == 0xF1:
             # unacceptable packet length on uart stream (>100 and not sat)
             print(line)
             try:
                 badLength = Formats.U2(logdata[:2])
             except:
                 badLength = ""<uknown>""
             readable += ""Bad UART len: "" + str(badLength)
         elif type == 0xF2:
             # number ENcoding error
             badNumber = (logdata[:4])
             format = (logdata[4:6])
             readable += ""Number encoding error: "" + str(badNumber) + "": "" + str(format)
         elif type == 0xF3:
             # number DEcoding error
             badNumber = (logdata[:4])
             format = (logdata[4:6])
             readable += ""Number decoding error: "" + str(badNumber) + "": "" + str(format)
         elif type == 0xF4:
             readable += ""Calibration t-o""
         elif type == 0xF5:
             readable += ""Reading t-o""
         elif type == 0xFE:
             readable += ""No storage space""
         elif type == 0xFF:
             readable += str(logdata)
         else:
             readable += ""UE: "" + str(type)
         if hash(readable) not in dups:
             # write to file - no check for file space :/
             fev.write(readable + ""\n"")
             dups.add(hash(readable))
         # csv line complete
         if csv[-1] != "","" and hash(csv) not in dups:
             fcsv.write(csv+""\n"")
             dups.add(hash(csv))
         # print(readable)
         # print(""------"")
     fcsv.flush()
     fcsv.close()
     fev.flush()
     fev.close()"
KO;39;songyoungwoon;yolov5-analog_gauge;950a85d9f625ee7cb897e6f6a87aa6cc45bdea27;"TensorRT PyTorch Hub inference fix (#7560)

Solution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.";"def forward(self, imgs, size=640, augment=False, profile=False):
         #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images
 
         t = [time_sync()]
         p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type
         autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference
         if isinstance(imgs, torch.Tensor):  # torch
             with amp.autocast(autocast):"
OK;39;songyoungwoon;yolov5-analog_gauge;950a85d9f625ee7cb897e6f6a87aa6cc45bdea27;"TensorRT PyTorch Hub inference fix (#7560)

Solution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.";"def forward(self, imgs, size=640, augment=False, profile=False):
         #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images
 
         t = [time_sync()]
         p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type
         autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference
         if isinstance(imgs, torch.Tensor):  # torch
             with amp.autocast(autocast):"
KO;41;songyoungwoon;yolov5-analog_gauge;950a85d9f625ee7cb897e6f6a87aa6cc45bdea27;"TensorRT PyTorch Hub inference fix (#7560)

Solution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.";"def forward(self, imgs, size=640, augment=False, profile=False):
         #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images
 
         t = [time_sync()]
         p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type
         autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference
         if isinstance(imgs, torch.Tensor):  # torch
             with amp.autocast(autocast):"
OK;41;songyoungwoon;yolov5-analog_gauge;950a85d9f625ee7cb897e6f6a87aa6cc45bdea27;"TensorRT PyTorch Hub inference fix (#7560)

Solution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.";"def forward(self, imgs, size=640, augment=False, profile=False):
         #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images
 
         t = [time_sync()]
         p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type
         autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference
         if isinstance(imgs, torch.Tensor):  # torch
             with amp.autocast(autocast):"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"def __init__(self, normalization, local_dataset, batch_size=32 ,epochs=1):
         self.epochs=epochs
 
 
     def train(self):
 
         optimizer = SGD(self.model.parameters(), lr=1e-3, weight_decay=5e-4)
 
         criterion = CrossEntropyLoss()
         criterion.cuda()
 
         self.model.cuda()
         self.model.train()
 
         for epoch in range(self.epochs):
             # training loop
             for i, data in enumerate(self.dataset):
                 imgs, labels = data
                 imgs, labels = imgs.cuda(), labels.cuda()
 
                 optimizer.zero_grad()
                 pred = self.model(imgs)
                 pred = pred.cuda()
                 
                 loss = criterion(pred, labels)
                 loss.backward()
                 optimizer.step()
 
         self.model = self.model.to('cpu')
 
         self.model_dict = self.model.state_dict()
         torch.cuda.empty_cache()
 
 
     def get_data(self, key):
         return self.model_dict[key]
 
     def set_model(self, model_dict):
         self.model = ResNet(self.normalization)
         self.model.load_state_dict(model_dict)
 
     
\ No newline at end of file"
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;"def __init__(self, normalization, local_dataset, batch_size=32 ,epochs=1):
         self.epochs=epochs
 
 
     
\ No newline at end of file"
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from torch.optim import SGD
 from torch.nn import CrossEntropyLoss
 from resnet50 import ResNet
 import torch
 
 class ClientSimulation:
 
     def __init__(self, n_clients, normalization):
         self.n_clients = n_clients
         self.normalization = normalization
         
 
     def train(self, clients, client_subset, server_model_dict):
 
         cl_data = dict()
         
         for index in client_subset:
             
             cl = _Client(self.normalization, clients[index].dataset, clients[index].epochs, server_model_dict)
             print(f""Training client {index}"")
             cl.train()
             print(""Done"")
             cl_data[index] = cl
 
         return cl_data
 
 
 class _Client:
 
     def __init__(self, normalization, local_dataset, epochs, model_dict):
         self.model = ResNet(normalization)
         self.model.load_state_dict(model_dict)
         self.dataset = local_dataset
         self.epochs = epochs
 
     def train(self):
 
         optimizer = SGD(self.model.parameters(), lr=1e-3, weight_decay=5e-4)
 
         criterion = CrossEntropyLoss()
         criterion.cuda()
 
         self.model.cuda()
         self.model.train()
 
         for epoch in range(self.epochs):
             # training loop
             for i, data in enumerate(self.dataset):
                 imgs, labels = data
                 imgs, labels = imgs.cuda(), labels.cuda()
 
                 optimizer.zero_grad()
                 pred = self.model(imgs)
                 pred = pred.cuda()
                 
                 loss = criterion(pred, labels)
                 loss.backward()
                 optimizer.step()
 
         self.model = self.model.to('cpu')
 
         self.model_dict = self.model.state_dict()
         torch.cuda.empty_cache()
 
     def get_data(self, key):
         return self.model_dict[key]
 "
KO;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from test import test_accuracy
 import copy
 import gc
 
 
 # global parameters : number of epochs locally, normalization type
def main(epochs, normalization, rounds, client_proportion, batch_size):
     dataset = get_dataset(transform)
     subdatasets = get_iid_split(dataset)
 
     # create clients
 
     clients = dict()
def main(epochs, normalization, rounds, client_proportion, batch_size):
 
         print(f""##### ROUND {round}"")
 
         client_subset = sample(range(N_CLIENTS), int(client_proportion*N_CLIENTS))
 
         for index in client_subset:
             print(f""Training client  {index}"")
             server_model_dict = server.get_model_dict()
 
             clients[index].set_model(server_model_dict)
             clients[index].train()
             print(""Done"")
 
         model_dict = average(clients, normalization, client_subset)
 
         server.update_model(model_dict)
 "
OK;7;ulysse1999;mldl-fl-project;e4c20686c9cbba63be79b8e977224d582fb946de;only store a few models at the same time, to avoid memory errors;" from test import test_accuracy
 import copy
 import gc
 from client_simulation import ClientSimulation
 
 
 # global parameters : number of epochs locally, normalization type
def main(epochs, normalization, rounds, client_proportion, batch_size):
     dataset = get_dataset(transform)
     subdatasets = get_iid_split(dataset)
 
     n_clients_each_round = int(client_proportion*N_CLIENTS)
 
     sim = ClientSimulation(n_clients_each_round, normalization)
 
     # create clients
 
     clients = dict()
def main(epochs, normalization, rounds, client_proportion, batch_size):
 
         print(f""##### ROUND {round}"")
 
         client_subset = sample(range(N_CLIENTS), n_clients_each_round)
 
         server_model_dict = server.get_model_dict()
         trained_models = sim.train(clients, client_subset, server_model_dict)
 
         model_dict = average(trained_models, normalization, client_subset)
 
         server.update_model(model_dict)
 "
KO;3;byrd-polar;s2s2net;d7f391c5f6fc4df24daaa4bb6dad56be226fad19;":zap: DeepSpeed ZeRO Stage 2 model parallel training (#2)

* :heavy_plus_sign: Add deepspeed

DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective! Using PyPI source for now until conda-forge package is released.

Also need to install newer gcc version to prevent error `Your compiler (c++ 4.8.5) may be ABI-incompatible with PyTorch! Please use a compiler that is ABI-compatible with GCC 5.0 and above` on the hpc server.

* :zap: Use DeepSpeed ZeRO Stage 2 model parallel strategy

Working towards conserving GPU memory for matters (inference on full-size images). Using DeepSpeed ZeRO Stage 2 which shards optimizer states (Stage 1) and gradients (Stage 2) across multiple GPUs. Have set devices to be auto instead of 2 so that I can run on 1 GPU on my laptop or 2 GPUs on the HPC server without changing values. Also needed to explicitly convert the input Sentinel-2 image tensor to float16 (if using 16-bit training) to avoid `RuntimeError: Input type (torch.cuda.ShortTensor) and weight type (torch.cuda.HalfTensor) should be the same`.";" # platform: linux-64
 @EXPLICIT
 https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ca-certificates-2021.10.8-ha878542_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/dcw-gmt-2.1.0-ha770c72_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-dejavu-sans-mono-2.37-hab24e00_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-inconsolata-3.000-h77eed37_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-source-code-pro-2.038-h77eed37_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-ubuntu-0.83-hab24e00_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gshhg-gmt-2.3.7-ha770c72_1003.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ld_impl_linux-64-2.36.1-hea4e1c9_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgfortran5-11.2.0-h5c6108e_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libstdcxx-ng-11.2.0-he4da1e4_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/mkl-include-2022.0.1-h8d4b97c_803.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pandoc-2.18-ha770c72_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/poppler-data-0.4.11-hd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/pytorch/noarch/pytorch-mutex-1.0-cuda.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tzdata-2022a-h191b570_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/fonts-conda-forge-1-0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgfortran-ng-11.2.0-h69a702a_16.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/fonts-conda-ecosystem-1-0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_kmp_llvm.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgcc-ng-11.2.0-h1d223b6_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/aom-3.3.0-h27087fc_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/bzip2-1.0.8-h7f98852_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/c-ares-1.18.1-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/charls-2.3.4-h9c3ff4c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cudatoolkit-11.3.1-ha36c431_10.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/expat-2.4.8-h27087fc_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fftw-3.3.10-nompi_h77c792f_102.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/freexl-1.0.6-h7f98852_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/libffi-3.4.2-h7f98852_5.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libiconv-1.16-h516909a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libnsl-2.0.0-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libpciaccess-0.16-h516909a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libsodium-1.0.18-h36c2ea0_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libspatialindex-1.9.3-h9c3ff4c_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libuuid-2.32.1-h7f98852_1000.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/openssl-1.1.1o-h166bdaf_0.tar.bz
 https://conda.anaconda.org/conda-forge/linux-64/pcre-8.45-h9c3ff4c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pixman-0.40.0-h36c2ea0_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pthread-stubs-0.4-h36c2ea0_1001.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/snappy-1.1.9-hbd366e4_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/svt-av1-0.9.1-h27087fc_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tbb-2021.5.0-h924138e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tzcode-2022a-h166bdaf_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/xorg-xproto-7.0.31-h7f98852_1007
 https://conda.anaconda.org/conda-forge/linux-64/xz-5.2.5-h516909a_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/yaml-0.2.5-h7f98852_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zfp-0.5.5-h9c3ff4c_8.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/blosc-1.21.1-hd32f23e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gettext-0.19.8.1-h73d1719_1008.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gnutls-3.6.13-h85f3911_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libavif-0.10.1-h166bdaf_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/xorg-fixesproto-5.0-h7f98852_100
 https://conda.anaconda.org/conda-forge/linux-64/xorg-libsm-1.2.3-hd9c2040_1000.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zeromq-4.3.4-h9c3ff4c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zlib-1.2.11-h166bdaf_1014.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zstd-1.5.2-ha95c52a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/boost-cpp-1.74.0-h6cacc03_7.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/brotli-bin-1.0.9-h166bdaf_7.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/c-blosc2-2.0.4-h5f21a17_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/hdf4-4.2.15-h10796ff_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/krb5-1.19.3-h3790be6_0.tar.bz2
 https://conda.anaconda.org/rapidsai/linux-64/libcucim-22.02.00-cuda11_g29bfe6f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libglib-2.70.2-h174f98d_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libnghttp2-1.47.0-h727a467_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libpng-1.6.37-h21135ba_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libprotobuf-3.20.0-h6239696_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libssh2-1.10.0-ha56f1ee_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libtiff-4.3.0-h542a066_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libxml2-2.9.12-h885dcf4_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libzip-1.8.0-h4de3113_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/mkl-2022.0.1-h8d4b97c_803.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/openh264-2.1.1-h780b84a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/sqlite-3.38.4-h4ff8645_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/xorg-libx11-1.7.2-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/brotli-1.0.9-h166bdaf_7.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/freetype-2.10.4-h0708190_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/lcms2-2.12-hddcbb42_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libblas-3.9.0-14_linux64_mkl.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libcurl-7.83.0-h7bff187_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libkml-1.3.0-h238a007_1014.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libpq-14.2-hd57d9b9_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libwebp-1.2.2-h3452ae3_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/brunsli-0.1-h9c3ff4c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cachetools-5.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cfitsio-4.1.0-hd9d235c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/charset-normalizer-2.0.12-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cloudpickle-2.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/curl-7.83.0-h7bff187_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cycler-0.11.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_3.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/decorator-5.1.1-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/entrypoints-0.4-pyhd8ed1ab_0.tar.b
 https://conda.anaconda.org/conda-forge/noarch/executing-0.8.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/flit-core-3.7.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fontconfig-2.14.0-h8e229c2_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/fsspec-2022.3.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/hdf5-1.12.1-nompi_h2386368_104.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/idna-3.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/iniconfig-1.1.1-pyh9f0ad1d_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/libva-2.14.0-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/locket-1.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/munkres-1.1.4-pyh9f0ad1d_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nest-asyncio-1.5.5-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pandocfilters-1.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/parso-0.8.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pathspec-0.9.0-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/py-1.11.0-pyh6c4a22f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyasn1-0.4.8-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pycparser-2.21-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pydeprecate-0.3.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyjwt-2.3.0-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyparsing-3.0.8-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/python-fastjsonschema-2.15.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/python_abi-3.9-2_cp39.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pytz-2022.1-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/tensorboard-plugin-wit-1.8.1-pyhd8
 https://conda.anaconda.org/conda-forge/noarch/threadpoolctl-3.1.0-pyh8a188c0_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tomli-2.0.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/toolz-0.11.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/traitlets-5.1.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/typing_extensions-4.2.0-pyha770c72_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/webencodings-0.5.1-py_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/websocket-client-1.3.2-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/asttokens-2.0.5-pyhd8ed1ab_0.tar.b
 https://conda.anaconda.org/conda-forge/noarch/babel-2.10.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/beautifulsoup4-4.11.1-pyha770c72_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cairo-1.16.0-ha12eb4b_1010.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/certifi-2021.10.8-py39hf3d152e_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cffi-1.15.0-py39h4bc2ebd_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/click-8.1.3-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cytoolz-0.11.2-py39hb9d737c_2.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/frozenlist-1.3.0-py39hb9d737c_1.
 https://conda.anaconda.org/conda-forge/linux-64/future-0.18.2-py39hf3d152e_5.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/geotiff-1.7.1-h509b78c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/importlib-metadata-4.11.3-py39hf3d152e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/importlib_resources-5.7.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/jedi-0.18.1-py39hf3d152e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/jupyter_core-4.9.2-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/kealib-1.4.14-h87e4c3c_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/kiwisolver-1.4.2-py39hf939315_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libdap4-3.20.6-hd7c4107_2.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/numpy-1.22.3-py39hc58783e_2.tar.
 https://conda.anaconda.org/conda-forge/noarch/packaging-21.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/partd-1.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pexpect-4.8.0-pyh9f0ad1d_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pillow-9.1.0-py39hae2aec6_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pluggy-1.0.0-py39hf3d152e_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/psutil-5.9.0-py39hb9d737c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/pysocks-1.7.1-py39hf3d152e_5.tar
 https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyu2f-0.1.5-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pyyaml-6.0-py39hb9d737c_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pyzmq-22.3.0-py39headdf64_2.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/rsa-4.8-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/rtree-1.0.0-py39hb102c33_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/setuptools-59.5.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/sniffio-1.2.0-py39hf3d152e_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tensorboard-data-server-0.6.0-py39hd97740a_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tiledb-2.8.2-h1e4a385_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tinycss2-1.1.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tornado-6.1-py39hb9d737c_3.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tqdm-4.64.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/typed-ast-1.5.3-py39hb9d737c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/typing-extensions-4.2.0-hd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/unicodedata2-14.0.0-py39hb9d737c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/aiosignal-1.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/anyio-3.5.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/argon2-cffi-bindings-21.2.0-py39hb9d737c_2.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/async-timeout-4.0.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/backports.functools_lru_cache-1.6.4-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/brotlipy-0.7.0-py39hb9d737c_1004
 https://conda.anaconda.org/conda-forge/linux-64/cftime-1.6.0-py39hd257fcd_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/click-plugins-1.1.1-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cligj-0.7.2-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cryptography-36.0.2-py39hd97740a_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cupy-10.4.0-py39hc3c280e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/dask-core-2022.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/einops-0.4.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fonttools-4.33.3-py39hb9d737c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/grpcio-1.45.0-py39h0f497a6_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/imagecodecs-2022.2.22-py39h9c0c3a3_4.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/imageio-2.19.0-pyhcf75d05_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/importlib_metadata-4.11.3-hd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jinja2-3.1.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/joblib-1.1.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jsonschema-4.5.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyter_client-7.3.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/markdown-3.3.7-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/munch-2.5.0-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/omegaconf-2.1.2-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pandas-1.4.2-py39h1832856_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pip-22.0.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/poppler-22.01.0-h1434ded_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/protobuf-3.20.0-py39h5a03fae_4.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pygments-2.12.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pyproj-3.3.1-py39hcadae2f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pytest-7.0.1-py39hf3d152e_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/scipy-1.8.0-py39hee8e79c_1.tar.b
 https://conda.anaconda.org/conda-forge/linux-64/shapely-1.8.2-py39h73b9895_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/snuggs-1.4.7-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/stack_data-0.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/terminado-0.13.3-py39hf3d152e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/yarl-1.7.2-py39hb9d737c_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/aiohttp-3.8.1-py39hb9d737c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/argon2-cffi-21.3.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/blas-2.114-mkl.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/geopandas-base-0.10.2-pyha770c72_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyterlab_pygments-0.2.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgdal-3.4.2-hb785293_6.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/matplotlib-base-3.5.2-py39h700656a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbformat-5.4.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/netcdf4-1.5.8-nompi_py39h64b754b_101.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/oauthlib-3.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyopenssl-22.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/scikit-learn-1.0.2-py39h4dfa638_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tifffile-2022.5.4-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/wcwidth-0.2.5-pyh9f0ad1d_2.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/xarray-2022.3.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gdal-3.4.2-py39hc691d54_6.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbclient-0.6.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/networkx-2.8-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/prompt-toolkit-3.0.29-pyha770c72_0.tar.bz2
 https://conda.anaconda.org/pytorch/linux-64/pytorch-1.11.0-py3.9_cuda11.3_cudnn8.2.0_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/rasterio-1.2.10-py39h2e4b6e6_5.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.9-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/efficientnet-pytorch-0.6.3-pyh9f0ad1d_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fiona-1.8.21-py39h83acdc4_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gmt-6.3.0-h793420d_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ipython-8.3.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/kornia-0.6.4-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbconvert-core-6.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/requests-2.27.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/rioxarray-0.10.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/scikit-image-0.19.2-py39hde0f152_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/torchmetrics-0.8.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/rapidsai/linux-64/cucim-22.02.00-cuda_11_py39_g29bfe6f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/google-auth-2.6.6-pyh6c4a22f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ipykernel-6.13.0-py39hef51801_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyter_server-1.17.0-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/pygmt-0.5.0-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/requests-oauthlib-1.3.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/pytorch/linux-64/torchvision-0.12.0-py39_cu113.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/google-auth-oauthlib-0.4.6-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyterlab_server-2.13.0-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbconvert-6.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/notebook-shim-0.1.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pretrainedmodels-0.7.4-pyhd8ed1ab_2.tar.bz2"
OK;3;byrd-polar;s2s2net;d7f391c5f6fc4df24daaa4bb6dad56be226fad19;":zap: DeepSpeed ZeRO Stage 2 model parallel training (#2)

* :heavy_plus_sign: Add deepspeed

DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective! Using PyPI source for now until conda-forge package is released.

Also need to install newer gcc version to prevent error `Your compiler (c++ 4.8.5) may be ABI-incompatible with PyTorch! Please use a compiler that is ABI-compatible with GCC 5.0 and above` on the hpc server.

* :zap: Use DeepSpeed ZeRO Stage 2 model parallel strategy

Working towards conserving GPU memory for matters (inference on full-size images). Using DeepSpeed ZeRO Stage 2 which shards optimizer states (Stage 1) and gradients (Stage 2) across multiple GPUs. Have set devices to be auto instead of 2 so that I can run on 1 GPU on my laptop or 2 GPUs on the HPC server without changing values. Also needed to explicitly convert the input Sentinel-2 image tensor to float16 (if using 16-bit training) to avoid `RuntimeError: Input type (torch.cuda.ShortTensor) and weight type (torch.cuda.HalfTensor) should be the same`.";" # platform: linux-64
 @EXPLICIT
 https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ca-certificates-2022.5.18.1-ha878542_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/dcw-gmt-2.1.1-ha770c72_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-dejavu-sans-mono-2.37-hab24e00_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-inconsolata-3.000-h77eed37_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-source-code-pro-2.038-h77eed37_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/font-ttf-ubuntu-0.83-hab24e00_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gshhg-gmt-2.3.7-ha770c72_1003.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/kernel-headers_linux-64-2.6.32-he073ed8_15.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ld_impl_linux-64-2.36.1-hea4e1c9_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgcc-devel_linux-64-11.2.0-h0952999_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgfortran5-12.1.0-hdcd56e2_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libstdcxx-devel_linux-64-11.2.0-h0952999_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libstdcxx-ng-12.1.0-ha89aaad_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/mkl-include-2022.0.1-h8d4b97c_803.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pandoc-2.18-ha770c72_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/poppler-data-0.4.11-hd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/pytorch/noarch/pytorch-mutex-1.0-cuda.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tzdata-2022a-h191b570_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/fonts-conda-forge-1-0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgfortran-ng-12.1.0-h69a702a_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgomp-12.1.0-h8d9b700_16.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/sysroot_linux-64-2.12-he073ed8_15.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/binutils_impl_linux-64-2.36.1-h193b22a_2.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/fonts-conda-ecosystem-1-0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/binutils_linux-64-2.36-hf3e587d_9.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_kmp_llvm.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgcc-ng-12.1.0-h8d9b700_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/aom-3.3.0-h27087fc_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/bzip2-1.0.8-h7f98852_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/c-ares-1.18.1-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/charls-2.3.4-h9c3ff4c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cudatoolkit-11.3.1-h9edb442_10.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/expat-2.4.8-h27087fc_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fftw-3.3.10-nompi_h77c792f_102.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/freexl-1.0.6-h7f98852_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/libffi-3.4.2-h7f98852_5.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libiconv-1.16-h516909a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libnsl-2.0.0-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libpciaccess-0.16-h516909a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libsanitizer-11.2.0-he4da1e4_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libsodium-1.0.18-h36c2ea0_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libspatialindex-1.9.3-h9c3ff4c_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libuuid-2.32.1-h7f98852_1000.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/openssl-1.1.1o-h166bdaf_0.tar.bz
 https://conda.anaconda.org/conda-forge/linux-64/pcre-8.45-h9c3ff4c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pixman-0.40.0-h36c2ea0_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pthread-stubs-0.4-h36c2ea0_1001.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/snappy-1.1.9-hbd366e4_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/svt-av1-0.9.1-h27087fc_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tbb-2021.5.0-h924138e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tzcode-2022a-h166bdaf_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/xorg-xproto-7.0.31-h7f98852_1007
 https://conda.anaconda.org/conda-forge/linux-64/xz-5.2.5-h516909a_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/yaml-0.2.5-h7f98852_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zfp-0.5.5-h9c3ff4c_8.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zlib-ng-2.0.6-h166bdaf_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gcc_impl_linux-64-11.2.0-h82a94d6_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gettext-0.19.8.1-h73d1719_1008.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gnutls-3.6.13-h85f3911_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libavif-0.10.1-h166bdaf_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/xorg-fixesproto-5.0-h7f98852_100
 https://conda.anaconda.org/conda-forge/linux-64/xorg-libsm-1.2.3-hd9c2040_1000.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zeromq-4.3.4-h9c3ff4c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zlib-1.2.11-h166bdaf_1014.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/zstd-1.5.2-h8a70e8d_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/blosc-1.21.1-h83bc5f7_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/boost-cpp-1.74.0-h6cacc03_7.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/brotli-bin-1.0.9-h166bdaf_7.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/c-blosc2-2.1.1-h7a311fb_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gcc_linux-64-11.2.0-h39a9532_9.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gxx_impl_linux-64-11.2.0-h82a94d6_16.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/hdf4-4.2.15-h10796ff_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/krb5-1.19.3-h3790be6_0.tar.bz2
 https://conda.anaconda.org/rapidsai/linux-64/libcucim-22.02.00-cuda11_g29bfe6f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libglib-2.70.2-h174f98d_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libnghttp2-1.47.0-h727a467_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libpng-1.6.37-h21135ba_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libprotobuf-3.20.1-h6239696_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libssh2-1.10.0-ha56f1ee_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libtiff-4.3.0-h542a066_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libxml2-2.9.12-h885dcf4_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libzip-1.8.0-h4de3113_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/mkl-2022.0.1-h8d4b97c_803.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/openh264-2.1.1-h780b84a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/sqlite-3.38.5-h4ff8645_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/xorg-libx11-1.7.2-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/brotli-1.0.9-h166bdaf_7.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/freetype-2.10.4-h0708190_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gxx_linux-64-11.2.0-hacbe6df_9.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/lcms2-2.12-hddcbb42_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libblas-3.9.0-14_linux64_mkl.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libcurl-7.83.1-h7bff187_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libkml-1.3.0-h238a007_1014.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libpq-14.2-hd57d9b9_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libwebp-1.2.2-h3452ae3_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/brunsli-0.1-h9c3ff4c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cachetools-5.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cfitsio-4.1.0-hd9d235c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/charset-normalizer-2.0.12-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cloudpickle-2.1.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/curl-7.83.1-h7bff187_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cycler-0.11.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_3.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/decorator-5.1.1-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/entrypoints-0.4-pyhd8ed1ab_0.tar.b
 https://conda.anaconda.org/conda-forge/noarch/executing-0.8.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/flit-core-3.7.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fontconfig-2.14.0-h8e229c2_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/fsspec-2022.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/hdf5-1.12.1-nompi_h2386368_104.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/idna-3.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/iniconfig-1.1.1-pyh9f0ad1d_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/libva-2.14.0-h7f98852_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/locket-1.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/munkres-1.1.4-pyh9f0ad1d_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nest-asyncio-1.5.5-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/networkx-2.8.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pandocfilters-1.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/parso-0.8.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pathspec-0.9.0-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/py-1.11.0-pyh6c4a22f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyasn1-0.4.8-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pycparser-2.21-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pydeprecate-0.3.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyjwt-2.4.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyparsing-3.0.9-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/python-fastjsonschema-2.15.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/python_abi-3.9-2_cp39.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pytz-2022.1-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/tensorboard-plugin-wit-1.8.1-pyhd8
 https://conda.anaconda.org/conda-forge/noarch/threadpoolctl-3.1.0-pyh8a188c0_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tomli-2.0.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/toolz-0.11.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/traitlets-5.2.1.post0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/typing_extensions-4.2.0-pyha770c72_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/webencodings-0.5.1-py_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/websocket-client-1.3.2-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/asttokens-2.0.5-pyhd8ed1ab_0.tar.b
 https://conda.anaconda.org/conda-forge/noarch/babel-2.10.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/beautifulsoup4-4.11.1-pyha770c72_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cairo-1.16.0-ha12eb4b_1010.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/certifi-2022.5.18.1-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cffi-1.15.0-py39h4bc2ebd_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/click-8.1.3-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cytoolz-0.11.2-py39hb9d737c_2.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/frozenlist-1.3.0-py39hb9d737c_1.
 https://conda.anaconda.org/conda-forge/linux-64/future-0.18.2-py39hf3d152e_5.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/geotiff-1.7.1-h509b78c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/importlib-metadata-4.11.3-py39hf3d152e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/importlib_resources-5.7.1-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/jedi-0.18.1-py39hf3d152e_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/jupyter_core-4.10.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/kealib-1.4.14-h87e4c3c_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/kiwisolver-1.4.2-py39hf939315_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libdap4-3.20.6-hd7c4107_2.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/numpy-1.22.3-py39hc58783e_2.tar.
 https://conda.anaconda.org/conda-forge/noarch/packaging-21.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/partd-1.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pexpect-4.8.0-pyh9f0ad1d_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pillow-9.1.1-py39hae2aec6_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pluggy-1.0.0-py39hf3d152e_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/psutil-5.9.0-py39hb9d737c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/pysocks-1.7.1-py39hf3d152e_5.tar
 https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyu2f-0.1.5-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pyyaml-6.0-py39hb9d737c_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pyzmq-23.0.0-py39headdf64_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/rsa-4.8-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/rtree-1.0.0-py39hb102c33_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/setuptools-59.5.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/sniffio-1.2.0-py39hf3d152e_3.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tensorboard-data-server-0.6.0-py39hd97740a_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tiledb-2.8.3-h1e4a385_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tinycss2-1.1.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/tornado-6.1-py39hb9d737c_3.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tqdm-4.64.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/typed-ast-1.5.3-py39hb9d737c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/typing-extensions-4.2.0-hd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/unicodedata2-14.0.0-py39hb9d737c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/aiosignal-1.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/anyio-3.6.1-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/argon2-cffi-bindings-21.2.0-py39hb9d737c_2.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/async-timeout-4.0.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/backports.functools_lru_cache-1.6.4-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/brotlipy-0.7.0-py39hb9d737c_1004
 https://conda.anaconda.org/conda-forge/linux-64/cftime-1.6.0-py39hd257fcd_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/click-plugins-1.1.1-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/cligj-0.7.2-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cryptography-37.0.2-py39hd97740a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/cupy-10.4.0-py39hc3c280e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/dask-core-2022.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/einops-0.4.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fonttools-4.33.3-py39hb9d737c_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/grpcio-1.46.1-py39h0f497a6_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/imagecodecs-2022.2.22-py39h9c0c3a3_5.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/imageio-2.19.2-pyhcf75d05_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/importlib_metadata-4.11.3-hd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jinja2-3.1.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/joblib-1.1.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jsonschema-4.5.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyter_client-7.3.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/markdown-3.3.7-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/munch-2.5.0-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/omegaconf-2.1.2-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pandas-1.4.2-py39h1832856_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pip-22.0.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/poppler-22.04.0-h1434ded_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/protobuf-3.20.1-py39h5a03fae_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pygments-2.12.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pyproj-3.3.1-py39hcadae2f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/pytest-7.0.1-py39hf3d152e_0.tar.bz2
https://conda.anaconda.org/conda-forge/linux-64/scipy-1.8.0-py39hee8e79c_1.tar.b
 https://conda.anaconda.org/conda-forge/linux-64/shapely-1.8.2-py39h73b9895_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/snuggs-1.4.7-py_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/stack_data-0.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/terminado-0.15.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/yarl-1.7.2-py39hb9d737c_2.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/aiohttp-3.8.1-py39hb9d737c_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/argon2-cffi-21.3.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/blas-2.114-mkl.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/geopandas-base-0.10.2-pyha770c72_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyterlab_pygments-0.2.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/libgdal-3.4.3-h56144a5_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/matplotlib-base-3.5.2-py39h700656a_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbformat-5.4.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/netcdf4-1.5.8-nompi_py39h64b754b_101.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/oauthlib-3.2.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pyopenssl-22.0.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/scikit-learn-1.1.1-py39h4037b75_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tifffile-2022.5.4-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/wcwidth-0.2.5-pyh9f0ad1d_2.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/xarray-2022.3.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gdal-3.4.3-py39hc691d54_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbclient-0.6.3-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/prompt-toolkit-3.0.29-pyha770c72_0.tar.bz2
 https://conda.anaconda.org/pytorch/linux-64/pytorch-1.11.0-py3.9_cuda11.3_cudnn8.2.0_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/rasterio-1.2.10-py39h2e4b6e6_5.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/scikit-image-0.19.2-py39hde0f152_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.9-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/rapidsai/linux-64/cucim-22.02.00-cuda_11_py39_g29bfe6f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/efficientnet-pytorch-0.6.3-pyh9f0ad1d_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/fiona-1.8.21-py39h83acdc4_1.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/gmt-6.3.0-h793420d_4.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ipython-8.3.0-py39hf3d152e_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/kornia-0.6.5-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbconvert-core-6.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/requests-2.27.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/rioxarray-0.10.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/torchmetrics-0.8.2-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/google-auth-2.6.6-pyh6c4a22f_0.tar.bz2
 https://conda.anaconda.org/conda-forge/linux-64/ipykernel-6.13.0-py39hef51801_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyter_server-1.17.0-pyhd8ed1ab_0.tar.bz2
https://conda.anaconda.org/conda-forge/noarch/pygmt-0.5.0-pyhd8ed1ab_1.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/requests-oauthlib-1.3.1-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/pytorch/linux-64/torchvision-0.12.0-py39_cu113.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/google-auth-oauthlib-0.4.6-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/jupyterlab_server-2.14.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/nbconvert-6.5.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/notebook-shim-0.1.0-pyhd8ed1ab_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/pretrainedmodels-0.7.4-pyhd8ed1ab_2.tar.bz2"
KO;3;byrd-polar;s2s2net;d7f391c5f6fc4df24daaa4bb6dad56be226fad19;":zap: DeepSpeed ZeRO Stage 2 model parallel training (#2)

* :heavy_plus_sign: Add deepspeed

DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective! Using PyPI source for now until conda-forge package is released.

Also need to install newer gcc version to prevent error `Your compiler (c++ 4.8.5) may be ABI-incompatible with PyTorch! Please use a compiler that is ABI-compatible with GCC 5.0 and above` on the hpc server.

* :zap: Use DeepSpeed ZeRO Stage 2 model parallel strategy

Working towards conserving GPU memory for matters (inference on full-size images). Using DeepSpeed ZeRO Stage 2 which shards optimizer states (Stage 1) and gradients (Stage 2) across multiple GPUs. Have set devices to be auto instead of 2 so that I can run on 1 GPU on my laptop or 2 GPUs on the HPC server without changing values. Also needed to explicitly convert the input Sentinel-2 image tensor to float16 (if using 16-bit training) to avoid `RuntimeError: Input type (torch.cuda.ShortTensor) and weight type (torch.cuda.HalfTensor) should be the same`.";"name: s2s2net
 channels:
   - conda-forge
   - rapidsai
 dependencies:
   - conda-forge::black=22.3.0
   - rapidsai::cucim=22.02.00
   - conda-forge::cudatoolkit=11.3.1
   - conda-forge::geopandas-base=0.10.2
   - conda-forge::jupyterlab=3.3.2
   - conda-forge::pip=22.0.3
   - conda-forge::pygmt=0.5.0
dependencies:
   - conda-forge::rioxarray=0.10.1
   - conda-forge::torchgeo=0.2.0
   - pip:
     - https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/mmcv_full-1.4.8-cp39-cp39-manylinux1_x86_64.whl
     - mmsegmentation==0.21.1"
OK;3;byrd-polar;s2s2net;d7f391c5f6fc4df24daaa4bb6dad56be226fad19;":zap: DeepSpeed ZeRO Stage 2 model parallel training (#2)

* :heavy_plus_sign: Add deepspeed

DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective! Using PyPI source for now until conda-forge package is released.

Also need to install newer gcc version to prevent error `Your compiler (c++ 4.8.5) may be ABI-incompatible with PyTorch! Please use a compiler that is ABI-compatible with GCC 5.0 and above` on the hpc server.

* :zap: Use DeepSpeed ZeRO Stage 2 model parallel strategy

Working towards conserving GPU memory for matters (inference on full-size images). Using DeepSpeed ZeRO Stage 2 which shards optimizer states (Stage 1) and gradients (Stage 2) across multiple GPUs. Have set devices to be auto instead of 2 so that I can run on 1 GPU on my laptop or 2 GPUs on the HPC server without changing values. Also needed to explicitly convert the input Sentinel-2 image tensor to float16 (if using 16-bit training) to avoid `RuntimeError: Input type (torch.cuda.ShortTensor) and weight type (torch.cuda.HalfTensor) should be the same`.";"name: s2s2net
 channels:
   - conda-forge
   - rapidsai
   - nodefaults
 dependencies:
   - conda-forge::black=22.3.0
   - rapidsai::cucim=22.02.00
   - conda-forge::cudatoolkit=11.3.1
   - conda-forge::geopandas-base=0.10.2
   - conda-forge::gxx_linux-64=11.2.0
   - conda-forge::jupyterlab=3.3.2
   - conda-forge::pip=22.0.3
   - conda-forge::pygmt=0.5.0
dependencies:
   - conda-forge::rioxarray=0.10.1
   - conda-forge::torchgeo=0.2.0
   - pip:
     - deepspeed==0.6.4
     - https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/mmcv_full-1.4.8-cp39-cp39-manylinux1_x86_64.whl
     - mmsegmentation==0.21.1"
KO;3;byrd-polar;s2s2net;d7f391c5f6fc4df24daaa4bb6dad56be226fad19;":zap: DeepSpeed ZeRO Stage 2 model parallel training (#2)

* :heavy_plus_sign: Add deepspeed

DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective! Using PyPI source for now until conda-forge package is released.

Also need to install newer gcc version to prevent error `Your compiler (c++ 4.8.5) may be ABI-incompatible with PyTorch! Please use a compiler that is ABI-compatible with GCC 5.0 and above` on the hpc server.

* :zap: Use DeepSpeed ZeRO Stage 2 model parallel strategy

Working towards conserving GPU memory for matters (inference on full-size images). Using DeepSpeed ZeRO Stage 2 which shards optimizer states (Stage 1) and gradients (Stage 2) across multiple GPUs. Have set devices to be auto instead of 2 so that I can run on 1 GPU on my laptop or 2 GPUs on the HPC server without changing values. Also needed to explicitly convert the input Sentinel-2 image tensor to float16 (if using 16-bit training) to avoid `RuntimeError: Input type (torch.cuda.ShortTensor) and weight type (torch.cuda.HalfTensor) should be the same`.";"def evaluate(
             - iou (Intersection over Union)
             - f1 (F1 score)
         """"""
         x: torch.Tensor = batch[""image""].float()  # Input Sentinel-2 image
         y: torch.Tensor = batch[""mask""]  # Groundtruth binary mask
         y_highres: torch.Tensor = batch[""hres""]  # High resolution image
         # y = torch.randn(8, 1, 2560, 2560)
def cli_main():
     trainer: pl.Trainer = pl.Trainer(
         # deterministic=True,
         accelerator=""auto"",
         devices=2,
         strategy=""ddp_find_unused_parameters_false"",
         logger=tensorboard_logger,
         max_epochs=27,
         precision=16,"
OK;3;byrd-polar;s2s2net;d7f391c5f6fc4df24daaa4bb6dad56be226fad19;":zap: DeepSpeed ZeRO Stage 2 model parallel training (#2)

* :heavy_plus_sign: Add deepspeed

DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective! Using PyPI source for now until conda-forge package is released.

Also need to install newer gcc version to prevent error `Your compiler (c++ 4.8.5) may be ABI-incompatible with PyTorch! Please use a compiler that is ABI-compatible with GCC 5.0 and above` on the hpc server.

* :zap: Use DeepSpeed ZeRO Stage 2 model parallel strategy

Working towards conserving GPU memory for matters (inference on full-size images). Using DeepSpeed ZeRO Stage 2 which shards optimizer states (Stage 1) and gradients (Stage 2) across multiple GPUs. Have set devices to be auto instead of 2 so that I can run on 1 GPU on my laptop or 2 GPUs on the HPC server without changing values. Also needed to explicitly convert the input Sentinel-2 image tensor to float16 (if using 16-bit training) to avoid `RuntimeError: Input type (torch.cuda.ShortTensor) and weight type (torch.cuda.HalfTensor) should be the same`.";"def evaluate(
             - iou (Intersection over Union)
             - f1 (F1 score)
         """"""
         dtype = torch.float16 if self.precision == 16 else torch.float
         x: torch.Tensor = batch[""image""].to(dtype=dtype)  # Input Sentinel-2 image
         y: torch.Tensor = batch[""mask""]  # Groundtruth binary mask
         y_highres: torch.Tensor = batch[""hres""]  # High resolution image
         # y = torch.randn(8, 1, 2560, 2560)
def cli_main():
     trainer: pl.Trainer = pl.Trainer(
         # deterministic=True,
         accelerator=""auto"",
         devices=""auto"",
         strategy=""deepspeed_stage_2"",
         logger=tensorboard_logger,
         max_epochs=27,
         precision=16,"
KO;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";"__pycache__/
 
 # Data files and folders
 *.csv"
OK;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";"__pycache__/
 
 # Data files and folders
 *.csv
 *.npy
 
 # Pytorch Lightning checkpoints
 lightning_logs/
 tb_logs/
 *.ckpt"
KO;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";" # S2S2Net Python package
 
 This folder contains Python scripts used to pre-process satellite data, as well
 as the neural network model architecture, data loaders, and training/testing"
OK;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";" # S2S2Net Python application
 
 This folder contains Python scripts used to pre-process satellite data, as well
 as the neural network model architecture, data loaders, and training/testing"
KO;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";"def __init__(self):
         ## Upsampling layers (Output). 1st one to get back original image size
         # 2nd upsample is to get a super-resolution result. Each of the two
         # upsample layers are followed by a Convolutional 2D layer.
         self.upsample_1 = torch.nn.Upsample(scale_factor=4, mode=""nearest"")
         self.post_upsample_conv_layer_1 = torch.nn.Conv2d(
             in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1
         )
         self.upsample_2 = torch.nn.Upsample(scale_factor=5, mode=""nearest"")
         self.post_upsample_conv_layer_2 = torch.nn.Conv2d(
             in_channels=8, out_channels=1, kernel_size=3, stride=1, padding=1
         )
 
         # Evaluation metrics to know how good the segmentation results are
         self.iou = torchmetrics.JaccardIndex(num_classes=2)
         self.f1_score = torchmetrics.F1Score(num_classes=1)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         """"""
         Forward pass (Inference/Prediction).
 
def forward(self, x: torch.Tensor) -> torch.Tensor:
         # print(""segformer_output:"", segformer_output.shape) # (8, 16, 128, 128)
 
         ## Step 3. Do a series of bilinear interpolation upsampling + Conv2d
         up1_output: torch.Tensor = self.upsample_1(segformer_output)
         up1_conv_output: torch.Tensor = self.post_upsample_conv_layer_1(up1_output)
         # print(""up1_output.shape:"", up1_output.shape)  # (8, 8, 512, 512)
         up2_output: torch.Tensor = self.upsample_2(up1_conv_output)
         up2_conv_output: torch.Tensor = self.post_upsample_conv_layer_2(up2_output)
         # print(""up2_output.shape:"", up2_output.shape)  # (8, 1, 2560, 2560)
 
         return up2_conv_output
 
     def training_step(
         self, batch: typing.Dict[str, torch.Tensor], batch_idx: int
def training_step(
         """"""
         x: torch.Tensor = batch[""image""].float()  # Input Sentinel-2 image
         y: torch.Tensor = batch[""mask""]  # Groundtruth binary mask
 
         y_hat: torch.Tensor = self(x)
 
         # Calculate loss value to minimize
         # loss: float = F.binary_cross_entropy_with_logits(input=y_hat, target=y)
         loss: float = torchvision.ops.sigmoid_focal_loss(
             inputs=y_hat, targets=y, alpha=0.75, gamma=2, reduction=""mean""
         )
 
         # Calculate metrics to determine how good results are
         iou_score: torch.Tensor = self.iou(
             preds=y_hat.squeeze(),
             target=(y > 0.5).squeeze().to(dtype=torch.int8),  # binarize
         )
         f1_score: torch.Tensor = self.f1_score(
             preds=y_hat.ravel(),
             target=(y > 0.5).ravel().to(dtype=torch.int8),  # binarize
         )
         metrics: typing.Dict[str, torch.Tensor] = {""iou"": iou_score, ""f1"": f1_score}
def training_step(
                     # epoch=self.current_epoch,
                 )
 
         return {""loss"": loss, **metrics}
 
     def predict_step(
         self,
def predict_step(
 
         y_hat: torch.Tensor = self(x)
 
         return torch.sigmoid(input=y_hat)
 
     def configure_optimizers(self):
         """"""
def cli_main():
     # torch.use_deterministic_algorithms(True, warn_only=True)
     trainer: pl.Trainer = pl.Trainer(
         # deterministic=True,
         gpus=1,
         logger=tensorboard_logger,
         max_epochs=27,
         precision=16,"
OK;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";"def __init__(self):
         ## Upsampling layers (Output). 1st one to get back original image size
         # 2nd upsample is to get a super-resolution result. Each of the two
         # upsample layers are followed by a Convolutional 2D layer.
         self.segmmask_upsample_0 = torch.nn.Upsample(scale_factor=4, mode=""nearest"")
         self.segmmask_post_upsample_conv_layer_0 = torch.nn.Conv2d(
             in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1
         )
         self.segmmask_upsample_1 = torch.nn.Upsample(scale_factor=5, mode=""nearest"")
         self.segmmask_post_upsample_conv_layer_1 = torch.nn.Conv2d(
             in_channels=8, out_channels=1, kernel_size=3, stride=1, padding=1
         )
 
         self.superres_upsample_0 = torch.nn.Upsample(scale_factor=4, mode=""nearest"")
         self.superres_post_upsample_conv_layer_0 = torch.nn.Conv2d(
             in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1
         )
         self.superres_upsample_1 = torch.nn.Upsample(scale_factor=5, mode=""nearest"")
         self.superres_post_upsample_conv_layer_1 = torch.nn.Conv2d(
             in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1
         )
 
         # Evaluation metrics to know how good the segmentation results are
         self.iou = torchmetrics.JaccardIndex(num_classes=2)
         self.f1_score = torchmetrics.F1Score(num_classes=1)
 
     def forward(self, x: torch.Tensor) -> typing.Dict[str, torch.Tensor]:
         """"""
         Forward pass (Inference/Prediction).
 
def forward(self, x: torch.Tensor) -> torch.Tensor:
         # print(""segformer_output:"", segformer_output.shape) # (8, 16, 128, 128)
 
         ## Step 3. Do a series of bilinear interpolation upsampling + Conv2d
         # Step 3a. Semantic Segmentation Super-Resolution (SSSR)
         segmmask_up_output_0: torch.Tensor = self.segmmask_upsample_0(segformer_output)
         segmmask_conv_output_0: torch.Tensor = self.segmmask_post_upsample_conv_layer_0(
             segmmask_up_output_0
         )
         # print(""segmmask_conv_output_0.shape:"", segmmask_conv_output_0.shape)  # (8, 8, 512, 512)
         segmmask_up_output_1: torch.Tensor = self.segmmask_upsample_1(
             segmmask_conv_output_0
         )
         segmmask_conv_output_1: torch.Tensor = self.segmmask_post_upsample_conv_layer_1(
             segmmask_up_output_1
         )
         # print(""segmmask_conv_output_1.shape:"", segmmask_conv_output_1.shape)  # (8, 1, 2560, 2560)
 
         # Step 3b. Single Image Super-Resolution (SISR)
         superres_up_output_0: torch.Tensor = self.superres_upsample_0(segformer_output)
         superres_conv_output_0: torch.Tensor = self.superres_post_upsample_conv_layer_0(
             superres_up_output_0
         )
         # print(""superres_conv_output_0.shape:"", superres_conv_output_0.shape)  # (8, 8, 512, 512)
         superres_up_output_1: torch.Tensor = self.superres_upsample_1(
             superres_conv_output_0
         )
         superres_conv_output_1: torch.Tensor = self.superres_post_upsample_conv_layer_1(
             superres_up_output_1
         )
         # print(""superres_conv_output_1.shape:"", superres_conv_output_1.shape)  # (8, 1, 2560, 2560)
 
         return {
             ""segmmask_conv_output_0"": segmmask_conv_output_0,  # for FA loss
             ""segmmask_conv_output_1"": segmmask_conv_output_1,  # segmentation output
             ""superres_conv_output_0"": superres_conv_output_0,  # for FA loss
             ""superres_conv_output_1"": superres_conv_output_1,  # super-resolution output
         }
 
     def training_step(
         self, batch: typing.Dict[str, torch.Tensor], batch_idx: int
def training_step(
         """"""
         x: torch.Tensor = batch[""image""].float()  # Input Sentinel-2 image
         y: torch.Tensor = batch[""mask""]  # Groundtruth binary mask
         y_highres: torch.Tensor = batch[""hres""]  # High resolution image
         # y = torch.randn(8, 1, 2560, 2560)
         # y_highres = torch.randn(8, 4, 2560, 2560)
 
         y_hat: typing.Dict[str, torch.Tensor] = self(x)
 
         ## Calculate loss values to minimize
         def similarity_matrix(f):
             # f expected shape (Bs, C', H', W')
             # before computing the relationship of every pair of pixels,
             # subsample the feature map to its 1/8
             f = F.interpolate(
                 f, size=(f.shape[2] // 8, f.shape[3] // 8), mode=""nearest""
             )
             f = f.permute((0, 2, 3, 1))
             f = torch.reshape(f, (f.shape[0], -1, f.shape[3]))  # shape (Bs, H'xW', C')
             f_n = torch.linalg.norm(f, ord=None, dim=2).unsqueeze(
                 -1
             )  # ord=None indicates 2-Norm,
             # unsqueeze last dimension to broadcast later
             eps = 1e-8
             f_norm = f / torch.max(f_n, eps * torch.ones_like(f_n))
             sim_mt = f_norm @ f_norm.transpose(2, 1)
             return sim_mt
 
         # 1: Feature Affinity loss calculation
         _segmmask_sim_matrix: torch.Tensor = similarity_matrix(
             f=y_hat[""segmmask_conv_output_0""]
         )
         _superres_sim_matrix: torch.Tensor = similarity_matrix(
             f=y_hat[""superres_conv_output_0""]
         )
         _n_elements: int = (
             _segmmask_sim_matrix.shape[-2] * _segmmask_sim_matrix.shape[-1]
         )
         _abs_dist: torch.Tensor = torch.abs(_segmmask_sim_matrix - _superres_sim_matrix)
         feature_affinity_loss: torch.Tensor = torch.mean(
             (1 / _n_elements) * torch.sum(input=_abs_dist, dim=[-2, -1])
         )
 
         # 2: Semantic Segmentation loss (Focal Loss)
         segmmask_loss: torch.Tensor = torchvision.ops.sigmoid_focal_loss(
             inputs=y_hat[""segmmask_conv_output_1""],
             targets=y,
             alpha=0.75,
             gamma=2,
             reduction=""mean"",
         )
         # 3: Super-Resolution loss (Mean Absolute Error)
         superres_loss: torch.Tensor = torchmetrics.functional.mean_absolute_error(
             preds=y_hat[""superres_conv_output_1""],
             target=y_highres.to(dtype=torch.float16),
         )
 
         # 1 + 2 + 3: Calculate total loss and log to console
         total_loss: torch.Tensor = (
             (1.0 * feature_affinity_loss) + segmmask_loss + (0.001 * superres_loss)
         )
         losses: typing.Dict[str, torch.Tensor] = {
             # Component losses (Feature Affinity, Segmentation, Super-Resolution)
             ""loss_feataffy"": feature_affinity_loss.detach(),
             ""loss_segmmask"": segmmask_loss.detach(),
             ""loss_superres"": superres_loss.detach(),
         }
         self.log_dict(dictionary=losses, prog_bar=True)
 
         # Calculate metrics to determine how good results are
         iou_score: torch.Tensor = self.iou(  # Intersection over Union
             preds=y_hat[""segmmask_conv_output_1""].squeeze(),
             target=(y > 0.5).squeeze().to(dtype=torch.int8),  # binarize
         )
         f1_score: torch.Tensor = self.f1_score(  # F1 Score
             preds=y_hat[""segmmask_conv_output_1""].ravel(),
             target=(y > 0.5).ravel().to(dtype=torch.int8),  # binarize
         )
         metrics: typing.Dict[str, torch.Tensor] = {""iou"": iou_score, ""f1"": f1_score}
def training_step(
                     # epoch=self.current_epoch,
                 )
 
         return total_loss  # {**losses, **metrics}
 
     def predict_step(
         self,
def predict_step(
 
         y_hat: torch.Tensor = self(x)
 
         return (
             torch.sigmoid(input=y_hat[""segmmask_conv_output_1""]),
             y_hat[""superres_conv_output_1""],
         )
 
     def configure_optimizers(self):
         """"""
def cli_main():
     # torch.use_deterministic_algorithms(True, warn_only=True)
     trainer: pl.Trainer = pl.Trainer(
         # deterministic=True,
         gpus=2,
         strategy=pl.plugins.DDPPlugin(find_unused_parameters=False),
         logger=tensorboard_logger,
         max_epochs=27,
         precision=16,"
KO;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";" https://github.com/PyTorchLightning/pytorch-lightning/blob/1.5.10/.github/CONTRIBUTING.md#how-to-add-new-tests
 """"""
 import pytorch_lightning as pl
 import s2s2net.model
 import torch
 
 
 class RandomDataset(torch.utils.data.Dataset):
     def __init__(self):
def __len__(self):
         return 2
 
     def __getitem__(self, idx: int) -> dict:
         return {""image"": torch.randn(4, 512, 512), ""mask"": torch.randn(1, 2560, 2560)}
 
 
 def test_s2s2net():
def test_s2s2net():
 
     # Test inference
     predictions = trainer.predict(model=model, dataloaders=dataloader)
     assert torch.cat(tensors=predictions).shape == (1, 1, 2560, 2560)"
OK;3;byrd-polar;s2s2net;979358749bdbca1587322ff4cd14504e882aa3b3;":busts_in_silhouette: Dual Network architecture for S2S2Net

Two branches - one for segmentation, one for super-resolution. Both joined using a Feature Affinity loss (1x weighting). In addition to the segmentation branch's focal loss (1x weighting), there is also a super-resolution branch mean absolute error loss (0.001 weighting). Code adapted from Abadal et al., 2021, but doing 5x super-resolution instead. Need to use 2 GPUs because this network uses a lot of GPU memory. Also gitignoring .npy files and Pytorch Lightning checkpoint files.";" https://github.com/PyTorchLightning/pytorch-lightning/blob/1.5.10/.github/CONTRIBUTING.md#how-to-add-new-tests
 """"""
 import pytorch_lightning as pl
 import torch
 
 import s2s2net.model
 
 
 class RandomDataset(torch.utils.data.Dataset):
     def __init__(self):
def __len__(self):
         return 2
 
     def __getitem__(self, idx: int) -> dict:
         return {
             ""image"": torch.randn(4, 512, 512),
             ""mask"": torch.randn(1, 2560, 2560),
             ""hres"": torch.randn(4, 2560, 2560),
         }
 
 
 def test_s2s2net():
def test_s2s2net():
 
     # Test inference
     predictions = trainer.predict(model=model, dataloaders=dataloader)
     segmmask, superres = predictions[0]
     assert segmmask.shape == (1, 1, 2560, 2560)
     assert superres.shape == (1, 4, 2560, 2560)"
KO;6;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import wordle as w
 import state as s
 
 
 if __name__ == '__main__':
     creds = botlib.Creds(

     
     bot = botlib.Bot(creds)
     prefix = os.environ.get('PREFIX', 'w!')
 
     try:
         s.load_state()
     except FileNotFoundError:
         s.save_state({})
 
     @bot.listener.on_message_event
     async def help_message(room, message):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command('h') or match.command('help'))):
             return
         
         response = (""## matrix-wordle\n""
async def help_message(room, message):
     @bot.listener.on_message_event
     @s.ensure_state
     async def start_game(room, message, state):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command('start') or match.command('s'))):
             return
         
         user = message.sender
         state[user] = 0
         response = (""Starting new Wordle game!\n""\
                     f""Guess a 5 letter word with \""{prefix}guess\""\n""\
                     ""6 guesses remaining.""
async def start_game(room, message, state):
     @bot.listener.on_message_event
     @s.ensure_state
     async def guess_word(room, message, state):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command('guess') or match.command('g'))):
             return
         
         user = message.sender
 
         result = w.check_guess(match.args()[0], w.get_daily())
         response = f""{'   '.join(x for x in match.args()[0])}\n{''.join(x for x in result)}""
         
         
         if not (user in state.keys()):
             response = f""Please start a new Wordle game with \""{prefix}start\"" before guessing!""
             await bot.api.send_text_message(room.room_id, response)
             return
         
         if not state[user] in range(0, 5):
             response = f""{response}\nOut of guesses!\n The answer may be revealed with \""{prefix}answer\"".""
             await bot.api.send_markdown_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
         
         if not (
             len(match.args()) == 1 and
             w.check_guess(
                 match.args()[0], w.get_daily()
                 )):
             response = ""Invalid guess. Please guess a valid 5 letter word""
             await bot.api.send_text_message(room.room_id, response)
             return
         
         if result == list(w.GREEN*5):
             response = f""{response}\nThe answer was {w.get_daily()}. You Won in {state[user]+1} guesses!""
             await bot.api.send_text_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
 
         state[user] = state[user] + 1
         s.save_state(state)
         response = f""{response}\n{6-state[user]} guesses remaining.""
         await bot.api.send_text_message(room.room_id, response)
 
     @bot.listener.on_message_event
     async def reveal_answer(room, message):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command(""answer"") or match.command(""a""))):
             return
         response = f""The answer is {w.get_daily()}.""
         await bot.api.send_text_message(room.room_id, response)
 
     bot.run()
\ No newline at end of file"
OK;6;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import wordle as w
 import state as s
 
 import string
 
 if __name__ == '__main__':
     creds = botlib.Creds(

     
     bot = botlib.Bot(creds)
     prefix = os.environ.get('PREFIX', 'w!')
     wordle = w.Wordle()
 
     try:
         s.load_state()
     except FileNotFoundError:
         s.save_state({})
 
     def bot_msg_match(room, message):
         """"""Wrapper function to reduce repetition""""""
         return botlib.MessageMatch(room, message, bot, prefix)
 
     @bot.listener.on_message_event
     async def help_message(room, message):
         """"""Display help message to user""""""
         match = bot_msg_match(room, message)
         if not valid_command(""help"", match):
             return
         
         response = (""## matrix-wordle\n""
async def help_message(room, message):
     @bot.listener.on_message_event
     @s.ensure_state
     async def start_game(room, message, state):
         """"""Initialize the state and game for requesting user""""""
         match = bot_msg_match(room, message)
         if not valid_command(""start"", match):
             return
         
         user = message.sender
         state[user] = {""guesses"": 0, ""guessed_letters"": [], ""letters_remaining"": list(string.ascii_uppercase)}
         response = (""Starting new Wordle game!\n""\
                     f""Guess a 5 letter word with \""{prefix}guess\""\n""\
                     ""6 guesses remaining.""
async def start_game(room, message, state):
     @bot.listener.on_message_event
     @s.ensure_state
     async def guess_word(room, message, state):
         """"""Check the word guessed by user""""""
         match = bot_msg_match(room, message)
         if not valid_command(""guess"", match):
             return
 
         user = message.sender
 
         # Game not started yet
         if not (user in state.keys()):
             response = f""Please start a new Wordle game with \""{prefix}start\"" before guessing!""
             await bot.api.send_text_message(room.room_id, response)
             return
 
         # Invalid guess
         if len(match.args()) != 1 or len(match.args()[0]) != 5:
             response = f""Invalid guess, please provide a valid 5 letter word with \""{prefix}guess <word>\"".""
             await bot.api.send_text_message(room.room_id, response)
             return
 
         # Build bot's response
         result = wordle.check_guess(match.args()[0])
         response = f""{'   '.join(x for x in match.args()[0])}\n{''.join(x for x in result['space'])}""
 
         # Unknown word
         if not wordle.known(match.args()[0]):
             response = f""Unknown word: \""{match.args()[0]}\""; please try again.""
             await bot.api.send_text_message(room.room_id, response)
             return
 
         # Out of guesses
         if state[user]['guesses'] > 5:
             response = f""Out of guesses!\n The answer may be revealed with \""{prefix}answer\"".""
             await bot.api.send_markdown_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
 
         # Answer guessed correctly
         if result['space'] == list(wordle.GREEN*5):
             response = f""{response}\nThe answer was {wordle.get_daily()}. You Won in {state[user]['guesses']+1} guesses!""
             await bot.api.send_text_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
 
         # Mid-game
         state[user]['guesses'] += 1
         for letter in result['valid']:
             if letter not in state[user]['guessed_letters']:
                 state[user]['guessed_letters'].append(letter)
                 state[user]['guessed_letters'] = sorted(state[user]['guessed_letters'])
             if letter in state[user]['letters_remaining']:
                 state[user]['letters_remaining'].remove(letter)
 
         for letter in result['invalid']:
             if letter in state[user]['letters_remaining']:
                 state[user]['letters_remaining'].remove(letter)
 
         s.save_state(state)
 
         response = (f""{response}\n""
             f""{6 - state[user]['guesses']} guesses remaining.\n""
             f""discovered: {''.join(state[user]['guessed_letters'])}\n""
             f""remaining: {''.join(state[user]['letters_remaining'])}""
         )
         await bot.api.send_text_message(room.room_id, response)
 
     @bot.listener.on_message_event
     async def reveal_answer(room, message):
         """"""Reveal the answer to today's puzzle""""""
         match = bot_msg_match(room, message)
         if not valid_command(""answer"", match):
             return
         response = f""The answer is {wordle.get_daily()}.""
         await bot.api.send_text_message(room.room_id, response)
 
\ No newline at end of file
     def valid_command(cmd, match) -> bool:
         """"""Check if the bot command is expected""""""
         if not match.prefix():
             return False
 
         cmd_start = cmd[0]
         if match.command(cmd) or match.command(cmd_start):
             return True
 
         return False
 
     bot.run()"
KO;6;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import json
 
 
 def load_state():
     with open('state.json', 'r') as f:
         return json.load(f)
 "
OK;6;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import json
 import os
 
 
 def load_state():
     if not os.path.isfile('state.json'):
         save_state({})
 
     with open('state.json', 'r') as f:
         return json.load(f)
 "
KO;6;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import datetime
 import os
 
 GREEN = '🟩'
 YELLOW = '🟨'
 BLACK = '⬛'
 
 def get_daily():
     now = datetime.datetime.now()
     month, day, year = now.strftime(""%b""), str(now.day).zfill(2), str(now.year)
 
     with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
         reader = csv.reader(f, delimiter=',')
         for row in reader:
             if (row[0], row[1], row[2]) == (month, day, year):
                 return row[5]
 
 def check_guess(guess: str, answer: str) -> list:
     guess = guess.upper()
 
     with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
         reader = csv.reader(f, delimiter=',')
         valid = False
         for row in reader:
             if row[5] == guess:
                 valid = True
         if not valid:
             return []
 
     check = []
     for i in range(5):
         if guess[i] == answer[i]:
             check.append(GREEN)
         elif guess[i] in answer:
             check.append(YELLOW)
         else:
             check.append(BLACK)
     return check
\ No newline at end of file"
OK;6;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import datetime
 import os
 
\ No newline at end of file
 class Wordle:
     """"""Game class for interacting with the known words""""""
     GREEN = '🟩'
     YELLOW = '🟨'
     BLACK = '⬛'
 
     def __init__(self):
         """"""Initialize and read in the words database""""""
         self.word_day = 0
         self.word_db = []
         self.word_of_the_day = """"
 
         # Read the words into the database
         with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
             reader = csv.reader(f, delimiter=',')
             for row in reader:
                 self.word_db.append(row[5])
         self.get_daily()
 
     def get_todays_word(self, month: str, day: str, year: str):
         """"""Retrieve the day's word from the CSV""""""
         with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
             reader = csv.reader(f, delimiter=',')
             for row in reader:
                 if (row[0], row[1], row[2]) == (month, day, year):
                     self.word_of_the_day = row[5]
                     break
 
     def get_daily(self) -> str:
         """"""Check the last retrieval date and get today's word if needed.""""""
         now = datetime.datetime.now()
         month, day, year = now.strftime(""%b""), str(now.day).zfill(2), str(now.year)
 
         if self.word_day == 0 or self.word_day != day:
             self.word_day = day
             self.get_todays_word(month, day, year)
 
         return self.word_of_the_day
 
     def check_guess(self, guess: str) -> list:
         """"""Check how correct the user guess is
 
         results = {
             ""space"": ""[green][black][black][yellow][black]"",
             ""valid"": ['c', 'h'],
             ""invalid"": ['a', 's']
         }
         """"""
         results = {""space"": [], ""valid"": [], ""invalid"": []}
         guess = guess.upper()
 
         if not self.known(guess):
             return results
 
         for i in range(5):
             if guess[i] == self.word_of_the_day[i]:
                 results['space'].append(self.GREEN)
                 if guess[i] not in results['valid']:
                     results['valid'].append(guess[i])
             elif guess[i] in self.word_of_the_day:
                 results['space'].append(self.YELLOW)
                 if guess[i] not in results['valid']:
                     results['valid'].append(guess[i])
             else:
                 results['space'].append(self.BLACK)
                 if guess[i] not in results['invalid']:
                     results['invalid'].append(guess[i])
 
         return results
 
     def known(self, guess: str) -> bool:
         """"""Check if the guessed word is known by the bot""""""
         guess = guess.upper()
 
         return guess in self.word_db"
KO;7;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import wordle as w
 import state as s
 
 
 if __name__ == '__main__':
     creds = botlib.Creds(

     
     bot = botlib.Bot(creds)
     prefix = os.environ.get('PREFIX', 'w!')
 
     try:
         s.load_state()
     except FileNotFoundError:
         s.save_state({})
 
     @bot.listener.on_message_event
     async def help_message(room, message):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command('h') or match.command('help'))):
             return
         
         response = (""## matrix-wordle\n""
async def help_message(room, message):
     @bot.listener.on_message_event
     @s.ensure_state
     async def start_game(room, message, state):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command('start') or match.command('s'))):
             return
         
         user = message.sender
         state[user] = 0
         response = (""Starting new Wordle game!\n""\
                     f""Guess a 5 letter word with \""{prefix}guess\""\n""\
                     ""6 guesses remaining.""
async def start_game(room, message, state):
     @bot.listener.on_message_event
     @s.ensure_state
     async def guess_word(room, message, state):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command('guess') or match.command('g'))):
             return
         
         user = message.sender
 
         result = w.check_guess(match.args()[0], w.get_daily())
         response = f""{'   '.join(x for x in match.args()[0])}\n{''.join(x for x in result)}""
         
         
         if not (user in state.keys()):
             response = f""Please start a new Wordle game with \""{prefix}start\"" before guessing!""
             await bot.api.send_text_message(room.room_id, response)
             return
         
         if not state[user] in range(0, 5):
             response = f""{response}\nOut of guesses!\n The answer may be revealed with \""{prefix}answer\"".""
             await bot.api.send_markdown_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
         
         if not (
             len(match.args()) == 1 and
             w.check_guess(
                 match.args()[0], w.get_daily()
                 )):
             response = ""Invalid guess. Please guess a valid 5 letter word""
             await bot.api.send_text_message(room.room_id, response)
             return
         
         if result == list(w.GREEN*5):
             response = f""{response}\nThe answer was {w.get_daily()}. You Won in {state[user]+1} guesses!""
             await bot.api.send_text_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
 
         state[user] = state[user] + 1
         s.save_state(state)
         response = f""{response}\n{6-state[user]} guesses remaining.""
         await bot.api.send_text_message(room.room_id, response)
 
     @bot.listener.on_message_event
     async def reveal_answer(room, message):
         match = botlib.MessageMatch(room, message, bot, prefix)
         if not (match.prefix() and (match.command(""answer"") or match.command(""a""))):
             return
         response = f""The answer is {w.get_daily()}.""
         await bot.api.send_text_message(room.room_id, response)
 
     bot.run()
\ No newline at end of file"
OK;7;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import wordle as w
 import state as s
 
 import string
 
 if __name__ == '__main__':
     creds = botlib.Creds(

     
     bot = botlib.Bot(creds)
     prefix = os.environ.get('PREFIX', 'w!')
     wordle = w.Wordle()
 
     try:
         s.load_state()
     except FileNotFoundError:
         s.save_state({})
 
     def bot_msg_match(room, message):
         """"""Wrapper function to reduce repetition""""""
         return botlib.MessageMatch(room, message, bot, prefix)
 
     @bot.listener.on_message_event
     async def help_message(room, message):
         """"""Display help message to user""""""
         match = bot_msg_match(room, message)
         if not valid_command(""help"", match):
             return
         
         response = (""## matrix-wordle\n""
async def help_message(room, message):
     @bot.listener.on_message_event
     @s.ensure_state
     async def start_game(room, message, state):
         """"""Initialize the state and game for requesting user""""""
         match = bot_msg_match(room, message)
         if not valid_command(""start"", match):
             return
         
         user = message.sender
         state[user] = {""guesses"": 0, ""guessed_letters"": [], ""letters_remaining"": list(string.ascii_uppercase)}
         response = (""Starting new Wordle game!\n""\
                     f""Guess a 5 letter word with \""{prefix}guess\""\n""\
                     ""6 guesses remaining.""
async def start_game(room, message, state):
     @bot.listener.on_message_event
     @s.ensure_state
     async def guess_word(room, message, state):
         """"""Check the word guessed by user""""""
         match = bot_msg_match(room, message)
         if not valid_command(""guess"", match):
             return
 
         user = message.sender
 
         # Game not started yet
         if not (user in state.keys()):
             response = f""Please start a new Wordle game with \""{prefix}start\"" before guessing!""
             await bot.api.send_text_message(room.room_id, response)
             return
 
         # Invalid guess
         if len(match.args()) != 1 or len(match.args()[0]) != 5:
             response = f""Invalid guess, please provide a valid 5 letter word with \""{prefix}guess <word>\"".""
             await bot.api.send_text_message(room.room_id, response)
             return
 
         # Build bot's response
         result = wordle.check_guess(match.args()[0])
         response = f""{'   '.join(x for x in match.args()[0])}\n{''.join(x for x in result['space'])}""
 
         # Unknown word
         if not wordle.known(match.args()[0]):
             response = f""Unknown word: \""{match.args()[0]}\""; please try again.""
             await bot.api.send_text_message(room.room_id, response)
             return
 
         # Out of guesses
         if state[user]['guesses'] > 5:
             response = f""Out of guesses!\n The answer may be revealed with \""{prefix}answer\"".""
             await bot.api.send_markdown_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
 
         # Answer guessed correctly
         if result['space'] == list(wordle.GREEN*5):
             response = f""{response}\nThe answer was {wordle.get_daily()}. You Won in {state[user]['guesses']+1} guesses!""
             await bot.api.send_text_message(room.room_id, response)
             state.pop(user)
             s.save_state(state)
             return
 
         # Mid-game
         state[user]['guesses'] += 1
         for letter in result['valid']:
             if letter not in state[user]['guessed_letters']:
                 state[user]['guessed_letters'].append(letter)
                 state[user]['guessed_letters'] = sorted(state[user]['guessed_letters'])
             if letter in state[user]['letters_remaining']:
                 state[user]['letters_remaining'].remove(letter)
 
         for letter in result['invalid']:
             if letter in state[user]['letters_remaining']:
                 state[user]['letters_remaining'].remove(letter)
 
         s.save_state(state)
 
         response = (f""{response}\n""
             f""{6 - state[user]['guesses']} guesses remaining.\n""
             f""discovered: {''.join(state[user]['guessed_letters'])}\n""
             f""remaining: {''.join(state[user]['letters_remaining'])}""
         )
         await bot.api.send_text_message(room.room_id, response)
 
     @bot.listener.on_message_event
     async def reveal_answer(room, message):
         """"""Reveal the answer to today's puzzle""""""
         match = bot_msg_match(room, message)
         if not valid_command(""answer"", match):
             return
         response = f""The answer is {wordle.get_daily()}.""
         await bot.api.send_text_message(room.room_id, response)
 
\ No newline at end of file
     def valid_command(cmd, match) -> bool:
         """"""Check if the bot command is expected""""""
         if not match.prefix():
             return False
 
         cmd_start = cmd[0]
         if match.command(cmd) or match.command(cmd_start):
             return True
 
         return False
 
     bot.run()"
KO;7;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import json
 
 
 def load_state():
     with open('state.json', 'r') as f:
         return json.load(f)
 "
OK;7;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import json
 import os
 
 
 def load_state():
     if not os.path.isfile('state.json'):
         save_state({})
 
     with open('state.json', 'r') as f:
         return json.load(f)
 "
KO;7;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import datetime
 import os
 
 GREEN = '🟩'
 YELLOW = '🟨'
 BLACK = '⬛'
 
 def get_daily():
     now = datetime.datetime.now()
     month, day, year = now.strftime(""%b""), str(now.day).zfill(2), str(now.year)
 
     with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
         reader = csv.reader(f, delimiter=',')
         for row in reader:
             if (row[0], row[1], row[2]) == (month, day, year):
                 return row[5]
 
 def check_guess(guess: str, answer: str) -> list:
     guess = guess.upper()
 
     with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
         reader = csv.reader(f, delimiter=',')
         valid = False
         for row in reader:
             if row[5] == guess:
                 valid = True
         if not valid:
             return []
 
     check = []
     for i in range(5):
         if guess[i] == answer[i]:
             check.append(GREEN)
         elif guess[i] in answer:
             check.append(YELLOW)
         else:
             check.append(BLACK)
     return check
\ No newline at end of file"
OK;7;i10b;matrix-wordle;5bed07e22ac082462716f6e2d9a03b559aa3b2d9;"Implement guessed character tracking

* Fixes off-by-one error when 1 guess remains, bot mistakenly revealed
answer in spite of one guess remaining and correct answer being guessed
* Fixes crash when no guess is supplied (w!g <blank>)
* Fixes state saving crash when file doesn't exist
* Expands the state module to track guessed and remaining letters to show
the user in response.
  e.x., example word: choke
   s   h   o   w   n
  5 guesses remaining.
  discovered: HO
  remaining: ABCDEFGIJKLMPQRTUVXYZ
* Read word list into memory for easier / faster checking
* Read day's word into memory to prevent file scanning
* Differentiate between invalid guesses and unknown words";" import datetime
 import os
 
\ No newline at end of file
 class Wordle:
     """"""Game class for interacting with the known words""""""
     GREEN = '🟩'
     YELLOW = '🟨'
     BLACK = '⬛'
 
     def __init__(self):
         """"""Initialize and read in the words database""""""
         self.word_day = 0
         self.word_db = []
         self.word_of_the_day = """"
 
         # Read the words into the database
         with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
             reader = csv.reader(f, delimiter=',')
             for row in reader:
                 self.word_db.append(row[5])
         self.get_daily()
 
     def get_todays_word(self, month: str, day: str, year: str):
         """"""Retrieve the day's word from the CSV""""""
         with open(os.path.join('resources', 'wordle_key.csv'), newline='') as f:
             reader = csv.reader(f, delimiter=',')
             for row in reader:
                 if (row[0], row[1], row[2]) == (month, day, year):
                     self.word_of_the_day = row[5]
                     break
 
     def get_daily(self) -> str:
         """"""Check the last retrieval date and get today's word if needed.""""""
         now = datetime.datetime.now()
         month, day, year = now.strftime(""%b""), str(now.day).zfill(2), str(now.year)
 
         if self.word_day == 0 or self.word_day != day:
             self.word_day = day
             self.get_todays_word(month, day, year)
 
         return self.word_of_the_day
 
     def check_guess(self, guess: str) -> list:
         """"""Check how correct the user guess is
 
         results = {
             ""space"": ""[green][black][black][yellow][black]"",
             ""valid"": ['c', 'h'],
             ""invalid"": ['a', 's']
         }
         """"""
         results = {""space"": [], ""valid"": [], ""invalid"": []}
         guess = guess.upper()
 
         if not self.known(guess):
             return results
 
         for i in range(5):
             if guess[i] == self.word_of_the_day[i]:
                 results['space'].append(self.GREEN)
                 if guess[i] not in results['valid']:
                     results['valid'].append(guess[i])
             elif guess[i] in self.word_of_the_day:
                 results['space'].append(self.YELLOW)
                 if guess[i] not in results['valid']:
                     results['valid'].append(guess[i])
             else:
                 results['space'].append(self.BLACK)
                 if guess[i] not in results['invalid']:
                     results['invalid'].append(guess[i])
 
         return results
 
     def known(self, guess: str) -> bool:
         """"""Check if the guessed word is known by the bot""""""
         guess = guess.upper()
 
         return guess in self.word_db"
KO;1.0;lucidrains;perceiver-ar-pytorch;be3765300f5aae03b779edf0e256b7a74bda5fc8;allow for processing heads in chunks in the initial cross attention layer, to save on peak memory;"def __init__(
         dim,
         dim_head = 64,
         heads = 8,
         dropout = 0.
     ):
         super().__init__()
         self.scale = dim_head ** -0.5
         self.heads = heads
         inner_dim = heads * dim_head
 
         self.norm = nn.LayerNorm(dim)
def forward(self, x, context, context_mask = None, rotary_pos_emb = None):
             q = apply_rotary_pos_emb(rotary_pos_emb, q)
             k = apply_rotary_pos_emb(rotary_pos_emb, k)
 
         sim = einsum('b h i d, b h j d -> b h i j', q, k)
 
         i, j = sim.shape[-2:]
 
         mask_value = -torch.finfo(sim.dtype).max
 
         if exists(context_mask):
             mask_len = context_mask.shape[-1]
             context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)
             context_mask = rearrange(context_mask, 'b j -> b 1 1 j')
             sim = sim.masked_fill(~context_mask, mask_value)
 
         causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)
         sim = sim.masked_fill(causal_mask, mask_value)
 
         attn = sim.softmax(dim = -1)
         attn = self.dropout(attn)
 
         out = einsum('b h i j, b h j d -> b h i d', attn, v)
 
         out = rearrange(out, 'b h n d -> b n (h d)')
 
def __init__(
         heads = 8,
         dropout = 0.,
         ff_mult = 4,
         perceive_depth = 1
     ):
         super().__init__()
         assert max_seq_len > cross_attn_seq_len, 'max_seq_len must be greater than cross_attn_seq_len, the length of the sequence for which to cross attend to ""perceiver"" style'
def __init__(
 
         for _ in range(perceive_depth):
             self.perceive_layers.append(nn.ModuleList([
                 CausalPrefixAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = dropout),
                 FeedForward(dim, mult = ff_mult, dropout = dropout)
             ]))
 "
OK;1.0;lucidrains;perceiver-ar-pytorch;be3765300f5aae03b779edf0e256b7a74bda5fc8;allow for processing heads in chunks in the initial cross attention layer, to save on peak memory;"def __init__(
         dim,
         dim_head = 64,
         heads = 8,
         max_heads_process = 2,
         dropout = 0.
     ):
         super().__init__()
         self.scale = dim_head ** -0.5
         self.heads = heads
         self.max_heads_process = max_heads_process
 
         inner_dim = heads * dim_head
 
         self.norm = nn.LayerNorm(dim)
def forward(self, x, context, context_mask = None, rotary_pos_emb = None):
             q = apply_rotary_pos_emb(rotary_pos_emb, q)
             k = apply_rotary_pos_emb(rotary_pos_emb, k)
 
         # take care of masking
 
         i, j = q.shape[-2], k.shape[-2]
         mask_value = -torch.finfo(q.dtype).max
 
         if exists(context_mask):
             mask_len = context_mask.shape[-1]
             context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)
             context_mask = rearrange(context_mask, 'b j -> b 1 1 j')
 
         causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)
 
         # process in chunks of heads
 
         out = []
 
         max_heads = self.max_heads_process
 
         for q_chunk, k_chunk, v_chunk in zip(q.split(max_heads, dim = 1), k.split(max_heads, dim = 1), v.split(max_heads, dim = 1)):
             sim = einsum('b h i d, b h j d -> b h i j', q_chunk, k_chunk)
 
             if exists(context_mask):
                 sim = sim.masked_fill(~context_mask, mask_value)
 
             sim = sim.masked_fill(causal_mask, mask_value)
 
             attn = sim.softmax(dim = -1)
             attn = self.dropout(attn)
 
             out_chunk = einsum('b h i j, b h j d -> b h i d', attn, v_chunk)
             out.append(out_chunk)
 
         # concat all the heads together
 
         out = torch.cat(out, dim = 1)
 
         # merge heads and then combine with linear
 
         out = rearrange(out, 'b h n d -> b n (h d)')
 
def __init__(
         heads = 8,
         dropout = 0.,
         ff_mult = 4,
         perceive_depth = 1,
         perceive_max_heads_process = 2 # processes the heads in the perceiver layer in chunks to lower peak memory, in the case the prefix is really long
     ):
         super().__init__()
         assert max_seq_len > cross_attn_seq_len, 'max_seq_len must be greater than cross_attn_seq_len, the length of the sequence for which to cross attend to ""perceiver"" style'
def __init__(
 
         for _ in range(perceive_depth):
             self.perceive_layers.append(nn.ModuleList([
                 CausalPrefixAttention(dim = dim, dim_head = dim_head, heads = heads, max_heads_process = perceive_max_heads_process, dropout = dropout),
                 FeedForward(dim, mult = ff_mult, dropout = dropout)
             ]))
 "
KO;9.0;tigert1998;opcounter;22d1dcd3aef379f4ac69e7b47602e05e9b87cd83;Fix incorrect memory calculation for Conv2DForwardHook;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
             mem = 4 * (b * module.in_channels * h * w + np.prod(module.kernel_size) +
                        b * module.out_channels * out_h * out_w)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem"
OK;9.0;tigert1998;opcounter;22d1dcd3aef379f4ac69e7b47602e05e9b87cd83;Fix incorrect memory calculation for Conv2DForwardHook;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
             mem = 4 * (b * module.in_channels * h * w + np.prod(module.weight.shape) +
                        b * module.out_channels * out_h * out_w)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem"
KO;9.0;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;" from opcounter.hooks import *
 
 if __name__ == ""__main__"":
     model = resnet18(pretrained=True)
     input_tuple = (torch.randn(1, 3, 224, 224),)
     dst = counter(model, input_tuple, [
         Conv2DForwardHook(),"
OK;9.0;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;" from opcounter.hooks import *
 
 if __name__ == ""__main__"":
     model = resnet50(pretrained=True)
     input_tuple = (torch.randn(1, 3, 224, 224),)
     dst = counter(model, input_tuple, [
         Conv2DForwardHook(),"
KO;9.0;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
             mem = b * module.in_channels * h * w + np.prod(module.kernel_size) + \
                 b * module.out_channels * out_h * out_w
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
 
def func(module, input_tensors, output_tensors):
             in_features, out_features = module.in_features, module.out_features
             b, _ = input_tensors[0].shape
             muladds = b * in_features * out_features
             mem = b * in_features + b * out_features + in_features * out_features
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
         return func"
OK;9.0;tigert1998;opcounter;eaf23a36b72320a82dc5540a9eb0a16994448350;Modify opcounter to use byte as memory unit;"def func(module, input_tensors, output_tensors):
             muladds = b * module.out_channels * out_h * out_w * \
                 module.kernel_size[0] * \
                 module.kernel_size[1] * module.in_channels
             mem = 4 * (b * module.in_channels * h * w + np.prod(module.kernel_size) +
                        b * module.out_channels * out_h * out_w)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
 
def func(module, input_tensors, output_tensors):
             in_features, out_features = module.in_features, module.out_features
             b, _ = input_tensors[0].shape
             muladds = b * in_features * out_features
             mem = 4 * (b * in_features + b * out_features +
                        in_features * out_features)
             dst[""muladds""] = dst.get(""muladds"", 0) + muladds
             dst[""mem""] = dst.get(""mem"", 0) + mem
         return func"
KO;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"benchmark.py
 index.py
 store_keys.txt
 b-tree.py"
OK;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"benchmark.py
 index.py
 store_keys.txt
 b-tree.py
 storemmap.txt
 exper.py"
KO;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def commands_cli():
         else:
             command_split = command.split()
             command_len = len(command_split)
             if command_len < 2:
                 print(""Few arguments given"", file=sys.stderr)
             elif command_len > 3:
                 print(""Too many arguments"", file=sys.stderr)
 
             if command_split[0] not in DATABASE_COMMANDS:
                 print(""Command not supported"", file=sys.stderr)
             else:
                 # socket is re-initialised everytime
                 with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                     try:
                         sock.connect((HOST, PORT))
                     except ConnectionRefusedError:
                         raise ConnectionRefusedError(""Cannot connect to host server, perhaps start db server"")
                     sock.sendall(bytes(command + ""\n"", ""utf-8""))
                     response = str(sock.recv(1024), ""utf-8"")
                     print(response)
 
 
 if __name__ == ""__main__"":"
OK;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def commands_cli():
         else:
             command_split = command.split()
             command_len = len(command_split)
 
             if command_len < 2:
                 print(""Few arguments given"", file=sys.stderr)
             elif command_len > 3:
                 print(""Too many arguments"", file=sys.stderr)
             else:
 
                 if command_split[0] not in DATABASE_COMMANDS:
                     print(""Command not supported"", file=sys.stderr)
                 else:
                     # socket is re-initialised everytime
                     with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                         try:
                             sock.connect((HOST, PORT))
                         except ConnectionRefusedError:
                             raise ConnectionRefusedError(""Cannot connect to host server, perhaps start db server"")
                         sock.sendall(bytes(command + ""\n"", ""utf-8""))
                         response = str(sock.recv(1024), ""utf-8"")
                         print(response)
 
 
 if __name__ == ""__main__"":"
KO;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def __init__(self, use_btrees=False):
 
     def insert(self, key, value):
         with open(self.STORE_KEYS_FILE, mode=""a"") as file:
             file.write(f""{key} {value} \n"")
             logger.info(f""{key} set in db"")
     
     def retrieve(self, key):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE, mode=""r"") as file:
                 keys = file.readlines()
                 for line in keys:
                     if line.startswith(key):
                         value = re.split(r'(\n|\s)', line)[2]
                         return value
 
     def update(self, key, new_value):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE) as file:
                 keys = file.readlines()
                 for line in keys:
                     if line.startswith(key):
                         # line.replace
                         pass
 "
OK;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;"def __init__(self, use_btrees=False):
 
     def insert(self, key, value):
         with open(self.STORE_KEYS_FILE, mode=""a"") as file:
             try:
                 file.write(f""{key} {value} \n"")
                 logger.info(f""{key} set in db"")
                 return True
             except Exception:
                 logger.error(f""[ATTEMPT-FAIL] {key} could not be set in db"")
                 return False
     
     def retrieve(self, key):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE, mode=""r"") as file:
                 data = file.readlines()
                 for line in data:
                     if line.startswith(key):
                         value = re.split(r'(\n|\s)', line)[2]
                         return value
 
     def update(self, key, new_value):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE) as file:
                 data = file.readlines()
                 for line in data:
                     if line.startswith(key):
                         # line.replace
                         pass
 
     def delete(self, key):
         if not self.use_btrees:
             with open(self.STORE_KEYS_FILE) as file:
                 data = file.readlines()
                 for line in data:
                     if line.startswith(key):
                         pass
 "
KO;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;" from keys import Key
 import os
 
 class TCPServerHandler(socketserver.StreamRequestHandler):
 
     def handle(self):
         db = Key()
         self.data = self.rfile.readline().strip().decode()
         command_split = self.data.split()
 
         if len(command_split) == 3:
def handle(self):
         elif len(command_split) == 2:
             db_command, key = command_split
             command_len = 2
 
         if hasattr(db, f""{db_command}""):
             command = getattr(db, f""{db_command}"")
             if command_len == 3:
                 command(key, value)
                 self.wfile.write(""0"".encode())
             elif command_len == 2:
                 get_key = command(key)
                 self.wfile.write(f""{get_key}"".encode())
 
 
 if __name__ == ""__main__"":
     HOST, PORT = ""localhost"", 4000
 
     with socketserver.TCPServer((HOST, PORT), TCPServerHandler) as server:
         try:
             process_id = os.getpid()
             logger.info(f""Starting process id {process_id}"")
             logger.info(f""Key value db server starting, address={HOST}:{PORT}"")
             server.serve_forever()
 
         except BaseException:
             logger.info(f""Closing process id {process_id}"")
             logger.info(f""Key value db server shutting down"")
             sys.exit()"
OK;16.0;Gaborade;Janus;a4f3164d1991a0a008135af7ef03c1ae8d5e2061;experiment: tracemalloc for memory tracing;" from keys import Key
 import os
 
 # experiment with tracemalloc
 import tracemalloc
 tracemalloc.start()
 
 
 def memory_trace(limit=10):
     print(""-"" *70)
     print(""-"" * 70)
     print(""-------------------------MEMORY USAGE----------------------------"")
     snapshot = tracemalloc.take_snapshot()
     memory_stats = snapshot.statistics(""lineno"")
     for stat in memory_stats[:limit]:
         print(stat)
    
 
 class TCPServerHandler(socketserver.StreamRequestHandler):
 
     def handle(self):
         db = Key()
         self.data = self.rfile.readline().strip().decode()
         logger.info(f""Message sent by key db cli client with address {self.client_address[0]}:{self.client_address[1]}"")
         command_split = self.data.split()
 
         if len(command_split) == 3:
def handle(self):
         elif len(command_split) == 2:
             db_command, key = command_split
             command_len = 2
         
         # callable to make sure no key db attributes which are not methods are called by getattr
         if hasattr(db, f""{db_command}"") and callable(getattr(db, f""{db_command}"")):
             command = getattr(db, f""{db_command}"")
             if command_len == 3:
                 if command(key, value):
                     self.wfile.write(""0"".encode())
             elif command_len == 2:
                 get_value = command(key)
                 self.wfile.write(f""{get_value}"".encode())
 
 
 if __name__ == ""__main__"":
     HOST, PORT = ""localhost"", 4000
     SERVER_IP_ADDR = ""127.0.0.1"" if HOST == ""localhost"" else HOST   
     try:
         DEBUG = sys.argv[1] if sys.argv[1] == ""debug"" else False
     except IndexError:
         DEBUG = False
 
     with socketserver.TCPServer((HOST, PORT), TCPServerHandler) as server:
         try:
             process_id = os.getpid()
             if DEBUG:
                 logger.info(""DEBUG=ON"")
             else:
                 logger.info(""DEBUG=OFF"")
             logger.info(f""Starting process id {process_id}"")
             logger.info(f""Key value db server starting"")
             logger.info(f""Server listening on address {SERVER_IP_ADDR}:{PORT}"")
             server.serve_forever()
 
         except KeyboardInterrupt:
             logger.info(f""Closing process id {process_id}"")
             logger.info(f""Key value db server shutting down"")
             if DEBUG:
                 memory_trace()
             sys.exit()
 
   "
KO;20.0;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
 sudo docker run --rm --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
 docker run -it --rm --gpus all milesial/unet
 ```
 
 "
OK;20.0;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
 sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
 docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet
 ```
 
 "
KO;21.0;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
 sudo docker run --rm --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
 docker run -it --rm --gpus all milesial/unet
 ```
 
 "
OK;21.0;Jhryu30;sem_depth_estimation;3a5f3a0c8c5069e837ade1997df7139f482bc985;Increase shared memory;"sudo systemctl restart docker
 ```
 3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)
 ```bash
 sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet
 ```
 
 4. Download the data and run training:
A docker image containing the code and the dependencies is available on [DockerH
 You can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):
 
 ```console
 docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet
 ```
 
 "
KO;45.0;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def standardize_inputs(self, x: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
         self.MeanIn = Mean
         self.RangeIn = Range
 
         #if hasattr(self,""MeanIn""):
         #    self.MeanIn = Mean
def standardize_outputs(self, input: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
         self.MeanOut = Mean
         self.RangeOut = Range
 
         #if hasattr(self,""MeanOut""):
         #    self.MeanOut = Mean.to(self.MeanOut.device)"
OK;45.0;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def standardize_inputs(self, x: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
         self.MeanIn = Mean.to(self.device_)
         self.RangeIn = Range.to(self.device_)
 
         #if hasattr(self,""MeanIn""):
         #    self.MeanIn = Mean
def standardize_outputs(self, input: torch.Tensor, print_values=False):
 
         Mean, Range = compute_mean_range(x, print_values)
 
         self.MeanOut = Mean.to(self.device_)
         self.RangeOut = Range.to(self.device_)
 
         #if hasattr(self,""MeanOut""):
         #    self.MeanOut = Mean.to(self.MeanOut.device)"
KO;45.0;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def fit(
             dataset = create_time_lagged_dataset(X,t,lag_time)
             train_loader = FastTensorDataLoader(dataset.tensors, batch_size=batch_size, shuffle=False) 
 
         # standardize inputs (unravel dataset and copy to device) #TODO check memory usage on GPU
         x_train = torch.cat([batch[0] for batch in train_loader]).to(self.device_)
         if standardize_inputs:
             self.standardize_inputs(x_train)
 "
OK;45.0;GPXue;mlcvs;ed05ffbc70588cb8ee889c89994248de71139696;[nn] fix memory issue when unraveling datasets for standardization - fix #5;"def fit(
             dataset = create_time_lagged_dataset(X,t,lag_time)
             train_loader = FastTensorDataLoader(dataset.tensors, batch_size=batch_size, shuffle=False) 
 
         # standardize inputs (unravel dataset to compute average)
         x_train = torch.cat([batch[0] for batch in train_loader])
         if standardize_inputs:
             self.standardize_inputs(x_train)
 "
KO;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/io.cod""
 
 5 malloc"
OK;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/memory.cod""
 include ""std/io.cod""
 
 5 malloc"
KO;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
OK;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/memory.cod""
 include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/math.cod"""
KO;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/io.cod""
 include ""std/math.cod""
 "
OK;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/memory.cod""
 include ""std/io.cod""
 include ""std/math.cod""
 "
KO;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/cstr.cod"""
OK;1.0;justlucdewit;cod;958fff8ca320ed225a54ae9fbfef36ef3324eb24;Changed tests to use std/memory.cod;" include ""std/memory.cod""
 include ""std/stack.cod""
 include ""std/io.cod""
 include ""std/cstr.cod"""
KO;1.0;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def generate_rt_calls(program, indent_count=1):
             result += f""{indent}stack_push(argc);\n""
         elif part[""type""] == ""argv"":
             result += f""{indent}stack_push((uint64_t)argv);\n""
         elif part[""type""] == ""malloc"":
             result += f""{indent}stack_malloc();\n""
         elif part[""type""] == ""free"":
             result += f""{indent}stack_free();\n""
         elif part[""type""] == ""realloc"":
             result += f""{indent}stack_realloc();\n""
         elif part[""type""] == ""write8"":
             result += f""{indent}stack_write8();\n""
         elif part[""type""] == ""read8"":
             result += f""{indent}stack_read8();\n""
         elif part[""type""] == ""read64"":
             result += f""{indent}stack_read64();\n""
 
         elif part[""type""] == ""parseInt"":
             result += f""{indent}stack_parse_int64();\n"""
OK;1.0;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def generate_rt_calls(program, indent_count=1):
             result += f""{indent}stack_push(argc);\n""
         elif part[""type""] == ""argv"":
             result += f""{indent}stack_push((uint64_t)argv);\n""
 
         elif part[""type""] == ""parseInt"":
             result += f""{indent}stack_parse_int64();\n"""
KO;1.0;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def parse_from_words(words, root=False):
     builtin_words = [
         ""argc"",
         ""argv"",
 
         ""malloc"",
         ""free"",
         ""realloc"",
         ""write8"",
         ""read8"",
         ""read64"",
         
         ""parseInt""
     ]"
OK;1.0;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;"def parse_from_words(words, root=False):
     builtin_words = [
         ""argc"",
         ""argv"",
         
         ""parseInt""
     ]"
KO;1.0;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;\ No newline at end of file
OK;1.0;justlucdewit;cod;a88f96163ba8b5d00b9d2672bd3986d5cdc4bbf0;Moved memory words to std/memory.cod;" subroutine malloc {
     raw ""stack_malloc();""
 }
 
 subroutine free {
     raw ""stack_free();""
 }
 
 subroutine realloc {
     raw ""stack_realloc();""
 }
 
 subroutine write8 {
     raw ""stack_write8();""
 }
 
 subroutine read8 {
     raw ""stack_read8();""
 }
 
 subroutine read64 {
     raw ""stack_read64();""
 }
\ No newline at end of file"
KO;10.0;Lincoln-LM;py-gdb-nx;4be70bb77e14d9b8c0443485c25fa5cd5c5880c9;Create methods for reading and writing memory;" """"""Wrapper around pygdbmi.GdbController for easier switch connection""""""
 
 from typing import Optional,List
 import os.path
 import pygdbmi.gdbcontroller
def attach(
         self,
         process_name: str = ""Application"",
     ):
         """"""Attach to process of name process_name
 
         Args:
             process_name (str, optional): Name of switch process to attach to.
def get_bases(
                 self.main_base, self.main_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[2:4])
 
     def add_breakpoint(
         self,
         bkpt: Breakpoint,
     ):
         """"""Activate breakpoint
 
         Args:
             bkpt (Breakpoint): Breakpoint object to activate
def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
     ) -> List[dict]:
         """"""Wait until response from gdb of type target_type
 
         Args:
             target_type (Optional[str], optional): mi3 type to wait for. Defaults to ""console"".
def log_response(
         response: List[dict],
         detailed: Optional[bool] = False,
     ):
         """"""Log a mi3 response List[dict]
 
         Args:
             response (List[dict]): mi3 response to log
def filter_response(
     def extract_payloads(
         response: List[dict],
     ) -> List[str]:
         """"""Extract only the payloads of a mi3 response
 
         Args:
             response (List[dict]): mi3 response to extract from"
OK;10.0;Lincoln-LM;py-gdb-nx;4be70bb77e14d9b8c0443485c25fa5cd5c5880c9;Create methods for reading and writing memory;" """"""Wrapper around pygdbmi.GdbController for easier switch connection""""""
 
 import struct
 from typing import Optional,List
 import os.path
 import pygdbmi.gdbcontroller
def attach(
         self,
         process_name: str = ""Application"",
     ):
         """"""
         Attach to process of name process_name
 
         Args:
             process_name (str, optional): Name of switch process to attach to.
def get_bases(
                 self.main_base, self.main_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[2:4])
 
     def read_instruction(
         self,
         address: int,
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ) -> str:
         """"""
         Read instruction from address
 
         Args:
             address (int): Address to read from
             offset_main (bool, optional): Whether or not to offset address by
             self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address by
             self.heap_base. Defaults to False
 
         Returns:
             str: Instruction information
         """"""
         if offset_main:
             address += self.main_base
         elif offset_heap:
             address += self.heap_base
         self.write(f""x/1iw {address}"", read_response = False)
         return self.filter_response(
             self.get_gdb_response(),
             ""console""
             )[0]['payload'].split("":"")[1].replace(""\\t"",""\t"").replace(""\\n"","""")
 
     def read_int(
         self,
         address: int,
         size: str = ""g"",
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ) -> int:
         """"""
         Read memory at address
 
         Args:
             address (int): Address to read from
             size (str, optional): GDB size of int to read. Defaults to ""g""
             offset_main (bool, optional): Whether or not to offset address by
             self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address by
             self.heap_base. Defaults to False
 
         Returns:
             int: Integer read from address
         """"""
         if offset_main:
             address += self.main_base
         elif offset_heap:
             address += self.heap_base
         self.write(f""x/1x{size} {address}"", read_response = False)
         return int(self.filter_response(
             self.get_gdb_response(),
             ""console""
             )[0]['payload'].split(""0x"")[-1][:-2].replace("":"",""""),16)
 
     def read_bytes(
         self,
         address: int,
         size: str = ""g"",
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ) -> bytes:
         """"""
         Read memory at address and convert it to bytes
 
         Args:
             address (int): Address to read from
             size (str, optional): GDB size of int to read. Defaults to ""g""
             offset_main (bool, optional): Whether or not to offset address by
             self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address by
             self.heap_base. Defaults to False
 
         Returns:
             bytes: Bytes read from address
         """"""
         if size == ""b"":
             struct_size = ""B""
         elif size == ""h"":
             struct_size = ""H""
         elif size == ""w"":
             struct_size = ""I""
         elif size == ""g"":
             struct_size = ""Q""
         return struct.pack(struct_size, self.read_int(address, size, offset_main, offset_heap))
 
     def read_float(
         self,
         address: int,
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ) -> float:
         """"""
         Read float at address
 
         Args:
             address (int): Address to read from
             offset_main (bool, optional): Whether or not to offset address by
             self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address by
             self.heap_base. Defaults to False
         """"""
         return struct.unpack(""f"", self.read_bytes(address, ""w"", offset_main, offset_heap))
 
     def write_int(
         self,
         address: int,
         value: int,
         size: str = ""g"",
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ):
         """"""
         Write integer of size to address
 
         Args:
             address (int): Address to write to
             value (int): Value to write to memory
             size (str, optional): GDB size of int to write. Defaults to ""g""
             offset_main (bool, optional): Whether or not to offset address by
             self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address by
             self.heap_base. Defaults to False
         """"""
         if size == ""b"":
             size = ""unsigned char""
         elif size == ""h"":
             size = ""unsigned short""
         elif size == ""w"":
             size = ""unsigned word""
         elif size == ""g"":
             size = ""unsigned long""
         if offset_main:
             address += self.main_base
         elif offset_heap:
             address += self.heap_base
         self.write(f""set {{{size}}}{address} = {value}"")
 
     def write_bytes(
         self,
         address: int,
         value: bytes,
         size: str = ""g"",
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ):
         """"""
         Write bytes of size to address
 
         Args:
             address (int): Address to write to
             value (bytes): Value to write to memory
             size (str, optional): GDB size of bytes to write. Defaults to ""g""
             offset_main (bool, optional): Whether or not to offset address by
             self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address by
             self.heap_base. Defaults to False
         """"""
         self.write_int(address, struct.unpack(""I"", value), size, offset_main, offset_heap)
 
     def write_float(
         self,
         address: int,
         value: float,
         offset_main: Optional[bool] = False,
         offset_heap: Optional[bool] = False,
     ):
         """"""
         Write float to address
 
         Args:
             address (int): Address to write to
             value (float): Value to write to memory
             offset_main (bool, optional): Whether or not to offset address
             by self.main_base. Defaults to False
             offset_heap (bool, optional): Whether or not to offset address
             by self.heap_base. Defaults to False
         """"""
         self.write_bytes(address, struct.pack(""f"", value), ""w"", offset_main, offset_heap)
 
     def add_breakpoint(
         self,
         bkpt: Breakpoint,
     ):
         """"""
         Activate breakpoint
 
         Args:
             bkpt (Breakpoint): Breakpoint object to activate
def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
     ) -> List[dict]:
         """"""
         Wait until response from gdb of type target_type
 
         Args:
             target_type (Optional[str], optional): mi3 type to wait for. Defaults to ""console"".
def log_response(
         response: List[dict],
         detailed: Optional[bool] = False,
     ):
         """"""
         Log a mi3 response List[dict]
 
         Args:
             response (List[dict]): mi3 response to log
def filter_response(
     def extract_payloads(
         response: List[dict],
     ) -> List[str]:
         """"""
         Extract only the payloads of a mi3 response
 
         Args:
             response (List[dict]): mi3 response to extract from"
KO;10.0;Lincoln-LM;py-gdb-nx;8fa4ecdd069e327eeee3aa01a81aab9b0ba07c01;Attach to process and read bases of memory regions;"def __init__(
         self.ip_address = ip_address
         self.active_breakpoints = {}
         self.main_base: int = None
         self.heap_base: int = None
         self.clear_responses()
         self.connect()
 
     def clear_responses(
         self,
def connect(
         self.write(f""target extended-remote {self.ip_address}:22225"", read_response=False)
         self.log_response(self.wait_for_response())
 
     def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
def log_response(
         if detailed:
             print(response)
         else:
             for line in self.extract_payloads(response):
                 print(line)
 
     @staticmethod"
OK;10.0;Lincoln-LM;py-gdb-nx;8fa4ecdd069e327eeee3aa01a81aab9b0ba07c01;Attach to process and read bases of memory regions;"def __init__(
         self.ip_address = ip_address
         self.active_breakpoints = {}
         self.main_base: int = None
         self.main_max: int = None
         self.heap_base: int = None
         self.heap_max: int = None
         self.stack_base: int = None
         self.stack_max: int = None
         self.clear_responses()
         self.connect()
         self.attach()
         self.get_bases()
         self.write(""set step-mode on"")
 
     def clear_responses(
         self,
def connect(
         self.write(f""target extended-remote {self.ip_address}:22225"", read_response=False)
         self.log_response(self.wait_for_response())
 
     def attach(
         self,
         process_name: str = ""Application"",
     ):
         """"""Attach to process of name process_name
 
         Args:
             process_name (str, optional): Name of switch process to attach to.
             Defaults to ""Application"".
         """"""
         self.write(""info os processes"", read_response = False)
         processes = self.wait_for_response()
         for line in reversed(processes): # sort by latest process started
             if line['type'] == ""console"" and process_name in line[""payload""]:
                 process_id = int(line[""payload""].split("" "",1)[0])
                 self.log_response(self.write(f""attach {process_id}""))
                 break
 
     def get_bases(
         self,
     ):
         """"""
         Read the base addresses of sections of the switch's memory
         """"""
         self.write(""monitor get base"", read_response = False)
         for line in self.filter_response(self.wait_for_response(""target""), ""target""):
             if ""Heap"" in line['payload']:
                 self.heap_base, self.heap_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[4:6])
             elif ""Stack"" in line['payload']:
                 self.stack_base, self.stack_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[3:5])
             elif "".nss"" in line['payload']:
                 self.main_base, self.main_max = \
                     (int(num, 16) for num in line['payload'].replace("" -"","""")[:-2].split("" "")[2:4])   
 
     def wait_for_response(
         self,
         target_type: Optional[str] = ""console"",
def log_response(
         if detailed:
             print(response)
         else:
             for line in self.extract_payloads(self.filter_response(response)):
                 print(line)
 
     @staticmethod"
KO;22.0;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
 
 
             
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 0
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
        
     def totalTime(self):
         for job in self.__job:
             self.__totalTime += job.jobTime()
         return self.__totalTime
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
                         print(f'Job {str(job.jobStream())}: {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms') 
                         break
                     
         
         while len(tempList) -1 >= 2:
def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
                         print(f'Job {str(job.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms')
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
                                     print(f'Job {str(job2.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job2.jobTime())} ms')
                                     break
                 else:
                     continue
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 0
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
         self.__totalWQ = {}
        
     def totalTime(self):
         for job in self.__job:
             self.__totalTime += job.jobTime()
         return self.__totalTime
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(self.__jobCount/self.__timer)} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
         print(self.__totalWT)
 
 
 # Worst Fit Memory Allocation
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = len(self.__job)
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
         self.__totalWQ = {}
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
 
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)        
         
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
         print(self.__totalTime)
         print(len(self.__job))
 
 
 
 def main():
     memoryBlockList = []
     jobList = []
     
def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
     while True:
         print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
         key = input("""")
         if key == '1':
             wf = WorstFit(memoryBlockList, jobList).worstFit()
         elif key == '2':
             bf = BestFit(memoryBlockList, jobList).bestFit()
         elif key == '3':
             ff = FirstFit(memoryBlockList, jobList).firstFit()
         else:
             print(""Invalid Key. Please try again.\n"")
     
     
 
 main()
 
"
OK;22.0;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
      
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 140  # total time from the given jobs based on the MP3
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
        
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
                         break
         
         for key,value in self.__allocation.items():
             self.__totalWT += value.jobTime()
             print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
             
                     
         
         while len(tempList) -1 >= 2:
def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
                                     break
                 else:
                     continue
             for key,value in self.__allocation.items():
                 if value != None:
                     self.__totalWT += value.jobTime()
                     print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
                     
                     
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.__totalWT/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 140  # constant based on the MP3 given jobs
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT)/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
 
 
 # Worst Fit Memory Allocation
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 140  # constant given in the description of MP
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
         self.__temp = job
 
 
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
 
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             self.sumIF(value,key) 
             print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
                    
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT + self.__totalTime) /self.__jobCount,2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
 
 
 
 def main():
 
     memoryBlockList = []
     jobList = []
     
def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
 
     print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
     key = input("""")
     if key == '1':
         wf = WorstFit(memoryBlockList, jobList).worstFit()
     elif key == '2':
         bf = BestFit(memoryBlockList, jobList).bestFit()
     elif key == '3':
         ff = FirstFit(memoryBlockList, jobList).firstFit()
     else:
         print(""Invalid Key. Please try again.\n"")
 
 main()
"
KO;27.0;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
 
 
             
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 0
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
        
     def totalTime(self):
         for job in self.__job:
             self.__totalTime += job.jobTime()
         return self.__totalTime
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
                         print(f'Job {str(job.jobStream())}: {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms') 
                         break
                     
         
         while len(tempList) -1 >= 2:
def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
                         print(f'Job {str(job.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job.jobTime())} ms')
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
                                     print(f'Job {str(job2.jobStream())} : {str(job.jobSize())} has been allocated in memory block {str(memory.memoryBlock())}:{str(memory.memorySize())} and will reside for {str(job2.jobTime())} ms')
                                     break
                 else:
                     continue
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 0
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
         self.__totalWQ = {}
        
     def totalTime(self):
         for job in self.__job:
             self.__totalTime += job.jobTime()
         return self.__totalTime
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(self.__jobCount/self.__timer)} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
         print(self.__totalWT)
 
 
 # Worst Fit Memory Allocation
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = len(self.__job)
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
         self.__totalWQ = {}
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             self.__totalWQ.update({memory.memoryBlock() : job.jobTime()})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
 
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)        
         
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())}:{str(key.jobSize())} has been allocated in memory block {str(value.memoryBlock())}:{str(value.memorySize())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.totalTime()/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.totalTime()/len(self.__job),2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
         print(self.__totalTime)
         print(len(self.__job))
 
 
 
 def main():
     memoryBlockList = []
     jobList = []
     
def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
     while True:
         print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
         key = input("""")
         if key == '1':
             wf = WorstFit(memoryBlockList, jobList).worstFit()
         elif key == '2':
             bf = BestFit(memoryBlockList, jobList).bestFit()
         elif key == '3':
             ff = FirstFit(memoryBlockList, jobList).firstFit()
         else:
             print(""Invalid Key. Please try again.\n"")
     
     
 
 main()
 
"
OK;27.0;DeinyRhed;CMSC-123--Memory-Management-and-Allocation-Strategies;c088a95eefd69a2d9be7406e8044da991debe329;Update memory_allocation_strategies.py;"def updateTime(self):
         if self.__jobTime <= 0:
             self.__jobTime = 0
 
      
 
 class MemoryBlock:
     def __init__(self, memoryBlock, memorySize):
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({memory : job})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 140  # total time from the given jobs based on the MP3
         self.__timer = 1        # For the time
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : job })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
        
     
     # This is for the internal fragmentation portion
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def firstFit(self):
                         self.__allocation.update({memory : job})
                         self.sumIF(memory,job)
                         self.__jobCount += 1
                         break
         
         for key,value in self.__allocation.items():
             self.__totalWT += value.jobTime()
             print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
             
                     
         
         while len(tempList) -1 >= 2:
def firstFit(self):
                     job.updateTime()
                     if job.jobTime() > 0:
                         self.__jobCount += 1
                     # If time == 1, then remove it from the jobList named tempList
                     # Also remove the value from the self.__allocation dictionary
                     else:
def firstFit(self):
                                     self.__allocation.update({memory : job2})
                                     self.sumIF(memory,job2)
                                     self.__jobCount += 1
                                     break
                 else:
                     continue
             for key,value in self.__allocation.items():
                 if value != None:
                     self.__totalWT += value.jobTime()
                     print(f'Job {str(value.jobStream())} has been allocated in memory block {str(key.memoryBlock())} and will reside for {str(value.jobTime())} ms')
                     
                     
         self.status()
             
     
     def status(self):
         print(f'\n===================================== FIRST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round(self.__totalWT/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 140  # constant based on the MP3 given jobs
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
     
     
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
def sumIF(self, memory:MemoryBlock, job:JobInfo):
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]}) # value is a list because we are storing the job and memory size difference in that block
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def bestFit(self):
 
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             self.sumIF(value,key)   # key = job and value = memory
             
         self.__jobCount += len(self.__allocation)
def bestFit(self):
 
             for key,value in self.__allocation.items():
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
                 self.sumIF(value,key)
 
         self.status()
def bestFit(self):
     def status(self):
         print(f'\n===================================== BEST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT)/self.__jobCount,2))} jobs per unit of time\n')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
             print(f'Block {str(memory.memoryBlock())}\'s total internal fragmentation (sum[block.size - job.size]): {str((sum(self.__sumIF[memory.memoryBlock()])))} units of memory')
             print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
 
 
 
 # Worst Fit Memory Allocation
def __init__(self, memory:[MemoryBlock], job:[JobInfo]):
         self.__job = job
         self.__allocation = {}  # ({job : memory})
         self.__jobCount = 0     # For the total assigned jobs
         self.__totalTime = 140  # constant given in the description of MP
         self.__timer = 1        # For the timer
         self.__sumBlock = 0
         self.__sumIF = {}       # Total Internal Fragmentation per block ({ memory : diffSize })
         self.__sumBlock = 0  
         self.__totalHUP = {}    # Total Heavily Used Partition ({ memoryBlock : jobSize })
         self.__totalUP = {}     # Total Unused Partition ({ memoryBlock : differenceSize})
         self.__totalWT = 0
         self.__temp = job
 
 
     def sumIF(self, memory:MemoryBlock, job:JobInfo):
         self.__sumBlock = 0
         if memory.memoryBlock() not in self.__sumIF:
             self.__sumBlock = (memory.memorySize() - job.jobSize()) + self.__sumBlock
             self.__sumIF.update({memory.memoryBlock() : [self.__sumBlock]})
             self.__totalHUP.update({memory.memoryBlock() : job.jobSize()})
             self.__totalUP.update({memory.memoryBlock() : self.__sumBlock})
 
             
         else:
             temp = (memory.memorySize() - job.jobSize())
def worstFit(self):
                             worstBlock = 0
                     else:
                         worstBlock = diffSize
             
         for key,value in self.__allocation.items():
             self.__totalWT += key.jobTime()
             self.sumIF(value,key) 
             print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
                    
         while len(self.__allocation) >= 1:
             if len(self.__allocation) == 1 and list(self.__allocation.keys())[0].jobTime() - 1 == 0:
                 break  
def worstFit(self):
             for key,value in self.__allocation.items():
                 self.sumIF(memory,job)
                 self.__totalWT += key.jobTime()
                 print(f'Job {str(key.jobStream())} has been allocated in memory block {str(value.memoryBlock())} and will reside for {str(key.jobTime())} ms')
             
 
         self.status()
def worstFit(self):
     def status(self):
         print(f'\n===================================== WORST FIT ===================================== ')
         print(f'AVERAGE THROUGHPUT (TOTAL ASSIGNED JOB COUNT/ TOTAL TIME: {str(round(self.__jobCount/self.__timer, 2))} jobs per unit of time')
         print(f'AVERAGE WAITING QUEUE (TOTAL WQ LENGTH/ TOTAL TIME: {str(round(self.__totalTime/self.__timer,2))} jobs per unit of time')
         print(f'AVERAGE WAITING TIME (TOTAL WT/ #JOBS): {str(round((self.__totalWT + self.__totalTime) /self.__jobCount,2))} jobs per unit of time')
         print(f'TOTAL UNUSED PARTITION ((TOTAL USED MEMORY / 50000) * 100): {str(round(((sum(self.__totalUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity')
         print(f'TOTAL HEAVILY USED PARTITION ((TOTAL EXHAUSTED MEMORY / 50000) * 100): {str(round(((sum(self.__totalHUP.values()))/50000)*100, 2))}% out of 50,000 memory capacity\n')
         print(f'----------------------------- INTERNAL FRAGMENTATION -----------------------------')
def status(self):
                 print(f'Block {str(memory.memoryBlock())}\'s average internal fragmentation (sum / totalTime): {str(round(sum(self.__sumIF[memory.memoryBlock()])/self.__timer, 2 ))} units of memory per unit of time\n')
             except:
                 print(f'Block {str(memory.memoryBlock())} was not allocated')
 
 
 
 def main():
 
     memoryBlockList = []
     jobList = []
     
def main():
             row = line.split()
             jobList.append(JobInfo(int(row[0]), int(row[1]), int(row[2])))
 
 
     print(f'Choose Algorithm [1] Worst Fit\t [2] Best Fit\t [3] First Fit')
     key = input("""")
     if key == '1':
         wf = WorstFit(memoryBlockList, jobList).worstFit()
     elif key == '2':
         bf = BestFit(memoryBlockList, jobList).bestFit()
     elif key == '3':
         ff = FirstFit(memoryBlockList, jobList).firstFit()
     else:
         print(""Invalid Key. Please try again.\n"")
 
 main()
"
KO;43.0;vanegascata;GAN;dec28e3dd38ec52c02c8fb3e4e8c4baf216555cf;fixed bug with pandas sampling frac -  decrease memory usage. Fixes for issue #14;"def preprocess_data(self, train_df, target, test_df, ) -> Tuple[pd.DataFrame, pd
     def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.DataFrame]:
         self._validate_data(train_df, target, test_df)
         train_df[self.TEMP_TARGET] = target
         generated_df = train_df.sample(frac=(1 + self.pregeneration_frac * self.get_generated_shape(train_df)),
                                        replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         gc.collect()
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         gc.collect()
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)"
OK;43.0;vanegascata;GAN;dec28e3dd38ec52c02c8fb3e4e8c4baf216555cf;fixed bug with pandas sampling frac -  decrease memory usage. Fixes for issue #14;"def preprocess_data(self, train_df, target, test_df, ) -> Tuple[pd.DataFrame, pd
     def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.DataFrame]:
         self._validate_data(train_df, target, test_df)
         train_df[self.TEMP_TARGET] = target
         generated_df = train_df.sample(frac=(1 + self.pregeneration_frac), replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         gc.collect()
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         del ad_model
         gc.collect()
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)"
KO;43.0;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;" todo write description
 """"""
 
 import logging
 import warnings
 from typing import Tuple
def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
         generated_df = train_df.sample(frac=(1 + self.pregeneration_frac * self.get_generated_shape(train_df)),
                                        replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
 
     def postprocess_data(self, train_df, target, test_df, ):
def postprocess_data(self, train_df, target, test_df, ):
             for cat_col in self.cat_cols:
                 filtered_df = train_df[train_df[cat_col].isin(test_df[cat_col].unique())]
                 train_df = filtered_df
         return train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True), train_df[self.TEMP_TARGET].reset_index(
             drop=True)
 
def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)
 
def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
             ].astype(data_dtype[i])
 
         train_df = pd.concat([train_df, generated_df, ]).reset_index(drop=True)
         return train_df.drop(self.TEMP_TARGET, axis=1), train_df[self.TEMP_TARGET]
 
 
 def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
     _logger = logging.getLogger(__name__)
     _logger.info(""Starting generating data:"")
     _logger.info(creator.generate_data_pipe(in_train, in_target, in_test))
     _logger.info(""Finished generatation\n"")
 
 
 if __name__ == ""__main__"":"
OK;43.0;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;" todo write description
 """"""
 
 import gc
 import logging
 import warnings
 from typing import Tuple
def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
         generated_df = train_df.sample(frac=(1 + self.pregeneration_frac * self.get_generated_shape(train_df)),
                                        replace=True, random_state=42)
         generated_df = generated_df.reset_index(drop=True)
         gc.collect()
         return generated_df.drop(self.TEMP_TARGET, axis=1), generated_df[self.TEMP_TARGET]
 
     def postprocess_data(self, train_df, target, test_df, ):
def postprocess_data(self, train_df, target, test_df, ):
             for cat_col in self.cat_cols:
                 filtered_df = train_df[train_df[cat_col].isin(test_df[cat_col].unique())]
                 train_df = filtered_df
         gc.collect()
         return train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True), train_df[self.TEMP_TARGET].reset_index(
             drop=True)
 
def adversarial_filtering(self, train_df, target, test_df, ):
         train_df[""test_similarity""] = ad_model.trained_model.predict(train_df.drop(self.TEMP_TARGET, axis=1))
         train_df.sort_values(""test_similarity"", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         gc.collect()
         return train_df.drop([""test_similarity"", self.TEMP_TARGET], axis=1).reset_index(drop=True), \
                train_df[self.TEMP_TARGET].reset_index(drop=True)
 
def generate_data(self, train_df, target, test_df) -> Tuple[pd.DataFrame, pd.Dat
             ].astype(data_dtype[i])
 
         train_df = pd.concat([train_df, generated_df, ]).reset_index(drop=True)
         gc.collect()
         return train_df.drop(self.TEMP_TARGET, axis=1), train_df[self.TEMP_TARGET]
 
 
 def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
     _logger = logging.getLogger(__name__)
     _logger.info(""Starting generating data:"")
     _logger.info(creator.generate_data_pipe(in_train, in_target, in_test))
     _logger.info(""Finished generation\n"")
 
 
 if __name__ == ""__main__"":"
KO;43.0;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;"def test_postprocess_data(self):
         new_train, new_target = self.sampler.postprocess_data(gen_train, gen_target, test_df)
         self.assertEqual(new_train.shape[0], new_target.shape[0])
         self.assertGreaterEqual(new_train.iloc[:, 0].min(), test_df.iloc[:, 0].min())
 
     def test_adversarial_filtering(self):
         new_train, new_target, test_df = self.sampler.preprocess_data(self.train.copy(),"
OK;43.0;vanegascata;GAN;acceaecc3ab376df4340cc3e1cb1324cc8884dc9;Added gc.collect after method calls - should decrease memory usage. Fixes for issue #14;"def test_postprocess_data(self):
         new_train, new_target = self.sampler.postprocess_data(gen_train, gen_target, test_df)
         self.assertEqual(new_train.shape[0], new_target.shape[0])
         self.assertGreaterEqual(new_train.iloc[:, 0].min(), test_df.iloc[:, 0].min())
         self.assertGreaterEqual(test_df.iloc[:, 0].max(), new_train.iloc[:, 0].max())
 
     def test_adversarial_filtering(self):
         new_train, new_target, test_df = self.sampler.preprocess_data(self.train.copy(),"
KO;1.0;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";"build-backend = ""setuptools.build_meta""
 
 [project]
 name = ""cardano-nft-vending-machine""
 version = ""0.3.0-beta2""
 
 description = ""Library to perform NFT mints automatically on the Cardano blockchain""
 readme = ""README.md"""
OK;1.0;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";"build-backend = ""setuptools.build_meta""
 
 [project]
 name = ""cardano-nft-vending-machine""
 version = ""0.3.0-beta3""
 
 description = ""Library to perform NFT mints automatically on the Cardano blockchain""
 readme = ""README.md"""
KO;1.0;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";" Repreentation of the Blockfrost web API used in retrieving metadata about txn i/o on the chain.
 """"""
 class BlockfrostApi(object):
     def __init__(self, project, mainnet=False):
         self.project = project
         self.mainnet = mainnet
def get_input_address(self, txn_hash):
         return utxo_inputs.pop()
 
     def get_utxos(self, address, exclusions):
         try:
             utxo_data = self.__call_get_api(f""addresses/{address}/utxos"")
         except requests.exceptions.HTTPError as e:
             if e.response.status_code == HTTPStatus.NOT_FOUND:
                 return []
             raise e
         available_utxos = set()
         #print('EXCLUSIONS\t', [f'{utxo.hash}#{utxo.ix}' for utxo in exclusions])
         for raw_utxo in utxo_data:
             balances = [Utxo.Balance(int(balance['quantity']), balance['unit']) for balance in raw_utxo['amount']]
             utxo = Utxo(raw_utxo['tx_hash'], raw_utxo['output_index'], balances)
             if utxo in exclusions:
                 print(f'Skipping {utxo.hash}#{utxo.ix}')
                 continue
             available_utxos.add(utxo)
         return available_utxos
 
     def get_protocol_parameters(self):"
OK;1.0;thaddeusdiamond;cardano-nft-vending-machine;23936f4c72aa9416f62100afa46630a73f51705c;"BlockfrostApi: Support UTXO Pagination

This isn't the most memory efficient, but we estimate each entry is
around 500 bytes, so 20K UTXOs would only use around 10M of RAM to
store.  These don't need to be kept in L1 cache, given that these are
not CPU intensive caches, they are mostly reference lookups for where to
how to compose transaction inputs that will be submitted to the
Blockfrost API.

To manually test this, rather than submit 100 different transactions, we
simply submit three transactions and temporary mock out the
UTXO_LIST_LIMIT VARIABLE to 1 to ensure that pagination of the 3
transactions work.

[ Documentation: None ]
[ Testing: Manual as described above ]";" Repreentation of the Blockfrost web API used in retrieving metadata about txn i/o on the chain.
 """"""
 class BlockfrostApi(object):
 
     _UTXO_LIST_LIMIT = 100
 
     def __init__(self, project, mainnet=False):
         self.project = project
         self.mainnet = mainnet
def get_input_address(self, txn_hash):
         return utxo_inputs.pop()
 
     def get_utxos(self, address, exclusions):
         available_utxos = set()
         current_page = 0
         while True:
             current_page += 1
             try:
                 utxo_data = self.__call_get_api(f""addresses/{address}/utxos?count={BlockfrostApi._UTXO_LIST_LIMIT}&page={current_page}"")
             except requests.exceptions.HTTPError as e:
                 if e.response.status_code == HTTPStatus.NOT_FOUND:
                     return []
                 raise e
             #print('EXCLUSIONS\t', [f'{utxo.hash}#{utxo.ix}' for utxo in exclusions])
             for raw_utxo in utxo_data:
                 balances = [Utxo.Balance(int(balance['quantity']), balance['unit']) for balance in raw_utxo['amount']]
                 utxo = Utxo(raw_utxo['tx_hash'], raw_utxo['output_index'], balances)
                 if utxo in exclusions:
                     print(f'Skipping {utxo.hash}#{utxo.ix}')
                     continue
                 available_utxos.add(utxo)
             if len(utxo_data) < BlockfrostApi._UTXO_LIST_LIMIT:
                 break
         return available_utxos
 
     def get_protocol_parameters(self):"
KO;1.0;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
                 result = self.sess.run(tf.nn.softmax(out_bid_logit), feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
OK;1.0;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.output_softmax = tf.nn.softmax(self.graph.get_tensor_by_name('out_bid_logit:0'))
         self.graph.finalize()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
                 result = self.sess.run(self.output_softmax, feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
KO;1.0;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";" import numpy as np
 import tensorflow as tf
 
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.model = self.init_model()
 
     def close(self):
def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
         return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1], 32))))#[:,-1,:]
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
         return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32))))#[:,-1,:]
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
OK;1.0;lorserker;ben;94ae908a7a8c5a0f8cfb80557355daea74e69fd8;"Merge pull request #15 from lorserker/fix-memory-leak

finalizing tensorflow graph to avoid memory leak";" import numpy as np
 import tensorflow as tf
 
 from scipy.special import softmax
 
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.graph.finalize()
         self.model = self.init_model()
 
     def close(self):
def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
         return softmax(card_logit.reshape((x.shape[0], x.shape[1], 32)), axis=2)
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
         return softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32)), axis=2)
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
KO;1.0;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
                 result = self.sess.run(tf.nn.softmax(out_bid_logit), feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
OK;1.0;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;"def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.output_softmax = tf.nn.softmax(self.graph.get_tensor_by_name('out_bid_logit:0'))
         self.graph.finalize()
         self.lstm_size = 128
         self.zero_state = (
             State(c=np.zeros((1, self.lstm_size)), h=np.zeros((1, self.lstm_size))),
def pred_fun_seq(x):
                     keep_prob: p_keep,
                     seq_in: x,
                 }
                 result = self.sess.run(self.output_softmax, feed_dict=feed_dict)
             return result
         
         return pred_fun_seq, pred_fun"
KO;1.0;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;" import numpy as np
 import tensorflow as tf
 
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.model = self.init_model()
 
     def close(self):
def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
         return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1], 32))))#[:,-1,:]
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
         return self.sess.run(tf.nn.softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32))))#[:,-1,:]
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
OK;1.0;lorserker;ben;01881d37335402e1534ae66374cb84ff257d00c3;finalizing tensorflow graph to avoid memory leak;" import numpy as np
 import tensorflow as tf
 
 from scipy.special import softmax
 
 
 SUIT_MASK = np.array([
     [1] * 8 + [0] * 24,
def __init__(self, name, model_path):
         self.graph = tf.Graph()
         self.sess = tf.Session(graph=self.graph)
         self.load_model()
         self.graph.finalize()
         self.model = self.init_model()
 
     def close(self):
def pred_fun(x):
         return pred_fun
 
     def reshape_card_logit(self, card_logit, x):
         return softmax(card_logit.reshape((x.shape[0], x.shape[1], 32)), axis=2)
 
     def next_cards_softmax(self, x):
         return self.model(x)[:,-1,:]
def next_cards_softmax(self, x):
 class BatchPlayerLefty(BatchPlayer):
 
     def reshape_card_logit(self, card_logit, x):
         return softmax(card_logit.reshape((x.shape[0], x.shape[1] - 1, 32)), axis=2)
 
 
 def follow_suit(cards_softmax, own_cards, trick_suit):"
KO;3.0;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
         cur_deltas = all_centers - old_centers  # (B, N, K, dim)
         dim = self.dim
         assert cur_deltas.shape == (B, N, K, dim)
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
         # cur_sumsq: (B, N, K), is the sum-squared error if we were to
         # make the n'th choice without making any of the other N-1 choices, i.e.
         # if we were to leave the other choices at the value we had at input.
         # Specifically, it is always supposed to equal the value of
         #  ((x_err + cur_deltas)**2).sum(dim=-1)
         # .. but we keep it around separately because it enables an optimization.
         modified_err = x_err + cur_deltas # (B, N, K, dim)
 
         # cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
         # We really want batched vector-vector product her, which torch does not
         # explicitly support, so we use a matrix multiplication with 1x1 output.
         cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
                                  modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
 
         assert cur_sumsq.shape == (B, N, K)
         # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
         gather_deltas = None # will be a lambda, see below.
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
                 # cur_indexes is (B, N, new_K, dim), but sorted from worst to best.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
                 if cur_deltas is not None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
                     cur_deltas = gather_deltas(this_indexes)
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
OK;3.0;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
         # At all points in the algorithm we maintain cur_sumsq and (conceptually)
         # cur_deltas (however in some parts cur_deltas is not instantiated, see
         # gather_deltas).
         #
         # cur_indexes: (B, N, K, L), initially (B, num_codebooks, codebook_size, 1),
         #   gives the codebook indexes corresponding to the k'th value of the n'th
         #   choice.  Initially this is just an arange expression but from the 1st
         #   iter of the algorithm it changes to something nontrivial.
         #
         # cur_sumsq: (B, N, K), is the sum-squared error of x versus its predicted value
         # from the codebooks, if we were to
         # make the n'th choice with value k without making any of the other N-1 choices, i.e.
         # if we were to leave the other choices at the value we had at input.
         # Specifically, it is always supposed to equal the value of
         #  ((x_err + cur_deltas)**2).sum(dim=-1)
         # .. but we keep it around separately because it enables an optimization.
         #
         # cur_deltas: (B, N, K, dim), is the change in x_err (with x_err =
         # x_approx - x and x_approx being a sum of codebook indexes) if we were
         # to make the n'th choice with value k without making any of the other
         # N-1 choices.
         # At the current point, i.e. at the start of the algorithm,
         # cur_deltas[b][n][k] says ""what would be the change in x_err if we
         # were to replace the current choice of the n'th codebook entry-- i.e.
         # the choice reflected in `indexes`-- with value k?  [In general,
         # cur_deltas[b][n][k] refers not directly to a codebook indexes, but
         # to an indexes into `cur_indexes` which corresponds to the sequence/combination
         # of codebook indexes that are stored in cur_indexes[b][n][k].
 
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
         # Conceptually we could do:
         # cur_deltas = all_centers - old_centers  # (B, N, K, dim)
         # ... however actually we won't be instantiating cur_deltas at this stage of the
         # algorithm.
         dim = self.dim
 
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
         if True:
             # compute cur_sumsq using an efficient approach
             x_err_sumsq = (x_err ** 2).sum(dim=-1) # (B, 1, 1)
 
             x_remaining = x_err - old_centers  # (B, num_codebooks, 1, dim): the x_err after subtracting
             # each of the codebooks; if we add back to this any given
             # codebook vector (from all_centers), we'll get the error
             # if we were to
             # choose that codebook entry instead of the one actually chosen.
 
             x_remaining_sumsq = (x_remaining ** 2).sum(dim=-1) # (B, num_codebooks, 1)
             # all_centers_sumsq is the sumsq of all the centers..
             all_centers_sumsq = (all_centers ** 2).sum(dim=-1) # (1, num_codebooks, codebook_size)
 
             cross_sum = torch.matmul(all_centers, # (1, num_codebooks, codebook_size, dim)
                                      x_remaining.permute(2, 1, 3, 0)  # (1, num_codebooks, dim, B)
             ) # (1, num_codebooks, codebook_size, B)
             cross_sum = cross_sum.squeeze(0).permute(2, 0, 1) # (B, num_codebooks, codebook_size)
             # (B, num_codebooks, codebook_size); interpret as (B, N, K)
             cur_sumsq = x_remaining_sumsq + all_centers_sumsq + 2 * cross_sum
             assert cur_sumsq.shape == (B, N, K)
 
             # gather_deltas (which will be re-defined below) is a lambda from
             # `this_indexes`, a LongTensor of shape (B, N, new_K, 1) [which
             # at the current iteration would equal (B, num_codebooks, new_K, 1)]
             # with elements in
             # {0..K-1} [i.e. 0..codebook_size-1], to the new ""cur_deltas"".
             # It is provided as a workaround in
             # case we did not physically instantiate cur_deltas on this iteration.
             # In general cur_deltas is supposed to represent ""change in encoded
             # value"" if we were to make a particular modified index choice, leaving
             # all other choices as they were on entry.
             # gather_deltas is supposed to be a lambda from this_indexes to the
             # something equivalent to following expression (if cur_deltas had actually
             # existed):
             #   torch.gather(input=cur_deltas, dim=2, index=this_indexes.expand(B, N, new_K, dim))
 
             gather_deltas = lambda this_indexes: (
                 torch.gather(input=all_centers.expand(B, N, K, dim), dim=2,
                              index=this_indexes.expand(B, N, -1, dim)) - old_centers
             )
         else:
             cur_deltas = all_centers - old_centers  # (B, N, K, dim)
             ## cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
             ## We really want batched vector-vector product her, which torch does not
             ## explicitly support, so we use a matrix multiplication with 1x1 output.
             modified_err = x_err + cur_deltas # (B, N, K, dim)
             cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
                                      modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
             gather_deltas = None
 
             # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
                 # cur_indexes is (B, N, new_K, L), but with only the chosen
                 # indexes kept.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
                 if gather_deltas is None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
                     # gather_deltas should be a lambda from:
                     # this_indexes: a LongTensor of shape (B, N, new_K, 1) containing elements in {0..K-1}
                     # to the new ""deltas"" which should be of shape
                     # (B, N, new_K, dim)
                     # representing the difference from the baseline ""x_offset"" if we choose this
                     # index for this codebook or range of codebooks, leaving other choices
                     # as they were at entry to this function.
                     cur_deltas = gather_deltas(this_indexes)
                     gather_deltas = None
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
KO;3.0;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
     #_test_train_from_file()
     _test_joint_predictor()"
OK;3.0;k2-fsa;multi_quantization;0ddde6bfb6ed04a79099626ad237c7552f7b6ac3;"Merge pull request #3 from danpovey/fast_quantization

Make quantization faster and more memory efficient.";"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
     _test_train_from_file()
     _test_joint_predictor()"
KO;3.0;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
         cur_deltas = all_centers - old_centers  # (B, N, K, dim)
         dim = self.dim
         assert cur_deltas.shape == (B, N, K, dim)
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
         # cur_sumsq: (B, N, K), is the sum-squared error if we were to
         # make the n'th choice without making any of the other N-1 choices, i.e.
         # if we were to leave the other choices at the value we had at input.
         # Specifically, it is always supposed to equal the value of
         #  ((x_err + cur_deltas)**2).sum(dim=-1)
         # .. but we keep it around separately because it enables an optimization.
         modified_err = x_err + cur_deltas # (B, N, K, dim)
 
         # cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
         # We really want batched vector-vector product her, which torch does not
         # explicitly support, so we use a matrix multiplication with 1x1 output.
         cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
                                  modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
 
         assert cur_sumsq.shape == (B, N, K)
         # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
         gather_deltas = None # will be a lambda, see below.
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
                 # cur_indexes is (B, N, new_K, dim), but sorted from worst to best.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
                 if cur_deltas is not None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
                     cur_deltas = gather_deltas(this_indexes)
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
OK;3.0;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def _refine_indexes(self,
         #     and doubles every 2 iterations to keep the work per iteration
         #     fairly constant.
 
         # At all points in the algorithm we maintain cur_sumsq and (conceptually)
         # cur_deltas (however in some parts cur_deltas is not instantiated, see
         # gather_deltas).
         #
         # cur_indexes: (B, N, K, L), initially (B, num_codebooks, codebook_size, 1),
         #   gives the codebook indexes corresponding to the k'th value of the n'th
         #   choice.  Initially this is just an arange expression but from the 1st
         #   iter of the algorithm it changes to something nontrivial.
         #
         # cur_sumsq: (B, N, K), is the sum-squared error of x versus its predicted value
         # from the codebooks, if we were to
         # make the n'th choice with value k without making any of the other N-1 choices, i.e.
         # if we were to leave the other choices at the value we had at input.
         # Specifically, it is always supposed to equal the value of
         #  ((x_err + cur_deltas)**2).sum(dim=-1)
         # .. but we keep it around separately because it enables an optimization.
         #
         # cur_deltas: (B, N, K, dim), is the change in x_err (with x_err =
         # x_approx - x and x_approx being a sum of codebook indexes) if we were
         # to make the n'th choice with value k without making any of the other
         # N-1 choices.
         # At the current point, i.e. at the start of the algorithm,
         # cur_deltas[b][n][k] says ""what would be the change in x_err if we
         # were to replace the current choice of the n'th codebook entry-- i.e.
         # the choice reflected in `indexes`-- with value k?  [In general,
         # cur_deltas[b][n][k] refers not directly to a codebook indexes, but
         # to an indexes into `cur_indexes` which corresponds to the sequence/combination
         # of codebook indexes that are stored in cur_indexes[b][n][k].
 
 
         # cur_deltas represents the change in x_err from making each choice (while
         # leaving all the other choices un-made by just keeping the passed-in/old
def _refine_indexes(self,
         N = self.num_codebooks
         K = self.codebook_size
         L = 1  # L is the number of codebooks covered by each choice.
         # Conceptually we could do:
         # cur_deltas = all_centers - old_centers  # (B, N, K, dim)
         # ... however actually we won't be instantiating cur_deltas at this stage of the
         # algorithm.
         dim = self.dim
 
         # cur_indexes is the codebook indexes corresponding to 'cur_deltas'.
         cur_indexes = torch.arange(K, device=x.device).reshape(1, 1, K, 1).expand(B, N, K, L)
 
         if True:
             # compute cur_sumsq using an efficient approach
             x_err_sumsq = (x_err ** 2).sum(dim=-1) # (B, 1, 1)
 
             x_remaining = x_err - old_centers  # (B, num_codebooks, 1, dim): the x_err after subtracting
             # each of the codebooks; if we add back to this any given
             # codebook vector (from all_centers), we'll get the error
             # if we were to
             # choose that codebook entry instead of the one actually chosen.
 
             x_remaining_sumsq = (x_remaining ** 2).sum(dim=-1) # (B, num_codebooks, 1)
             # all_centers_sumsq is the sumsq of all the centers..
             all_centers_sumsq = (all_centers ** 2).sum(dim=-1) # (1, num_codebooks, codebook_size)
 
             cross_sum = torch.matmul(all_centers, # (1, num_codebooks, codebook_size, dim)
                                      x_remaining.permute(2, 1, 3, 0)  # (1, num_codebooks, dim, B)
             ) # (1, num_codebooks, codebook_size, B)
             cross_sum = cross_sum.squeeze(0).permute(2, 0, 1) # (B, num_codebooks, codebook_size)
             # (B, num_codebooks, codebook_size); interpret as (B, N, K)
             cur_sumsq = x_remaining_sumsq + all_centers_sumsq + 2 * cross_sum
             assert cur_sumsq.shape == (B, N, K)
 
             # gather_deltas (which will be re-defined below) is a lambda from
             # `this_indexes`, a LongTensor of shape (B, N, new_K, 1) [which
             # at the current iteration would equal (B, num_codebooks, new_K, 1)]
             # with elements in
             # {0..K-1} [i.e. 0..codebook_size-1], to the new ""cur_deltas"".
             # It is provided as a workaround in
             # case we did not physically instantiate cur_deltas on this iteration.
             # In general cur_deltas is supposed to represent ""change in encoded
             # value"" if we were to make a particular modified index choice, leaving
             # all other choices as they were on entry.
             # gather_deltas is supposed to be a lambda from this_indexes to the
             # something equivalent to following expression (if cur_deltas had actually
             # existed):
             #   torch.gather(input=cur_deltas, dim=2, index=this_indexes.expand(B, N, new_K, dim))
 
             gather_deltas = lambda this_indexes: (
                 torch.gather(input=all_centers.expand(B, N, K, dim), dim=2,
                              index=this_indexes.expand(B, N, -1, dim)) - old_centers
             )
         else:
             cur_deltas = all_centers - old_centers  # (B, N, K, dim)
             ## cur_sumsq: (B, N, K), equivalent to: ((x_err + cur_deltas)**2).sum(dim=-1)
             ## We really want batched vector-vector product her, which torch does not
             ## explicitly support, so we use a matrix multiplication with 1x1 output.
             modified_err = x_err + cur_deltas # (B, N, K, dim)
             cur_sumsq = torch.matmul(modified_err.unsqueeze(-2),
                                      modified_err.unsqueeze(-1)).squeeze(-1).squeeze(-1)
             gather_deltas = None
 
             # x_err_sumsq: (B, 1, 1), is the sum-squared of x_err; we'll need it in the loop.
         x_err_sumsq = (x_err**2).sum(dim=-1)
 
         K_cutoff_base = 8 if self.codebook_size <= 16 else 16
 
def get_K_cutoff():
 
                 this_indexes = this_indexes.unsqueeze(-1)
 
                 # cur_indexes is (B, N, new_K, L), but with only the chosen
                 # indexes kept.
                 cur_indexes = torch.gather(input=cur_indexes, dim=2,
                                            index=this_indexes.expand(B, N, new_K, L))
 
                 if gather_deltas is None:
                     # also sort cur_deltas in the same way
                     cur_deltas = torch.gather(input=cur_deltas, dim=2,
                                               index=this_indexes.expand(B, N, new_K, dim))
                 else:
                     # gather_deltas should be a lambda from:
                     # this_indexes: a LongTensor of shape (B, N, new_K, 1) containing elements in {0..K-1}
                     # to the new ""deltas"" which should be of shape
                     # (B, N, new_K, dim)
                     # representing the difference from the baseline ""x_offset"" if we choose this
                     # index for this codebook or range of codebooks, leaving other choices
                     # as they were at entry to this function.
 
                     #if cur_deltas is not None:
                     #    cur_deltas_alt = torch.gather(input=cur_deltas, dim=2,
                     #                                  index=this_indexes.expand(B, N, new_K, dim))
                     cur_deltas = gather_deltas(this_indexes)
                     #if cur_deltas is not None and cur_deltas.shape == cur_deltas_alt.shape:
                     #    print(""cur_deltas: "", cur_deltas[:3,:3,:3,:3])
                     #    print(""cur_deltas_alt: "", cur_deltas_alt[:3,:3,:3,:3])
                     #    assert torch.allclose(cur_deltas, cur_deltas_alt)
                     gather_deltas = None
                 K = new_K
             else:
                 # Combine pairs of choices.  We know that N > 1."
KO;3.0;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
     #_test_train_from_file()
     _test_joint_predictor()"
OK;3.0;k2-fsa;multi_quantization;15ba47302b884bd91726cd6acb7d2895e4522b7f;Make quantization faster and more memory efficient.;"def minibatch_generator(data: Tensor,
 
 if __name__ == ""__main__"":
     logging.getLogger().setLevel(logging.INFO)
     _test_train_from_file()
     _test_joint_predictor()"
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" 
 A Python shared memory toolkit for process picture between different processes.
 
 ### How to use
 
 main process - 主进程
 
 ```python
 # optional
 import cv2
 from shared_memory_toolkit import dump_image_into_shared_memory
 
 image = cv2.imread('test.pic')
 dump_image_into_shared_memory('uuid_content', image)
 ```
 
 sub process - 子进程
 
 ```python
 from shared_memory_toolkit import load_image_from_shared_memory
 
 # ... some other codes
 
 image = load_image_from_shared_memory('uuid_content')
 ```
 
 主进程和子进程中的image将会完全保持一致：由同样的bytes转换而来。
 
 #### TODO:
 
 1. unittest for raw_image.
 2. base64_image module.
 3. README and docs."
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" 
 A Python shared memory toolkit for process picture between different processes.
 
 # Upgrade: 支持读写不同形状的图片，读写不同大小的共享内存
 
 ### How to use
 
 main process - 主进程
 
 ```python
 from shared_memory_toolkit import load_image_from_shared_memory, dump_image_into_shared_memory
 
 # load image
 import cv2
 
 raw_image = cv2.imread('image')
 
 image_shm_name = 'camera_1817'
 dump_image_into_shared_memory(image_shm_name, raw_image)
 
 # in other process
 
 image = load_image_from_shared_memory('camera_1817')
 
 # raw_image == image
 ```
 
 #### TODO:
 
 1. base64_image module."
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""setup with setuptools.""""""
 
 from setuptools import setup, find_packages
 
 setup(
     name='stream_watcher',
     version='0.1',
     keywords='Stream',
     description='A Pythonic way to manage streams in one file.',
     author='Logic',
     author_email='logic.irl@outlook.com',
     url='https://github.com/TheStar-LikeDust/shared_memory_toolkit.git',
     python_requires='>=3.8',
     packages=find_packages(exclude=['tests*']),
     license='Apache License 2.0'
 )"
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""
 
 """"""
 
 from .raw_image import dump_image_into_shared_memory, load_image_from_shared_memory
 
 from .sync import initial_sync_in_fork, initial_sync_in_spawn"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""
 
 """"""
 # system
 from .sync import initial_sync_in_fork, initial_sync_in_spawn
 
 # base
 from .core import get_share_memory
 from .raw_image import dump_image_into_shared_memory, load_image_from_shared_memory
 
 # extra
 # TODO:"
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""Dict[str, SharedMemory]: 存储共享内存名字和共享内存实体对象的共享内存对象字典""""""
 
 
 def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
     """"""从共享内存映射表中加载一个共享内存，返回共享内存对象和对应的锁。
 
     如果不存在（第一次加载）共享内存，则会创建一个FIX_LENGTH大小的共享内存，和相应的锁。
 
     Note:
         如果不在同一个进程下也可以相互访问共享内存。
 
     Note:
         如果执行了initial_shared_memory_lock，则会通过一个跨进程字典来使不同进程之间保持同一个锁。
 
     Args:
         shared_memory_name (str): 共享内存名字。
 
     Returns:
         Tuple[SharedMemory, Lock]: 共享内存和锁的元组。
     """"""
     # 获取此操作的锁
     lock = get_shm_lock(shared_memory_name)
def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
             # 同步：尝试创建
             # case: 系统中不存在此共享内存，则创建一个新的共享内存区
             try:
                 shared = SharedMemory(name=shared_memory_name, create=True, size=FIX_LENGTH)
                 assert len(shared.buf) == FIX_LENGTH
             # case: 系统中存在此共享内存，则创建共享内存对象并放入当前进程的共享内存对象字典
             except FileExistsError:
                 shared = SharedMemory(name=shared_memory_name, create=False)"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" """"""Dict[str, SharedMemory]: 存储共享内存名字和共享内存实体对象的共享内存对象字典""""""
 
 
 def get_share_memory(shared_memory_name: str, memory_size: int = None) -> Tuple[SharedMemory, Lock]:
     """"""从共享内存映射表中加载一个共享内存，返回共享内存对象和对应的锁。
 
     如果不存在此（第一次加载）共享内存，则会创建一个FIX_LENGTH大小的共享内存，和相应的锁。
 
     Args:
         shared_memory_name (str): 共享内存名
         memory_size (int, optional): 共享内存大小. Defaults to None.
 
     Returns:
         Tuple[SharedMemory, Lock]: 共享内存和对应的锁
     """"""
     # 获取此操作的锁
     lock = get_shm_lock(shared_memory_name)
def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
             # 同步：尝试创建
             # case: 系统中不存在此共享内存，则创建一个新的共享内存区
             try:
                 shared = SharedMemory(
                     name=shared_memory_name,
                     create=True,
                     size=memory_size if memory_size else FIX_LENGTH
                 )
             # case: 系统中存在此共享内存，则创建共享内存对象并放入当前进程的共享内存对象字典
             except FileExistsError:
                 shared = SharedMemory(name=shared_memory_name, create=False)"
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""numpy格式图片的读取和写入。
 
 
 全局变量:
 
     1. IMAGE_SHAPE
 
 函数:
 
     1. dump_image_into_shared_memory::
 
         Dump image
 
     2. load_image_from_shared_memory::
 
         Load image
 
 Note:
 
     可跨进程使用
 
 
 """"""
 import numpy
 
 from .core import _get_share_memory, FIX_LENGTH
 
 IMAGE_SHAPE = (1080, 1920, 3)
 """"""图像的默认形状""""""
 
 
 def dump_image_into_shared_memory(shared_memory_name: str, image: numpy.ndarray) -> memoryview:
     """"""将当前的图片dump成共享内存放入当前的共享内存映射中，此操作加锁
 
     Args:
         shared_memory_name (str): 共享内存名
         image (numpy.ndarray): numpy格式图片
 
     Returns:
         memoryview: 内存对象（共享内存的.buf属性）
     """"""
     shared_memory, lock = _get_share_memory(shared_memory_name)
 
     with lock:
         shared_memory.buf[:FIX_LENGTH] = image.tobytes()
     return shared_memory.buf
 
 
 def load_image_from_shared_memory(shared_memory_name: str) -> numpy.ndarray:
     """"""从当前的共享内存映射中读取相应的共享内存并转换为图像，此操作加锁
 
     Args:
         shared_memory_name (str): 共享内存名
 
     Returns:
         numpy.ndarray: numpy格式图片
     """"""
     shared_memory, lock = _get_share_memory(shared_memory_name)
 
     with lock:
         image = numpy.frombuffer(shared_memory.buf, dtype=numpy.uint8)[:FIX_LENGTH].reshape(IMAGE_SHAPE)
     return image"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""numpy格式图片的读取和写入。
 
 全局变量:
 
     1. IMAGE_SHAPE: 默认图片的形状
 
 函数:
 
     1. dump_image_into_shared_memory: Dump image
     2. load_image_from_shared_memory: Load image
 
 TODO:
 
     1. 基于shm的numpy数组
 """"""
 from typing import Tuple
 from multiprocessing.shared_memory import SharedMemory
 
 import numpy
 
 from .core import get_share_memory, FIX_LENGTH
 
 DEFAULT_IMAGE_SHAPE: Tuple[int, int, int] = (1080, 1920, 3)
 """"""图像的默认形状""""""
 
 get_image_size = lambda x: x[0] * x[1] * x[2]
 """"""获取图像大小""""""
 
 
 def dump_image_into_shared_memory(
         shared_memory_name: str,
         image: numpy.ndarray,
         memory_size: int = get_image_size(DEFAULT_IMAGE_SHAPE),
 ) -> SharedMemory:
     """"""将当前的图片dump成共享内存放入当前的共享内存映射中，此操作加锁
 
     Args:
         shared_memory_name (str): 共享内存名
         image (numpy.ndarray): numpy格式图片
         memory_size (int): 图片格式大小，默认为默认图像形状的大小. Default is 6220800
 
     Returns:
         SharedMemory: 共享内存对象
     """"""
     shared_memory, lock = get_share_memory(shared_memory_name, memory_size)
 
     with lock:
         shared_memory.buf[:FIX_LENGTH] = image.tobytes()
     return shared_memory
 
 
 def load_image_from_shared_memory(
         shared_memory_name: str,
         image_shape: Tuple[int, int, int] = DEFAULT_IMAGE_SHAPE,
 ) -> numpy.ndarray:
     """"""从当前的共享内存映射中读取相应的共享内存并转换为图像，此操作加锁
 
     Args:
         shared_memory_name (str): 共享内存名
         image_shape (Tuple[int, int, int]): 默认图像形状. Default is (1080, 1920, 3)
 
     Returns:
         numpy.ndarray: numpy格式图片
     """"""
     shared_memory, lock = get_share_memory(shared_memory_name)
 
     with lock:
         image = numpy.frombuffer(shared_memory.buf, dtype=numpy.uint8)[:get_image_size(image_shape)].reshape(
             image_shape)
     return image"
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" def get_shm_lock(shm_name) -> Lock:
     """"""获取共享内存对应的跨进程锁""""""
 
     # 同步：先查找映射字典中是否有该共享内存
     with _lock:
         # case: 不存在共享内存，则需要创建。
def initial_sync_in_fork(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], L
 
 
 def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], List[Lock]]:
     """"""Spawn方式的启动：需要在主进程先调用此方法，再将同步对象手动放入子进程中
 
     Args:
         lock_number (int, optional): 子进程的对应的锁数量. Defaults to 64.
def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int],
 
 
 def synchronization_setter(lock: Lock, name_index_mapper: Dict[str, int], lock_list: List[Lock]) -> NoReturn:
     """"""用于Spawn启动，手动设置子进程的同步对象
 
     Args:
         lock (Lock): 控制映射表的锁"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" def get_shm_lock(shm_name) -> Lock:
     """"""获取共享内存对应的跨进程锁""""""
 
     # 如果没有初始化进程间锁，则会使用默认的锁。
     if not _lock_list:
         return _lock
 
     # 同步：先查找映射字典中是否有该共享内存
     with _lock:
         # case: 不存在共享内存，则需要创建。
def initial_sync_in_fork(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], L
 
 
 def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int], List[Lock]]:
     """"""Spawn方式的启动：需要在主进程先调用此方法，再将同步对象手动放入子进程中。
 
     Args:
         lock_number (int, optional): 子进程的对应的锁数量. Defaults to 64.
def initial_sync_in_spawn(lock_number: int = 64) -> Tuple[Lock, Dict[str, int],
 
 
 def synchronization_setter(lock: Lock, name_index_mapper: Dict[str, int], lock_list: List[Lock]) -> NoReturn:
     """"""用于Spawn启动时，手动传入子进程的同步对象。
 
     Args:
         lock (Lock): 控制映射表的锁"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" import unittest
 
 
 class MyTestCase(unittest.TestCase):
     def test_something(self):
         self.assertEqual(True, False)  # add assertion here
 
 
 if __name__ == '__main__':
     unittest.main()"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""Unitest testcases.
 
 
 """""""
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" # -*- coding: utf-8 -*-
 """"""Example Google style docstrings.
 
 Example:
     Examples can be given using either the ``Example`` or ``Examples``
     sections. Sections support any reStructuredText formatting, including
     literal blocks::
 
         $ python example_google.py
 
 Attributes:
     module_level_variable1 (int): Module level variables may be documented in
         either the ``Attributes`` section of the module docstring, or in an
         inline docstring immediately following the variable.
 
 Todo:
     * For module TODOs
     * You have to also use ``sphinx.ext.todo`` extension
 
 .. _Google Python Style Guide:
    http://google.github.io/styleguide/pyguide.html
 
 """""""
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" import time
 import unittest
 
 import random
 from multiprocessing import Process, shared_memory
 from concurrent.futures import ProcessPoolExecutor
 
 from shared_memory_toolkit.core import get_share_memory
 
 
 def dump_bytes(shm_name: str, random_bytes: bytes):
     shm, lock = get_share_memory(shm_name)
 
     shm.buf[:len(random_bytes)] = random_bytes
 
 
 def load_bytes(shm_name: str) -> bytes:
     shm, lock = get_share_memory(shm_name)
 
     return bytes(shm.buf)
 
 
 def dump_bytes_in_process(shm_name: str, random_bytes: bytes):
     # cannot pickle local function as subprocess
     p = Process(target=dump_bytes, args=(shm_name, random_bytes))
     p.daemon = True
     p.start()
     p.join()
 
 
 def get_lock_id(shm_name):
     shm, lock = get_share_memory(shm_name)
     print(id(lock))
 
     return id(lock)
 
 
 class CoreTestCase(unittest.TestCase):
 
     def test_default_property(self):
         """"""默认的各项属性""""""
         from shared_memory_toolkit import core
 
         with self.subTest('Default FIX_LENGTH: 6220800'):
             assert core.FIX_LENGTH == 6220800
 
         with self.subTest('Default _share_memory_cache_mapper: empty'):
             assert core._share_memory_cache_mapper == {}
 
     def test_get_share_memory_same_name(self):
         """"""_get_share_memory: 相同名字加载相同的共享内存""""""
         same_name = 'same_name'
 
         shm_0, lock_0 = get_share_memory(same_name)
         shm_1, lock_1 = get_share_memory(same_name)
 
         assert shm_0.name == shm_1.name
         assert shm_0.buf == shm_1.buf
 
     def test_get_shm_in_different_process(self):
         """"""主进程创建后，子进程写入，共享内存产生相同变化""""""
         random_bytes_content = random.randbytes(1920 * 1080 * 3)
 
         # 必须由主进程先创建共享内存 否则子进程后退出时会自动销毁
         shm_in_main, lock_main = get_share_memory('same_name')
 
         # 子进程写入
         dump_bytes_in_process(shm_name='same_name', random_bytes=random_bytes_content)
 
         assert bytes(shm_in_main.buf) == random_bytes_content
 
 
 if __name__ == '__main__':
     unittest.main()"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" import unittest
 
 from shared_memory_toolkit.sync import get_shm_lock
 
 
 class MyTestCase(unittest.TestCase):
     def test_get_shm_lock_without_initial(self):
         """"""不初始化""""""
 
         lock_0 = get_shm_lock('test_memory_0')
         lock_1 = get_shm_lock('test_memory_1')
 
         assert lock_0 == lock_1
 
 
 if __name__ == '__main__':
     unittest.main()"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" import time
 import unittest
 
 from multiprocessing import get_start_method, Process
 from concurrent.futures import ProcessPoolExecutor
 from shared_memory_toolkit.sync import initial_sync_in_fork, get_shm_lock
 
 
 def process_state(*args):
     from shared_memory_toolkit.sync import _lock, _lock_list, _name_index_mapper
 
     return _lock._id, _name_index_mapper._id, _lock_list._id
 
 
 def process_id(n: str = 'name'):
     lock = get_shm_lock(n)
     return lock._id
 
 
 @unittest.skipUnless(condition=get_start_method() == 'fork', reason='fork')
 class SyncForkTestCase(unittest.TestCase):
 
     def test_initial_sync_in_fork(self):
         syncs = initial_sync_in_fork()
 
         with ProcessPoolExecutor(4) as pool:
             sub_results = [pool.submit(process_state).result() for _ in range(10)]
 
         with self.subTest('same lock id'):
             for sub_result in sub_results:
                 self.assertEquals(sub_result[0], syncs[0]._id)
 
         with self.subTest('same mapper id'):
             for sub_result in sub_results:
                 self.assertEquals(sub_result[1], syncs[1]._id)
 
         with self.subTest('same lock_list id'):
             for sub_result in sub_results:
                 self.assertEquals(sub_result[2], syncs[2]._id)
 
     def test_get_shm_lock_same_lock(self):
         """"""一致的名称返回一致的锁""""""
         initial_sync_in_fork()
 
         lock_0 = get_shm_lock('uuid')
         lock_1 = get_shm_lock('uuid')
 
         assert lock_0._id == lock_1._id
 
     def test_get_shm_lock_between_process(self):
         initial_sync_in_fork()
 
         with ProcessPoolExecutor(4) as pool:
             futures = [pool.submit(process_id) for _ in range(10)]
             ids = [_.result() for _ in futures]
 
         [self.assertEquals(_, ids[0]) for _ in ids]
 
 
 if __name__ == '__main__':
     unittest.main()"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" import time
 import unittest
 from concurrent.futures import ProcessPoolExecutor
 from multiprocessing import get_start_method, Process
 
 from shared_memory_toolkit.sync import initial_sync_in_spawn, get_shm_lock, synchronization_setter
 
 
 def process_state(*args):
     synchronization_setter(*args)
 
     from shared_memory_toolkit.sync import _lock, _lock_list, _name_index_mapper
 
     return _lock._id, _name_index_mapper._id, _lock_list._id
 
 
 def process_id(n: str = 'name'):
     lock = get_shm_lock(n)
     return lock._id
 
 
 @unittest.skipUnless(condition=get_start_method() == 'spawn', reason='spawn')
 class SyncSpawnTestCase(unittest.TestCase):
     def test_initial_sync_in_spawn(self):
         syncs = initial_sync_in_spawn()
 
         with ProcessPoolExecutor(4) as pool:
             sub_results = [pool.submit(process_state, *syncs).result() for _ in range(10)]
 
         with self.subTest('same lock id'):
             for sub_result in sub_results:
                 self.assertEquals(sub_result[0], syncs[0]._id)
 
         with self.subTest('same mapper id'):
             for sub_result in sub_results:
                 self.assertEquals(sub_result[1], syncs[1]._id)
 
         with self.subTest('same lock_list id'):
             for sub_result in sub_results:
                 self.assertEquals(sub_result[2], syncs[2]._id)
 
     def test_get_shm_lock_same_lock(self):
         """"""一致的名称返回一致的锁""""""
         sync = initial_sync_in_spawn()
         synchronization_setter(*sync)
 
         lock_0 = get_shm_lock('uuid')
         lock_1 = get_shm_lock('uuid')
 
         assert lock_0._id == lock_1._id
 
     def test_get_shm_lock_between_process(self):
         _, d, l = initial_sync_in_spawn()
 
         from shared_memory_toolkit.sync import synchronization_setter
         with ProcessPoolExecutor(4, initializer=synchronization_setter, initargs=(_, d, l)) as pool:
             futures = [pool.submit(process_id) for _ in range(10)]
             ids = [_.result() for _ in futures]
 
         [self.assertEquals(_, ids[0]) for _ in ids]
 
 
 if __name__ == '__main__':
     unittest.main()"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;eb5e619def82c8986747de19fbea1bece06f45b0;"Feat: dynamic shared_memory and picture shape.

1. The refactor tests.
2. Dynamic load/get
3. Fix some docstring
4. setup.py";" import unittest
 
 
 class MyTestCase(unittest.TestCase):
     def test_something(self):
         pass
 
 if __name__ == '__main__':
     unittest.main()"
KO;4.0;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";" # -*- coding: utf-8 -*-
 """"""
 
 """"""
 
 from .core import initial_shared_memory_lock
 
 from .raw_image import dump_image_into_shared_memory, load_image_from_shared_memory"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";" # -*- coding: utf-8 -*-
 """"""The core of shared memory toolkit.
 
 基础的初始化以及最为核心的加载共享内存函数。
 
 全局变量:
 
     1. FIX_LENGTH::
 
         共享内存的固定大小，一般美容嗯为图片的大小: 1920 * 1080 * 3 == 6220800
 
     2. _share_memory_cache_mapper::
 
         共享内存对象字典，通过此字典来缓存共享内存对象而不需要每次实例化SharedMemory
 
     3. _share_memory_lock_mapper 和 _manager::
 
         用于控制共享内存的同步读取/写入
 
 
 """"""
 
 from multiprocessing.shared_memory import SharedMemory
 from multiprocessing import Manager, Lock
 from typing import Dict, Tuple, Optional, NoReturn
 
 FIX_LENGTH: int = 6220800
 """"""固定数据块（图片）的大小，用于从不定大小的共享内存中获取定长数据，默认为1920*1080*3大小的图片。""""""
 
 _share_memory_cache_mapper: Dict[str, SharedMemory] = {}
 """"""Dict[str, SharedMemory]: 存储共享内存名字和共享内存实体对象的共享内存对象字典""""""
 
 _share_memory_lock_mapper: Optional[Dict[str, Lock]] = {}
 """"""Dict[str, Lock]: 存储共享内存名和对应的锁的映射关系字典""""""
 
 _manager: Optional[Manager] = None
 """"""共享内存模块的Manager，主要用于生成限制对memory同一时间的写入和读取""""""
 
 
 def initial_shared_memory_lock() -> NoReturn:
     """"""初始化共享内存和锁的映射，需要在使用模块前在主进程执行。
 
     执行此函数，锁字典会变成跨进程字典。
 
     不执行此函数时，会产生一个普通的跨进程锁
     """"""
     global _manager, _share_memory_lock_mapper
     _manager = Manager()
     _share_memory_lock_mapper = _manager.dict()
 
 
 def _get_share_memory(shared_memory_name: str) -> Tuple[SharedMemory, Lock]:
     """"""从共享内存映射表中加载一个共享内存，返回共享内存对象和对应的锁。
 
     如果不存在（第一次加载）共享内存，则会创建一个FIX_LENGTH大小的共享内存，和相应的锁。
 
     Note:
         如果不在同一个进程下也可以相互访问共享内存。
 
     Note:
         如果执行了initial_shared_memory_lock，则会通过一个跨进程字典来使不同进程之间保持同一个锁。
 
     Args:
         shared_memory_name (str): 共享内存名字。
 
     Returns:
         Tuple[SharedMemory, Lock]: 共享内存和锁的元组。
     """"""
     # 从当前进程的共享内存对象字典中获取共享内存对象
     shared = _share_memory_cache_mapper.get(shared_memory_name)
 
     # case: 当前进程中暂存的共享内存对象字典中不存在该共享内存
     if shared is None:
         # 尝试创建
         # case: 系统中不存在此共享内存，则创建一个新的共享内存区
         try:
             shared = SharedMemory(name=shared_memory_name, create=True, size=FIX_LENGTH)
             assert len(shared.buf) == FIX_LENGTH
         # case: 系统中存在此共享内存，则创建共享内存对象并放入当前进程的共享内存对象字典
         except FileExistsError:
             shared = SharedMemory(name=shared_memory_name, create=False)
 
         _share_memory_cache_mapper[shared_memory_name] = shared
 
     # case: 当前找到的共享内存大小小于FIX_LENGTH，则关闭改共享内存然后重新创建
     if len(shared.buf) <= FIX_LENGTH:
         shared.close()
         shared.unlink()
         shared = SharedMemory(name=shared_memory_name, create=True, size=FIX_LENGTH)
 
         _share_memory_cache_mapper[shared_memory_name] = shared
 
     # 如果不存在锁对象，则创建一个锁对象
     if shared_memory_name not in _share_memory_lock_mapper:
         # 如果没有manager，则会创建一个普通的进程锁
         lock = _manager.Lock() if _manager is not None else Lock()
         _share_memory_lock_mapper[shared_memory_name] = lock
 
     return _share_memory_cache_mapper[shared_memory_name], _share_memory_lock_mapper[shared_memory_name]"
OK;4.0;TheStar-LikeDust;shared_memory_toolkit;dd83a8908ffdb3499d46074833626399ad0ef3bf;"initial

1. core: manage the shared memory
2. raw_image: dump/load image";" # -*- coding: utf-8 -*-
 """"""numpy格式图片的读取和写入。
 
 
 全局变量:
 
     1. IMAGE_SHAPE
 
 函数:
 
     1. dump_image_into_shared_memory::
 
         Dump image
 
     2. load_image_from_shared_memory::
 
         Load image
 
 Note:
 
     可跨进程使用
 
 
 """"""
 import numpy
 
 from .core import _get_share_memory, FIX_LENGTH
 
 IMAGE_SHAPE = (1080, 1920, 3)
 """"""图像的默认形状""""""
 
 
 def dump_image_into_shared_memory(shared_memory_name: str, image: numpy.ndarray) -> memoryview:
     """"""将当前的图片dump成共享内存放入当前的共享内存映射中，此操作加锁
 
     Args:
         shared_memory_name (str): 共享内存名
         image (numpy.ndarray): numpy格式图片
 
     Returns:
         memoryview: 内存对象（共享内存的.buf属性）
     """"""
     shared_memory, lock = _get_share_memory(shared_memory_name)
 
     with lock:
         shared_memory.buf[:FIX_LENGTH] = image.tobytes()
     return shared_memory.buf
 
 
 def load_image_from_shared_memory(shared_memory_name: str) -> numpy.ndarray:
     """"""从当前的共享内存映射中读取相应的共享内存并转换为图像，此操作加锁
 
     Args:
         shared_memory_name (str): 共享内存名
 
     Returns:
         numpy.ndarray: numpy格式图片
     """"""
     shared_memory, lock = _get_share_memory(shared_memory_name)
 
     with lock:
         image = numpy.frombuffer(shared_memory.buf, dtype=numpy.uint8)[:FIX_LENGTH].reshape(IMAGE_SHAPE)
     return image"
KO;5.0;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" import os
 import os.path as osp
 
 from PIL import Image
 from pycocotools.coco import COCO
 from pycocotools.cocoeval import COCOeval

 from utils.utils import get_classes, get_coco_label_map
 from utils.utils_map import Make_json, prep_metrics
 
 if __name__ == '__main__':
     #------------------------------------------------------------------------------------------------------------------#
     #   map_mode用于指定该文件运行时计算的内容"
OK;5.0;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" import os
 import os.path as osp
 
 import tensorflow as tf
 from PIL import Image
 from pycocotools.coco import COCO
 from pycocotools.cocoeval import COCOeval

 from utils.utils import get_classes, get_coco_label_map
 from utils.utils_map import Make_json, prep_metrics
 
 gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
 for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)
 
 if __name__ == '__main__':
     #------------------------------------------------------------------------------------------------------------------#
     #   map_mode用于指定该文件运行时计算的内容"
KO;5.0;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" 
 import cv2
 import numpy as np
 from PIL import Image
 
 from mask_rcnn import MASK_RCNN
 
 if __name__ == ""__main__"":
     mask_rcnn = MASK_RCNN()
     #----------------------------------------------------------------------------------------------------------#"
OK;5.0;bubbliiiing;mask-rcnn-tf2;39da8479407198fe9189d027fb6982d482697d62;update set_memory_growth;" 
 import cv2
 import numpy as np
 import tensorflow as tf
 from PIL import Image
 
 from mask_rcnn import MASK_RCNN
 
 gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
 for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)
 
 if __name__ == ""__main__"":
     mask_rcnn = MASK_RCNN()
     #----------------------------------------------------------------------------------------------------------#"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"jobs:
           - 14.x
   publish_sdk:
     name: Publish SDKs
     runs-on: ${{ matrix.language == 'nodejs' && 'macos-latest' || 'ubuntu-latest' }}
     needs: publish_binary
     steps:
       - name: Checkout Repo"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;"jobs:
           - 14.x
   publish_sdk:
     name: Publish SDKs
     runs-on: 'ubuntu-latest'
     needs: publish_binary
     steps:
       - name: Checkout Repo"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" VERSION         := 0.1.11
 
 PACK            := azure-justrun
 PROJECT         := github.com/pulumi/pulumi-${PACK}
gen_nodejs_sdk::
 build_nodejs_sdk:: gen_nodejs_sdk
 	cd sdk/nodejs/ && \
 		yarn install && \
 		yarn run tsc --version && \
 		yarn run tsc && \
 		cp -R scripts/ bin && \
 		cp ../../README.md ../../LICENSE package.json yarn.lock ./bin/ && \
 		sed -i.bak -e ""s/\$${VERSION}/$(VERSION)/g"" ./bin/package.json && \
"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" VERSION         := 0.1.12
 
 PACK            := azure-justrun
 PROJECT         := github.com/pulumi/pulumi-${PACK}
gen_nodejs_sdk::
 build_nodejs_sdk:: gen_nodejs_sdk
 	cd sdk/nodejs/ && \
 		yarn install && \
 		NODE_OPTIONS=--max-old-space-size=8192 yarn run tsc --diagnostics \
 		cp -R scripts/ bin && \
 		cp ../../README.md ../../LICENSE package.json yarn.lock ./bin/ && \
 		sed -i.bak -e ""s/\$${VERSION}/$(VERSION)/g"" ./bin/package.json && \
"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" *.pyc
 venv/"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" *.pyc
 venv/
"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" """"""An Azure RM Python Pulumi program""""""
 
 import pulumi
 import pulumi_azure_justrun 
 
 
 webapp = pulumi_azure_justrun.Webapp(""mywebapp"", file_path=""./www"")
 
 pulumi.export(""url"",webapp.url)"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" """"""An Azure RM Python Pulumi program""""""
 
 import pulumi
 import pulumi_azure_justrun 
 
 
 webapp = pulumi_azure_justrun.Webapp(""mywebapp"", file_path=""./www"")
 
 pulumi.export(""url"",webapp.url)
"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" pulumi>=3.0.0,<4.0.0
 pulumi-azure-native>=1.0.0,<2.0.0"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" pulumi>=3.0.0,<4.0.0
 pulumi-azure-native>=1.0.0,<2.0.0
"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""name"": ""@pulumi/azure-justrun"",
   ""version"": ""0.1.11"",
   ""devDependencies"": {
     ""@types/node"": ""^17.0.40"",
     ""@vercel/ncc"": ""^0.28.6"",
"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""name"": ""@pulumi/azure-justrun"",
   ""version"": ""0.1.12"",
   ""devDependencies"": {
     ""@types/node"": ""^17.0.40"",
     ""@vercel/ncc"": ""^0.28.6"",
"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""azure-justrun"",
     ""version"": ""v0.1.11"",
     ""types"": {
         ""azure-justrun:index:PublicAccess"":{
             ""type"": ""string"",
"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""azure-justrun"",
     ""version"": ""v0.1.12"",
     ""types"": {
         ""azure-justrun:index:PublicAccess"":{
             ""type"": ""string"",
"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
   ""version"": ""0.1.11""
 }"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
   ""resource"": true,
   ""name"": ""azure-justrun"",
   ""version"": ""0.1.12""
 }"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""@pulumi/azure-justrun"",
     ""version"": ""0.1.11"",
     ""scripts"": {
         ""build"": ""tsc"",
         ""install"": ""node scripts/install-pulumi-plugin.js resource azure-justrun 0.1.11""
     },
     ""dependencies"": {
         ""@pulumi/azure-native"": ""^1.0.0"",

     },
     ""pulumi"": {
         ""resource"": true,
         ""version"": ""0.1.11""
     }
 }"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" {
     ""name"": ""@pulumi/azure-justrun"",
     ""version"": ""0.1.12"",
     ""scripts"": {
         ""build"": ""tsc"",
         ""install"": ""node scripts/install-pulumi-plugin.js resource azure-justrun 0.1.12""
     },
     ""dependencies"": {
         ""@pulumi/azure-native"": ""^1.0.0"",

     },
     ""pulumi"": {
         ""resource"": true,
         ""version"": ""0.1.12""
     }
 }"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" # Pulumi Component Provider Boilerplate (TypeScript)
 
 This repo is a boilerplate showing how to create a Pulumi component provider written in TypeScript. You can search-replace `xyz` with the name of your desired provider as a starting point for creating a component provider for your component resources.
 
 ## Background
 This repository is part of the [guide for authoring and publishing a Pulumi Package](https://www.pulumi.com/docs/guides/pulumi-packages/how-to-author).
 
 Learn about the concepts behind [Pulumi Packages](https://www.pulumi.com/docs/guides/pulumi-packages/#pulumi-packages) and, more specifically, [Pulumi Components](https://www.pulumi.com/docs/intro/concepts/resources/components/)
 
 ## Sample xyz Component Provider
 
 An example `StaticPage` [component resource](https://www.pulumi.com/docs/intro/concepts/resources/#components) is available in `provider/cmd/pulumi-resource-xyz/staticPage.ts`. This component creates a static web page hosted in an AWS S3 Bucket. There is nothing special about `StaticPage` -- it is a typical component resource written in TypeScript.
 
 The component provider makes component resources available to other languages. The implementation is in `provider/cmd/pulumi-resource-xyz/provider.ts`. Each component resource in the provider must have an implementation in the `construct` method to create an instance of the requested component resource and return its `URN` and state (outputs). There is an initial implementation that demonstrates an implementation of `construct` for the example `StaticPage` component.
 
 A code generator is available which generates SDKs in TypeScript, Python, Go and .NET which are also checked in to the `sdk` folder. The SDKs are generated from a schema in `schema.json`. This file should be kept aligned with the component resources supported by the component provider implementation.
 
 An example of using the `StaticPage` component in TypeScript is in `examples/simple`.
 
 Note that the provider plugin (`pulumi-resource-xyz`) must be on your `PATH` to be used by Pulumi deployments. In this case, `pulumi-resource-xyz` is a platform-specific binary that includes its Node.js dependency along with the provider code, created using [pkg](https://github.com/vercel/pkg). By default, running `make install` will create the binary specific to your host environment.
 
 After running `make install`, `pulumi-resource-xyz` will be available in the `./bin` directory. You can add this to your path in bash with `export PATH=$PATH:$PWD/bin`.
 
 If creating a provider for distribution to other users, they will need the `pulumi-resource-xyz` directory on their `PATH`. See the Packaging section below for more on distributing the provider to users.
 
 ## Prerequisites
 
 - Pulumi CLI
 - Node.js
 - Yarn
 - Go 1.17 (to regenerate the SDKs)
 - Python 3.6+ (to build the Python SDK)
 - .NET Core SDK (to build the .NET SDK)
 
 ## Build and Test
 
 ```bash
 # Build and install the provider
 make install_provider
 
 # Regenerate SDKs
 make generate
 
 # Ensure the pulumi-provider-xyz script is on PATH
 $ export PATH=$PATH:$PWD/bin
 
 # Test Node.js SDK
 $ make install_nodejs_sdk
 $ cd examples/simple
 $ yarn install
 $ yarn link @pulumi/xyz
 $ pulumi stack init test
 $ pulumi config set aws:region us-east-1
 $ pulumi up
 ```
 
 ## Naming
 
 The `xyz` provider's plugin must be named `pulumi-resource-xyz` (in the format `pulumi-resource-<provider>`).
 
 While the provider plugin must follow this naming convention, the SDK package naming can be customized. TODO explain.
 
 ## Packaging
 
 The provider plugin can be packaged into a tarball and hosted at a custom server URL to make it easier to distribute to users.
 
 Currently, five tarball files are necessary for Linux, macOS, and Windows (`pulumi-resource-xyz-v0.0.1-linux-amd64.tar.gz`, `pulumi-resource-xyz-v0.0.1-linux-arm64.tar.gz` `pulumi-resource-xyz-v0.0.1-darwin-amd64.tar.gz`, `pulumi-resource-xyz-v0.0.1-darwin-arm64.tar.gz`, `pulumi-resource-xyz-v0.0.1-windows-amd64.tar.gz`) each containing the same files: the platform-specific binary `pulumi-resource-xyz`, README and LICENSE. The fill set of binaries can be automatically generated using the command `make dist`.
 
 TODO explain custom server hosting in more detail.
 
 ## Configuring CI and releases
 
 1. Follow the instructions laid out in the [deployment templates](./deployment-templates/README-DEPLOYMENT.md).
 
 
 ## Example component
 
 Let's look at the example `StaticPage` component resource in more detail.
 
 ### Schema
 
 The example `StaticPage` component resource is defined in `schema.json`:
 
 ```json
 ""resources"": {
     ""xyz:index:StaticPage"": {
         ""isComponent"": true,
         ""inputProperties"": {
             ""indexContent"": {
                 ""type"": ""string"",
                 ""description"": ""The HTML content for index.html.""
             }
         },
         ""requiredInputs"": [
             ""indexContent""
         ],
         ""properties"": {
             ""bucket"": {
                 ""$ref"": ""/aws/v3.30.0/schema.json#/resources/aws:s3%2Fbucket:Bucket"",
                 ""description"": ""The bucket resource.""
             },
             ""websiteUrl"": {
                 ""type"": ""string"",
                 ""description"": ""The website URL.""
             }
         },
         ""required"": [
             ""bucket"",
             ""websiteUrl""
         ]
     }
 }
 ```
 
 The component resource's type token is `xyz:index:StaticPage` in the format of `<package>:<module>:<type>`. In this case, it's in the `xyz` package and `index` module. This is the same type token passed inside the implementation of `StaticPage` in `provider/cmd/pulumi-resource-xyz/staticPage.ts`, and also the same token referenced in `construct` in `provider/cmd/pulumi-resource-xyz/provider.ts`.
 
 This component has a required `indexContent` input property typed as `string`, and two required output properties: `bucket` and `websiteUrl`. Note that `bucket` is typed as the `aws:s3/bucket:Bucket` resource from the `aws` provider (in the schema the `/` is escaped as `%2F`).
 
 Since this component returns a type from the `aws` provider, each SDK must reference the associated Pulumi `aws` SDK for the language. For the .NET, Node.js, and Python SDKs, dependencies are specified in the `language` section of the schema:
 
 ```json
 ""language"": {
     ""csharp"": {
         ""packageReferences"": {
             ""Pulumi"": ""2.*"",
             ""Pulumi.Aws"": ""3.*""
         }
     },
     ""nodejs"": {
         ""dependencies"": {
             ""@pulumi/aws"": ""^3.30.0""
         },
         ""devDependencies"": {
             ""typescript"": ""^3.7.0""
         }
     },
     ""python"": {
         ""requires"": {
             ""pulumi"": "">=2.21.2,<3.0.0"",
             ""pulumi-aws"": "">=3.30.0,<4.0.0""
         }
     }
 }
 ```
 
 For the Go SDK, dependencies are specified in the `sdk/go.mod` file.
 
 ### Implementation
 
 The implementation of this component is in `provider/cmd/pulumi-resource-xyz/staticPage.ts` and the structure of the component's inputs and outputs aligns with what is defined in `schema.json`:
 
 ```typescript
 export interface StaticPageArgs {
     indexContent: pulumi.Input<string>;
 }
 
 export class StaticPage extends pulumi.ComponentResource {
     public readonly bucket: aws.s3.Bucket;
     public readonly websiteUrl: pulumi.Output<string>;
 
     constructor(name: string, args: StaticPageArgs, opts?: pulumi.ComponentResourceOptions) {
         super(""xyz:index:StaticPage"", name, args, opts);
 
         ...
     }
 }
 ```
 
 The provider makes this component resource available in the `construct` method in `provider/cmd/pulumi-resource-xyz/provider.ts`. When `construct` is called and the `type` argument is `xyz:index:StaticPage`, we create an instance of the `StaticPage` component resource and return its `URN` and outputs as its state.
 
 
 ```typescript
 async function constructStaticPage(name: string, inputs: pulumi.Inputs,
     options: pulumi.ComponentResourceOptions): Promise<provider.ConstructResult> {
 
     // Create the component resource.
     const staticPage = new StaticPage(name, inputs as StaticPageArgs, options);
 
     // Return the component resource's URN and outputs as its state.
     return {
         urn: staticPage.urn,
         state: {
             bucket: staticPage.bucket,
             websiteUrl: staticPage.websiteUrl,
         },
     };
 }
 ```
\ No newline at end of file"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" # Pulumi Azure JustRun
 
 Azure-JustRun allows you to deploy a static site to Azure in just a few lines of code.
 
 ## Contributing
 When contributing to this package, make sure to bump the version in the schema.json as well as the makefile when preparing a new release, and then regenerate the SDKs before pushing. 
 Add tags of the form v0.0.0 AND sdk/v0.0.0
\ No newline at end of file"
KO;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" from subprocess import check_call
 
 
 VERSION = ""0.1.11""
 PLUGIN_VERSION = ""0.1.11""
 
 class InstallPluginCommand(install):
     def run(self):"
OK;8.0;pulumi;pulumi-azure-justrun;94db53d88439ecba4cc938aafd8fe569b95f53c1;added more memory to node and moved back to ubuntu;" from subprocess import check_call
 
 
 VERSION = ""0.1.12""
 PLUGIN_VERSION = ""0.1.12""
 
 class InstallPluginCommand(install):
     def run(self):"
KO;13.0;fmathiou;Replay-SLDA;f371be6bf9d9ed294740ba47541be1eec3c93976;Delete memory.py;" from torch.utils.data import Dataset
 from torchvision import transforms
 import torch
 import random
 
 
 class Memory(Dataset):
     """"""Memory buffer used for rehearsal.
 
     Attributes:
         max_samples (int): Maximum allowed number of samples to be stored.
         reservoir (list): Contains stored instances in the form [instance, label].
         seen_samples (int): Number of instances ecnountered so far.
         transform (Tansform): Transformation operation when replaying data.
     """"""
     
     def __init__(self, max_samples=200):
         super(Memory, self).__init__()
         self.max_samples = max_samples
         self.reservoir =[]
         self.seen_samples = 0
         self.transform = transforms.Compose([transforms.Resize(256), 
                                              transforms.CenterCrop(224),
                                              transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                                   std=[0.229, 0.224, 0.225])])
 
     def __len__ (self):
         return len(self.reservoir)
         
     def __getitem__(self, index):
         sample = self.reservoir[index]
         return sample
         
     def reservoir_sampling(self, samples, labels):
         """"""Perform reservoir sampling.""""""
         nr_of_smaples = labels.shape[0]
         for i in range(nr_of_smaples):
             if(self.seen_samples < self.max_samples):
                 self.seen_samples += 1
                 self.reservoir.append([samples[i], labels[i]])
             else:
                 self.seen_samples += 1   
                 random_index = random.randrange(self.seen_samples)
                 if(random_index < self.max_samples):
                     self.reservoir[random_index] = [samples[i], labels[i]]
 
     def random_replay(self, batch_size):
         """"""Draw instances from the reservoir uniformly at random.
 
         Args:
             batch_size (int): Numbber of instances to sample.
 
         Returns:
             tuple: Samples and corresponding labels.
         """"""
         if(len(self.reservoir)) >= batch_size:
             random_indices = random.sample(range(len(self.reservoir)), batch_size)
             batch = list(map(self.__getitem__, random_indices))
             samples = list(map(lambda x: x[0], batch))
             samples = torch.stack(samples)
             samples = self.transform(samples)
             labels = list(map(lambda x: x[1], batch)) 
             labels = torch.stack(labels)
             
         else:
             samples = float('NaN')
             labels = float('NaN')
             
         return samples, labels
\ No newline at end of file"
OK;13.0;fmathiou;Replay-SLDA;f371be6bf9d9ed294740ba47541be1eec3c93976;Delete memory.py;\ No newline at end of file
KO;13.0;fmathiou;Replay-SLDA;1d1898771f816f796a6c071f6592f21aa308fb72;Delete memory.py;" from torch.utils.data import Dataset
 from torchvision import transforms
 import torch
 import random
 
 
 class Memory(Dataset):
     """"""Memory buffer used for rehearsal.
 
     Attributes:
         max_samples (int): Maximum allowed number of samples to be stored.
         reservoir (list): Contains stored instances in the form [instance, label].
         seen_samples (int): Number of instances ecnountered so far.
         transform (Tansform): Transformation operation when replaying data.
     """"""
     
     def __init__(self, max_samples=200):
         super(Memory, self).__init__()
         self.max_samples = max_samples
         self.reservoir =[]
         self.seen_samples = 0
         self.transform = transforms.Compose([transforms.Resize(256), 
                                              transforms.CenterCrop(224),
                                              transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                                   std=[0.229, 0.224, 0.225])])
 
     def __len__ (self):
         return len(self.reservoir)
         
     def __getitem__(self, index):
         sample = self.reservoir[index]
         return sample
         
     def reservoir_sampling(self, samples, labels):
         """"""Perform reservoir sampling.""""""
         nr_of_smaples = labels.shape[0]
         for i in range(nr_of_smaples):
             if(self.seen_samples < self.max_samples):
                 self.seen_samples += 1
                 self.reservoir.append([samples[i], labels[i]])
             else:
                 self.seen_samples += 1   
                 random_index = random.randrange(self.seen_samples)
                 if(random_index < self.max_samples):
                     self.reservoir[random_index] = [samples[i], labels[i]]
 
     def random_replay(self, batch_size):
         """"""Draw instances from the reservoir uniformly at random.
 
         Args:
             batch_size (int): Numbber of instances to sample.
 
         Returns:
             tuple: Samples and corresponding labels.
         """"""
         if(len(self.reservoir)) >= batch_size:
             random_indices = random.sample(range(len(self.reservoir)), batch_size)
             batch = list(map(self.__getitem__, random_indices))
             samples = list(map(lambda x: x[0], batch))
             samples = torch.stack(samples)
             samples = self.transform(samples)
             labels = list(map(lambda x: x[1], batch)) 
             labels = torch.stack(labels)
             
         else:
             samples = float('NaN')
             labels = float('NaN')
             
         return samples, labels
\ No newline at end of file"
OK;13.0;fmathiou;Replay-SLDA;1d1898771f816f796a6c071f6592f21aa308fb72;Delete memory.py;\ No newline at end of file
KO;26.0;appbox;shadowsocksr;9bb52acaf54f6e6ce32360474b286bddf7e4edbf;"Merge pull request #25 from mengskysama/patch-4

memory leak";"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
OK;26.0;appbox;shadowsocksr;9bb52acaf54f6e6ce32360474b286bddf7e4edbf;"Merge pull request #25 from mengskysama/patch-4

memory leak";"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
         self._dns_cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
KO;26.0;appbox;shadowsocksr;b059b9ad8562df90251419685edeada96ae6560c;memory leak;"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
OK;26.0;appbox;shadowsocksr;b059b9ad8562df90251419685edeada96ae6560c;memory leak;"def handle_periodic(self):
                 logging.info('closed UDP port %d', self._listen_port)
         before_sweep_size = len(self._sockets)
         self._cache.sweep()
         self._dns_cache.sweep()
         if before_sweep_size != len(self._sockets):
             logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))
         self._client_fd_to_server_addr.sweep()"
KO;27.0;WoLeo-Z;tgmsbot;7550e4fc4b33b1792b3c6951cb1b50d8a6f49457;free memory and fix BadRequest;"def dist_cards_btn_click(update, context):
     data = update.callback_query.data
     user = update.callback_query.from_user
     omsg = update.callback_query.message
     try:
         (_, rphash) = data.split(' ')
         red_packets = context.chat_data.setdefault('red_packets', dict())
         rp = red_packets.get(str(rphash), None)
         if rp:
             (cards, damount) = [int(a) for a in rp]
def __floating(value):
                 return randrange(5000,15000)/10000 * value
             got_cards = int(__floating(cards/damount))
             got_cards = got_cards if got_cards <= cards else cards
             got_cards = 1 if randrange(0,10000)/10000 < 0.2 and got_cards == 0 else got_cards
             got_cards = got_cards if damount != 1 else cards
             rp[0] -= got_cards
             rp[1] -= 1
def __floating(value):
             update.callback_query.answer()
         except Exception:
             pass
         omsg.edit_text(omsg.text_markdown + ""褪裙了"", parse_mode=""Markdown"", reply_markup=None)"
OK;27.0;WoLeo-Z;tgmsbot;7550e4fc4b33b1792b3c6951cb1b50d8a6f49457;free memory and fix BadRequest;"def dist_cards_btn_click(update, context):
     data = update.callback_query.data
     user = update.callback_query.from_user
     omsg = update.callback_query.message
     red_packets = context.chat_data.setdefault('red_packets', dict())
     try:
         (_, rphash) = data.split(' ')
         rp = red_packets.get(str(rphash), None)
         if rp:
             (cards, damount) = [int(a) for a in rp]
def __floating(value):
                 return randrange(5000,15000)/10000 * value
             got_cards = int(__floating(cards/damount))
             got_cards = got_cards if got_cards <= cards else cards
             got_cards = 1 if got_cards == 0 and randrange(0,10000)/10000 < 0.2 else got_cards
             got_cards = got_cards if damount != 1 else cards
             rp[0] -= got_cards
             rp[1] -= 1
def __floating(value):
             update.callback_query.answer()
         except Exception:
             pass
         def free_mem(job_context):
             try:
                 red_packets.pop(rphash)
             except KeyError:
                 pass
         if rphash:
             rp = red_packets.get(rphash, [0, 0])
             if rp[0] != -1:
                 rp[0] = -1
                 omsg.edit_text(omsg.text_markdown + ""褪裙了"", parse_mode=""Markdown"", reply_markup=None)
                 context.job_queue.run_once(free_mem, 5)"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def my_load_checkpoint(model, filename, map_location=None, strict=False, logger=
     elif isinstance(checkpoint, dict) and 'model' in checkpoint:
         state_dict = checkpoint['model']  # for classification weights
     else:
         raise RuntimeError(
             'No state_dict found in checkpoint file {}'.format(filename))
     # strip prefix of state_dict
     if list(state_dict.keys())[0].startswith('module.'):
         state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def my_load_checkpoint(model, filename, map_location=None, strict=False, logger=
     elif isinstance(checkpoint, dict) and 'model' in checkpoint:
         state_dict = checkpoint['model']  # for classification weights
     else:
         state_dict = checkpoint #  fix ""No state_dict found in checkpoint file""
         # raise RuntimeError(
         #     'No state_dict found in checkpoint file {}'.format(filename))
     # strip prefix of state_dict
     if list(state_dict.keys())[0].startswith('module.'):
         state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, dim=768):
     def forward(self, x, H, W):
         B, N, C = x.shape
         n = N // 21
         x1 = x[:, 0:16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2)
         x2 = x[:, 16 * n:20 * n, :].transpose(1, 2).view(B, C, H, W)
         x3 = x[:, 20 * n:, :].transpose(1, 2).view(B, C, H // 2, W // 2)
         x1 = self.dwconv(x1).flatten(2).transpose(1, 2)
         x2 = self.dwconv(x2).flatten(2).transpose(1, 2)
         x3 = self.dwconv(x3).flatten(2).transpose(1, 2)
def __init__(self, dim, num_heads=6, n_points=4, norm_layer=partial(nn.LayerNorm
         if extra_extractor:
             self.extra_extractors = nn.Sequential(*[
                 Extractor(dim=dim, num_heads=num_heads, n_points=n_points, norm_layer=norm_layer,
                           with_cffn=with_cffn, cffn_ratio=cffn_ratio, deform_ratio=deform_ratio)
                 for _ in range(2)
             ])
         else:
def __init__(self, inplanes=64, embed_dim=384):
         self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc2 = nn.Conv2d(2 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc3 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc4 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0,  bias=True)
 
     def forward(self, x):
         c1 = self.stem(x)"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, dim=768):
     def forward(self, x, H, W):
         B, N, C = x.shape
         n = N // 21
         x1 = x[:, 0:16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2).contiguous()
         x2 = x[:, 16 * n:20 * n, :].transpose(1, 2).view(B, C, H, W).contiguous()
         x3 = x[:, 20 * n:, :].transpose(1, 2).view(B, C, H // 2, W // 2).contiguous()
         x1 = self.dwconv(x1).flatten(2).transpose(1, 2)
         x2 = self.dwconv(x2).flatten(2).transpose(1, 2)
         x3 = self.dwconv(x3).flatten(2).transpose(1, 2)
def __init__(self, dim, num_heads=6, n_points=4, norm_layer=partial(nn.LayerNorm
         if extra_extractor:
             self.extra_extractors = nn.Sequential(*[
                 Extractor(dim=dim, num_heads=num_heads, n_points=n_points, norm_layer=norm_layer,
                           with_cffn=with_cffn, cffn_ratio=cffn_ratio, deform_ratio=deform_ratio,
                           drop=drop, drop_path=drop_path)
                 for _ in range(2)
             ])
         else:
def __init__(self, inplanes=64, embed_dim=384):
         self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc2 = nn.Conv2d(2 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc3 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
         self.fc4 = nn.Conv2d(4 * inplanes, embed_dim, kernel_size=1, stride=1, padding=0, bias=True)
 
     def forward(self, x):
         c1 = self.stem(x)"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" 
 import torch
 import torch.nn.functional as F
 from mmcv.runner import load_checkpoint
 from mmdet.utils import get_root_logger
 from timm.models.layers import DropPath
def forward(self, hidden_states, input_tensor):
 
 class BertLayer(nn.Module):
     def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=12,
                  drop_path_ratio=0.1, windowed=False, window_size=14):
 
         super(BertLayer, self).__init__()
         self.attention = BertAttention(hidden_size, num_attention_heads,
                                        drop_path_ratio, windowed, window_size)
 
def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=
                                  drop_path_ratio=drop_path_ratio)
 
     def forward(self, hidden_states, H, W):
         attention_output = self.attention(hidden_states, H, W)
         intermediate_output = self.intermediate(attention_output)
         layer_output = self.output(intermediate_output, attention_output)
         return layer_output
 
 
 class VisualPatchEmbedding(nn.Module):
def forward(self, x):
 class UnifiedBertEncoder(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,
                  num_heads=12, mlp_ratio=4., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
                  embed_layer=VisualPatchEmbedding, window_attn=False, window_size=14, pretrained=None):
 
         super(UnifiedBertEncoder, self).__init__()
         self.embed_dim = embed_dim
def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth
             layers.append(
                 BertLayer(hidden_size=embed_dim, intermediate_size=int(embed_dim * mlp_ratio),
                           num_attention_heads=num_heads, drop_path_ratio=drop_path_rate,
                           windowed=window_attn[i], window_size=window_size[i])
             )
 
         self.layers = nn.ModuleList(layers)"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" 
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint as cp
 from mmcv.runner import load_checkpoint
 from mmdet.utils import get_root_logger
 from timm.models.layers import DropPath
def forward(self, hidden_states, input_tensor):
 
 class BertLayer(nn.Module):
     def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=12,
                  drop_path_ratio=0.1, windowed=False, window_size=14, with_cp=False):
 
         super(BertLayer, self).__init__()
         self.with_cp = with_cp
         self.attention = BertAttention(hidden_size, num_attention_heads,
                                        drop_path_ratio, windowed, window_size)
 
def __init__(self, hidden_size=768, intermediate_size=3072, num_attention_heads=
                                  drop_path_ratio=drop_path_ratio)
 
     def forward(self, hidden_states, H, W):
         
         def _inner_forward(hidden_states):
             attention_output = self.attention(hidden_states, H, W)
             intermediate_output = self.intermediate(attention_output)
             layer_output = self.output(intermediate_output, attention_output)
             return layer_output
 
         if self.with_cp and hidden_states.requires_grad:
             x = cp.checkpoint(_inner_forward, hidden_states)
         else:
             x = _inner_forward(hidden_states)
 
         return x
 
 
 class VisualPatchEmbedding(nn.Module):
def forward(self, x):
 class UnifiedBertEncoder(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,
                  num_heads=12, mlp_ratio=4., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
                  embed_layer=VisualPatchEmbedding, window_attn=False, window_size=14,
                  with_cp=False, pretrained=None):
 
         super(UnifiedBertEncoder, self).__init__()
         self.embed_dim = embed_dim
def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth
             layers.append(
                 BertLayer(hidden_size=embed_dim, intermediate_size=int(embed_dim * mlp_ratio),
                           num_attention_heads=num_heads, drop_path_ratio=drop_path_rate,
                           windowed=window_attn[i], window_size=window_size[i], with_cp=with_cp)
             )
 
         self.layers = nn.ModuleList(layers)"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.runner import BaseModule
 from mmcv_custom import my_load_checkpoint as load_checkpoint
 from mmdet.utils import get_root_logger
def forward(self, x, H, W):
 
 
 class Block(nn.Module):
     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
                  attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                  windowed=False, window_size=14, pad_mode='constant', layer_scale=False):
         super().__init__()
         self.norm1 = norm_layer(dim)
         if windowed:
             self.attn = WindowedAttention(dim, num_heads=num_heads,
def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
             self.gamma2 = nn.Parameter(torch.ones((dim)), requires_grad=True)
 
     def forward(self, x, H, W):
         if self.layer_scale:
             x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), H, W))
             x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
         else:
             x = x + self.drop_path(self.attn(self.norm1(x), H, W))
             x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
class TIMMVisionTransformer(BaseModule):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768,
                  depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,
                  drop_path_rate=0., layer_scale=True, embed_layer=PatchEmbed, norm_layer=partial(nn.LayerNorm, eps=1e-6),
                  act_layer=nn.GELU, window_attn=False, window_size=14, pretrained=None):
         """"""
         Args:
             img_size (int, tuple): input image size
def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
             embed_layer (nn.Module): patch embedding layer
             norm_layer: (nn.Module): normalization layer
             pretrained: (str): pretrained path
         """"""
         super().__init__()
         self.num_classes = num_classes
def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
                   qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
                   drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer,
                   windowed=window_attn[i], window_size=window_size[i],
                   layer_scale=layer_scale) for i in range(depth)
         ])
 
         self.init_weights(pretrained)"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;" import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.utils.checkpoint as cp
 from mmcv.runner import BaseModule
 from mmcv_custom import my_load_checkpoint as load_checkpoint
 from mmdet.utils import get_root_logger
def forward(self, x, H, W):
 
 
 class Block(nn.Module):
     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., with_cp=False,
                  attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                  windowed=False, window_size=14, pad_mode='constant', layer_scale=False):
         super().__init__()
         self.with_cp = with_cp
         self.norm1 = norm_layer(dim)
         if windowed:
             self.attn = WindowedAttention(dim, num_heads=num_heads,
def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,
             self.gamma2 = nn.Parameter(torch.ones((dim)), requires_grad=True)
 
     def forward(self, x, H, W):
         
         def _inner_forward(x):
             if self.layer_scale:
                 x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), H, W))
                 x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
             else:
                 x = x + self.drop_path(self.attn(self.norm1(x), H, W))
                 x = x + self.drop_path(self.mlp(self.norm2(x)))
             return x
 
         if self.with_cp and x.requires_grad:
             x = cp.checkpoint(_inner_forward, x)
         else:
             x = _inner_forward(x)
         
         return x
 
 
class TIMMVisionTransformer(BaseModule):
     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768,
                  depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,
                  drop_path_rate=0., layer_scale=True, embed_layer=PatchEmbed, norm_layer=partial(nn.LayerNorm, eps=1e-6),
                  act_layer=nn.GELU, window_attn=False, window_size=14, with_cp=False, pretrained=None):
         """"""
         Args:
             img_size (int, tuple): input image size
def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
             embed_layer (nn.Module): patch embedding layer
             norm_layer: (nn.Module): normalization layer
             pretrained: (str): pretrained path
             with_cp: (bool): use checkpoint or not
         """"""
         super().__init__()
         self.num_classes = num_classes
def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, em
                   qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
                   drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer,
                   windowed=window_attn[i], window_size=window_size[i],
                   layer_scale=layer_scale, with_cp=with_cp) for i in range(depth)
         ])
 
         self.init_weights(pretrained)"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, conv_inplane=64, n_points=4, deform_num_he
         self.version = version
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, conv_inplane=64, n_points=4, deform_num_he
         self.version = version
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.layers)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.layers)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.flags = [i for i in range(-1, self.num_block, self.num_block // 4)][1:]
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
OK;1.0;czczup;ViT-Adapter;a48725eaa11b2fb8aaa851572e691b8db5dd52ea;support with_cp for detection to save memory;"def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,
         self.cls_token = None
         self.num_block = len(self.blocks)
         self.pretrain_size = (pretrain_size, pretrain_size)
         self.interaction_indexes = interaction_indexes
         self.add_vit_feature = add_vit_feature
         embed_dim = self.embed_dim"
KO;1.0;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;" CWD = os.path.dirname(__file__)
 sys.path.append(os.path.join(CWD, '..'))
 from scan.modules import *
 from utils.net import get_all_ip
 from utils.base import multi_thread, multi_process, process_bar, save_res
 
 
 class Base:
class CameraScanner(Base):
     def __init__(self, in_file: str, out_file: str) -> None:
         super().__init__(in_file, out_file)
         self.scanner_name = 'camera scanner'
         self.ip_list = []
         self.lock = Lock()
         self.total = 0
         self.found = 0
         self.done = 0
def __init__(self, in_file: str, out_file: str) -> None:
     def _get_ip(self):
         with open(self.in_file, 'r') as f:
             for line in f:
                 if line.strip():
                     if not line.startswith('#'):
                         if '-' in line or '/' in line:
                             self.ip_list.extend(get_all_ip(line.strip()))
                         else:
                             self.ip_list.append(line.strip())
         self.total = len(self.ip_list)
 
     def _step(self, *args, **kwargs):
         with self.lock:
             if kwargs['found']:
                 self.found += 1
             self.bar(self.total, self.done + 1, self.found, timer=True, start_time=self.start_time)
 
     def scan(self, ip):
         for mod in self.modules:
             found = False
             try:
                 res = mod(ip)
                 if res[0]:
                     found = True
                     save_res(self.out_file, [ip] + res[1:])
             except Exception as e: pass  # print(e)
             finally: self._step(found=found)
         with self.lock: self.done += 1
 
 
     def __call__(self, args):
def __call__(self, args):
             if args.cve_2021_36260: self.modules.append(cve_2021_36260)
             if args.cve_2020_25078: self.modules.append(cve_2020_25078)
             if args.cve_2021_33044: self.modules.append(cve_2021_33044)
         multi_thread(self.scan, self.ip_list, processes=args.th_num)"
OK;1.0;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;" CWD = os.path.dirname(__file__)
 sys.path.append(os.path.join(CWD, '..'))
 from scan.modules import *
 from utils.net import get_all_ip, get_ip_seg_len
 from utils.base import multi_thread, process_bar, save_res
 
 
 class Base:
class CameraScanner(Base):
     def __init__(self, in_file: str, out_file: str) -> None:
         super().__init__(in_file, out_file)
         self.scanner_name = 'camera scanner'
         self.lock = Lock()
         self.ip_list = []
         self.total = 0
         self.found = 0
         self.done = 0
def __init__(self, in_file: str, out_file: str) -> None:
     def _get_ip(self):
         with open(self.in_file, 'r') as f:
             for line in f:
                 if line.strip() and not line.startswith('#'):
                     self.total += get_ip_seg_len(line.strip()) if '-' in line or '/' in line else 1
                     self.ip_list.append(line.strip())
 
     def _step(self, *args, **kwargs):
         with self.lock:
             if kwargs['found']:
                 self.found += 1
             self.bar(self.total, self.done + 1, self.found, timer=True, start_time=self.start_time)
 
     def scan(self, ip_term):
         for ip in get_all_ip(ip_term):
             for mod in self.modules:
                 found = False
                 try:
                     res = mod(ip)
                     if res[0]:
                         found = True
                         save_res(self.out_file, [ip] + res[1:])
                 except Exception as e: pass  # print(e)
                 finally: self._step(found=found)
             with self.lock: self.done += 1
 
 
     def __call__(self, args):
def __call__(self, args):
             if args.cve_2021_36260: self.modules.append(cve_2021_36260)
             if args.cve_2020_25078: self.modules.append(cve_2020_25078)
             if args.cve_2021_33044: self.modules.append(cve_2021_33044)
         
         multi_thread(self.scan, self.ip_list, processes=args.th_num)"
KO;1.0;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def wrapper(total, done, found=0, timer=False, start_time=0):
         _found = 'Found ' + output_formatter(found, color='red', bold=True) if found else ''
         count = f""{_done}/{_total} ({_percent}) {_found}""
 
         print(f""\r{icon} {count}  {_time}"", end='')
     return wrapper
 
 "
OK;1.0;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def wrapper(total, done, found=0, timer=False, start_time=0):
         _found = 'Found ' + output_formatter(found, color='red', bold=True) if found else ''
         count = f""{_done}/{_total} ({_percent}) {_found}""
 
         print(f""\r{icon} {count}  {_time:<55}"", end='')
     return wrapper
 
 "
KO;1.0;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def get_ip_segment(start: str, end: str) -> str:
     return IPy.IP(f""{start}-{end}"", make_net=True).strNormal()
 
 
 def get_all_ip(ip_seg: str) -> list:
     return [i.strNormal() for i in IPy.IP(f""{ip_seg}"", make_net=True)]
 
 
 def get_user_agent(name='random'):"
OK;1.0;jorhelp;Ingram;8933fd352ecf3e452580c047a529693074ef242c;Fixed memory explosion bug;"def get_ip_segment(start: str, end: str) -> str:
     return IPy.IP(f""{start}-{end}"", make_net=True).strNormal()
 
 
 def get_ip_seg_len(ip_seg: str) -> int:
     return IPy.IP(ip_seg, make_net=True).len()
 
 
 def get_all_ip(ip_seg: str) -> list:
     return [i.strNormal() for i in IPy.IP(ip_seg, make_net=True)]
 
 
 def get_user_agent(name='random'):"
KO;2.0;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
 the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of current user and software\microsoft\windows\currentVersion\run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
OK;2.0;enghossamshady;RansomWare;988c295c80715a735bd555660c9b8cd0258ea4e9;memory;"file that is imposible to return data cause the space was busy then after that m
 and if you want it more advanced you can encrypt the key by using RSA encrytion. here are many things advanced like making file encrypt
 itself after finishing its task to prevent anyone from analysing it 
 
 the most advanced method of preventing the ransome from encrypting data many times I copied the path of it to appdata with windows.exe and moved it to the memory of HKCU\Software\Microsoft\Windows\CurrentVersion\Run to make it encrypt all the new files and data every time the device restart and make it impossible to be killed 
 
 
 "
KO;2.0;CarbonCollective;fusion-dUQtools;9e8b10b081cfc7275998055e377496bf1da9a172;"Fix memory issue with IDS mapping (#90)

Fix memory issue with mapping";"def recursive_defaultdict():
 
 
 class IDSMapping:
     # All fields in the core profile in a single dict
     flat_fields: dict = {}
     # All fields, in the core profile in a nested dict
     fields: dict = defaultdict(recursive_defaultdict)
 
     def __init__(self, ids):
         self.dive(ids, [])
         self.fields = self.ddict_to_dict(self.fields)
 "
OK;2.0;CarbonCollective;fusion-dUQtools;9e8b10b081cfc7275998055e377496bf1da9a172;"Fix memory issue with IDS mapping (#90)

Fix memory issue with mapping";"def recursive_defaultdict():
 
 
 class IDSMapping:
 
     def __init__(self, ids):
         # All fields in the core profile in a single dict
         self.flat_fields: dict = {}
         # All fields, in the core profile in a nested dict
         self.fields: dict = defaultdict(recursive_defaultdict)
 
         self.dive(ids, [])
         self.fields = self.ddict_to_dict(self.fields)
 "
KO;2.0;dansanderson;mega65-welcome-guide;d39da1b335c04b9bf5afe8d71653a2d7f4fad035;Mention screen memory arrays in Recent Features;"Some of the new features that have been added since the factory-installed ROM wa
 - Single-letter BASIC variables are ""fast"" variables stored in fixed memory addresses `$FD00-$FEFF`.
 - The `PLAY` and `SOUND` commands have improved background playback and use of SID voices, so BASIC games can sensibly have both background music and sound effects.
 - Some disk commands can access files on the SD card directly (and not via a mounted D81 disk image) using the virtual device `U12`. `DIR U12` lists the files on the SD card. `DLOAD ""FILE.PRG"",U12` loads a `PRG` file.
 
 ## New BASIC commands
 "
OK;2.0;dansanderson;mega65-welcome-guide;d39da1b335c04b9bf5afe8d71653a2d7f4fad035;Mention screen memory arrays in Recent Features;"Some of the new features that have been added since the factory-installed ROM wa
 - Single-letter BASIC variables are ""fast"" variables stored in fixed memory addresses `$FD00-$FEFF`.
 - The `PLAY` and `SOUND` commands have improved background playback and use of SID voices, so BASIC games can sensibly have both background music and sound effects.
 - Some disk commands can access files on the SD card directly (and not via a mounted D81 disk image) using the virtual device `U12`. `DIR U12` lists the files on the SD card. `DLOAD ""FILE.PRG"",U12` loads a `PRG` file.
 - BASIC programs can access screen and color memory via special byte arrays `T@&(COLUMN, ROW)` and `C@&(COLUMN, ROW)`. Screen coordinates are intuitive in both 40-column and 80-column modes.
 
 ## New BASIC commands
 "
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
 *.parquet
\ No newline at end of file"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"examples/covid.py
 examples/covid.toml
 
 # Ignore exported simulations
 *.ipc
 *.parquet
\ No newline at end of file"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
 A smol simulator framework built on top of PyGame.
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" # Violet
 
 A smol simulator framework built on top of [PyGame](https://www.pygame.org/docs/).
 
 - Automatic agent wandering behaviour
 - Automatic obstacle avoidance"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
 from vi import Agent, BaseConfig, Simulation, Snapshot, dataclass
 
 
 @dataclass
 class MySnapshot(Snapshot):  # 👈 inherit Snapshot to collect base metrics.
     # We want to keep track of how many other agents were in our agent's radius,
     # so we add an extra `in_radius` metric to our Snapshot!
     in_radius: int
 
 
 class MyAgent(Agent):
     def update(self):
         # If at least one agent is within our agent's radius, then we turn red!
         if len(self.in_radius()) > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
     def snapshot(self) -> MySnapshot:
         return MySnapshot(
             # Automatically fill-in all the Snapshot attributes such as agent, frame, x and y.
             **super().snapshot().as_dict(),
             # Then add our own metric: in_radius!
             in_radius=len(self.in_radius()),
         )
 
 
 print(
     # We're using a seed to collect the same data every time.
def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
     # convert the output of the simulation into a Polars DataFrame
     .to_polars()
     .groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" import polars as pl
 
 from vi import Agent, BaseConfig, Simulation
 
 
 class MyAgent(Agent):
     def every_frame(self):
         # As radius calculation is quite performance heavy,
         # we only calculate it once per frame.
         in_radius = len(self.in_radius())
 
         # We want to keep track of how many other agents were in our agent's radius,
         # so we add data to the `in_radius` column of our dataframe!
         self.save_data(""in_radius"", in_radius)
 
         # If at least one agent is within our agent's radius, then we turn red!
         if in_radius > 0:
             self.change_image(index=1)
         else:
             # Otherwise we turn white.
             self.change_image(index=0)
 
 
 print(
     # We're using a seed to collect the same data every time.
def snapshot(self) -> MySnapshot:
         ],
     )
     .run()
     .snapshots.groupby(""frame"")
     # Count the number of agents (per frame) that see at least one other agent (making them red)
     .agg((pl.col(""in_radius"") > 0).sum().alias(""# red agents""))
     .select(""# red agents"")"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
 optional = true
 python-versions = "">=3.8""
 
 [[package]]
 name = ""pandas""
 version = ""1.4.2""
 description = ""Powerful data structures for data analysis, time series, and statistics""
 category = ""main""
 optional = true
 python-versions = "">=3.8""
 
 [package.dependencies]
 numpy = [
     {version = "">=1.18.5"", markers = ""platform_machine != \""aarch64\"" and platform_machine != \""arm64\"" and python_version < \""3.10\""""},
     {version = "">=1.19.2"", markers = ""platform_machine == \""aarch64\"" and python_version < \""3.10\""""},
     {version = "">=1.20.0"", markers = ""platform_machine == \""arm64\"" and python_version < \""3.10\""""},
     {version = "">=1.21.0"", markers = ""python_version >= \""3.10\""""},
 ]
 python-dateutil = "">=2.8.1""
 pytz = "">=2020.1""
 
 [package.extras]
 test = [""hypothesis (>=5.5.3)"", ""pytest (>=6.0)"", ""pytest-xdist (>=1.31)""]
 
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
 optional = true
 python-versions = "">=3.7""
 
 [package.dependencies]
toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
 [[package]]
 name = ""python-dateutil""
 version = ""2.8.2""
 description = ""Extensions to the standard Python datetime module""
 category = ""main""
 optional = true
 python-versions = ""!=3.0.*,!=3.1.*,!=3.2.*,>=2.7""
 
 [package.dependencies]
 six = "">=1.5""
 
 [[package]]
 name = ""pytz""
 version = ""2022.1""
 description = ""World timezone definitions, modern and historical""
 category = ""main""
 optional = true
 python-versions = ""*""
 
 [[package]]
 name = ""six""
 version = ""1.16.0""
 description = ""Python 2 and 3 compatibility utilities""
 category = ""main""
 optional = true
 python-versions = "">=2.7, !=3.0.*, !=3.1.*, !=3.2.*""
 
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
 [extras]
 full = [""pandas"", ""polars""]
 pandas = [""pandas""]
 polars = [""polars""]
 
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
 content-hash = ""8fda347438c855d8fcb0e5b6319823d331690eba7e5f28f2a7cfaa4d69b2020d""
 
 [metadata.files]
 black = [
numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
 pandas = [
     {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_universal2.whl"", hash = ""sha256:be67c782c4f1b1f24c2f16a157e12c2693fd510f8df18e3287c77f33d124ed07""},
     {file = ""pandas-1.4.2-cp310-cp310-macosx_10_9_x86_64.whl"", hash = ""sha256:5a206afa84ed20e07603f50d22b5f0db3fb556486d8c2462d8bc364831a4b417""},
     {file = ""pandas-1.4.2-cp310-cp310-macosx_11_0_arm64.whl"", hash = ""sha256:0010771bd9223f7afe5f051eb47c4a49534345dfa144f2f5470b27189a4dd3b5""},
     {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:3228198333dd13c90b6434ddf61aa6d57deaca98cf7b654f4ad68a2db84f8cfe""},
     {file = ""pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:5b79af3a69e5175c6fa7b4e046b21a646c8b74e92c6581a9d825687d92071b51""},
     {file = ""pandas-1.4.2-cp310-cp310-win_amd64.whl"", hash = ""sha256:5586cc95692564b441f4747c47c8a9746792e87b40a4680a2feb7794defb1ce3""},
     {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_universal2.whl"", hash = ""sha256:061609334a8182ab500a90fe66d46f6f387de62d3a9cb9aa7e62e3146c712167""},
     {file = ""pandas-1.4.2-cp38-cp38-macosx_10_9_x86_64.whl"", hash = ""sha256:b8134651258bce418cb79c71adeff0a44090c98d955f6953168ba16cc285d9f7""},
     {file = ""pandas-1.4.2-cp38-cp38-macosx_11_0_arm64.whl"", hash = ""sha256:df82739e00bb6daf4bba4479a40f38c718b598a84654cbd8bb498fd6b0aa8c16""},
     {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:385c52e85aaa8ea6a4c600a9b2821181a51f8be0aee3af6f2dcb41dafc4fc1d0""},
     {file = ""pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:295872bf1a09758aba199992c3ecde455f01caf32266d50abc1a073e828a7b9d""},
     {file = ""pandas-1.4.2-cp38-cp38-win32.whl"", hash = ""sha256:95c1e422ced0199cf4a34385ff124b69412c4bc912011ce895582bee620dfcaa""},
     {file = ""pandas-1.4.2-cp38-cp38-win_amd64.whl"", hash = ""sha256:5c54ea4ef3823108cd4ec7fb27ccba4c3a775e0f83e39c5e17f5094cb17748bc""},
     {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_universal2.whl"", hash = ""sha256:c072c7f06b9242c855ed8021ff970c0e8f8b10b35e2640c657d2a541c5950f59""},
     {file = ""pandas-1.4.2-cp39-cp39-macosx_10_9_x86_64.whl"", hash = ""sha256:f549097993744ff8c41b5e8f2f0d3cbfaabe89b4ae32c8c08ead6cc535b80139""},
     {file = ""pandas-1.4.2-cp39-cp39-macosx_11_0_arm64.whl"", hash = ""sha256:ff08a14ef21d94cdf18eef7c569d66f2e24e0bc89350bcd7d243dd804e3b5eb2""},
     {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl"", hash = ""sha256:8c5bf555b6b0075294b73965adaafb39cf71c312e38c5935c93d78f41c19828a""},
     {file = ""pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:51649ef604a945f781105a6d2ecf88db7da0f4868ac5d45c51cb66081c4d9c73""},
     {file = ""pandas-1.4.2-cp39-cp39-win32.whl"", hash = ""sha256:d0d4f13e4be7ce89d7057a786023c461dd9370040bdb5efa0a7fe76b556867a0""},
     {file = ""pandas-1.4.2-cp39-cp39-win_amd64.whl"", hash = ""sha256:09d8be7dd9e1c4c98224c4dfe8abd60d145d934e9fc1f5f411266308ae683e6a""},
     {file = ""pandas-1.4.2.tar.gz"", hash = ""sha256:92bc1fc585f1463ca827b45535957815b7deb218c549b7c18402c322c7549a12""},
 ]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
 python-dateutil = [
     {file = ""python-dateutil-2.8.2.tar.gz"", hash = ""sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86""},
     {file = ""python_dateutil-2.8.2-py2.py3-none-any.whl"", hash = ""sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9""},
 ]
 pytz = [
     {file = ""pytz-2022.1-py2.py3-none-any.whl"", hash = ""sha256:e68985985296d9a66a881eb3193b0906246245294a881e7c8afe623866ac6a5c""},
     {file = ""pytz-2022.1.tar.gz"", hash = ""sha256:1e760e2fe6a8163bc0b3d9a19c4f84342afa0a2affebfaa84b01b978a02ecaa7""},
 ]
 six = [
     {file = ""six-1.16.0-py2.py3-none-any.whl"", hash = ""sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254""},
     {file = ""six-1.16.0.tar.gz"", hash = ""sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926""},
 ]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"name = ""numpy""
 version = ""1.22.4""
 description = ""NumPy is the fundamental package for array computing with Python.""
 category = ""main""
 optional = false
 python-versions = "">=3.8""
 
 [[package]]
 name = ""pathspec""
 version = ""0.9.0""
name = ""polars""
 version = ""0.13.38""
 description = ""Blazingly fast DataFrame library""
 category = ""main""
 optional = false
 python-versions = "">=3.7""
 
 [package.dependencies]
toml = [""toml""]
 yaml = [""pyyaml""]
 numpy = [""numpy (>=1.21.0,<1.22.0)"", ""numpy (>1.21.0)"", ""numpy (>1.21.0)"", ""numpy (>1.22.0)""]
 
 [[package]]
 name = ""stringcase""
 version = ""1.2.0""
python-versions = ""*""
 mypy-extensions = "">=0.3.0""
 typing-extensions = "">=3.7.4""
 
 [metadata]
 lock-version = ""1.1""
 python-versions = ""^3.9""
 content-hash = ""5509c339bc3154af4b5c349b84ad6a79801ec11bd63a83881a76c9c664791262""
 
 [metadata.files]
 black = [
numpy = [
     {file = ""numpy-1.22.4-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"", hash = ""sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207""},
     {file = ""numpy-1.22.4.zip"", hash = ""sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af""},
 ]
 pathspec = [
     {file = ""pathspec-0.9.0-py2.py3-none-any.whl"", hash = ""sha256:7d15c4ddb0b5c802d161efc417ec1a2558ea2653c2e8ad9c19098201dc1c993a""},
     {file = ""pathspec-0.9.0.tar.gz"", hash = ""sha256:e564499435a2673d586f6b2130bb5b95f04a3ba06f81b8f895b651a3c76aabb1""},
pyserde = [
     {file = ""pyserde-0.7.3-py3-none-any.whl"", hash = ""sha256:6206a5692cb85150ca1cd690441afa53c40d96a4e5425f3a6e49ffdf2ad707d5""},
     {file = ""pyserde-0.7.3.tar.gz"", hash = ""sha256:f4ec94e6b5260ef1c7c955c587963e176952f02248fe932de62a95bbb718fecf""},
 ]
 stringcase = [
     {file = ""stringcase-1.2.0.tar.gz"", hash = ""sha256:48a06980661908efe8d9d34eab2b6c13aefa2163b3ced26972902e3bdfd87008""},
 ]"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
 
 # Optional DataFrame libraries
 pandas = { version = ""^1.4.2"", optional = true }
 polars = { version = ""^0.13.38"", optional = true }
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
 [tool.poetry.extras]
 pandas = [""pandas""]
 polars = [""polars""]
 full = [""pandas"", ""polars""] # all features
 
 [tool.isort]
 profile = ""black""
 "
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;"repository = ""https://github.com/m-rots/violet""
 python = ""^3.9""
 pygame = ""^2.1.2""
 pyserde = { extras = [""toml""], version = ""^0.7.3"" }
 polars = ""^0.13.38""
 
 [tool.poetry.dev-dependencies]
 black = ""^22.3.0""
 isort = ""^5.10.1""
 
 [tool.isort]
 profile = ""black""
 "
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
 from .metrics import Snapshot
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" 
 from .agent import Agent
 from .config import BaseConfig, Window
 from .replay import TimeMachine
 from .simulation import Simulation
 from .util import probability"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
 from typing import TYPE_CHECKING, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask

 from pygame.surface import Surface
 
 from .config import BaseConfig
 from .metrics import Snapshot
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
     # Sites
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
     # Proximity
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
     # Config (shared with other agents too)
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
 
         self.__proximity = proximity
 
def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
     def update(self):
         """"""Run your own agent logic at every tick of the simulation.
         Every frame of the simulation, this update method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
def there_is_no_escape(self) -> bool:
 
         return changed
 
     def update_position(self):
         """"""Update the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
def change_image(self, index: int):
 
         self._image_index = index
 
     def snapshot(self) -> Snapshot:
         """"""Create a Snapshot of agent data that you're interested in.
 
         By default the Agent will produce a Snapshot with the following data:
         - agent identifier
         - current frame
         - x and y coordinates
 
         However, you can also add your own data by inheriting the Snapshot dataclass.
         Add any fields that you like and then overwrite this method to produce your custom Snapshot.
 
         Make sure to call `super().snapshot()` to collect the default Snapshot data.
         """"""
 
         return Snapshot(
             x=self.pos.x,
             y=self.pos.y,
             id=self.id,
             frame=self.shared.counter,
             image_index=self._image_index,
         )"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Optional, TypeVar
 
 import pygame as pg
 from pygame.mask import Mask

 from pygame.surface import Surface
 
 from .config import BaseConfig
 from .metrics import Metrics
 from .util import random_angle, random_pos, round_pos
 
 if TYPE_CHECKING:
class Agent(Sprite):
     obstacles: Group
     """"""The group of obstacles the agent can collide with.""""""
 
     sites: Group
     """"""The group of sites on which the agent can appear.""""""
 
     __proximity: ProximityEngine
     """"""The Proximity Engine used for all proximity-related methods.
     
     The proximity engine is private (double underscore prefix) as one could retrieve all agents with it.
     Therefore, the Agent class provides the (public) `in_proximity`, `in_close_proximity` and `in_radius` wrapper methods instead.
     """"""
 
     config: BaseConfig
     """"""The config of the simulation that's shared with all agents.
     
class Agent(Sprite):
     shared: Shared
     """"""Attributes that are shared between the simulation and all agents.""""""
 
     __metrics: Metrics
     """"""Data collection of the snapshots.""""""
 
     def __init__(
         self,
         id: int,  # unique identifier used in e.g. proximity calculation and stats engine
def __init__(
         proximity: ProximityEngine,
         config: BaseConfig,
         shared: Shared,
         metrics: Metrics,
     ):
         Sprite.__init__(self, *containers)
 
         self.id = id
         self.config = config
         self.shared = shared
         self.__metrics = metrics
 
         self.__proximity = proximity
 
def mask(self) -> Mask:
 
         return pg.mask.from_surface(self.image)
 
     def every_frame(self):
         """"""Run your own agent logic at every tick of the simulation.
         Every frame of the simulation, this method is called automatically for every agent of the simulation.
 
         To add your own logic, inherit the `Agent` class and override this method with your own.
         """"""
 
         ...
 
     def update(self):
         self._collect_replay_data()
         self.every_frame()
 
     def on_spawn(self):
         """"""Run any code when the agent is spawned into the simulation.
 
def there_is_no_escape(self) -> bool:
 
         return changed
 
     def change_position(self):
         """"""Change the position of the agent.
 
         The agent's new position is calculated as follows:
         1. The agent checks whether it's outside of the visible screen area.
def change_image(self, index: int):
 
         self._image_index = index
 
     def save_data(self, column: str, value: Any):
         """"""Add extra data to the simulation's metrics.
 
         The following data is collected automatically:
         - agent identifier
         - current frame
         - x and y coordinates
 
         Examples
         --------
 
         Saving the number of agents in radius:
 
         >>> from vi import Agent
         >>> class MyAgent(Agent):
         ...     def every_frame(self):
         ...         in_radius = len(self.in_radius())
         ...         self.save_data(""in_radius"", in_radius)
         """"""
 
         self.__metrics._temporary_snapshots[column].append(value)
 
     def _collect_replay_data(self):
         """"""Add the minimum data needed for the replay simulation to the dataframe.""""""
 
         x, y = round_pos(self.pos)
         snapshots = self.__metrics._temporary_snapshots
 
         snapshots[""frame""].append(self.shared.counter)
         snapshots[""id""].append(self.id)
 
         snapshots[""x""].append(x)
         snapshots[""y""].append(y)
 
         snapshots[""image_index""].append(self._image_index)
 
         if self.config.image_rotation:
             angle = self.move.angle_to(Vector2((0, -1)))
             snapshots[""angle""].append(round(angle))"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
 import dataclasses
 from dataclasses import dataclass, field
 from typing import TYPE_CHECKING, Any
 
 if TYPE_CHECKING:
     from pandas import DataFrame as PandasDataFrame
     from polars import DataFrame as PolarsDataFrame
     from polars import Series as PolarsSeries
 
 
 @dataclass
 class Snapshot:
     """"""Data that's collected for every agent in every frame of the simulation.""""""
 
     frame: int
     """"""The current frame of the simulation.""""""
 
     id: int
     """"""The identifier of the agent.""""""
 
     x: float
     """"""The x coordinate of the agent.""""""
 
     y: float
     """"""The y coordinate of the agent.""""""
 
     image_index: int
     """"""The current index of the image list.""""""
 
     def as_dict(self) -> dict[str, Any]:
         """"""Convert this Snapshot into a dictionary.""""""
 
         return dataclasses.asdict(self)
 
 
 @dataclass
class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
     def to_polars(self) -> PolarsSeries:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
 @dataclass
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
     fps: Fps = field(default_factory=Fps)
     """"""The frames-per-second history to analyse performance.""""""
 
     snapshots: list[dict[str, Any]] = field(default_factory=list)
     """"""The most important data (snapshot) of every agent at every moment in time.""""""
 
     def to_pandas(self) -> PandasDataFrame:
         import pandas as pd
 
         return pd.DataFrame(self.snapshots)
 
     def to_polars(self) -> PolarsDataFrame:
         import polars as pl
 
         return pl.from_dicts(self.snapshots)"
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from __future__ import annotations
 
 from collections import defaultdict
 from dataclasses import dataclass, field
 from typing import Any
 
 import polars as pl
 
 
 @dataclass
class Fps:
     def _push(self, fps: float):
         self.__fps.append(fps)
 
     def to_polars(self) -> pl.Series:
         import polars as pl
 
         return pl.Series(""fps"", self.__fps)
 
 
 class Metrics:
     """"""A container hosting all the accumulated simulation data over time.""""""
 
     fps: Fps
     """"""The frames-per-second history to analyse performance.""""""
 
     _temporary_snapshots: defaultdict[str, list[Any]]
     snapshots: pl.DataFrame
 
     def __init__(self):
         self.fps = Fps()
         self._temporary_snapshots = defaultdict(list)
         self.snapshots = pl.DataFrame()
 
     def merge(self):
         df = pl.from_dict(self._temporary_snapshots)
 
         self.snapshots.vstack(df, in_place=True)
 
         self._temporary_snapshots = defaultdict(list)"
KO;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
 from .util import load_image, load_images, round_pos
 
 if TYPE_CHECKING:
     from .agent import Agent
def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
             )
 
         return self
def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
         )
 
         return self
def tick(self):
         # Update all agents
         self._all.update()
 
         # Snapshot marked agent data
         self.__save_snapshots()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
             agent.update_position()
 
     def __save_snapshots(self):
         """"""Save a Snapshot of each agent and add it to Metrics.""""""
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
             snapshot = agent.snapshot()
 
             self.__metrics.snapshots.append(snapshot.as_dict())
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
OK;3.0;m-rots;violet;8b370f80a44302e53caa48ed9d9322f8cbb2cfb4;feat: renamed methods and optimised memory usage;" from .metrics import Metrics
 from .obstacle import Obstacle
 from .proximity import ProximityEngine
 from .util import load_image, load_images
 
 if TYPE_CHECKING:
     from .agent import Agent
def batch_spawn_agents(
                 proximity=self._proximity,
                 config=self.config,
                 shared=self.shared,
                 metrics=self.__metrics,
             )
 
         return self
def spawn_agent(
             proximity=self._proximity,
             config=self.config,
             shared=self.shared,
             metrics=self.__metrics,
         )
 
         return self
def tick(self):
         # Update all agents
         self._all.update()
 
         # Merge the collected snapshots into the dataframe.
         self.__metrics.merge()
 
         # Draw everything to the screen
         self._all.draw(self._screen)
def __update_positions(self):
 
         for sprite in self._agents.sprites():
             agent: Agent = sprite  # type: ignore
             agent.change_position()
 
     def __visualise_chunks(self):
         """"""Visualise the proximity chunks by drawing their borders."""""""
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 3,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""B""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 2,
             ""execution_time"": 2,
             ""deadline"": 3,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
         ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
             ""name"": ""C"",
             ""arrival_time"": 4,
             ""execution_time"": 1,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 1
         },
         ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 9,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 6,
             ""execution_time"": 3,
             ""deadline"": 8,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 12,
         ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": [
                     0,
                     1,

                     7,
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""6"": {

             ""already_exec"": 3
         },
         ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
         ""time"": 13,
         ""started_time"": 12,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,

                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 1
             }
         }
     }
 }
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"                     7,
                     8,
                     9
                 ]
             }
         },
         ""memory_counter"": {
             ""A"": 1
         }
     },
     ""2"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 2,
             ""execution_time"": 2,
             ""deadline"": 3,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": [
                     0,
                     1,
                     2,

                     7,
                     8,
                     9
                 ]
             },
             ""B"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ]
             }
         },
         ""memory_counter"": {
             ""A"": 0,
             ""B"": 0
         }
     },
     ""3"": {
         ""process"": {
             ""name"": ""C"",
             ""arrival_time"": 4,
             ""execution_time"": 1,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 1
         },
         ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
         ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": [
                     0,
                     1,
                     2,

                     7,
                     8,
                     9
                 ]
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ]
             }
         },
         ""memory_counter"": {
             ""A"": 0,
             ""B"": 0,
             ""C"": 0
         }
     },
     ""4"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 6,
             ""execution_time"": 3,
             ""deadline"": 8,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 10,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": [
                     0,
                     1,
                     2,

                     7,
                     8,
                     9
                 ]
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ]
             }
         },
         ""memory_counter"": {
             ""A"": 0,
             ""B"": 0,
             ""C"": 0,
             ""D"": 1
         }
     },
     ""5"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 3,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 13,
         ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,

                     7,
                     8,
                     9
                 ]
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": null,
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ]
             }
         },
         ""memory_counter"": {
             ""A"": 0,
             ""B"": 0,
             ""C"": 0,
             ""D"": 0
         }
     },
     ""6"": {

             ""already_exec"": 3
         },
         ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
         ""time"": 15,
         ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,

                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ]
             }
         },
         ""memory_counter"": {
             ""A"": 0,
             ""B"": 0,
             ""C"": 0,
             ""D"": 0
         }
     }
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
 
 scalonator_translate = {
     ""FIFO"": fifo,

 }
 
 swap_translate = {
     ""FIFO"": swap_fifo
 }
 
 "
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from cpu.scalers.RR import rr
 from cpu.scalers.EDF import edf
 from cpu.memory.swap_algorithm.swap_fifo import swap_fifo
 from cpu.memory.swap_algorithm.swap_lru import swap_lru
 
 
 scalonator_translate = {
     ""FIFO"": fifo,

 }
 
 swap_translate = {
     ""FIFO"": swap_fifo,
     ""LRU"":swap_lru
 }
 
 "
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
         [
             ""A"",
             4,
             0
         ],
         [
             ""B"",
             7,
             2
         ],
         [
             ""C"",
             9,
             4
         ],
         [
             ""D"",
             13,
             6
         ]
     ],
     ""0"": 5.25
 }
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" {
     ""1"": [
         [
             ""B"",
             5,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""A"",
             13,
             0
         ],
         [
             ""D"",
             15,
             6
         ]
     ],
     ""0"": 7.0
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
 
 
     def initialize(self, queue:List[ProcessIn]):
def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
         #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return False #Tudo certo! o processo já está carregado na memoria
             #Não precisa de OVERHEAD
         
def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0 
                 self.swap(process)
                 return True
 
def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         #TODO Must teste this approach! It is the correct one!
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
                 self.p_order)
         # old_p_name= self.swap_algorithm(
             # self.p_order)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
             self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
         self.real_virtual_map[process.name][""uses""] = 0 
 
         return True
          
def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
     "
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from typing import Dict, List, Union, Callable, Tuple, Set
 from collections import deque, Counter
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
def __init__(self,
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
         self.counter = Counter()
 
 
     def initialize(self, queue:List[ProcessIn]):
def initialize(self, queue:List[ProcessIn]):
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
         self.counter[process.name] +=1
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return False #Tudo certo! o processo já está carregado na memoria
             #Não precisa de OVERHEAD
         
def load_context(self, process: ProcessIn)-> bool:
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
def load_context(self, process: ProcessIn)-> bool:
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = None
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.swap(process)
                 return True
 
def add_to_memory(
     def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         removed_p_count = 1
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
                 self.p_order, self.counter,removed_p_count)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
             self.counter[old_p_name] = 0
 
             removed_p_count+=1
 
         #cadastra o novo processo na memoria real
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
         self.real_virtual_map[process.name][""real""] = real_used_indexes 
         self.counter[process.name] = 0 
 
         return True
          
def garbage_collector(self,process:ProcessIn):
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
 
             self.counter[p_name] = 0
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
 
     def show_counter(self):
         copy = json.dumps(dict(self.counter))
         return copy"
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from collections import deque
 
 def swap_fifo(
     p_order:deque
 ):
     old_process_name = p_order[-1] #pega a primeira posição
     p_order.rotate()"
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from collections import deque, Counter
 
 def swap_fifo(
     p_order:deque,
     counter:Counter,
     removed_p_count:int
 ):
     old_process_name = p_order[-1] #pega a primeira posição
     p_order.rotate()"
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from collections import deque,Counter
 
 def swap_lru(
     p_order:deque,
     counter:Counter,
     removed_p_count:int
 ):
     least_used = counter.most_common()[:-removed_p_count-1:-1] 
 
     return least_used[removed_p_count-1][0] #retorna o nome do processo menos usado
 
 "
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;" from collections import Counter
 
 d = Counter()
 
 d[0]+=1
 d[0]+=1
 d[1]+=1
 d[1]+=1
 d[1]+=1
 
 print()
 "
KO;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
         # if p.name != cache_name: #Caso o process não esteja carregado na cache
         #     result = mmu.load_context(p)
             
         #     is_overhead = True
         #     if not first:
         #         sleep(overhead)
         #         time_count+=overhead
         #     first = False
             
         # else:
         #     is_overhead = False
         # is_process_done = False
         # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
 
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
             real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
     real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
                 ""real_virtual_map"": memory_map
             }
     
 "
OK;4.0;caiovinisl;simulador-processos-memoria;0d40bdac288e82dcd099995afdc495371e0aebb5;feat: Add memory lru! I need a break from coding in this project, testing and validatiing the algorithm is so annoying;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
     mmu_counter = """"
     print(""enters main loop!"")
 
     while True:
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             is_overhead = False
         is_process_done = False
         
 
         
 
         for quantum in range(1, threshold_quantum+1):
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
                 
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         mmu_counter = mmu.show_counter()
         cicle_data = create_cicle_data(
             0,0,
             0,p,
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
             real_virtual_map,
             mmu_counter
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
def create_cicle_data(
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
     real_virtual_map: str,
     mmu_counter:str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     memory_counter=json.loads(mmu_counter)
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
def create_cicle_data(
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
                 ""real_virtual_map"": memory_map,
                 ""memory_counter"":memory_counter
             }
     
 "
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""B""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 2,
             ""execution_time"": 2,
             ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 6,
         ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
             ""name"": ""C"",
             ""arrival_time"": 4,
             ""execution_time"": 1,
             ""deadline"": 8,
             ""pages"": 10,
             ""already_exec"": 1
         },
         ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
         ""started_time"": 6,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 6,
             ""execution_time"": 3,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 9,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": [
                     0,
                     1,

                     9
                 ],
                 ""uses"": 0
             }
         }
     },

             ""already_exec"": 3
         },
         ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
         ""time"": 10,
         ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,

                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 1
             }"
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"     },
     ""2"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 2,
             ""execution_time"": 2,
             ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 5,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ],
                 ""uses"": 0
             }
         }
     },
     ""3"": {
         ""process"": {
             ""name"": ""C"",
             ""arrival_time"": 4,
             ""execution_time"": 1,
             ""deadline"": 8,
             ""pages"": 10,
             ""already_exec"": 1
         },
         ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
         ""started_time"": 5,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 6,
             ""execution_time"": 3,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 10,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": [
                     0,
                     1,
                     2,

                     8,
                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 1,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 13,
         ""started_time"": 10,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,

                     9
                 ],
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": null,
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ],
                 ""uses"": 0
             }
         }
     },

             ""already_exec"": 3
         },
         ""quantum"": 1,
         ""overhead"": 1,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
         ""time"": 15,
         ""started_time"": 13,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,

                     9
                 ],
                 ""virtual"": [
                     10,
                     11,
                     12,
                     13,
                     14,
                     15,
                     16,
                     17,
                     18,
                     19
                 ],
                 ""uses"": 1
             }"
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;\ No newline at end of file
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""config"":{
         ""scale_algorithm"": ""RR"",
         ""page_algorithm"": ""FIFO"",
         ""quantum"": 10,
         ""overhead"":0
     },
     ""processes"":[
         {
             ""name"":""p5"",
             ""arrival_time"":35,
             ""execution_time"":5,
             ""pages"":10,
             ""deadline"":10
         },
         {
             ""name"":""p4"",
             ""arrival_time"":21,
             ""execution_time"":13,
             ""pages"":10,
             ""deadline"":10
         },
         {
             ""name"":""p3"",
             ""arrival_time"":19,
             ""execution_time"":10,
             ""pages"":10,
             ""deadline"":10
         },
         {
             ""name"":""p2"",
             ""arrival_time"":13,
             ""execution_time"":75,
             ""pages"":10,
             ""deadline"":8
         },
         {
             ""name"":""p1"",
             ""arrival_time"":5,
             ""execution_time"":17,
             ""pages"":10,
             ""deadline"":5
         },
         {
             ""name"":""p0"",
             ""arrival_time"":0,
             ""execution_time"":25,
             ""pages"":10,
             ""deadline"":7
         }
     ]
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
         [
             ""A"",
             4,
             0
         ],
         [
             ""B"",
             6,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""D"",
             10,
             6
         ]
     ],
     ""0"": 3.75
 }
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" {
     ""1"": [
         [
             ""B"",
             5,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""A"",
             13,
             0
         ],
         [
             ""D"",
             15,
             6
         ]
     ],
     ""0"": 7.0
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         #TODO Must teste this approach! It is the correct one!
         # while not self.memory_real.does_it_fit(process.pages): 
         #     old_p_name= self.swap_algorithm(
         #         self.p_order)
         old_p_name= self.swap_algorithm(
             self.p_order)
 
 
         #Remove o index do processo antigo da memoria real
         list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
         self.memory_real.remove(list_index_to_remove)
         self.real_virtual_map[old_p_name][""real""] = None
         self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         #TODO Must teste this approach! It is the correct one!
         while not self.memory_real.does_it_fit(process.pages): 
             old_p_name= self.swap_algorithm(
                 self.p_order)
         # old_p_name= self.swap_algorithm(
             # self.p_order)
 
             #Remove o index do processo antigo da memoria real
             list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
             self.memory_real.remove(list_index_to_remove)
             self.real_virtual_map[old_p_name][""real""] = None
             self.real_virtual_map[old_p_name][""uses""] = 0
 
         #cadastra o novo processo na memoria
         real_used_indexes = self.add_to_memory(process.pages,self.memory_real)"
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 from typing import Union, List
 
 
 
def fifo(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
         d.append(x)
     return d
 
 def fifo_dont_use(
     process_list:list[ProcessIn],
     add_p:Union[list[ProcessIn],ProcessIn],
     time_count:int=None,
 )-> deque[ProcessIn]:
     d = deque()
     print('fazendo fifo')
     for x in process_list:
         d.append(x)
 
     if type(add_p) is list:
         for p in add_p:
             d.appendleft(p)
     elif type(add_p) is ProcessIn:
         d.append(add_p)
 
     return d
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 
 
 
 def rr(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;" # sys.path.append(result)
 from cpu.models.process import ProcessIn
 from collections import deque
 from typing import Union, List
 
 
 
 def rr_v1(process_list:list[ProcessIn],time_count:int=None)-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
     d.append(process_list[len(process_list) - 1])
     for x in range(len(process_list)-2):
         d.append(process_list[x])
     return d
 
 
 def rr_v2(
     process_list:list[ProcessIn],
     time_count:int=None,
     right:bool=False
 )-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
 
     for x in process_list:
         d.append(x)
 
 
     if right:
         d.rotate()#rodar para a direita!
 
     return d
 
 def rr_v3(
     process_list:list[ProcessIn],
     add_p:Union[list[ProcessIn],ProcessIn],
     time_count:int=None,
 )-> deque[ProcessIn]:
     d = deque()
     print('fazendo rr')
 
     for x in process_list:
         d.append(x)
 
     if type(add_p) is list:
         for p in add_p:
             d.append(p)
     elif type(add_p) is ProcessIn:
         d.appendleft(add_p)
    
     return d
 
 def rr(process_list:list[ProcessIn],time_count:int=None):
     d = deque()
     lesser = 9999
     for x in process_list:
         if x.already_exec < lesser:
             lesser = x.already_exec
     normalized = all([p.already_exec==lesser if p.already_exec!=0 else False for p in process_list])
     if not normalized:        
         process_list.sort(key=lambda x: x.already_exec - lesser, reverse=True)
     else:
         process_list=reversed(process_list)
 
 
     for x in process_list:
         d.append(x)
     return d
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
             queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
             sleep(1)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
OK;4.0;caiovinisl;simulador-processos-memoria;4021e908f6896962dedfcac8f32feb0739d25754;feat: Add fix to the memory not swapping enough pages and added Round-Robin algorithm based on equal time;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             for ent in to_enter:
                 queue.appendleft(ent)
             # print(""processos no escalonador em "" + str(time_count) + "":  "" + str(queue))
             queue: deque[process.ProcessIn] = scalonator_engine(list(queue),time_count=time_count)
 
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
             if not first:
                 is_overhead = True
                 sleep(overhead)
                 time_count+=overhead
             first = False
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         for quantum in range(1, threshold_quantum+1):
             p.already_exec +=1
             time_count+= 1
             sleep(0)
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count=time_count)
 
         cicle_data = create_cicle_data(
             0,0,"
KO;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 10,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
         ""started_time"": 0
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 10,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 4,
         ""started_time"": 2
     },
     ""3"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 10,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 6
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 6,
         ""started_time"": 4
     },
     ""4"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 10,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 8
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B"",
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 8,
         ""started_time"": 6
     },
     ""5"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 10,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 10
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 10,
         ""started_time"": 8
     },
     ""6"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 0,
             ""execution_time"": 6,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 12,
         ""started_time"": 10
     },
     ""7"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 0,
             ""execution_time"": 6,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C"",
             ""B""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 14,
         ""started_time"": 12
     },
     ""8"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 0,
             ""execution_time"": 6,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 6
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D"",
             ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 16,
         ""started_time"": 14
     },
     ""9"": {
         ""process"": {
             ""name"": ""C"",
             ""arrival_time"": 0,
             ""execution_time"": 2,
             ""deadline"": 8,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 18,
         ""started_time"": 16
     },
     ""10"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E"",
             ""D""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 20,
         ""started_time"": 18
     },
     ""11"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 22,
         ""started_time"": 20
     },
     ""12"": {
         ""process"": {
             ""name"": ""E"",
             ""arrival_time"": 0,
             ""execution_time"": 8,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 24,
         ""started_time"": 22
     },
     ""13"": {
         ""process"": {
             ""name"": ""E"",
             ""arrival_time"": 0,
             ""execution_time"": 8,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 26,
         ""started_time"": 24
     },
     ""14"": {
         ""process"": {
             ""name"": ""E"",
             ""arrival_time"": 0,
             ""execution_time"": 8,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 6
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""E""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 28,
         ""started_time"": 26
     },
     ""15"": {
         ""process"": {
             ""name"": ""E"",
             ""arrival_time"": 0,
             ""execution_time"": 8,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 8
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
         ""time"": 30,
         ""started_time"": 28
     }
 }
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""A""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 2,
         ""started_time"": 0,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""2"": {
         ""process"": {
             ""name"": ""A"",
             ""arrival_time"": 0,
             ""execution_time"": 4,
             ""deadline"": 7,
             ""pages"": 10,
             ""already_exec"": 4
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""B""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 4,
         ""started_time"": 2,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 1
             }
         }
     },
     ""3"": {
         ""process"": {
             ""name"": ""B"",
             ""arrival_time"": 2,
             ""execution_time"": 2,
             ""deadline"": 5,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""C""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 6,
         ""started_time"": 4,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""4"": {
         ""process"": {
             ""name"": ""C"",
             ""arrival_time"": 4,
             ""execution_time"": 1,
             ""deadline"": 8,
             ""pages"": 10,
             ""already_exec"": 1
         },
         ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": true,
         ""time"": 7,
         ""started_time"": 6,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""5"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 6,
             ""execution_time"": 3,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 2
         },
         ""quantum"": 2,
         ""overhead"": 0,
         ""next_processess"": [
             ""D""
         ],
         ""done_in_this_cicle"": false,
         ""time"": 9,
         ""started_time"": 7,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 0
             }
         }
     },
     ""6"": {
         ""process"": {
             ""name"": ""D"",
             ""arrival_time"": 6,
             ""execution_time"": 3,
             ""deadline"": 10,
             ""pages"": 10,
             ""already_exec"": 3
         },
         ""quantum"": 1,
         ""overhead"": 0,
         ""next_processess"": [],
         ""done_in_this_cicle"": true,
         ""time"": 10,
         ""started_time"": 9,
         ""real_virtual_map"": {
             ""A"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""B"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""C"": {
                 ""real"": null,
                 ""virtual"": null,
                 ""uses"": 0
             },
             ""D"": {
                 ""real"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""virtual"": [
                     0,
                     1,
                     2,
                     3,
                     4,
                     5,
                     6,
                     7,
                     8,
                     9
                 ],
                 ""uses"": 1
             }
         }
     }
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
             10,
             0
         ],
         [
             ""B"",
             16,
             0
         ],
         [
             ""C"",
             18,
             0
         ],
         [
             ""D"",
             22,
             0
         ],
         [
             ""E"",
             30,
             0
         ]
     ],
     ""0"": 19.2
 }
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;"     ""1"": [
         [
             ""A"",
             4,
             0
         ],
         [
             ""B"",
             6,
             2
         ],
         [
             ""C"",
             7,
             4
         ],
         [
             ""D"",
             10,
             6
         ]
     ],
     ""0"": 3.75
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 
 
 class MMU:
     
def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
 
         old_p_name= self.swap_algorithm(
             self.p_order)
 
def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
 
     "
OK;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
 import json
 
 class MMU:
     
def add_to_memory(
 
 
     def swap(self, process: ProcessIn):
         #Enquanto não tiver espaço, fazer  o swap para ter espaço!
         
         #TODO Must teste this approach! It is the correct one!
         # while not self.memory_real.does_it_fit(process.pages): 
         #     old_p_name= self.swap_algorithm(
         #         self.p_order)
         old_p_name= self.swap_algorithm(
             self.p_order)
 
def garbage_collector(self,process:ProcessIn):
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
     def show_real_virtual_map(self):
         copy = json.dumps(self.real_virtual_map)
         return copy
     "
KO;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
 from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
     print(""enters main loop!"")
 
     while True:
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time
             }
     
 "
OK;4.0;caiovinisl;simulador-processos-memoria;c1aef2c233c5f6d5bcf2b342a7c0e6cde470f5f7;feat: Add to the cicle_data information about the memory allocations;" from distutils.command.build_scripts import first_line_re
 import json
 import re
 from typing import Callable, List, Tuple, Dict
 from collections import deque
 
 from cpu.models import config_model
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     first = True
     queue: deque = deque()
     number_process = len(process_list)
     real_virtual_map = None
     print(""enters main loop!"")
 
     while True:
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 real_virtual_map = mmu.show_real_virtual_map()
 
                 # real_virtual_map = mmu.show_real_virtual_map()
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         #Fora do processador
         
         if not p.is_it_done():
             real_virtual_map = mmu.show_real_virtual_map()
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             quantum,is_overhead,
             overhead,is_process_done,
             queue,time_count,
             started_time,
             real_virtual_map
         )
 
         json_driver.write(path,file_name,cicle_id,cicle_data=cicle_data)
def create_cicle_data(
     is_process_done:bool,
     queue:deque[process.ProcessIn],
     time_count:int,
     started_time:int,
     real_virtual_map: str
 ) -> dict:
     if is_overhead:
         overhead_response = overhead
def create_cicle_data(
 
     #do something with the arguments
     p_dict = process.dict()
     memory_map =json.loads(real_virtual_map)
     return {
                 ""process"":p_dict,
                 ""quantum"":quantum,
                 ""overhead"":overhead_response,
                 ""next_processess"":next_processess,
                 ""done_in_this_cicle"":is_process_done,
                 ""time"":time_count,
                 ""started_time"":started_time,
                 ""real_virtual_map"": memory_map
             }
     
 "
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
     ""FIFO"": swap_fifo,
 
 }
 
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" }
 
 swap_translate = {
     ""FIFO"": swap_fifo
 }
 
 
 path = ""cpu/configs""
 file_name = ""cicles_log.json""
 turnover_file_name = ""turnover.json"""
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;\ No newline at end of file
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" {
     ""config"":{
         ""scale_algorithm"": ""FIFO"",
         ""page_algorithm"": ""FIFO"",
         ""quantum"": 2,
         ""overhead"":0
     },
     ""processes"":[
        
         {
             ""name"":""A"",
             ""arrival_time"":0,
             ""execution_time"":10,
             ""pages"":10,
             ""deadline"":10
         },
         {
             ""name"":""B"",
             ""arrival_time"":0,
             ""execution_time"":6,
             ""pages"":10,
             ""deadline"":10
         },
         {
             ""name"":""C"",
             ""arrival_time"":0,
             ""execution_time"":2,
             ""pages"":10,
             ""deadline"":8
         },
         {
             ""name"":""D"",
             ""arrival_time"":0,
             ""execution_time"":4,
             ""pages"":10,
             ""deadline"":5
         },
         {
             ""name"":""E"",
             ""arrival_time"":0,
             ""execution_time"":8,
             ""pages"":10,
             ""deadline"":7
         }
     ]
 }
\ No newline at end of file"
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from typing import Dict, List, Union, Callable, Tuple
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
     page_algorithm: Callable
     
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
         page_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
         self.page_algorithm = page_algorithm
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
             if not self.memory_real.is_memory_full(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
             elif not self.memory_virtual.is_memory_full(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
def initialize(self, queue:List[ProcessIn]):
                 
 
 
     def load_context(self, process: ProcessIn):
         real_virtual_map = self.real_virtual_map
         self.memory_real.add_stack(process)
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return True #Tudo certo! o processo já está carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
def load_context(self, process: ProcessIn):
 
 
         else:#O processo não ta nem na memoria real nem na virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
         old_p_name= self.page_algorithm(
             self.memory_real)
 
 
         #Remove o index do processo antigo da memoria real
def swap(self, process: ProcessIn):
 
 
 
     def garbage_collector(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
 
     def init_memories(self):
         pass
 
     "
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from typing import Dict, List, Union, Callable, Tuple, Set
 from collections import deque
 from cpu.models.process import ProcessIn
 from cpu.memory.schemas.memory_real import Memory
class MMU:
     real_virtual_map: Dict[str,Dict[str,int]]
     memory_real: Memory
     memory_virtual: Memory
     swap_algorithm: Callable
     p_count: any
     p_order: deque
 
     def __init__(self,
         memory_real: Memory,
         memory_virtual:Memory,
         swap_algorithm:Callable
     )-> None:
         self.memory_real = memory_real
         self.memory_virtual = memory_virtual
         self.real_virtual_map = {}
         self.swap_algorithm = swap_algorithm
         self.p_count = None
         self.p_order = deque()
 
 
     def initialize(self, queue:List[ProcessIn]):
         for p in queue:
 
             if self.memory_real.does_it_fit(p.pages):
 
                 real_used_index = self.add_to_memory(p.pages,self.memory_real)
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
def initialize(self, queue:List[ProcessIn]):
                     ""virtual"":virtual_used_index,
                     ""uses"": 1
                 }
             elif self.memory_virtual.does_it_fit(p.pages):
                 virtual_used_index = self.add_to_memory(p.pages,self.memory_virtual)
                 self.real_virtual_map[p.name] = {
                     ""real"":None,
def initialize(self, queue:List[ProcessIn]):
                 
 
 
     def load_context(self, process: ProcessIn)-> bool:
         real_virtual_map = self.real_virtual_map
         if not process.name in real_virtual_map:
             self.p_order.appendleft(process.name)
         
         #|TODO Make add_stack and Count 1 function inside the corresponding swap_algorithm
         
         if True and\
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return False #Tudo certo! o processo já está carregado na memoria
             #Não precisa de OVERHEAD
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
             if self.memory_real.does_it_fit(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
def load_context(self, process: ProcessIn):
 
 
         else:#O processo não ta nem na memoria real nem na virtual
             if self.memory_real.does_it_fit(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.pages,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
 
                 self.real_virtual_map[process.name] = {}
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
                 self.real_virtual_map[process.name][""uses""] = 0
                 return True
 
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 virtual_used_indexes = self.add_to_memory(process.pages,self.memory_virtual)
def add_to_memory(
 
     def swap(self, process: ProcessIn):
 
         old_p_name= self.swap_algorithm(
             self.p_order)
 
 
         #Remove o index do processo antigo da memoria real
def swap(self, process: ProcessIn):
 
 
 
     def garbage_collector_all(self,process_done:Tuple[str,int,int]):
         real_virtual_map = self.real_virtual_map
         for p_name,_,_ in process_done:
             free_real_indexes = real_virtual_map[p_name][""real""]
def garbage_collector(self,process_done:Tuple[str,int,int]):
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
     def garbage_collector(self,process:ProcessIn):
             p_name = process.name
             real_virtual_map = self.real_virtual_map
             free_real_indexes = real_virtual_map[p_name][""real""]
             free_virtual_indexes = real_virtual_map[p_name][""virtual""]
 
             self.memory_real.remove(free_real_indexes)
             self.memory_virtual.remove(free_virtual_indexes)
 
 
             real_virtual_map[p_name][""real""] = None
             real_virtual_map[p_name][""virtual""] = None
             real_virtual_map[p_name][""uses""] = 0
             self.real_virtual_map = real_virtual_map 
             print(f""Removed processes = {p_name}"")
 
 
     "
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
     current_memory_space: int
     space_graph: Dict[str, bool]
     process_stack: deque
     type_name:str
 
def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
     def is_memory_full(self, number_of_page_in: int):
         if number_of_page_in < self.current_space_occupied:
             return True
         return False
     
def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
         for index in index_list:
             self.space_graph[index] = False
             self.current_space_occupied -= 1
             print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 class Memory:
     total_memory_pages: int
     current_space_occupied: int
     space_graph: Dict[int, bool]
     process_stack: deque
     type_name:str
 
def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
     def does_it_fit(self, number_of_page_in: int):
         free_space = self.total_memory_pages - self.current_space_occupied 
         if free_space >= number_of_page_in:
             return True
         return False
     
def add(self,used_index:List,pages:int)-> List[int]:
       
 
     def remove(self,index_list:List[int]):
         if index_list is None:
             print(""Process is not in memory"")
         else:
             for index in index_list:
                 self.space_graph[index] = False
                 self.current_space_occupied -= 1
                 print(f""Removed {index} from real memory"")
 
     def add_stack(self,process):
         name = process.name"
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" 
 
 def swap_fifo(
     memory_real
 ):
     old_process_name = memory_real.process_stack[-1] #pega a primeira posição
     memory_real.process_stack.rotate()
     return old_process_name
\ No newline at end of file"
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from collections import deque
 
 def swap_fifo(
     p_order:deque
 ):
\ No newline at end of file
     old_process_name = p_order[-1] #pega a primeira posição
     p_order.rotate()
     return old_process_name"
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
 from typing import List, Tuple
 from collections import deque
 
 from cpu.models import config_model
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
     scalonator_engine =  scalonator_translate[config.scale_algorithm] 
     page_algorithm = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
     # real_memory = Memory(""real"",total_memory_pages=20)
     # virtual_memory = Memory(""virtual"",total_memory_pages=100)
     # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
     cache_name = False
     is_process_done = False
     time_count = 0
     first = True
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
             # result = mmu.load_context(p)
             
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
             
         else:
             is_overhead = False
         is_process_done = False
         cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
             queue.appendleft(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
     for index,p in enumerate(process_list):
         if time_count >= p.arrival_time:
             result.append(p)
             process_list.pop(index)
     return result
         
 "
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" from distutils.command.build_scripts import first_line_re
 import json
 import re
 from typing import Callable, List, Tuple
 from collections import deque
 
 from cpu.models import config_model
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
 
     print(""###########################"")
 
     scalonator_engine: Callable =  scalonator_translate[config.scale_algorithm] 
     swap_algorithm: Callable = swap_translate[config.page_algorithm]
     
     json_driver.create_file(path=path,file_name=file_name)
 
     real_memory = Memory(""real"",total_memory_pages=10)
     virtual_memory = Memory(""virtual"",total_memory_pages=100)
     mmu = MMU(real_memory,virtual_memory,swap_algorithm)
     # mmu.initialize(process_list)
     # main-loop variables
     cicle_id = 1
     threshold_quantum = config.quantum
     overhead = config.overhead
     done_process = []
     is_overhead = False
     is_process_done = False
     time_count = 0
     first = True
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
         if len(queue) != 0:
             p = queue.pop() #Dentro do processador
             started_time = time_count
             is_process_done = False
 
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
         #Retorna True caso precise trocar de contexto!
         if mmu.load_context(p):
             is_overhead = True
             if not first:
                 sleep(overhead)
                 time_count+=overhead
             first = False
         else: #Retorna False caso o contexto já estava carregado
             is_overhead = False
         is_process_done = False
         
         # if p.name != cache_name: #Caso o process não esteja carregado na cache
         #     result = mmu.load_context(p)
             
         #     is_overhead = True
         #     if not first:
         #         sleep(overhead)
         #         time_count+=overhead
         #     first = False
             
         # else:
         #     is_overhead = False
         # is_process_done = False
         # cache_name = p.name
         
 
         for quantum in range(1, threshold_quantum+1):
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             if p.is_it_done():
                 is_process_done = True
                 done_process.append((p.name,time_count,p.arrival_time))
                 mmu.garbage_collector(p)
 
                 print(f""process={p.name} its done!"")
                 break 
         
         #Fora do processador
         
         if not p.is_it_done():
             queue.append(p) 
             queue: deque = scalonator_engine(list(queue),time_count)
 
         cicle_data = create_cicle_data(
def p_ready_to_enter(
     time_count:int
 )-> List[process.ProcessIn]:
     result = []
 
     process_copy = process_list.copy()
 
     for p in process_copy:
         if time_count >= p.arrival_time:
             result.append(p)
             process_list.remove(p)
 
             # process_list.pop(index)
     return result
         
 "
KO;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
 #TODO: Need delete cicle_log data!
 
 app = FastAPI()
 "
OK;4.0;caiovinisl;simulador-processos-memoria;6ba886db72ded69ca53361a64ec57dc7285a89b4;fix: Fix p_to_enter order, fix re0queue of recently process in cpu, fix memory_garbage_collector;" import asyncio
 
 #TODO: Need response models!
 
 app = FastAPI()
 "
KO;4.0;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
     real_memory = MemoryReal(total_memory_pages=50)
     virtual_memory = MemoryVirtual(total_memory_frames=100)
     mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
             result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4.0;caiovinisl;simulador-processos-memoria;6e79e37961294de92e2acf862fb84cc8f1e64f21;fix: comment memory undone code;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
     # real_memory = MemoryReal(total_memory_pages=50)
     # virtual_memory = MemoryVirtual(total_memory_frames=100)
     # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             # mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             # mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
             # result = mmu.load_context(p)
             
             is_overhead = True
             if not first:"
KO;4.0;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             return True #Tudo certo! o processo já está carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_index = self.add_to_memory(process.page,self.memory_real)
                 self.real_virtual_map[process.name][""real""] = real_used_index #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 real_virtual_map = self.swap(real_virtual_map)
                 pass
 
 
         else:#O processo não tinha sido cadastrado antes!
             pass
 
         
         
def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
         if not memory.is_memory_full(pages):
             for _ in range(pages): 
                used_index = memory.add(used_index)
             return used_index
         else:
             print(""memory full!"")
 
 
     def swap(self, process: ProcessIn):
         for k in self.real_virtual_map.keys():
             self.real_virtual_map[k]
 
         self.page_algorithm()
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
     def clean_done_process():
         pass
 
     def init_memory(self, process_list: List[ProcessIn]):
         pass
 
     def init_disk(self, process_list: List[ProcessIn]):
         pass
 
     "
OK;4.0;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def load_context(self, process: ProcessIn):
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""real"", None) 
         ):
             self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
             return True #Tudo certo! o processo já está carregado na memoria
         
         elif True and \
             ( real_virtual_map.get(process.name, None) ) and\
             ( real_virtual_map[process.name].get(""virtual"", None)
         ): #O processo não está na memo_real, mas está na memo_virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 real_used_indexes = self.add_to_memory(process.page,self.memory_real)
                 self.real_virtual_map[process.name][""real""] = real_used_indexes #Fazer update da tabela hash
                 self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
                 return True
             else: #Caso a memoria esteja cheia, vamos ao swap!
                 self.swap(process)
                 
 
                 return True
 
 
         else:#O processo não ta nem na memoria real nem na virtual
             if not self.memory_real.is_memory_full(process.pages): #caso a memoria real NÃO esteja cheia, alocar o processo
                 self.memory_real.add_stack(process)
                 real_used_indexes = self.add_to_memory(process.page,self.memory_real)
                 virtual_used_indexes = self.add_to_memory(process.page,self.memory_virtual)
 
                 self.real_virtual_map[process.name][""real""] = real_used_indexes
                 self.real_virtual_map[process.name][""virtual""] = virtual_used_indexes
             else:#Caso a memoria real esteja cheia! Vamos de swap dnovo!
                 self.swap(process)
 
 
 
         
         
def add_to_memory(
         memory: Union[MemoryReal,MemoryVirtual]
     )-> List[int]:
         used_index = []
         for _ in range(pages): 
             used_index = memory.add(used_index)
         return used_index
 
 
     def swap(self, process: ProcessIn):
 
         new_p_real_index, old_p_name= self.page_algorithm(
             self.memory_real,
             process,
             self.real_virtual_map)
 
         #Remove o index do processo antigo da memoria real
         list_index_to_remove = self.real_virtual_map[old_p_name][""real""]
         self.memory_real.remove(list_index_to_remove)
 
         #Atualiza a remoção na hash table
         self.real_virtual_map[old_p_name][""real""] = None
         self.real_virtual_map[old_p_name][""uses""] = 0 #Fazer update da tabela hash
 
         #
         self.real_virtual_map[process.name][""real""] = new_p_real_index #Fazer update da tabela hash
         self.real_virtual_map[process.name][""uses""] += 1 #Fazer update da tabela hash
 
         return True
          
 
     def update_special_queue(self,page:Page):
         index = self.special_queue.index(page)
         self.special_queue.remove(page)
 
     def garbage_collector(self,process_done:List[ProcessIn]):
         real_virtual_map = self.real_virtual_map
         for p in process_done:
             free_real_index = real_virtual_map[p.name][""real""]
             free_virtual_index = real_virtual_map[p.name][""virtual""]
 
             self.memory_real.remove(free_real_index)
             self.memory_virtual.remove(free_virtual_index)
 
 
             real_virtual_map[p.name][""real""] = None
             real_virtual_map[p.name][""virtual""] = None
             real_virtual_map[p.name][""uses""] = None
         self.real_virtual_map = real_virtual_map 
         print(""Removed unused processes"")
 
 
     def init_memories(self):
         pass
 
     "
KO;4.0;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
     def is_memory_full(self, page_in: int):
         if page_in > self.current_memory_space:
             return True
         return False
     
def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
     def add(self,used_index: List[any])-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
     def remove(self,index:int):
         self.space_graph[str(index)] = False
         self.current_space_occupied -= 1
 
 "
OK;4.0;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def space_initializer(self):
             space_graph[i] = False # lugar da memoria começa vazio
         return space_graph
 
     def is_memory_full(self, number_of_page_in: int):
         if number_of_page_in > self.current_memory_space:
             return True
         return False
     
def empty_spaces(self):
                 empty_list.append(i)
         return empty_list
 
     def add(self,used_index:List)-> List[int]:
         for i in range(self.total_memory_pages):
             if not self.space_graph[i]:
                 self.space_graph[i] = True
def add(self,used_index: List[any])-> List[int]:
         return used_index
       
 
     def remove(self,index_list:List[int]):
         for index in index_list:
             self.space_graph[index] = False
             self.current_space_occupied -= 1
             print(f""Removed {index} from real memory"")
 "
KO;4.0;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
     # real_memory = MemoryReal(total_memory_pages=50)
     # virtual_memory = MemoryVirtual(total_memory_frames=100)
     # mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
             # result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
OK;4.0;caiovinisl;simulador-processos-memoria;de463f34ec7d1f3bcb6f630e62f0b49a638bf2e4;feat: finish memory logic! Still missing LRU nad FIFO page_swap_algorithm;"def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     
     json_driver.create_file(path=path,file_name=file_name)
 
     real_memory = MemoryReal(total_memory_pages=50)
     virtual_memory = MemoryVirtual(total_memory_frames=100)
     mmu = MMU(real_memory,virtual_memory,page_algorithm)
     # MMU.initialize(process_list)
 
     # main-loop variables
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
     while True:
 
         if len(done_process) >= number_process:
             mmu.garbage_collector(done_process)
             break
 
         to_enter = p_ready_to_enter(process_list,time_count)
def start(config:config_model.ConfigIn, process_list:List[process.ProcessIn]):
             started_time = time_count
         else:
             time_count+=1
             mmu.garbage_collector(done_process)
             continue
 
         if p.name != cache_name: #Caso o process não esteja carregado na cache
             result = MMU.load_context(p)
             
             is_overhead = True
             if not first:"
KO;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";
OK;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" [common]
 type = magnitude
 
 [directory]
 user = /home/edgar
 work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
 
 [gp]
 components = 100
 batch_size = 100"
OK;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" """"""Prepare raw images for Fourier Analysis""""""
 
 ###############################################################################
 from configparser import ConfigParser, ExtendedInterpolation
 import glob
 import pickle
 import random
 import time
 
 import numpy as np
 from sklearn.random_projection import GaussianRandomProjection
 
 from leosAi.utils.managefiles import FileDirectory
 ###############################################################################
 start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""gauss_rp.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
 # Handle configuration file
 # configuration = ConfigurationFile()
 ###############################################################################
 # location of data
 data_directory = parser.get(""directory"", ""data"")
 data_type = parser.get(""common"", ""type"")
 path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
 
 number_of_files = len(path_to_files)
 batch_size = parser.getint(""gp"", ""batch_size"")
 number_of_batches = number_of_files // batch_size
 
 # Complete a batch in case number of batches
 # does not fit all files
 if number_of_batches % batch_size !=0:
 
     number_of_batches += 1
 
     remaining_number_of_files = batch_size - number_of_batches % batch_size
     # randomly pick already used images
     path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
 
 
 
 image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
 
 batch_shape = (batch_size, ) + image_shape
 batch_of_images = np.empty(batch_shape).astype(np.float32)
 
 n_components = parser.getint(""gp"", ""components"")
 transformer = GaussianRandomProjection(n_components=n_components)
 
 save_to = f""{data_directory}/gauss_rp""
 check.check_directory(save_to, exit_program=False)
 
 for batch in range(number_of_batches):
 
 
     # load images to current batch of images
     index_of_images = range(batch_size*batch, batch_size*(batch+1))
 
     for idx_batch, idx_image in enumerate(index_of_images):
 
         batch_of_images[idx_batch, ...] = np.load(
             path_to_files[idx_image]
         ).astype(np.float32)
 
     print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
     # fit grp
     embedding = transformer.fit_transform(
         batch_of_images.reshape(batch_size, -1)
     )
     ###########################################################################
     np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
 
 
 ###############################################################################
 with open(
     f""{save_to}/{config_file_name}"",
     ""w"", encoding=""utf8""
 ) as config_file:
 
     parser.write(config_file)
 ###############################################################################
 finish_time = time.time()
 print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
OK;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" [common]
 type = magnitude
 
 [directory]
 user = /home/edgar
 work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
 
 [pca]
 components = 50
 batch_size = 50"
OK;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" """"""Prepare raw images for Fourier Analysis""""""
 
 ###############################################################################
 from configparser import ConfigParser, ExtendedInterpolation
 import glob
 import pickle
 import random
 import time
 
 import numpy as np
 from sklearn.decomposition import IncrementalPCA
 
 from leosAi.utils.managefiles import FileDirectory
 ###############################################################################
 start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""ipca.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
 # Handle configuration file
 # configuration = ConfigurationFile()
 ###############################################################################
 # location of data
 data_directory = parser.get(""directory"", ""data"")
 data_type = parser.get(""common"", ""type"")
 path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
 
 number_of_files = len(path_to_files)
 batch_size = parser.getint(""pca"", ""batch_size"")
 number_of_batches = number_of_files // batch_size
 
 # Complete a batch in case number of batches
 # does not fit all files
 if number_of_batches % batch_size !=0:
 
     number_of_batches += 1
 
     remaining_number_of_files = batch_size - number_of_batches % batch_size
     # randomly pick already used images
     path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
 
 
 
 image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
 
 batch_shape = (batch_size, ) + image_shape
 batch_of_images = np.empty(batch_shape).astype(np.float32)
 
 n_components = parser.getint(""pca"", ""components"")
 assert n_components <= batch_size
 transformer = IncrementalPCA(n_components = n_components)
 
 save_to = f""{data_directory}/gauss_rp""
 check.check_directory(save_to, exit_program=False)
 
 for batch in range(number_of_batches):
 
 
     # load images to current batch of images
     index_of_images = range(batch_size*batch, batch_size*(batch+1))
 
     for idx_batch, idx_image in enumerate(index_of_images):
 
         batch_of_images[idx_batch, ...] = np.load(
             path_to_files[idx_image]
         ).astype(np.float32)
 
     print(f""IPCA of batch {batch:02d}"", end=""\n"")
     # fit pca
     transformer.fit(batch_of_images.reshape(batch_size, -1))
 ###############################################################################
 with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
 
     pickle.dump(transformer, file)
 
 ###############################################################################
 with open(
     f""{save_to}/{config_file_name}"",
     ""w"", encoding=""utf8""
 ) as config_file:
 
     parser.write(config_file)
 ###############################################################################
 finish_time = time.time()
 print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()

 
     with pyfits.open(path_to_file) as hdu:
 
         # get magnitude scale to better distinguis objects
         # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
     image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5.0;CLEOsat-group;leos-ai;4a64a1badc8e47e36034225fa614cb79f8136a3e;"Merge pull request #5 from ed-ortizm/pca

pca: implement PCA and Gaussian random projections to get embedding of data as requested in issue #1.
To account for memory capacity in normal desktops or notebooks, instead of PCA, IncrementalPCA was implemented.";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()

 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
     image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
OK;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" [common]
 type = magnitude
 
 [directory]
 user = /home/edgar
 work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
 
 [gp]
 components = 100
 batch_size = 100"
OK;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" """"""Prepare raw images for Fourier Analysis""""""
 
 ###############################################################################
 from configparser import ConfigParser, ExtendedInterpolation
 import glob
 import pickle
 import random
 import time
 
 import numpy as np
 from sklearn.random_projection import GaussianRandomProjection
 
 from leosAi.utils.managefiles import FileDirectory
 ###############################################################################
 start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""gauss_rp.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
 # Handle configuration file
 # configuration = ConfigurationFile()
 ###############################################################################
 # location of data
 data_directory = parser.get(""directory"", ""data"")
 data_type = parser.get(""common"", ""type"")
 path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
 
 number_of_files = len(path_to_files)
 batch_size = parser.getint(""gp"", ""batch_size"")
 number_of_batches = number_of_files // batch_size
 
 # Complete a batch in case number of batches
 # does not fit all files
 if number_of_batches % batch_size !=0:
 
     number_of_batches += 1
 
     remaining_number_of_files = batch_size - number_of_batches % batch_size
     # randomly pick already used images
     path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
 
 
 
 image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
 
 batch_shape = (batch_size, ) + image_shape
 batch_of_images = np.empty(batch_shape).astype(np.float32)
 
 n_components = parser.getint(""gp"", ""components"")
 transformer = GaussianRandomProjection(n_components=n_components)
 
 save_to = f""{data_directory}/gauss_rp""
 check.check_directory(save_to, exit_program=False)
 
 for batch in range(number_of_batches):
 
 
     # load images to current batch of images
     index_of_images = range(batch_size*batch, batch_size*(batch+1))
 
     for idx_batch, idx_image in enumerate(index_of_images):
 
         batch_of_images[idx_batch, ...] = np.load(
             path_to_files[idx_image]
         ).astype(np.float32)
 
     print(f""Gaussian random projection of batch {batch:03d}"", end=""\n"")
     # fit grp
     embedding = transformer.fit_transform(
         batch_of_images.reshape(batch_size, -1)
     )
     ###########################################################################
     np.save(f""{save_to}/embedding_{batch:03d}.npy"", embedding)
 
 
 ###############################################################################
 with open(
     f""{save_to}/{config_file_name}"",
     ""w"", encoding=""utf8""
 ) as config_file:
 
     parser.write(config_file)
 ###############################################################################
 finish_time = time.time()
 print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]"
OK;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";"work = ${user}/leos-ai
 data = ${user}/leos-data/leos-ai/data/korea
 
 [file]
 
 [pca]
 components = 50
 batch_size = 50"
OK;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" """"""Prepare raw images for Fourier Analysis""""""
 
 ###############################################################################
 from configparser import ConfigParser, ExtendedInterpolation
 import glob
 import pickle
 import random
 import time
 
 import numpy as np
 from sklearn.decomposition import IncrementalPCA
 
 from leosAi.utils.managefiles import FileDirectory
 ###############################################################################
 start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""ipca.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
 # Handle configuration file
 # configuration = ConfigurationFile()
 ###############################################################################
 # location of data
 data_directory = parser.get(""directory"", ""data"")
 data_type = parser.get(""common"", ""type"")
 path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
 
 number_of_files = len(path_to_files)
 batch_size = parser.getint(""pca"", ""batch_size"")
 number_of_batches = number_of_files // batch_size
 
 # Complete a batch in case number of batches
 # does not fit all files
 if number_of_batches % batch_size !=0:
 
     number_of_batches += 1
 
     remaining_number_of_files = batch_size - number_of_batches % batch_size
     # randomly pick already used images
     path_to_files += random.choices(path_to_files, k=remaining_number_of_files)
 
 
 
 image_shape = np.load(path_to_files[0], mmap_mode=""r"").shape
 
 batch_shape = (batch_size, ) + image_shape
 batch_of_images = np.empty(batch_shape).astype(np.float32)
 
 n_components = parser.getint(""pca"", ""components"")
 assert n_components <= batch_size
 transformer = IncrementalPCA(n_components = n_components)
 
 save_to = f""{data_directory}/gauss_rp""
 check.check_directory(save_to, exit_program=False)
 
 for batch in range(number_of_batches):
 
 
     # load images to current batch of images
     index_of_images = range(batch_size*batch, batch_size*(batch+1))
 
     for idx_batch, idx_image in enumerate(index_of_images):
 
         batch_of_images[idx_batch, ...] = np.load(
             path_to_files[idx_image]
         ).astype(np.float32)
 
     print(f""IPCA of batch {batch:02d}"", end=""\n"")
     # fit pca
     transformer.fit(batch_of_images.reshape(batch_size, -1))
 ###############################################################################
 with open(f""{save_to}/ipca.pkl"", ""wb"") as file:
 
     pickle.dump(transformer, file)
 
 ###############################################################################
 with open(
     f""{save_to}/{config_file_name}"",
     ""w"", encoding=""utf8""
 ) as config_file:
 
     parser.write(config_file)
 ###############################################################################
 finish_time = time.time()
 print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
KO;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" """"""Prepare raw images for Fourier Analysis""""""
 
 ###############################################################################
 from configparser import ConfigParser, ExtendedInterpolation
 import glob
 import time
 
 from astropy.io import fits as pyfits
 import numpy as np
 
 from leosAi.utils.managefiles import FileDirectory
 ###############################################################################
 start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""ir_pca.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()
 # Handle configuration file
 # configuration = ConfigurationFile()
 ###############################################################################
 # location of data
 data_directory = parser.get(""directory"", ""data"")
 data_type = parser.get(""common"", ""type"")
 path_to_files = glob.glob(f""{data_directory}/*/{data_type}/*.npy"")
 
 for idx, path_to_file in enumerate(path_to_files):
 
     file_name = path_to_file.split(""/"")[-1].split(""."")[0]
     print(f""{idx:04d}: {file_name}"", end=""\r"")
 
     image = np.load(path_to_file)
 ###############################################################################
 # with open(
 #     f""{save_to}/{config_file_name}"",
 #     ""w"", encoding=""utf8""
 # ) as config_file:
 #
 #     parser.write(config_file)
 ###############################################################################
 finish_time = time.time()
 print(f""\n Run time: {finish_time-start_time:.2f}"", end=""\n"")"
OK;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";
KO;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""raw.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()

 
     with pyfits.open(path_to_file) as hdu:
 
         # get magnitude scale to better distinguis objects
         # image = np.log10(hdu[0].data)
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
     image = np.log10(image)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
OK;5.0;CLEOsat-group;leos-ai;5cbfcba5a95a0d04ffe2b00ee6563b8ded61e4f4;"feature: implement incremental PCA and Gaussian random projection on data to get lower dimensional representations

iPCA with 50 images and 50 components takes ~8 GBs in memory [image_shape = 2048x2048]. Gaussian RP takes less and is way faster, therefore it is feaseble to compute 100 latent dimensions on 100 images at a time";" start_time = time.time()
 ###############################################################################
 parser = ConfigParser(interpolation=ExtendedInterpolation())
 config_file_name = ""magnitude.ini""
 parser.read(f""{config_file_name}"")
 # Check files and directory
 check = FileDirectory()

 
     with pyfits.open(path_to_file) as hdu:
 
         image = hdu[0].data
 
     # replace NaNs with background
     image = np.where(~np.isfinite(image), np.nanmedian(image), image)
     # replace negative and null counts with median
     image = np.where(image <= 0, np.nanmedian(image), image)
     # compute magnitude
     image = np.log10(image, dtype=np.float32)
     # Set background to zero
     image = np.where(image <= np.median(image), 0., image-np.median(image))
     # Normalize image"
KO;5.0;BinFlush;scripts;f6b8b28ef6c4ca975f9ee38e9cef9830b74032e0;added memory sanitation and clipboard option;"def main():
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()"
OK;5.0;BinFlush;scripts;f6b8b28ef6c4ca975f9ee38e9cef9830b74032e0;added memory sanitation and clipboard option;"def main():
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-f', action='store_true', help=""Require password to include every specified type of characters"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()"
KO;5.0;BinFlush;scripts;610dc5bb1e7fbed110ac160f2034b26df18ce21f;added option to copy password to clipboard, and some memory sanitation of sensitive data;" import secrets
 import string
 import argparse
 
 
 def main():
     # Defining command line arguments
def main():
     parser.add_argument('-u', action='store_true', help=""use uppercase"")
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set \""in quotes\"""")
     args = parser.parse_args()
 
     # Checking how many sets of characters are to be used
def main():
 
     # Add sets to superset
     chars = """"
     if args.l:    chars += string.ascii_lowercase
     if args.u:
         chars += string.ascii_uppercase
     if args.d:
def main():
 
     # Build and print actual password
     password = ''.join(secrets.choice(chars) for i in range(args.n))
     print(password)
 
 def count_arguments(args) -> int:
     """""" Counts valid command line arguments except -n""""""
     n: int = 0
     for arg in vars(args):
         if not arg == ""n"":
             n += bool(getattr(args, arg))
     return n
 "
OK;5.0;BinFlush;scripts;610dc5bb1e7fbed110ac160f2034b26df18ce21f;added option to copy password to clipboard, and some memory sanitation of sensitive data;" import secrets
 import string
 import argparse
 import gc
 import pyperclip # remember to pip3 install pyperclip
 
 def main():
     # Defining command line arguments
def main():
     parser.add_argument('-u', action='store_true', help=""use uppercase"")
     parser.add_argument('-d', action='store_true', help=""use numbers"")
     parser.add_argument('-s', action='store_true', help=""use symbols"")
     parser.add_argument('-c', action='store_true', help=""copy password to clipboard"")
     parser.add_argument('-n', default=12, type=int, help=""password length (default 12)"")
     parser.add_argument('--charset', default="""", help=""custom character set 'in singlequotes'"")
     args = parser.parse_args()
 
     # Checking how many sets of characters are to be used
def main():
 
     # Add sets to superset
     chars = """"
     if args.l:    
         chars += string.ascii_lowercase
     if args.u:
         chars += string.ascii_uppercase
     if args.d:
def main():
 
     # Build and print actual password
     password = ''.join(secrets.choice(chars) for i in range(args.n))
     if args.c:
         pyperclip.copy(password)
     else:
         print(password)
 
     
     # Clean up sensitive data
     del(password)
     del(chars)
     gc.collect()
 
 def count_arguments(args) -> int:
     """""" Counts valid command line arguments except -n""""""
     n: int = 0
     for arg in vars(args):
         if arg in [""u"", ""l"", ""d"", ""charset""]:
             n += bool(getattr(args, arg))
     return n
 "
KO;6.0;NTT123;a0-jax;df6a898f5b2d72b537c3c31d25ffbc7e4138a935;reduce memory usage;"def train(
         buffer.extend(data)
         data = list(buffer)
         shuffler.shuffle(data)
         data = jax.tree_map(lambda *xs: np.stack(xs), *data)
         N = data.state.shape[0]
         losses = []
         old_agent = jax.tree_map(lambda x: jnp.copy(x), agent)
         agent = agent.train()
         with click.progressbar(
             range(0, N - batch_size, batch_size), label=""  train agent""
         ) as bar:
             for i in bar:
                 batch = jax.tree_map(lambda x: x[i : (i + batch_size)], data)
                 agent, optim, loss = train_step(agent, optim, batch)
                 losses.append(loss)
 "
OK;6.0;NTT123;a0-jax;df6a898f5b2d72b537c3c31d25ffbc7e4138a935;reduce memory usage;"def train(
         buffer.extend(data)
         data = list(buffer)
         shuffler.shuffle(data)
         N = len(data)
         losses = []
         old_agent = jax.tree_map(lambda x: jnp.copy(x), agent)
         agent = agent.train()
         with click.progressbar(
             range(0, N - batch_size, batch_size), label=""  train agent""
         ) as bar:
             for i in bar:
                 batch = data[i : (i + batch_size)]
                 batch = jax.tree_map(lambda *xs: np.stack(xs), *batch)
                 agent, optim, loss = train_step(agent, optim, batch)
                 losses.append(loss)
 "
KO;7.0;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
   </component>
 </module>
\ No newline at end of file"
OK;7.0;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"     <content url=""file://$MODULE_DIR$"" />
     <orderEntry type=""jdk"" jdkName=""Python 3.8"" jdkType=""Python SDK"" />
     <orderEntry type=""sourceFolder"" forTests=""false"" />
     <orderEntry type=""module"" module-name=""state_machine"" />
   </component>
 </module>
\ No newline at end of file"
KO;7.0;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
OK;7.0;a5892731;GUI_template;c1b922d997a90267984bb6f9844d4de43f86a77f;add States data memory class;"   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/GUI_template.iml"" filepath=""$PROJECT_DIR$/.idea/GUI_template.iml"" />
       <module fileurl=""file://$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/../state_machine/.idea/state_machine.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
KO;8.0;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el número de taps que realice el usuario
 
     if taps==64:
        up()
        goto(0, 0)
        color (""red"")"
OK;8.0;a01655338;RepoEvidencia;00eb7d7a60eabb43e05ffad30b5651cbe685ebda;Update memory.py;"def draw():
     goto(-190, 180)
     write(taps,  align=""center"", font=(""Arial"", 20, ""bold"")) # Cuenta el número de taps que realice el usuario
 
     if taps==300:
        up()
        goto(0, 0)
        color (""red"")"
KO;9.0;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
         self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
OK;9.0;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def __init__(self, logger=None, manager=None, config=None):
         if not logger:
             self.logger = logging.getLogger()
         self.logger.setLevel(logging.INFO)
         # self.logger.addHandler(logging.StreamHandler())
         
         self.max_players = (config.get('max_players') if 'max_players' in config else 100)
         self.blocking = (config.get('blocking') if 'blocking' in config else 0)"
KO;9.0;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
         print(cache.actual_data)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
         cache.actual_data['w'] = keys[pygame.K_w] * 6
         cache.actual_data['s'] = keys[pygame.K_s] * 6
         cache.actual_data['a'] = keys[pygame.K_a] * 6
         cache.actual_data['d'] = keys[pygame.K_d] * 6
         for key in cache.actual_data:
             if cache.actual_data[key] == 0:
                 continue
             cache.actual_data[key] = cache.actual_data[key] - 1
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
             if e.type == pygame.KEYDOWN:
                 if e.unicode in cache.actual_data:
                     cache.actual_data[e.unicode] = 5
         
         screen.fill('black')
         data = cache.get_last_data()"
OK;9.0;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"def transfer(client, fps=50):
     time_sleep = 1 / fps
     while client.transfer_live:
         client.call_udp(method=""get_data"", data={}, address=SERVER_ADDRESS, response=True, caching=True)
         client.call_udp(method=""send_data"", data={""keys"": cache.actual_data}, address=SERVER_ADDRESS, response=False)
         time.sleep(time_sleep)
 
def change_server():
     is_playing = True
     while is_playing:
         keys = pygame.key.get_pressed()
         cache.actual_data['w'] = keys[pygame.K_w]
         cache.actual_data['s'] = keys[pygame.K_s]
         cache.actual_data['a'] = keys[pygame.K_a]
         cache.actual_data['d'] = keys[pygame.K_d]
         for e in pygame.event.get():
             if e.type == pygame.QUIT:
                 _client.transfer_live = False
def change_server():
                 change_color(_client)
             elif e.type == pygame.KEYDOWN and e.key == pygame.K_2:
                 change_server()
         
         screen.fill('black')
         data = cache.get_last_data()"
KO;9.0;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
     simulation_id = request['data'].get(""id"")
     if not uid:
         return
     elif not simulation_id:
         return
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
OK;9.0;GoodDay-lab;online_game;380c2ba0c9af97d11f5395df3bcb1e98195fbb56;Optimized output into console! Now it takes lower memory and CPU... in 2 times!;"async def connecting(addr, request):
 @server.add_udp_handler(""enter_simulation"")
 async def enter_sim(addr, request):
     uid = request['cookie'].get(""uid"")
     old_simulation_id = request['cookie'].get(""id"")
     simulation_id = request['data'].get(""id"")
     
     if not uid: return
     if not simulation_id: return
     if not old_simulation_id: return
     
     user = storage.get_unit(""users"", id=uid)
     if not user:
         return
     simulation = storage.get_unit(""simulations"", simulation_id=simulation_id)
     if not simulation:
         return
     old_simulation = storage.get_unit(""simulations"", simulation_id=old_simulation_id)
     if not old_simulation:
         return
     old_simulation[""users""].remove(user['id'])
     simulation[""users""].append(uid)
     simulation[""updated""] = False
     response = {""id"": simulation_id}
async def getting_data(addr, request):
 async def sending_data(addr, request):
     uid = request['cookie'].get(""uid"")
     if not uid: return
     for key in request['data']['keys']:
         request['data']['keys'][key] *= 6
     storage.update_unit(""users"", control_data={""id"": uid}, relevant_data=request['data'])
 
 "
KO;9.0;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"Displays stock information relative to now, in years. Replace the 1 with any num
 ## Up and Down Command:
 
 `!up three_letter_stock_name value_to_reach`
 `!down three_letter_stock_name value_to_reach`
 
 Sets up an automatic alert for when the specified stock value exceeds or falls under a specified value.
 
 ## Buy and Sell Command:
 
 `!buy money_to_buy_shares_with three_letter_stock_name`
Deletes all pending alerts for the provided stock ticker that are also from the
 
 Deletes any pending alert that matches the stock ticker, `!up` and `!down` command, and also the value.
 
 # Administration Commands:
 ## Stop Command:
 "
OK;9.0;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"Displays stock information relative to now, in years. Replace the 1 with any num
 ## Up and Down Command:
 
 `!up three_letter_stock_name value_to_reach`
 
 `!down three_letter_stock_name value_to_reach`
 
 Sets up an automatic alert for when the specified stock value exceeds or falls under a specified value.
 
 `!up three_letter_stock_name percentage %`
 
 `!down three_letter_stock_name percentage %`
 
 Sets a relative pricing alert to it's current price.
 
 ## Buy and Sell Command:
 
 `!buy money_to_buy_shares_with three_letter_stock_name`
Deletes all pending alerts for the provided stock ticker that are also from the
 
 Deletes any pending alert that matches the stock ticker, `!up` and `!down` command, and also the value.
 
 ## Portfolio:
 
 ### This command only works in Direct Messages with the bot. It will send you a DM if you execute this command in any server.
 
 `!portfolio torn_api_key`
 
 Lists all stocks that you own with all transactions with their change in price relative to now. All transactions for that stock are ordered newest first to oldest last.
 
 `!portfolio torn_api_key stock_ticker`
 
 Lists all transactions for the specified ticker.
 
 `!portfolio torn_api_key stock_ticker number_of_transactions`
 
 Lists `number_of_transactions` of the selected stock before stopping. If you have two or more transactions, and use a it'll show the most recent transaction.
 
 # Administration Commands:
 ## Stop Command:
 "
KO;9.0;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							break
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)
async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							break
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)"
OK;9.0;Jordach;TornStonksLive;ae8dc377f722908fdeb9e5266b16f197ff340c95;"Fix percentage adds breaking memory integrity when a stock isn't found
Add documentation";"async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					found_stock = False
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							found_stock = True
 							break
 					if not found_stock:	
 						userdata[""value""].append(0)
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)
async def alerts(self, message, prefix):
 					userdata[""value""].append(float(self.strip_commas(command[2])))
 				elif command[3] == ""%"":
 					perc = 1 + (float(self.strip_commas(command[2])) / 100)
 					found_stock = False
 					for data in json_data[""data""]:
 						if data[""stock""] == command[1].upper():
 							userdata[""value""].append(float(data[""price""]) * perc)
 							found_stock = True
 							break
 					if not found_stock:	
 						userdata[""value""].append(0)
 				else:
 					err_embed = discord.Embed(title="":no_entry_sign: Invalid Argument :no_entry_sign:"")
 					self.set_author(message, err_embed)"
KO;12.0;js0522;tgn_review;00428db9374655b3ecf0ce9806b2d8e1254101c0;Add vanilla RNN memory updater;"def update_memory(self, unique_node_ids, unique_messages, timestamps):
     pass
 
 
 class GRUMemoryUpdater(MemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
     super(GRUMemoryUpdater, self).__init__()
     self.memory = memory
     self.layer_norm = torch.nn.LayerNorm(memory_dimension)
     self.message_dimension = message_dimension
     self.device = device
 
     self.memory_updater = nn.GRUCell(input_size=message_dimension,
                                      hidden_size=memory_dimension)
 
   def update_memory(self, unique_node_ids, unique_messages, timestamps):
     if len(unique_node_ids) <= 0:
       return
def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):
     updated_last_update[unique_node_ids] = timestamps
 
     return updated_memory, updated_last_update"
OK;12.0;js0522;tgn_review;00428db9374655b3ecf0ce9806b2d8e1254101c0;Add vanilla RNN memory updater;"def update_memory(self, unique_node_ids, unique_messages, timestamps):
     pass
 
 
 class SequenceMemoryUpdater(MemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
     super(SequenceMemoryUpdater, self).__init__()
     self.memory = memory
     self.layer_norm = torch.nn.LayerNorm(memory_dimension)
     self.message_dimension = message_dimension
     self.device = device
 
   def update_memory(self, unique_node_ids, unique_messages, timestamps):
     if len(unique_node_ids) <= 0:
       return
def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):
     updated_last_update[unique_node_ids] = timestamps
 
     return updated_memory, updated_last_update
 
 
 class GRUMemoryUpdater(SequenceMemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
     super(GRUMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)
 
     self.memory_updater = nn.GRUCell(input_size=message_dimension,
                                      hidden_size=memory_dimension)
 
 
 class RNNMemoryUpdater(SequenceMemoryUpdater):
   def __init__(self, memory, message_dimension, memory_dimension, device):
     super(RNNMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)
 
     self.memory_updater = nn.RNNCell(input_size=message_dimension,
                                      hidden_size=memory_dimension)
 
 
 def get_memory_updater(module_type, memory, message_dimension, memory_dimension, device):
   if module_type == ""gru"":
     return GRUMemoryUpdater(memory, message_dimension, memory_dimension, device)
   elif module_type == ""rnn"":
     return RNNMemoryUpdater(memory, message_dimension, memory_dimension, device)"
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;\ No newline at end of file
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" <?xml version=""1.0"" encoding=""UTF-8""?>
 <project version=""4"">
   <component name=""ProjectModuleManager"">
     <modules>
       <module fileurl=""file://$PROJECT_DIR$/.idea/state_machine.iml"" filepath=""$PROJECT_DIR$/.idea/state_machine.iml"" />
     </modules>
   </component>
 </project>
\ No newline at end of file"
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" <?xml version=""1.0"" encoding=""UTF-8""?>
 <project version=""4"">
   <component name=""RSettings"" path="""" />
 </project>
\ No newline at end of file"
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" <?xml version=""1.0"" encoding=""UTF-8""?>
 <project version=""4"">
   <component name=""ProjectId"" id=""29QNMxx7EeoT8aAdnSWICPXeq0h"" />
 </project>
\ No newline at end of file"
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" from resources.state_machine.states.s03_test_state_2 import Test2StateBody
 
 class Initialization(InitializationBody):
     def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
             self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         else:
             info = "">>> Info: transition error in {} state"".format(self)
             return CloseProgram(info)
 
 class CloseProgram(CloseProgramBody):
     def on_event(self, event):
         self.action()
 
 class Test1State(Test1StateBody):
     def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
             self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
             info = "">>> Info: transition error in {} state"".format(self)
             return CloseProgram(info)
 
 class Test2State(Test2StateBody):
     def on_event(self, event):
         if event == 'device_locked':
             self.action()
         else:
             self.status = ""error""
 
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
             info = "">>> Info: transition error in {} state"".format(self)
             return CloseProgram(info)"
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" from resources.state_machine.states.s03_test_state_2 import Test2StateBody
 
 class Initialization(InitializationBody):
     def on_event(self, event, states_data):
         '''import memory from States class'''
         self = states_data.Initialization
 
         '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
             states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
             return CloseProgram()
 
         '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             return Test1State()
         else:
             states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
             return CloseProgram()
 
 class CloseProgram(CloseProgramBody):
     def on_event(self, event, states_data):
         '''import memory from States class'''
         self = states_data.CloseProgram
 
         self.action()
 
 class Test1State(Test1StateBody):
     def on_event(self, event, states_data):
         '''import memory from States class'''
         self = states_data.Test1State
 
         '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
             states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
             return CloseProgram()
 
         '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             states_data.Test1State = Test1State()
             return Test2State()
         else:
             states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
             return CloseProgram()
 
 class Test2State(Test2StateBody):
     def on_event(self, event, states_data):
         '''import memory from States class'''
         self = states_data.Test2State
 
         '''control_word'''
         if event == 'device_locked':
             self.action()
         else:
             states_data.CloseProgram.info = "">>> Info: device unlocked in {} state"".format(self)
             return CloseProgram()
 
         '''transition conditions'''
         if self.status == ""GO TO TEST1"":
             '''clear data before transition'''
             states_data.Test2State = Test2State()
             return Test1State()
         elif self.status == ""GO TO TEST2"":
             return Test2State()
         else:
             states_data.CloseProgram.info = "">>> Info: transition error in {} state"".format(self)
             return CloseProgram()"
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 
 '''import all your states here'''
 from resources.state_machine.my_states import Initialization
 
 
 class StateLoader(object): #in Karen project is a SimpleDevice class
def __init__(self):
         """""" Initialize the components. """"""
 
         # Start with a default state.
         self.state = Initialization()
 
     def on_event(self, event):
         """"""
def on_event(self, event):
         #
 
         # The next state will be the result of the on_event function.
         self.state = self.state.on_event(event)
 
 
 "
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 
 '''import all your states here'''
 from resources.state_machine.my_states import Initialization, CloseProgram, Test1State, Test2State
 
 class States():
     def __init__(self):
         self.Initialization = Initialization()
         self.CloseProgram = CloseProgram()
         self.Test1State = Test1State()
         self.Test2State = Test2State()
 
 
 class StateLoader(object): #in Karen project is a SimpleDevice class
def __init__(self):
         """""" Initialize the components. """"""
 
         # Start with a default state.
         self.states_data = States()
         self.state = self.states_data.Initialization
 
     def on_event(self, event):
         """"""
def on_event(self, event):
         #
 
         # The next state will be the result of the on_event function.
         self.state = self.state.on_event(event=event, states_data=self.states_data)
 
 
 "
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" class InitializationBody(object):
 
     def __init__(self,):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
         self.status = None
 
     def action(self):"
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" class InitializationBody(object):
     def __init__(self,):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
 
         self.status = None
 
     def action(self):"
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 class CloseProgramBody(object):
 
     def __init__(self, info):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
         self.info = info
 
     def action(self):
         print(self)"
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;" 
 class CloseProgramBody(object):
 
     def __init__(self):
         """"""
         We define a state object which provides some utility functions for the
         individual states within the state machine.
         """"""
         self.info = ""info""
 
     def action(self):
         print(self)"
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
         self.status = None
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
         while self.counter < 5:
             self.counter += 1
             print(self.counter)
             sleep(0.5)
         self.status = ""GO TO TEST2""
 
     def __repr__(self):
         """"""
def __str__(self):
         return self.__class__.__name__
 
 "
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
         self.status = ""GO TO TEST1""
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
         self.counter += 1
         print(self.counter)
         sleep(0.5)
         if self.counter >= 3:
             self.status = ""GO TO TEST2""
 
     def __repr__(self):
         """"""
def __str__(self):
         return self.__class__.__name__
 
 
 "
KO;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
         self.status = None
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
         while self.counter < 5:
             self.counter += 1
             print(self.counter)
             sleep(0.5)
         self.status = ""GO TO TEST1""
 
     def __repr__(self):
         """""""
OK;12.0;a5892731;state_machine;a103350c897039c2831227f981e87d6069a3af6f;add States data memory class;"def __init__(self,):
         individual states within the state machine.
         """"""
         self.counter = 0
         self.status = ""GO TO TEST2""
 
     def action(self):
         print(self)
         self.state_loop()
 
     def state_loop(self):
         self.counter += 1
         print(self.counter)
         sleep(0.5)
         if self.counter >= 3:
             self.status = ""GO TO TEST1""
 
     def __repr__(self):
         """""""
KO;13.0;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
 				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
 				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
 				if(starget < etarget){
 					max_token=mid;
 				}"
OK;13.0;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;"__kernel void GMEMD_gradient(__global float *data, __global float *diff, __globa
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
 				tx=x+list_x[max_token];
 				ty=y+list_y[max_token];
 				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
 					starget=data[ty*width+tx];
 				}
 				else starget=0;
 				
 				tx=x+list_x[mid];
 				ty=y+list_y[mid];
 				if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
 					etarget=data[ty*width+tx];
 				}
 				else etarget=0;
 				
 				if(starget < etarget){
 					max_token=mid;
 				}"
KO;13.0;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;
OK;13.0;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;" 
 __kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
 						__global int *list_x, __global int *list_y, __global float *list_deg,
 						int list_len, int width, int height) {
 	//kernel index
 	int x=get_global_id(0); //x
 	int y=get_global_id(1); //y
 
 	if(x<height && y<width){
 		//init variable
 		float pos_avg=0,neg_avg=0;
 		int pos_count=0,neg_count=0,weight=0,start,end,max_weight=0,max_token;
 		int tx,ty;
 		float current,target;
 		current=data[y*width+x];
 		start=list_len*3/4;
 		end=list_len/4;
 		
 		//init direct weight
 		for(int i=0;i<end;++i){
 			tx=x+list_x[i];
 			ty=y+list_y[i];
 			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				target=data[ty*width+tx];
 				if(target > current) ++weight;
 			}
 		}
 		for(int i=start;i<list_len;++i){
 			tx=x+list_x[i];
 			ty=y+list_y[i];
 			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				target=data[ty*width+tx];
 				if(target > current) ++weight;
 			}
 		}
 		
 		//calc direct
 		for(int i=0;i<list_len;++i){
 			start=(start+1)%list_len;
 			end=(end+1)%list_len;
 			tx=x+list_x[start];
 			ty=y+list_y[start];
 			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				target=data[ty*width+tx];
 				if(target > current){ --weight; }
 			}
 			tx=x+list_x[end];
 			ty=y+list_y[end];
 			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				target=data[ty*width+tx];
 				if(target > current){ ++weight; }
 			}
 			//if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				if(weight>max_weight){
 					max_token=i;
 					max_weight=weight;
 				}
 				else if(weight==max_weight){
 					target=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
 					if(target < current){
 						max_token=i;
 					}
 				}
 			//}
 			
 		}
 		
 		//calc diff
 		for(int i=0;i<list_len;++i){
 			tx=x+list_x[i];
 			ty=y+list_y[i];
 			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				target=data[ty*width+tx];
 				if(target > current){
 					pos_avg+=target;
 					++pos_count;
 				}
 				else{
 					neg_avg+=target;
 					++neg_count;
 				}
 			}
 		}
 		
 		//diff finish
 		if(pos_count){ pos_avg/=(float)pos_count; }
 		else{ pos_avg=current; }
 		if(neg_count){ neg_avg/=(float)neg_count; }
 		else{ neg_avg=current; }
 		diff[y*width+x]=pos_avg-neg_avg;
 	
 		//direct finish
 		direct[y*width+x]=list_deg[max_token];
 	}
 }
 
 
 __kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
                         __global int *list_x, __global int *list_y, __global float *list_deg,
 						int list_len, int width, int height) {
 	
 	//kernel index
 	int x=get_global_id(0); //x
 	int y=get_global_id(1); //y
 	
 	if(x<height && y<width){
 		int tx,ty;
 		result[y*width+x]=0;
 		for(int i=0;i<list_len;++i){
 			tx=x+list_x[i];
 			ty=y+list_y[i];
 			if( (tx>=0 && tx<height) && (ty>=0 && ty<width) ){
 				result[y*width+x]-=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
 			}
 		}
 	}
 }
 "
OK;13.0;CardLin;SFEGO_PyOpenCL;f0bf25c13ac53a1c68749ea44f4f90eeb901279f;fix kernel memory violation and upload old kernel;" 
 __kernel void GMEMD_gradient(__global float *data, __global float *diff, __global float *direct,
 						__global int *list_x, __global int *list_y, __global float *list_deg,
 						int list_len, int width, int height) {
 	//kernel index
 	int x=get_global_id(0); //x
 	int y=get_global_id(1); //y
 
 	if(x<width && y<height){
 		//init variable
 		float pos_avg=0,neg_avg=0;
 		int pos_count=0,neg_count=0,weight=0,start,end,mid,max_weight=0,max_token,dist_token;
 		int tx,ty,otx,oty,stx,sty,etx,ety;
 		float current,target,otarget,starget,etarget;
 		
 		//mid point
 		current=data[y*width+x];
 
 		//init direction (sliding windows, init weight)
 		start=0;
 		end=list_len/2+1;
 		mid=end/2+1;
 		weight=0;
 		
 		max_token=mid;
 		max_weight=weight;	//weight can be negative, just searching for largest weight
 		
 		//calc direct
 		for(int i=0;i<list_len;++i){
 			stx=x+list_x[start];
 			sty=y+list_y[start];
 			etx=x+list_x[end];
 			ety=y+list_y[end];
 			
 			//check start target of sliding window
 			if( (stx>=0 && stx<width) && (sty>=0 && sty<height) ){
 				starget=data[sty*width+stx];
 			}
 			else starget=-1;
 			
 			//check end target of sliding window (assume it is opposite of start target)
 			if( (etx>=0 && etx<width) && (ety>=0 && ety<height) ){
 				etarget=data[ety*width+etx];
 			}
 			else etarget=-1;
 			
 			//compare start and end to the middle point
 			if(starget>current && etarget>current){
 				//both is larger than middle point
 				if(starget>etarget) --weight;
 				else ++weight;
 			}
 			else{
 				if(starget>current) --weight;
 				if(etarget>current) ++weight;
 			}
 			
 			//update max_weight
 			if(weight>max_weight){
 				max_token=mid;
 				max_weight=weight;
 			}
 			else if(weight==max_weight){
 				starget=data[(y+list_y[max_token])*width+(x+list_x[max_token])];
 				etarget=data[(y+list_y[mid])*width+(x+list_x[mid])];
 				if(starget < etarget){
 					max_token=mid;
 				}
 			}
 			
 			//move sliding window
 			start=(start+1)%list_len;
 			end=(end+1)%list_len;
 			mid=(mid+1)%list_len;
 		}
 		
 		//calculate diff (magnitude)
 		for(int i=0;i<list_len;++i){
 			tx=x+list_x[i];
 			ty=y+list_y[i];
 			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
 				target=data[ty*width+tx];
 				if(i>max_token) dist_token=i-max_token;
 				else dist_token=max_token-i;
 				if(dist_token>list_len/2) dist_token=list_len-dist_token;
 				
 				if(dist_token>list_len/4){
 					pos_avg+=target;
 					++pos_count;
 				}
 				else{
 					neg_avg+=target;
 					++neg_count;
 				}
 			}
 		}
 		
 		//finish diff (magnitude)
 		if(pos_count){ pos_avg/=(float)pos_count; }
 		else{ pos_avg=current; }
 		if(neg_count){ neg_avg/=(float)neg_count; }
 		else{ neg_avg=current; }
 		diff[y*width+x]=pos_avg-neg_avg;
 		
 		//direct finish
 		direct[y*width+x]=list_deg[max_token];
 	}
 }
 
 
 __kernel void GMEMD_integral(__global float *result, __global float *diff, __global float *direct,
                         __global int *list_x, __global int *list_y, __global float *list_deg,
 						int list_len, int width, int height) {
 	
 	//kernel index
 	int x=get_global_id(0); //x
 	int y=get_global_id(1); //y
 	
 	if(x<width && y<height){
 		int tx,ty;
 		result[y*width+x]=0;
 		for(int i=0;i<list_len;++i){
 			tx=x+list_x[i];
 			ty=y+list_y[i];
 			if( (tx>=0 && tx<width) && (ty>=0 && ty<height) ){
 				result[y*width+x]+=cos(direct[ty*width+tx]-list_deg[i])*diff[ty*width+tx];
 			}
 		}
 	}
 }
 "
KO;13.0;Boavizta;cloud-bill;cd5b941be98275fd0549d8abc4a5a460f713c4cc;fix meminfo total memory reader #1;" from typing import List
 
 from .model import MemoryDevice
def get_total_memory_in_kb() -> int:
             if 'MemTotal' in line:
                 mem_total_line = line.strip()
                 break
     total_size_kb = int(mem_total_line.split()[0])
     return total_size_kb
 
 "
OK;13.0;Boavizta;cloud-bill;cd5b941be98275fd0549d8abc4a5a460f713c4cc;fix meminfo total memory reader #1;" import re
 
 from typing import List
 
 from .model import MemoryDevice
def get_total_memory_in_kb() -> int:
             if 'MemTotal' in line:
                 mem_total_line = line.strip()
                 break
     total_size_kb = int(re.search(r'[0-9]+', mem_total_line)[0])
     return total_size_kb
 
 "
KO;14.0;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
         self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
OK;14.0;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;"def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
         self.seg_head = SegHead(classes, 1024, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)"
KO;14.0;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 # tf.keras.mixed_precision.set_global_policy('mixed_float16')
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
 mirrored_strategy = tf.distribute.MirroredStrategy()
 
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",

 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
                                                                                    ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,

 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
 args.add_argument(""--random_crop_height"", type=int, default=None,
                   help=""Height of random crop, random_crop_width must be given with this"")
 args.add_argument(""--random_crop_width"", type=int, default=None,
                   help=""Width of random crop, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
 args = args.parse_args()
 
 tf.random.set_seed(args.random_seed)
 random_crop_size = (args.random_crop_width, args.random_crop_height) \
     if args.random_crop_width is not None and args.random_crop_height is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset

 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
 logdir = os.path.join(args.save_dir, ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
                                                                                       batch_size,
                                                                                       optimizer_name, lr,
                                                                                       args.lr_scheduler,
                                                                                       backbone,
                                                                                       model_name,
                                                                                       time))
 
 # =========== Load Dataset ============ #
 

 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,

 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
 processed_train = dataset_train.map(get_images_processed)
 processed_train = processed_train.map(augmentor)
 processed_val = dataset_validation.map(get_images_processed)
 processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).prefetch(
     tf.data.experimental.AUTOTUNE)
 processed_val = processed_val.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)

         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
     model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True) if random_crop_size is None else model(tf.random.uniform((1, random_crop_size[0], random_crop_size[1], 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):

 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
         train_logits = model((mini_batch[0] / 127.5) - 1, training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
     return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
     val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
     return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
                # tf.concat(train_labs.values, axis=0), \
                # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
         processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
                             tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
 for epoch in range(START_EPOCH, EPOCHS):
     print(""\n ----------- Epoch {} --------------\n"".format(epoch))
     step = 0
     if epoch % args.save_interval == 0:
         model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
         print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
     for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
         c_step = (epoch * total_samples // args.batch_size) + step
         loss, train_labs, train_logits = distributed_train_step(mini_batch)
         step += 1
 
         # ======== mIoU calculation ==========
         mIoU.reset_states()
         gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
         pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
         mIoU.update_state(gt, pred)
         # ====================================
         # print(""Epoch {}: {}/{}, Loss: {}, mIoU: {}"".format(epoch, step * batch_size, total_samples,
         #                                                    loss.numpy(), mIoU.result().numpy()))
         write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
 
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
                         tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))"
OK;14.0;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 args = argparse.ArgumentParser(description=""Train a network with specific settings"")
 args.add_argument(""--backbone"", type=str, default="""",
                   help=""Backbone in case applicable"",

 args.add_argument(""-si"", ""--save_interval"", type=int, default=5, help=""Save interval for model"")
 args.add_argument(""-wis"", ""--write_image_summary_steps"", type=int, default=50, help=""Add images to tfrecords ""
 
                                                                                     ""after these many logging steps"")
 args.add_argument(""-m"", ""--model"", type=str, default=""bisenetv2"", help=""Select model"")
 args.add_argument(""-l_m"", ""--load_model"", type=str,
                   default=None,

 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=512, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 args.add_argument(""--fp16"", action=""store_true"", default=False, help=""Give to enable mixed precision training."")
 # ============ Augmentation Arguments ===================== #
 args.add_argument(""--flip_up_down"", action=""store_true"", default=False, help=""Randomly flip images up and down"")
 args.add_argument(""--flip_left_right"", action=""store_true"", default=False, help=""Randomly flip images right left"")
 args.add_argument(""--random_crop_min"", type=float, default=None,
                   help=""minimum value for crop height/width relative to original image"")
 args.add_argument(""--random_crop_max"", type=float, default=None,
                   help=""Width of random crop as ratio of original width, random_crop_height must be given with this"")
 args.add_argument(""--random_hue"", action=""store_true"", default=False, help=""Randomly change hue"")
 args.add_argument(""--random_saturation"", action=""store_true"", default=False, help=""Randomly change saturation"")
 args.add_argument(""--random_brightness"", action=""store_true"", default=False, help=""Randomly change brightness"")
 args.add_argument(""--random_contrast"", action=""store_true"", default=False, help=""Randomly change contrast"")
 args.add_argument(""--random_quality"", action=""store_true"", default=False, help=""Randomly change jpeg quality"")
 args.add_argument(""--all_augs"", action=""store_true"", default=False, help=""Add all augmentations except flip_up_down"")
 args = args.parse_args()
 
 if args.fp16:
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
 
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)
 mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())
 
 tf.random.set_seed(args.random_seed)
 random_crop_size = (args.random_crop_min, args.random_crop_max) \
     if args.random_crop_max is not None and args.random_crop_min is not None \
     else None
 backbone = args.backbone
 dataset_name = args.dataset

 EPOCHS = args.epochs
 time = str(datetime.datetime.now())
 time = time.translate(str.maketrans('', '', string.punctuation)).replace("" "", ""-"")[:-8]
 logdir = os.path.join(args.save_dir,
                       ""{}_epochs-{}_{}_bs-{}_{}_lr_{}-{}_{}_{}_{}"".format(dataset_name, epochs, args.loss,
                                                                           batch_size,
                                                                           optimizer_name, lr,
                                                                           args.lr_scheduler,
                                                                           backbone,
                                                                           model_name,
                                                                           time))
 
 # =========== Load Dataset ============ #
 

 dataset_validation = TFRecordsSeg(
     tfrecord_path=
     ""{}/{}_val.tfrecords"".format(args.tf_record_path, dataset_name)).read_tfrecords()
 if args.all_augs:
     args.flip_left_right = True
     random_crop_size = (0.5, 0.95)
     args.random_hue = True
     args.random_saturation = True
     args.random_brightness = True
     args.random_contrast = True
 
 augmentor = lambda image, label: aug.augment_seg(image, label,
                                                  args.flip_up_down,
                                                  args.flip_left_right,

 eval_dataset = dataset_validation
 get_images_processed = lambda image, label: get_images_custom(image, label, (args.height, args.width), cs_19)
 
 processed_train = dataset_train.map(augmentor)
 processed_train = processed_train.map(get_images_processed)
 processed_val = dataset_validation.map(get_images_processed)
 processed_train = processed_train.shuffle(args.shuffle_buffer).batch(batch_size, drop_remainder=True).repeat(
     EPOCHS).prefetch(
     tf.data.experimental.AUTOTUNE)
 processed_val = processed_val.batch(batch_size, drop_remainder=True) \
     if (dataset_validation is not None) else None
 processed_train = mirrored_strategy.experimental_distribute_dataset(processed_train)
 processed_val = mirrored_strategy.experimental_distribute_dataset(processed_val)

         optimizer = K.optimizers.SGD(learning_rate=lr_scheduler, momentum=momentum)
     model = get_model(model_name, classes=classes, in_size=(args.height, args.width), aux=aux,
                       backbone=args.backbone)
     model(tf.random.uniform((1, args.height, args.width, 3), dtype=tf.float32), True)
     model.summary()
     if args.load_model:
         if os.path.exists(os.path.join(args.load_model)):

 
 def train_step(mini_batch, aux=False, pick=None):
     with tf.GradientTape() as tape:
         train_logits = model(tf.image.per_image_standardization(mini_batch[0]), training=True)
         train_labs = tf.one_hot(mini_batch[1][..., 0], classes)
         if aux:
             losses = [tf.reduce_mean(calc_loss(train_labs, tf.image.resize(train_logit, size=train_labs.shape[
def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
     return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3],
                                              method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
     val_logits = model(tf.image.per_image_standardization(mini_batch[0]),
                        training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
def val_step(mini_batch, aux=False):
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
     return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3],
                                                method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
def write_summary_images(batch, logits):
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
         processed_labs = tf.concat(batch[1].values[0], axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]
def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                 conf_matrix = tf.math.confusion_matrix(gt, pred,
                                                        num_classes=classes)
                 conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
                         tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
                 tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=curr_step)
                 write_summary_images(batch, logits)
     with writer.as_default():
         tmp = lr_scheduler(step=curr_step)
         tf.summary.scalar(""Learning Rate"", tmp, curr_step)
 
 
 def evaluate():
     global val_writer
     mIoU.reset_states()
     conf_matrix_list = []
     total_val_loss = []
def write_to_tensorboard(curr_step, image_write_step, writer, logits, batch):
                           step=c_step)
         if val_mini_batch is not None:
             conf_matrix = tf.cast(conf_matrix, dtype=tf.float64) / (
                     tf.cast(tf.reduce_sum(conf_matrix, axis=1), dtype=tf.float64) + 1e-6)
             tf.summary.image(""conf_matrix"", conf_matrix[tf.newaxis, ..., tf.newaxis], step=c_step)
             write_summary_images(val_mini_batch, val_logits)
     print(""Val Epoch {}: {}, mIoU: {}"".format(epoch, val_loss, mIoU.result().numpy()))
 
 
 epoch = 0
 while epoch < EPOCHS:
     print(""\n ----------- Epoch {} --------------\n"".format(epoch))
     step = 0
     if epoch % args.save_interval == 0:
         model.save_weights(os.path.join(logdir, model_name, str(epoch), ""saved_model""))
         print(""Model at Epoch {}, saved at {}"".format(epoch, os.path.join(logdir, model_name, str(epoch))))
     for mini_batch in tqdm.tqdm(processed_train, total=total_samples // args.batch_size):
         c_step = (epoch * total_samples // args.batch_size) + step
         loss, train_labs, train_logits = distributed_train_step(mini_batch)
         step += 1
 
         # ======== mIoU calculation ==========
         mIoU.reset_states()
         gt = tf.reshape(tf.argmax(train_labs, axis=-1), -1)
         pred = tf.reshape(tf.argmax(train_logits, axis=-1), -1)
         mIoU.update_state(gt, pred)
         # ====================================
         write_to_tensorboard(c_step, image_write_step, train_writer, train_logits, mini_batch)
         if step == total_samples // args.batch_size:
             epoch += 1
             break
 
     evaluate()"
KO;14.0;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
                 crop=(256, 256),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
     if crop is not None:
         image_crop = list(crop) + [image.shape[-1]]
         label_crop = list(crop) + [label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
OK;14.0;AhmedBadar512;TF2-Segmentation;2d65af954e15da38104959a04efd223e33fdade1;Fix memory leakage for multi-gpu, improve memory usage and add mixed precision support;" def augment_seg(image, label,
                 v_flip=False,
                 h_flip=False,
                 crop_scale=(0.05, 0.95),
                 rand_hue=False,
                 rand_sat=False,
                 rand_brightness=False,
def augment_seg(image, label,
     if v_flip:
         image = tf.image.random_flip_up_down(image, seed=0)
         label = tf.image.random_flip_up_down(label, seed=0)
     if crop_scale is not None:
         img_shp = tf.cast(tf.shape(image), tf.float32)
         h_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[0]
         w_scale = tf.random.uniform([], crop_scale[0], crop_scale[1]) * img_shp[1]
         image_crop = [h_scale, w_scale, image.shape[-1]]
         label_crop = [h_scale, w_scale, label.shape[-1]]
         image = tf.image.random_crop(image, image_crop, seed=0)
         label = tf.image.random_crop(label, label_crop, seed=0)
     if rand_brightness:"
KO;14.0;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
         # b = tf.image.resize(y_b * x_b, tf.shape(a)[1:3])
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
     def __init__(self, mid_channels, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
 
 
 class BiSeNetv2(K.Model):
     def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=64, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
         self.seg_head = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         self.seg_head1 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         self.seg_head2 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         self.seg_head3 = K.Sequential([ConvBlock(seg_channels, 3, padding='same'), K.layers.Conv2D(classes, 1, padding='same')])
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
     def call(self, inputs, training=None, mask=None):
         original_size = tf.shape(inputs)[1:3]
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
         x2_s5 = self.ge3(x2_s4)                     # Stride /32
         x2_ce = self.ce(x2_s5)
 
         final_feat = self.seg_head(tf.image.resize(self.aggregator((x1_s3, x2_ce)), original_size))
         if training:
             out_s3 = tf.image.resize(self.seg_head1(x2_s3), original_size, method=tf.image.ResizeMethod.BILINEAR)
             out_s4 = tf.image.resize(self.seg_head2(x2_s4), original_size, method=tf.image.ResizeMethod.BILINEAR)
             out_s5 = tf.image.resize(self.seg_head3(x2_s5), original_size, method=tf.image.ResizeMethod.BILINEAR)
             return final_feat, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
     bs = 7
     x = tf.random.normal((bs, 512, 1024, 3))
     bisenet = BiSeNetv2(classes=19)
     optimizer = K.optimizers.SGD()
     for _ in tqdm.tqdm(range(1000)):
         with tf.GradientTape() as tape:
             final, z1, z2, z3 = bisenet(x, True)
             # final = bisenet(x, False)
             loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
         vars = bisenet.trainable_variables
         optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file
\ No newline at end of file"
OK;14.0;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def call(self, inputs, *args, **kwargs):
         y_b = tf.nn.sigmoid(self.semantic_1x1conv_b(y_b))
 
         a = y_a * x_a
         b = self.upsample(y_b * x_b)
         return self.final_conv(a + b)
 
 
 class SegHead(K.layers.Layer):
     def __init__(self, n_classes=19, mid_channels=64, upfactor=8, aux=True):
         super(SegHead, self).__init__()
         self.conv = ConvBlock(mid_channels, 3, 1, padding='same')
         self.drop = K.layers.Dropout(0.1)
         self.upfactor = upfactor
         self.aux = aux
         if self.aux:
             self.conv_aux = K.Sequential([
                 K.layers.UpSampling2D(2),
                 ConvBlock(upfactor * upfactor, 3, 1, padding='same'),
             ])
             upfactor //= 2
         self.conv_final = K.layers.Conv2D(n_classes, 1, 1)
         # self.upsample = K.layers.UpSampling2D(upfactor, interpolation=""bilinear"")
 
     def call(self, inputs, **kwargs):
         x = self.conv(inputs)
         x = self.drop(x)
         if self.aux:
             x = self.conv_aux(x)
         return self.conv_final(x)
         # else:
         #     return self.upsample(self.conv_final(x))
 
 
 class BiSeNetv2(K.Model):
     def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_channels=128, **kwargs):
         super(BiSeNetv2, self).__init__()
         self.backbone = backbone
         # =========== Detail Branch =========== #
def __init__(self, classes=21, activation='relu', backbone=None, alpha=1.0, seg_
         ])
         self.ce = ContextEmbeddingBlock()
         # =========== Segmentation Head =========== #
         self.seg_head = SegHead(classes, seg_channels, 8, aux=False)
         self.aux_head1 = SegHead(classes, seg_channels, 4, aux=True)
         self.aux_head2 = SegHead(classes, seg_channels, 8, aux=True)
         self.aux_head3 = SegHead(classes, seg_channels, 16, aux=True)
         self.aux_head4 = SegHead(classes, seg_channels, 32, aux=True)
         # ========== Aggregation Head ============ #
         self.aggregator = Aggregator()
 
     def call(self, inputs, training=True, mask=None):
         # ========= Detail ============ #
         x1_s1 = self.detail_convblock1(inputs)         # Stride /2
         x1_s2 = self.detail_convblock2(x1_s1)             # Stride /4
def call(self, inputs, training=None, mask=None):
         x2_s2 = self.stem(inputs)                   # Stride /4
         x2_s3 = self.ge1(x2_s2)                     # Stride /8
         x2_s4 = self.ge2(x2_s3)                     # Stride /16
         x2_s5 = self.ge3(x2_s4)                     # Stride /32z
         x2_ce = self.ce(x2_s5)
 
         final_feat = self.aggregator((x1_s3, x2_ce))
         final_feat = self.seg_head(final_feat)
         if training:
             out_s2 = self.aux_head1(x2_s2)
             out_s3 = self.aux_head2(x2_s3)
             out_s4 = self.aux_head3(x2_s3)
             out_s5 = self.aux_head4(x2_s5)
             return final_feat, out_s2, out_s3, out_s4, out_s5
         return final_feat
 
 if __name__ == ""__main__"":
     import tqdm
 
     physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
     for gpu in physical_devices:
         tf.config.experimental.set_memory_growth(gpu, True)
     tf.keras.mixed_precision.set_global_policy('mixed_float16')
     bs = 14
     x = tf.random.normal((bs, 1024, 2048, 3))
     bisenet = BiSeNetv2(classes=19)
     # bisenet.build((bs, 1024, 2048, 3))
     optimizer = K.optimizers.SGD()
\ No newline at end of file
     final, z1, z2, z3 = bisenet(x, True)
     bisenet.summary()
     # for _ in tqdm.tqdm(range(1000)):
     #     final, z1, z2, z3 = bisenet(x, True)
         # with tf.GradientTape() as tape:
         #     final, z1, z2, z3 = bisenet(x, True)
         #     # final = bisenet(x, False)
         #     loss = 100 * tf.reduce_mean(tf.cast(final, tf.float32) - tf.random.normal((bs, 512, 1024, 19)))
         # vars = bisenet.trainable_variables
         # optimizer.apply_gradients(zip(tape.gradient(loss, vars), vars))
\ No newline at end of file"
KO;14.0;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
                  sn=False,
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
         if sn:
             self.conv2d = tfa.layers.SpectralNormalization(self.conv2d)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         elif norm_layer == 'instance':
             self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 
def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         elif norm_layer == 'instance':
             self.normalization = tfa.layers.InstanceNormalization()
         else:
             self.normalization = tf.identity
 "
OK;14.0;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;"def __init__(self,
                  use_bias=True,
                  norm_layer=""batch"",
                  activation='linear',
                  dilation_rate=(1, 1),
                  **kwargs):
         super(ConvBlock, self).__init__()
def __init__(self,
                                       dilation_rate=dilation_rate,
                                       use_bias=use_bias,
                                       **kwargs)
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 
def __init__(self,
         self.activation = K.layers.Activation(activation)
         if norm_layer == 'batch':
             self.normalization = K.layers.BatchNormalization()
         else:
             self.normalization = tf.identity
 "
KO;14.0;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)

 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
 args.add_argument(""--lr"", type=float, default=1e-5, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",

 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.25, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
     return loss, train_labs, train_logits
 
 
 def val_step(mini_batch, aux=False):
     val_logits = model((mini_batch[0] / 127.5) - 1, training=False) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=False)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
     # if aux:
     #     losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
     #                                                                                    1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
     #         calc_loss(
     #             val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
     #               enumerate(val_logits)]
     #     val_loss = tf.reduce_sum(losses)
     #     val_logits = val_logits[0]
     # else:
     val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
     return val_loss, val_labs, val_logits
 
 
 @tf.function
def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
         tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         processed_labs = tf.concat(batch[1].values, axis=0)
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
OK;14.0;AhmedBadar512;TF2-Segmentation;4d6da98a777c30dae6839317b092b9270b5eb7f4;Improve memory usage efficency;" from utils.create_seg_tfrecords import TFRecordsSeg
 from visualization_dicts import gpu_cs_labels, generate_random_colors, gpu_random_labels
 
 # tf.keras.mixed_precision.set_global_policy('mixed_float16')
 physical_devices = tf.config.experimental.list_physical_devices(""GPU"")
 for gpu in physical_devices:
     tf.config.experimental.set_memory_growth(gpu, True)

 args.add_argument(""-lrs"", ""--lr_scheduler"", type=str, default=""exp_decay"", help=""Select learning rate scheduler"",
                   choices=[""poly"", ""exp_decay""])
 args.add_argument(""-e"", ""--epochs"", type=int, default=100, help=""Number of epochs to train"")
 args.add_argument(""--lr"", type=float, default=1e-3, help=""Initial learning rate"")
 args.add_argument(""--momentum"", type=float, default=0.9, help=""Momentum"")
 args.add_argument(""-l"", ""--logging_freq"", type=int, default=10, help=""Add to tfrecords after this many steps"")
 args.add_argument(""--loss"", type=str, default=""cross_entropy"",

 args.add_argument(""--width"", type=int, default=1024, help=""Size of the shuffle buffer"")
 args.add_argument(""--height"", type=int, default=512, help=""Size of the shuffle buffer"")
 args.add_argument(""--aux"", action=""store_true"", default=False, help=""Auxiliary losses included if true"")
 args.add_argument(""--aux_weight"", type=float, default=0.2, help=""Auxiliary losses included if true"")
 args.add_argument(""--random_seed"", type=int, default=1, help=""Set random seed to this if true"")
 args.add_argument(""--bg_class"", type=int, default=0, help=""Select bg class for visualization shown as black"")
 # ============ Augmentation Arguments ===================== #
def train_step(mini_batch, aux=False, pick=None):
         trainable_vars = model.trainable_variables
     grads = tape.gradient(loss, trainable_vars)
     optimizer.apply_gradients(zip(grads, trainable_vars))
     return loss, train_labs, tf.image.resize(train_logits, tf.shape(train_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 def val_step(mini_batch, aux=False):
     val_logits = model((mini_batch[0] / 127.5) - 1, training=True) if random_crop_size is None else model((tf.image.resize(mini_batch[0], random_crop_size) / 127.5) - 1, training=True)
     val_labs = tf.one_hot(mini_batch[1][..., 0], classes)
     if random_crop_size is not None:
         val_labs = tf.image.resize(val_labs, random_crop_size)
     if aux:
         losses = [tf.reduce_mean(calc_loss(val_labs, tf.image.resize(train_logit, size=val_labs.shape[
                                                                                        1:3]))) if n == 0 else args.aux_weight * tf.reduce_mean(
             calc_loss(
                 val_labs, tf.image.resize(train_logit, size=val_labs.shape[1:3]))) for n, train_logit in
                   enumerate(val_logits)]
         val_loss = tf.reduce_sum(losses)
         val_logits = val_logits[0]
     else:
         val_loss = calc_loss(val_labs, val_logits)
     val_loss = tf.reduce_mean(val_loss)
     return val_loss, val_labs, tf.image.resize(val_logits, tf.shape(val_labs)[1:3], method=tf.image.ResizeMethod.BILINEAR)
 
 
 @tf.function
def distributed_train_step(dist_inputs):
         return loss, \
                tf.concat(train_labs.values, axis=0), \
                tf.concat(train_logits.values, axis=0)
                # tf.concat(train_labs.values, axis=0), \
                # tf.concat(train_logits.values, axis=0)
     else:
         return loss, \
                train_labs, \
def distributed_val_step(dist_inputs):
 
 def write_summary_images(batch, logits):
     if len(physical_devices) > 1:
         # tf.summary.image(""images"", tf.concat(batch[0].values, axis=0) / 255, step=c_step)
         # processed_labs = tf.concat(batch[1].values, axis=0)
         tf.summary.image(""images"", batch[0].values[0] / 255, step=c_step)
         processed_labs = batch[1].values[0]
     else:
         tf.summary.image(""images"", batch[0] / 255, step=c_step)
         processed_labs = batch[1]"
KO;14.0;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
 systemdan/__main__.spec
\ No newline at end of file
\ No newline at end of file"
OK;14.0;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"tests/__pycache__/
 systemdan/__pycache__/
 systemdan/build/
 systemdan/dist/
\ No newline at end of file
 systemdan/__main__.spec
 systemdan/command
 systemdan/commandlinux
 systemdan/distlinux/
 __main__.spex
\ No newline at end of file"
KO;14.0;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
OK;14.0;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;"def info() -> None:
     typer.echo(f'CPU list: {cpu_info[""cpu_list""]}')
     typer.echo(f'CPU percent: {cpu_info[""cpu_percent""]}')
 
     typer.echo(typer.style(f'\nMemory\n', fg=typer.colors.BLUE))
     mem_info = system_info.get_memory_info()
     typer.echo(f'Total memory: {mem_info[""total""]}')
     typer.echo(f'Available memory: {mem_info[""available""]}')
     typer.echo(f'Used memory: {mem_info[""used""]}')
 
     typer.echo(f'Swap Percentage: {mem_info[""percentage""]}')
     typer.echo(f'Swap total: {mem_info[""swap_total""]}')
     typer.echo(f'Swap free: {mem_info[""swap_free""]}')
     typer.echo(f'Swap used: {mem_info[""swap_used""]}')
     typer.echo(f'Swap percentage: {mem_info[""swap_percentage""]}')
 
 
 @app.command(name='os')
 def os_info(vars: bool = False,"
KO;14.0;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result"
OK;14.0;motahharm;systemdan;57970fa92ee29c241da27fa9d73490a5c0de5edc;add memory info;" 
 uname = platform.uname()
 
 def get_size(bytes, suffix=""B""):
     """"""
     e.g:
         1253656 => '1.20MB'
         1253656678 => '1.17GB'
     """"""
     factor = 1024
     for unit in ["""", ""K"", ""M"", ""G"", ""T"", ""P""]:
         if bytes < factor:
             return f""{bytes:.2f}{unit}{suffix}""
         bytes /= factor
 
 def get_system_name() -> str:
     return uname.system
 def get_node_name() -> str:
def get_cpu_info() -> dict:
     result['cpu_percent'] = psutil.cpu_percent()
 
     return result
 
 def get_memory_info() -> dict:
     """"""Get the memory information""""""
     result = {}
     svmem = psutil.virtual_memory()
     result['total'] = get_size(svmem.total)
     result['available'] = get_size(svmem.available)
     result['used'] = get_size(svmem.used)
     result['percentage'] = get_size(svmem.free)
     swap = psutil.swap_memory()
     result['swap_total'] = get_size(swap.total)
     result['swap_free'] = get_size(swap.free)
     result['swap_used'] = get_size(swap.used)
     result['swap_percentage'] = swap.percent
 
     return result"
KO;15.0;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
OK;15.0;ktiyab;pulsar;fdd84f603166db907350921e0f6d69a6742fbd2a;Update process to save function memory into BigQuery for analyze capabilities;"locals {
   CONTEXT = <<-EOT
 APP_NAME = ""${var.PULSAR_NAME}""
 RUNTIME = ""${var.PULSAR_RUNTIME}""
 MEMORY = ""${var.PULSAR_MEMORY}""
 PROJECT_ID = ""${var.PROJECT_ID}""
 REGION = ""${var.PULSAR_REGION}""
 SERVICE_ACCOUNT_EMAIL = ""${var.SERVICE_ACCOUNT_EMAIL}"""
KO;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
         self.state = None
         self.alert_level = ""false""
         self.owners = None
def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
OK;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"def __init__(self):
         self.app = deployment_context.APP_NAME
         self.service_account = deployment_context.SERVICE_ACCOUNT_EMAIL
         self.runtime = deployment_context.RUNTIME
         self.memory = deployment_context.MEMORY
         self.state = None
         self.alert_level = ""false""
         self.owners = None
def to_dict(self):
             ""region"": str(self.region) if self.region else """",
             ""service_account"": str(self.service_account) if self.service_account else """",
             ""runtime"": str(self.runtime) if self.runtime else """",
             ""memory"": str(self.memory) if self.memory else """",
             ""alert_level"": str(self.alert_level) if self.alert_level else """",
             ""owners"": str(self.owners) if self.owners else """",
             ""parameters"": str(self.parameters) if self.parameters else """",
def load(self, run_context):
             self.task.region = run_context[app_configs.REGION_KEY]
             self.task.service_account = run_context[app_configs.SERVICE_ACCOUNT_KEY]
             self.task.runtime = deployment_context.RUNTIME
             self.task.memory = deployment_context.MEMORY
 
             # Check if context (allowed Project and region) is valid
             is_valid_context, check_context_message = self.is_valid_context(run_context)
def load_proto_payload(self, proto_payload, gcp_context, event_id):
             self.task.description = app_configs.TRIGGERED_DESCRIPTION
             self.task.app = deployment_context.APP_NAME
             self.task.runtime = deployment_context.RUNTIME
             self.task.memory = deployment_context.MEMORY
             self.task.project_id = gcp_context[app_configs.PROJECT_ID_KEY]
             self.task.region = gcp_context[app_configs.REGION_KEY]
             self.task.service_account = gcp_context[app_configs.SERVICE_ACCOUNT_KEY]"
KO;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
 PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
OK;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_INTERRUPTED_TABLE_NAME=""interrupted""
 PULSAR_INTERRUPTED_TABLE_DESCRIPTION=""The ${PULSAR_NAME} terminated tasks table.""
 
 PULSAR_DATASET_DESCRIPTION=""${PULSAR_NAME} analytical logs.""
 PULSAR_TASK_SCHEMA=""id:STRING,name:STRING,description:STRING,state:STRING,app:STRING,project_id:STRING,region:STRING,service_account:STRING,runtime:STRING,memory:STRING,alert_level:STRING,owners:STRING,parameters:STRING,acknowledge_timestamp:STRING,processed_timestamp:STRING,success:STRING,details:STRING"""
KO;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
OK;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"then
   # Set Context information
   echo ""APP_NAME = \""$PULSAR_NAME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""RUNTIME = \""$PULSAR_RUNTIME\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""MEMORY = \""$PULSAR_MEMORY\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""PROJECT_ID = \""$PROJECT_ID\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""REGION = \""$REGION\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH""
   echo ""SERVICE_ACCOUNT_EMAIL = \""$SERVICE_ACCOUNT_EMAIL\"""" >> ..""$PULSAR_CONTEXT_PY_ROOT_PATH"""
KO;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
OK;15.0;ktiyab;pulsar;5404aa8a9d2c06a8ac1fad3ed8f4238914511d9a;Update process to save function memory into BigQuery for analyze capabilities;"PULSAR_TASK_SCHEMA= <<EOF
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
   {
     ""name"": ""memory"",
     ""type"": ""STRING"",
     ""mode"": ""NULLABLE""
   },
   {
     ""name"": ""alert_level"",
     ""type"": ""STRING"","
KO;17.0;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;" import create from 'zustand'
 
 
 export interface Tokens {
   access: string;
interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
 const useStore = create<State>(set => ({
     tokens: {access: """", refresh: """"},
     setTokens: (tokens) => set({tokens})
 }))
 
 export default useStore;
\ No newline at end of file"
OK;17.0;taraldga;ladhub;5fa06a88812ada5f02ef48dbc844e1fabb68454d;feat: update usestore to use localstorage instead of in memory;" import create from ""zustand"";
 
 export interface Tokens {
   access: string;
interface State {
   setTokens: (tokens: Tokens) => void;
 }
 
 const getLocalStorage = (key: string) =>
   JSON.parse(window.localStorage.getItem(key) as string);
 const setLocalStorage = (key: string, value: any) =>
   window.localStorage.setItem(key, JSON.stringify(value));
 
 const useStore = create<State>((set) => ({
   tokens: getLocalStorage(""tokens"") || { access: """", refresh: """" },
   setTokens: (tokens) =>
     set(() => {
       setLocalStorage(""tokens"", tokens);
       return { tokens };
     }),
 }));
 
 
\ No newline at end of file
 export default useStore;"
KO;18.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;18.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
 
 .idea/
\ No newline at end of file"
KO;18.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;18.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;" import pygame
 from pygame.locals import (
     K_UP,
     K_DOWN,
     K_LEFT,
     K_RIGHT,
     K_ESCAPE,
     KEYDOWN,
     QUIT,
 )
 mat = [
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0]
 ]
 def placematrix(i,j):
   #insx = round(x//100)
   #insy = round(y//100)
   mat[i][j] = 1
 
 def print_matrix(matrix):
   for i in matrix:
     print(*i)
   print()
 pygame.init()
 
 def isStop(i,j):
     return i >= 5 or mat[i + 1][j] == 1
 def contgame(i,j):
     placematrix(i,j)
     print_matrix(mat)
     #shuld summon new block
 
 screen = pygame.display.set_mode([600, 600])
 i = 0
 j = 2
 running = True
 x = 200
 y = 100
 z = 0
 while running:
     for event in pygame.event.get():
         if event.type == KEYDOWN:
             if event.key == K_ESCAPE:
                 running = False
             if event.key == K_LEFT:
                 if j >= 0:
                     #x = x - 100
                     j = j - 1
             if event.key == K_RIGHT:
                 if j <= 4:
                     #x = x + 100
                     j = j + 1
     screen.fill((0, 0, 0))
 
     pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
     if i < 9:
         if z == 500:
             if isStop(i,j):
                 contgame(i,j)
                 running = False
             #y = y + 100
             i = i + 1
             z = 0
         z = z + 1
 
     pygame.display.flip()
 
 pygame.quit()
\ No newline at end of file"
KO;18.0;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
 
 import torch
 import torchvision.transforms as T
def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
         return self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
             single_layer_size = (json_data['layer_width'], json_data['layer_height'])
             asset_image_path = str(self.data_folder / artboard['layerassets'])
             asset_image_rgb = Image.open(asset_image_path).convert('RGB')
             asset_image_tensor = self.img_transform(asset_image_rgb)
             images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
             names = []
             bboxes = []
             colors = []
def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
             data.append((images, names, bboxes, colors, classes, labels))
         return data
 
 "
OK;18.0;Borealin;sketch-model;0bbebc99f8948dcc9f3ee163da08b83ea4b8e7aa;[FIX] delay loading of image to save memory;" import json
 from pathlib import Path
 from typing import Any, Dict, List
 
 import torch
 import torchvision.transforms as T
def __init__(self, index_json_path: str, tokenizer: PreTrainedTokenizerBase):
             T.ToTensor(),
             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
         ])
         self.artboard_detail: List[Dict[str, Any]] = []
         self.data = self.load_data(tokenizer)
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
         artboard = self.index_json[idx]
         json_data = self.artboard_detail[idx]
         single_layer_size = (json_data['layer_width'], json_data['layer_height'])
         asset_image_path = str(self.data_folder / artboard['layerassets'])
         asset_image_rgb = Image.open(asset_image_path).convert('RGB')
         asset_image_tensor = self.img_transform(asset_image_rgb)
         images = torch.stack(asset_image_tensor.split(single_layer_size[1], dim=1))
         return images, *self.data[idx]
 
     def load_data(self, tokenizer: PreTrainedTokenizerBase):
         data = []
         for artboard in tqdm(self.index_json, desc='Loading Artboards'):
             json_path = self.data_folder / artboard['json']
             json_data = json.load(open(json_path, 'r'))
             self.artboard_detail.append(json_data)
             names = []
             bboxes = []
             colors = []
def load_data(self, tokenizer: PreTrainedTokenizerBase):
             colors = torch.as_tensor(colors, dtype=torch.float32)
             classes = torch.as_tensor(classes, dtype=torch.int64)
             labels = torch.as_tensor(labels, dtype=torch.int64)
             data.append((names, bboxes, colors, classes, labels))
         return data
 
 "
KO;19.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
\ No newline at end of file"
OK;19.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;"dmypy.json
 
 # Pyre type checker
 .pyre/
 
 .idea/
\ No newline at end of file"
KO;19.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;\ No newline at end of file
OK;19.0;gual14;CodingFestival-Tetris;fa8b2ec9d2cdbe9759e7eda79e99a05dabb9595e;rendering cube, adding movment and represent the movment on memory.;" import pygame
 from pygame.locals import (
     K_UP,
     K_DOWN,
     K_LEFT,
     K_RIGHT,
     K_ESCAPE,
     KEYDOWN,
     QUIT,
 )
 mat = [
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0],
   [0,0,0,0,0]
 ]
 def placematrix(i,j):
   #insx = round(x//100)
   #insy = round(y//100)
   mat[i][j] = 1
 
 def print_matrix(matrix):
   for i in matrix:
     print(*i)
   print()
 pygame.init()
 
 def isStop(i,j):
     return i >= 5 or mat[i + 1][j] == 1
 def contgame(i,j):
     placematrix(i,j)
     print_matrix(mat)
     #shuld summon new block
 
 screen = pygame.display.set_mode([600, 600])
 i = 0
 j = 2
 running = True
 x = 200
 y = 100
 z = 0
 while running:
     for event in pygame.event.get():
         if event.type == KEYDOWN:
             if event.key == K_ESCAPE:
                 running = False
             if event.key == K_LEFT:
                 if j >= 0:
                     #x = x - 100
                     j = j - 1
             if event.key == K_RIGHT:
                 if j <= 4:
                     #x = x + 100
                     j = j + 1
     screen.fill((0, 0, 0))
 
     pygame.draw.rect(screen, (255, 255, 255), pygame.Rect(j*100, i*100, 50, 50))
     if i < 9:
         if z == 500:
             if isStop(i,j):
                 contgame(i,j)
                 running = False
             #y = y + 100
             i = i + 1
             z = 0
         z = z + 1
 
     pygame.display.flip()
 
 pygame.quit()
\ No newline at end of file"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .netserialize import *
 from .dockernet import *
 from .dockershm import *
 from .dockernetserialize import *"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import os
 
 def get_data():
    testfile = ""airbnb.csv""
    datadir = ""/data/""
    currdir = os.path.dirname(os.path.abspath(__file__)) 
    f = open(currdir + datadir + testfile, ""r"")
    data = f.read()
    return data
 
 if __name__ == ""__main__"":
     get_data()"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
 print(f""recv reading from {__file__}..."")
 print(f""recv data: {contents}"")
 f.close()"
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
 print(f""recv: read from {__file__}"")
 f.close()"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
 print(f""send writing to shared mem from {__file__}..."")
 f.write(""a very simple message for recv"")
 f.close()"
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from data import get_data
 
 f = open(""test.txt"", ""w+"")
 print(f""send: writing to shared mem from {__file__}..."")
 f.write(get_data())
 f.close()"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_data_to_volume(volume): 
     data_script = ""data.py""
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     data_dir = curr_dir + ""/data""
     dst_dir = volume[""Mountpoint""]+f""/data""
     if os.path.exists(dst_dir):
         shutil.rmtree(dst_dir)
     shutil.copytree(data_dir, dst_dir)
     shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
     copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
 def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
     data = sendsock.recv(1024)
     print(f""send {data}"")
     
     sendsock.close()
 
def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
     sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 from data import get_data
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
 def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
 
     sendsock.sendall(b""my simple message"")
     
     # wait for confirmation
     recvdata = sendsock.recv(1024)
     print(f""send: {recvdata}"")
     
     sendsock.close()
 
def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
     sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
 def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
     print(""send writing to shared mem..."")
     f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
     print(f""recv data: {contents}"")
     f.close()
 
 def main():
def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
     sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;20.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
 from data import get_data
 
 def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
     print(""send: writing to shared mem..."")
     f.write(data)
     f.close()
 
     # wait until file is written
def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
     print(f""recv: received data"")
     f.close()
 
 def main():
def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
     data = get_data()
     sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
OK;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"ENV_FILE_PATH = ""./""
 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 DEFAULT_FLAG_RFI = True
 DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Parameters Checks
 AVAILABLE_STAT = [""SNR_XX"", ""SNR_YY"", ""RFIPercentage_XX""]"
KO;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
OK;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);" 
 # Flag strategy (*.rfis / *.lua) path
 FLAG_STRATEGY_FILE_PATH = ""./""
 DEFAULT_FLAG_RFI = True
 DEFAULT_FLAG_MEMORYPERC = 30 # not set via parameters
 
 # Send or not Slack messages in the #alerte-nickel-preprocessing channel
 SEND_SLACK_MESSAGE = True"
KO;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
     FLAG_STRATEGY_FILE_PATH
 )
 
 
def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
                     ""check_function"": self._check_avg_timestep,
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
                     ""check_function"": self._check_avg_freqstep,
                 },
                 ""startchan"": {
                     ""value"": None,
def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
                     ""check_function"": self._check_compress,
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
                     ""check_function"": self._check_flag_strategy,
                 }
             },
             ""quality"": {
def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
                     step_dict[key_lower][""value""] = value
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
OK;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"     AVERAGE_FREQSTEP_MIN,
     DEFAULT_AVERAGE_FREQSTEP,
     DEFAULT_STARTCHAN,
     FLAG_STRATEGY_FILE_PATH,
     DEFAULT_FLAG_RFI,
     DEFAULT_FLAG_MEMORYPERC
 )
 
 
def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_TIMESTEP,
                     ""parsing_function"": (lambda x: int(x)),
                     ""check_function"": self._check_avg_timestep
                 },
                 ""avg_freqstep"": {
                     ""value"": None,
                     ""default"": DEFAULT_AVERAGE_FREQSTEP,
                     ""parsing_function"": (lambda x: int(x)),
                     ""check_function"": self._check_avg_freqstep
                 },
                 ""startchan"": {
                     ""value"": None,
def __init__(self, file_name: str, channelization: int, dumptime: int, subbands:
                     ""value"": None,
                     ""default"": False,
                     ""parsing_function"": (lambda x: False if x.lower()==""false"" else True),
                     ""check_function"": self._check_compress
                 },
                 ""flag_strategy"": {
                     ""value"": None,
                     ""default"": os.path.join(FLAG_STRATEGY_FILE_PATH, 'NenuFAR-64C1S.rfis'),
                     ""parsing_function"": (lambda f: os.path.join(FLAG_STRATEGY_FILE_PATH, f) if not os.path.isabs(f) else f),
                     ""check_function"": self._check_flag_strategy
                 },
                 ""flag_rfi"": {
                     ""value"": None,
                     ""default"": DEFAULT_FLAG_RFI,
                     ""parsing_function"": (lambda x: bool(x)),
                     ""check_function"": self._check_flag_rfi
                 },
                 ""flag_memoryperc"": {
                     ""value"": DEFAULT_FLAG_MEMORYPERC, # This prevents any update from the parameter field
                     ""default"": DEFAULT_FLAG_MEMORYPERC,
                     ""parsing_function"": (lambda x: int(x)),
                     ""check_function"": self._check_flag_memoryperc
                 }
             },
             ""quality"": {
def _set_parameters(self, parameters: dict) -> None:
                     except:
                         log.warning(f""Parameter '{key}': parsing error. Considering no value."")
                         value = None
                     if step_dict[key_lower][""value""] is None:
                         step_dict[key_lower][""value""] = value
                     else:
                         # The parameter has a fixed value that cannot be set
                         break
                     log.info(f""'{key_lower}' set to '{value}'."")
                     break
             else:
                 # If the loop has not broken, it means the parameter is not expected
                 log.warning(
                     f""Unexpected parset parameter key '{key}': skipped.""
                 )
def _check_compress(compress: bool) -> bool:
         return isinstance(compress, bool)
 
 
     @staticmethod
     def _check_flag_rfi(flag_rfi: bool) -> bool:
         """""" """"""
         return isinstance(flag_rfi, bool)
 
 
     @staticmethod
     def _check_flag_strategy(flag_strategy: str) -> bool:
         """""" """"""
def _check_flag_strategy(flag_strategy: str) -> bool:
         return file_exists
 
 
     @staticmethod
     def _check_flag_memoryperc(flag_memoryperc: int) -> bool:
         """""" """"""
         is_int = isinstance(flag_memoryperc, int)
         is_percent = 0 <= flag_memoryperc <= 100
         return is_int & is_percent
 
 
     def _check_sws(self, sws: str) -> bool:
         """""" """"""
         matches = re.findall(r""(\d+)-(\d+)-(\d+)"", str(sws))"
KO;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
OK;22.0;AlanLoh;copper_preprocessing_config;753437b4137349206116ab67e33fde8636f66938;Added `flag_rfi` and `flag_memoryperc` (this fixes #2);"def test_tml_writing():
         'nchan = 60\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         'flag_rfi = True\n'
         'flag_memoryperc = 30\n'
         '\n[quality]\n'
         ""sws = ['SW01-106-200', 'SW02-202-300', 'SW03-306-418']\n""
         ""stat_pols = ['SNR_XX', 'SNR_YY', 'RFIPercentage_XX']\n""
def test_empty_param():
         'nchan = 64\n'
         'compress = False\n'
         ""flag_strategy = './NenuFAR-64C1S.rfis'\n""
         'flag_rfi = True\n'
         'flag_memoryperc = 30\n'
     )]
 
     open_mock.return_value.write.assert_has_calls(calls)"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .dockernet import *
 from .dockershm import *"
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from .net import *
 from .shm import *
 from .netserialize import *
 from .dockernet import *
 from .dockershm import *
 from .dockernetserialize import *"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import os
 
 def get_data():
    testfile = ""airbnb.csv""
    datadir = ""/data/""
    currdir = os.path.dirname(os.path.abspath(__file__)) 
    f = open(currdir + datadir + testfile, ""r"")
    data = f.read()
    return data
 
 if __name__ == ""__main__"":
     get_data()"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
 print(f""recv reading from {__file__}..."")
 print(f""recv data: {contents}"")
 f.close()"
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""r"")
 contents = f.read()
 print(f""recv: read from {__file__}"")
 f.close()"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" f = open(""test.txt"", ""w+"")
 print(f""send writing to shared mem from {__file__}..."")
 f.write(""a very simple message for recv"")
 f.close()"
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" from data import get_data
 
 f = open(""test.txt"", ""w+"")
 print(f""send: writing to shared mem from {__file__}..."")
 f.write(get_data())
 f.close()"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" 
 tQ = mpc.Queue()
 
 def copy_data_to_volume(volume): 
     data_script = ""data.py""
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     data_dir = curr_dir + ""/data""
     dst_dir = volume[""Mountpoint""]+f""/data""
     if os.path.exists(dst_dir):
         shutil.rmtree(dst_dir)
     shutil.copytree(data_dir, dst_dir)
     shutil.copy(curr_dir + ""/"" + data_script, volume[""Mountpoint""]+f""/{data_script}"")
 
 def copy_file_to_volume(script: str, volume):
     curr_dir = os.path.dirname(os.path.abspath(__file__))
     scripts_dir = curr_dir + ""/dockerscripts/""
def main():
     # get scripts
     copy_file_to_volume(read_script, volume)
     copy_file_to_volume(write_script, volume)
     copy_data_to_volume(volume)
 
     # setup Pipe to synchronize
     read, write = mpc.Pipe()"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
 def send(port: int, host: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
     sendsock.sendall(b""my simple message"")
     data = sendsock.recv(1024)
     print(f""send {data}"")
     
     sendsock.close()
 
def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
     sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import multiprocessing as mpc
 import socket
 
 from data import get_data
 
 # TODO: integrate pyspark
 
 def get_port():
     # going to hardcode for now
     pass
 
 def send(port: int, host: str, data: str, pipe):
     print(f""send pid {os.getpid()} port {port}"")
     
     # use Pipe to synchronize
def send(port: int, host: str, pipe):
     sendsock.connect((host, port))
     
     # simply send
 
     sendsock.sendall(b""my simple message"")
     
     # wait for confirmation
     recvdata = sendsock.recv(1024)
     print(f""send: {recvdata}"")
     
     sendsock.close()
 
def main():
     HOSTNAME = socket.gethostname()
     HOSTIP = socket.gethostbyname(HOSTNAME)
 
     sendproc = mpc.Process(target=send, args=(LISTENING_PORT, HOSTIP, get_data(), read,))
     recvproc = mpc.Process(target=recv, args=(LISTENING_PORT, HOSTIP, write,))
     
     # start and join processes"
KO;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
 def send(pipe, fname: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
     print(""send writing to shared mem..."")
     f.write(""a very simple message recv"")
     f.close()
 
     # wait until file is written
def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
     print(f""recv data: {contents}"")
     f.close()
 
 def main():
def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
     sendproc = mpc.Process(target=send, args=(write, fname,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
OK;28.0;bbytiger;cs260r-final-project-src;2273a1e37f7f4b0a45845585372537a7415f5c30;support larger data size for shared memory models;" import tempfile
 import multiprocessing as mpc
 
 from data import get_data
 
 def send(pipe, fname: str, data: str):
     print(f""send {os.getpid()} fname {fname}"")
 
     f = open(fname, ""a+"")
     print(""send: writing to shared mem..."")
     f.write(data)
     f.close()
 
     # wait until file is written
def recv(pipe, fname: str):
     # open, read contents, and close
     f = open(fname, ""r"")
     contents = f.read()
     print(f""recv: received data"")
     f.close()
 
 def main():
def main():
     read, write = mpc.Pipe()
 
     # setup processes and arguments
     data = get_data()
     sendproc = mpc.Process(target=send, args=(write, fname, data,))
     recvproc = mpc.Process(target=recv, args=(read, fname,))
 
     # start and join all processes"
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   VQSR: ""8g""
   OTHER: ""4g""
 
 ##############################"
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['OTHER']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
     resources: cpus=1, mem_mb=4000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['VQSR']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
     resources: cpus=1, mem_mb=10000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Specify type of NGS sequencing ('WES' or 'WGS'). Use for VQSR filtering
 DATA: ""WES""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
   MARK_DUP: ""40g""
   HC: ""20g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 # Specify type of NGS sequencing ('WES' or 'WGS'). Used for choosing memory. Also used for VQSR filtering.
 DATA: ""WES""
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
   MARK_DUP_WES: ""35g""
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 #### Set up report #####
 
 report: ""report/workflow.rst"""
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 def get_bwa_memory(resource):
     if config['DATA'] == ""WES"":
         return 15000
     if config['DATA'] == ""WGS"":
         return 50000
     else:
         return 15000
 
 def get_mkdup_memory(resource):
     if config['DATA'] == ""WES"":
         return 40000
     if config['DATA'] == ""WGS"":
         return 80000
     else:
         return 40000
 
 def get_mkdup_xmx(resource):
     if config['DATA'] == ""WES"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
     if config['DATA'] == ""WGS"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WGS'])
     else:
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
 
 def get_HC_memory(resource):
     if config['DATA'] == ""WES"":
         return 10000
     if config['DATA'] == ""WGS"":
         return 20000
     else:
         return 10000
 
 def get_HC_xmx(resource):
     if config['DATA'] == ""WES"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
     if config['DATA'] == ""WGS"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WGS'])
     else:
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
         
 #### Set up report #####
 
 report: ""report/workflow.rst"""
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
     resources: cpus=28, mem_mb=15000, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | ""
"
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
     resources: cpus=28, mem_mb=get_bwa_memory, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | ""
"
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP']),
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
     resources: cpus=28, mem_mb=40000, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \
"
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
         maxmemory = get_mkdup_xmx,
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
     resources: cpus=28, mem_mb=get_mkdup_memory, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \
"
KO;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC']),
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
     resources: cpus=1, mem_mb=20000, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \
"
OK;32.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
         maxmemory = get_HC_xmx,
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
     resources: cpus=1, mem_mb=get_HC_memory, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \
"
KO;40.0;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
\ No newline at end of file"
OK;40.0;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" # cpu-simulator
  
 Computer Architecture project to simulate a CPU, including memory and cache 
 
 1. What does the program do? 
    Program will prompt user to point the program to two input files, the cpu instructions file as well as data input 
 
 2. What data do you need?
    Program needs CPU instructions as well as data input to initialize memory bus 
 
 3. What aspects of a CPU can you simulate using Python?
    The program can simulate 
 
 4. How will your CPU handle incoming instructions?
    The cpu will receive the insructions from the cpu instructions file and run the appropriate ISA instructions
 
 5. How will your CPU output instructions?
    CPU will output instructions to the terminal
  
 6. How will your CPU store data?
    CPU will store the data in memory using the Memory class. 
\ No newline at end of file"
KO;40.0;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;
OK;40.0;rxfxt;cpu-simulator;cb0be629d4f720fc1a2d110f384e9811aff40950;Add project readme and memory bus class;" MEMORY_BUS_SIZE = 128
 
 # This class implements a memory bus
 class Memory:
     def __init__(self):
         self.memory_bus = {}
         self.initialize_memory_bus()
 
     # Method to initialize memory bus with size based on MEMORY_BUS_SIZE variable
     # Memory bus is setup as a dict with binary address being key, and value being value 
     def initialize_memory_bus(self):
         for i in range(MEMORY_BUS_SIZE):
             self.memory_bus[f'{i:08b}'] = 0
     
     # Method to search for address in memory bus and return value if available 
     def search_memory_bus(self, address):
         if self.memory_bus.get(address) != None:
             return self.memory_bus.get(address)
         return None 
     
     # Method to write in memory bus only if address is within the bus size 
     def write_memory_bus(self, address, value):
         if self.memory_bus.get(address) != None:
             self.memory_bus[address] = value"
KO;40.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"MAXMEMORY:
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   VQSR: ""8g""
   OTHER: ""4g""
 
 ##############################"
KO;40.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['OTHER']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
     resources: cpus=1, mem_mb=4000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;ba5e0593f1be5b81d5a3ca2b39b7ce0c6b57a4d7;increased vqsr memory;"rule VQSR_snp:
         recal_snp = ""../results/vqsr/snp.recal"",
         tranches_snp = ""../results/vqsr/snp.tranches""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['VQSR']),
         tdir = config['TEMPDIR'],
         hapmap = config['HAPMAP'],
         omni = config['OMNI'],
rule VQSR_snp:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_VQSR for SNPs""
     resources: cpus=1, mem_mb=10000, time_min=1440, partition=""serial""
     shell:
         """"""
         gatk VariantRecalibrator --java-options {params.maxmemory} \"
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;" ##############################
 ###### Overall workflow ######
 ##############################
 # Specify type of NGS sequencing ('WES' or 'WGS'). Used for choosing memory. Also used for VQSR filtering.
 DATA: ""WES""
 
 # Specify the path to .ped file if you want to use it. Otherwise leave it empty.
 # Check https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format
GNOMAD: ""/work/gr-fe/saadat/pri/known_sites/af-only-gnomad.hg38.vcf.gz""
 # Temporary file directory
 TEMPDIR: ""/scratch/saadat/temp/""
 
 # Inbreeding Coefficient: This option is used in VQSR. if you have less than 10 samples, or if samples are related (families), put 'EXCLUDE'. Otherwise put 'INCLUDE'
 INBREED_COEFF_FILTER: ""EXCLUDE""
 
WES:
 
 # Maximum memory usage per rule/sample (eg. '40g' for 40 gigabytes, this should suffice for exomes)
 MAXMEMORY: 
   MARK_DUP_WES: ""35g""
   MARK_DUP_WGS: ""70g""
   HC_WES: ""9g""
   HC_WGS: ""18g"" #HaplotypeCaller
   OTHER: ""4g""
 
 ##############################"
KO;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 #### Set up report #####
 
 report: ""report/workflow.rst"""
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"def get_wes_padding_command(resource):
         command = """"
     return command
 
 def get_bwa_memory(resource):
     if config['DATA'] == ""WES"":
         return 15000
     if config['DATA'] == ""WGS"":
         return 50000
     else:
         return 15000
 
 def get_mkdup_memory(resource):
     if config['DATA'] == ""WES"":
         return 40000
     if config['DATA'] == ""WGS"":
         return 80000
     else:
         return 40000
 
 def get_mkdup_xmx(resource):
     if config['DATA'] == ""WES"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
     if config['DATA'] == ""WGS"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WGS'])
     else:
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP_WES'])
 
 def get_HC_memory(resource):
     if config['DATA'] == ""WES"":
         return 10000
     if config['DATA'] == ""WGS"":
         return 20000
     else:
         return 10000
 
 def get_HC_xmx(resource):
     if config['DATA'] == ""WES"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
     if config['DATA'] == ""WGS"":
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WGS'])
     else:
         expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC_WES'])
         
 #### Set up report #####
 
 report: ""report/workflow.rst"""
KO;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
     resources: cpus=28, mem_mb=15000, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | ""
"
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule bwa_mem:
         ""../envs/bwa.yaml""
     message:
         ""Fastp, BWA-MEM, and Smatools for {wildcards.sample}""
     resources: cpus=28, mem_mb=get_bwa_memory, time_min=1440, partition=""parallel""
     shell:
         ""fastp -i {input.R1} -I {input.R2} --stdout --thread 2 -j {log.fastp_json} -h {log.fastp_html} 2> {log.fastp_log} | ""
         ""bwa mem -v 2 -M -t 22 -p -R {params.readgroup} {input.refgenome} - 2> {log.bwa} | ""
"
KO;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['MARK_DUP']),
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
     resources: cpus=28, mem_mb=40000, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \
"
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_MarkDuplicates:
         bam = ""../results/mapped/{sample}_mkdups.bam"",
         metrics = ""../results/mapped/{sample}_mkdups_metrics.txt""
     params:
         maxmemory = get_mkdup_xmx,
         tdir = config['TEMPDIR']
     log:
         ""logs/gatk_MarkDuplicates/{sample}.log""
rule gatk_MarkDuplicates:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_MarkDuplicates for {input}""
     resources: cpus=28, mem_mb=get_mkdup_memory, time_min=1440, partition=""parallel""
     shell:
         """"""gatk MarkDuplicatesSpark --java-options {params.maxmemory} \
         -I {input} \
"
KO;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
         maxmemory = expand('""-Xmx{maxmemory}""', maxmemory = config['MAXMEMORY']['HC']),
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
     resources: cpus=1, mem_mb=20000, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \
"
OK;40.0;AliSaadatV;Variant_Calling_Snakemake;4ee616e17b61c8302529201780f19c31aef1fba8;input functionsfor memory selection for HC, MKDUP, BWA;"rule gatk_HaplotypeCaller:
     output:
         vcf = ""../results/called/{sample}.g.vcf.gz""
     params:
         maxmemory = get_HC_xmx,
         tdir = config['TEMPDIR'],
         padding = get_wes_padding_command,
         intervals = get_wes_intervals_command,
rule gatk_HaplotypeCaller:
         ""../envs/gatk4.yaml""
     message:
         ""gatk_HaplotypeCaller for {input.bams}""
     resources: cpus=1, mem_mb=get_HC_memory, time_min=1440, partition=""serial""
     shell:
         """"""gatk HaplotypeCaller --java-options {params.maxmemory} \
         -I {input.bams} \
"
KO;42.0;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
   # sage.interfaces.gap.gap_cmd = 'gap -r -o 24G '
   sage.interfaces.gap.gap_cmd = 'gap -r -o 50G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
     set_gap_memory_pool_size(50* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
OK;42.0;LaisRast;point-groups;42397614a1985f19d452e09868e2baefe23fb9a2;computation needs 256GB memory;"print(f""elapsed time for loading: {time()-start_time}s"", file=file, flush=True)
 
 ## increase GAP pre-set memory limit
 if sage.misc.banner.require_version(major=9, minor=3):
   sage.interfaces.gap.gap_cmd = 'gap -r -o 256G '
 else:
     # The following works in sage 9.2, but no longer in sage 9.5:
     from sage.interfaces.gap import set_gap_memory_pool_size, get_gap_memory_pool_size
     print(f""GAP default memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
     set_gap_memory_pool_size(256* 10**9)
     print(f""GAP adjusted memory pool size {get_gap_memory_pool_size()}"", file=file, flush=True)
 
 ## ensure enough memory for GAP"
KO;45.0;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"def evaluate_samples(self,p,model,forwarding='head',knn=100):
             ri, ci = assign_classes_hungarian(C)
             accuracy = accuracy_from_assignment(C,ri,ci)
             print('Accuracy: ',accuracy)
             # §------------------------------------------------
 
             feature_tensor = torch.nn.functional.normalize(feature_tensor, dim = 1)
             similarity_matrix = torch.einsum('nd,cd->nc', [feature_tensor, feature_tensor]) # removed .cpu()
 
             #self.knn = knn
             scores, idx_k = similarity_matrix.topk(k=knn, dim=1)
             #self.proximity = torch.mean(scores_k,dim=1)
             #self.kNN_indices = idx_k
             labels_topk = torch.zeros_like(idx_k)"
OK;45.0;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);"def evaluate_samples(self,p,model,forwarding='head',knn=100):
             ri, ci = assign_classes_hungarian(C)
             accuracy = accuracy_from_assignment(C,ri,ci)
             print('Accuracy: ',accuracy)
         # §------------------------------------------------
 
             feature_tensor = torch.nn.functional.normalize(feature_tensor, dim = 1)
 
             idx_list = []
             for i in range(len(feature_tensor)):
                 feature = torch.unsqueeze(feature_tensor[i],dim=0)
                 similarities = torch.mm(feature,feature_tensor.t())
                 scores, idx_ = similarities.topk(k=knn, dim=1)
                 idx_list.append(idx_)
             idx_k = torch.cat(idx_list)
             #similarity_matrix = torch.einsum('nd,cd->nc', [feature_tensor, feature_tensor]) # removed .cpu()
 
             #self.knn = knn
             #scores, idx_k = similarity_matrix.topk(k=knn, dim=1)
             #self.proximity = torch.mean(scores_k,dim=1)
             #self.kNN_indices = idx_k
             labels_topk = torch.zeros_like(idx_k)"
KO;45.0;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);
OK;45.0;MT-Blachetta;MERGE-unsupervised_clustering;ac027234ed284a64a6c43acdeb343c67a3dda106;fixed memory issue (similarity matrix);" {
  ""cells"": [
   {
    ""cell_type"": ""markdown"",
    ""id"": ""afd3e0db"",
    ""metadata"": {},
    ""source"": [
     ""## @ref = tv_res18""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 1,
    ""id"": ""dcd1c27d"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""ResNet(\n"",
        ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
        ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""  (relu): ReLU(inplace=True)\n"",
        ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
        ""  (layer1): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (layer2): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (downsample): Sequential(\n"",
        ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
        ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      )\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (layer3): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (downsample): Sequential(\n"",
        ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
        ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      )\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (layer4): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (downsample): Sequential(\n"",
        ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
        ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      )\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
        ""  (fc): Linear(in_features=512, out_features=1000, bias=True)\n"",
        "")""
       ]
      },
      ""execution_count"": 1,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""import torchvision\n"",
     ""\n"",
     ""resnet18 = torchvision.models.resnet18()\n"",
     ""resnet18""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 2,
    ""id"": ""c9ecd56d"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""ResNet(\n"",
        ""  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"",
        ""  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""  (relu): ReLU(inplace=True)\n"",
        ""  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n"",
        ""  (layer1): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (layer2): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (downsample): Sequential(\n"",
        ""        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
        ""        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      )\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (layer3): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (downsample): Sequential(\n"",
        ""        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
        ""        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      )\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (layer4): Sequential(\n"",
        ""    (0): BasicBlock(\n"",
        ""      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (downsample): Sequential(\n"",
        ""        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"",
        ""        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      )\n"",
        ""    )\n"",
        ""    (1): BasicBlock(\n"",
        ""      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""      (relu): ReLU(inplace=True)\n"",
        ""      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"",
        ""      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"",
        ""    )\n"",
        ""  )\n"",
        ""  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n"",
        ""  (fc): Linear(in_features=512, out_features=10, bias=True)\n"",
        "")""
       ]
      },
      ""execution_count"": 2,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""import torch.nn as nn\n"",
     ""resnet18.fc = nn.Linear(512,10)\n"",
     ""resnet18""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 4,
    ""id"": ""511ae48e"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": [
     ""import torch\n"",
     ""\n"",
     ""demoBatch = torch.rand([4,3,32,32])\n"",
     ""modelSample = resnet18(demoBatch)""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 5,
    ""id"": ""13b59406"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""torch.Size([4, 10])""
       ]
      },
      ""execution_count"": 5,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""modelSample.shape""
    ]
   },
   {
    ""cell_type"": ""markdown"",
    ""id"": ""f545a7ad"",
    ""metadata"": {},
    ""source"": [
     ""## @ref = singleSoftmaxUse""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 10,
    ""id"": ""19283bac"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": [
     ""soft = torch.nn.Softmax(dim = 1) # create function class instance <callable>""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 11,
    ""id"": ""928045a9"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": [
     ""test = soft(modelSample) # softmax""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 14,
    ""id"": ""70fb3bbe"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""tensor(1.0000, grad_fn=<AddBackward0>)""
       ]
      },
      ""execution_count"": 14,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""sum(test[0])""
    ]
   },
   {
    ""cell_type"": ""markdown"",
    ""id"": ""0911bd73"",
    ""metadata"": {},
    ""source"": [
     ""## @ref = matrixC""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 12,
    ""id"": ""915c9184"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""torch.Size([128])""
       ]
      },
      ""execution_count"": 12,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""import torch\n"",
     ""\n"",
     ""dummy_features = torch.rand([105000,128])\n"",
     ""dummy_features = torch.nn.functional.normalize(dummy_features, dim = 1)\n"",
     ""single_feature = dummy_features[0]\n"",
     ""single_feature.shape""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 16,
    ""id"": ""063902c3"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""105000""
       ]
      },
      ""execution_count"": 16,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""len(dummy_features)""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 13,
    ""id"": ""ad491fdd"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""torch.Size([1, 105000])""
       ]
      },
      ""execution_count"": 13,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""matmul = torch.mm(torch.unsqueeze(single_feature,dim=0),dummy_features.t())\n"",
     ""matmul.shape""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 14,
    ""id"": ""4f6da2a1"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""torch.Size([1, 100])""
       ]
      },
      ""execution_count"": 14,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""scores, idx_k = matmul.topk(k=100, dim=1)\n"",
     ""idx_k.shape""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": 15,
    ""id"": ""397dd28a"",
    ""metadata"": {},
    ""outputs"": [
     {
      ""data"": {
       ""text/plain"": [
        ""tensor([[1.0000, 0.8648, 0.8608, 0.8602, 0.8599, 0.8583, 0.8579, 0.8547, 0.8541,\n"",
        ""         0.8538, 0.8535, 0.8535, 0.8532, 0.8530, 0.8523, 0.8522, 0.8518, 0.8518,\n"",
        ""         0.8514, 0.8513, 0.8507, 0.8501, 0.8499, 0.8495, 0.8494, 0.8491, 0.8486,\n"",
        ""         0.8485, 0.8482, 0.8478, 0.8462, 0.8459, 0.8459, 0.8458, 0.8458, 0.8455,\n"",
        ""         0.8451, 0.8449, 0.8449, 0.8448, 0.8448, 0.8446, 0.8446, 0.8443, 0.8442,\n"",
        ""         0.8441, 0.8441, 0.8441, 0.8440, 0.8440, 0.8437, 0.8437, 0.8436, 0.8435,\n"",
        ""         0.8433, 0.8433, 0.8432, 0.8432, 0.8432, 0.8431, 0.8430, 0.8430, 0.8429,\n"",
        ""         0.8428, 0.8426, 0.8423, 0.8422, 0.8422, 0.8422, 0.8422, 0.8421, 0.8421,\n"",
        ""         0.8421, 0.8420, 0.8420, 0.8418, 0.8418, 0.8417, 0.8415, 0.8415, 0.8414,\n"",
        ""         0.8414, 0.8414, 0.8413, 0.8413, 0.8413, 0.8411, 0.8411, 0.8410, 0.8408,\n"",
        ""         0.8408, 0.8408, 0.8407, 0.8407, 0.8407, 0.8407, 0.8406, 0.8405, 0.8405,\n"",
        ""         0.8405]])""
       ]
      },
      ""execution_count"": 15,
      ""metadata"": {},
      ""output_type"": ""execute_result""
     }
    ],
    ""source"": [
     ""scores""
    ]
   },
   {
    ""cell_type"": ""code"",
    ""execution_count"": null,
    ""id"": ""5b5e42ed"",
    ""metadata"": {},
    ""outputs"": [],
    ""source"": []
   }
  ],
  ""metadata"": {
   ""kernelspec"": {
    ""display_name"": ""Python 3 (ipykernel)"",
    ""language"": ""python"",
    ""name"": ""python3""
   },
   ""language_info"": {
    ""codemirror_mode"": {
     ""name"": ""ipython"",
     ""version"": 3
    },
    ""file_extension"": "".py"",
    ""mimetype"": ""text/x-python"",
    ""name"": ""python"",
    ""nbconvert_exporter"": ""python"",
    ""pygments_lexer"": ""ipython3"",
    ""version"": ""3.7.10""
   }
  },
  ""nbformat"": 4,
  ""nbformat_minor"": 5
 }"
KO;1.0;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
KO;1.0;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
         M, G, T to specify MiB, GiB, or TiB, respectively.
         Values without suffix are interpreted as bytes.
         Values are rounded to the closest MiB.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
 def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
 def max_memory(max_memory):
     """"""Interpret the config parameter --max-memory.""""""
     try:
         matches = re.match(r""(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[%MGT])?"", max_memory)
         value = float(matches[""value""])
         unit = matches[""unit""]
         if unit == ""%"":
             max_memory = share_of_ram(share=(value / 100.0))
         else:
             # convert to MiB
             if unit is None:
                 value *= 2**-10
                 if value < 1:
                     value = 1
             # elif unit == ""M"":
             #    value *= 2 ** 1
             elif unit == ""G"":
                 value *= 2**10
             elif unit == ""T"":
                 value *= 2**20
             max_memory = round(value)
     except TypeError:
         raise ValueError(f""Could not interpret --max-memory: {max_memory}"")
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
def max_memory(max_memory):
     return max_memory
 
 
 MAX_JVM_MEMORY = max_memory(arguments.max_memory)"
OK;1.0;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
         K, M, G, T to specify KiB, MiB, GiB, or TiB, respectively.
         Values are rounded to the closest MiB.
         Values without suffix are interpreted as bytes.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
 def _share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
 def _parse_max_memory_string(max_memory):
     """"""
     Extract maximum memory value and unit from text input.
 
     Arguments
     ---------
     max_memory : str
         Input text from the config parameter --max-memory.
 
     Returns
     -------
     tuple: a tuple containing
         - value (float): Amount of memory to be allocated in a given unit.
         - unit (str): The unit of memory.
     """"""
     try:
         matches = re.match(
             r""^(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[^0-9])?$"", max_memory
         )
         value = float(matches[""value""])
         unit = matches[""unit""]
 
         if unit is not None and unit not in ""%KMGT"":
             raise ValueError(
                 ""Could not interpret the memory unit from --max-memory.""
                 ""The suffix for --max-memory should be '%', 'K', 'M', 'G' or 'T'.""
                 ""For example to allocate five gigabytes of memory, use: '5G'""
             )
         return value, unit
     except TypeError:
         raise ValueError(
             f""Could not interpret --max-memory: {max_memory}.""
             f""To allocate memory, use e.g. '5G' for five gigabytes of memory.""
         )
 
 
 def _get_max_memory(max_memory):
     """"""
     Interpret the config parameter --max-memory.
 
     Arguments
     ---------
 
     max_memory : str
         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
         K, M, G, T suffix specify KiB, MiB, GiB, or TiB, respectively.
         Values are rounded to the closest MiB.
         Values without suffix are interpreted as bytes.
 
     Returns
     -------
     float
         Maximum amount of memory allocated for R5 in MiB.
     """"""
 
     value, unit = _parse_max_memory_string(max_memory)
 
     if unit == ""%"":
         max_memory = _share_of_ram(share=(value / 100.0))
     else:
         # convert to MiB
         if unit is None:
             value *= 2**-20
         elif unit == ""K"":
             value *= 2**-10
         elif unit == ""M"":
             value *= 2**1
         elif unit == ""G"":
             value *= 2**10
         elif unit == ""T"":
             value *= 2**20
 
         if value < 1:
             value = 1
 
         max_memory = round(value)
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
def max_memory(max_memory):
     return max_memory
 
 
 MAX_JVM_MEMORY = _get_max_memory(arguments.max_memory)"
KO;1.0;facebookresearch;metaseq;ca180bb6474d0e55051e2bb3db9af69ddb3725e3;"fix off by one in API (#145)

* making prompt_len independent of batchfy implmentation

* modify for echo=True case

* returning logprobs, to support logprob input

* setting need_logprobs depends on each request to save memory";"def generate(
             self.cfg.generation.max_len_a = 0
 
             logger.info(f""Preparing generator with settings {self.cfg.generation}"")
             generator = self.task.build_generator(
                 self.models, self.cfg.generation, extra_gen_cls_kwargs={""stop"": stop}
             )
 
             # okay actually generate
def generate(
                     tokens, scores, distributions = GeneratorInterface._filter_special(
                         tokens, scores, distributions
                     )
                     prompt_len = src_lengths[i]
                     if echo:
                         # don't cut off prompt
                         tokens = tokens[: prompt_len + max_tokens[i] - 1]
                         scores = scores[: prompt_len + max_tokens[i] - 1]
                         if logprobs > 0:
                             distributions = distributions[
                                 : prompt_len + max_tokens[i] - 1
                             ]
                     else:
                         # cut off prompt
                         tokens = tokens[prompt_len - 1 :][: max_tokens[i]]
                         scores = scores[prompt_len - 1 :][: max_tokens[i]]
                         if logprobs > 0:
                             distributions = distributions[prompt_len - 1 :][
                                 : max_tokens[i]
                             ]
                     # turn it into a string
                     text = self.bpe.bpe.decode(tokens)
                     # re-encode it so we get offsets"
OK;1.0;facebookresearch;metaseq;ca180bb6474d0e55051e2bb3db9af69ddb3725e3;"fix off by one in API (#145)

* making prompt_len independent of batchfy implmentation

* modify for echo=True case

* returning logprobs, to support logprob input

* setting need_logprobs depends on each request to save memory";"def generate(
             self.cfg.generation.max_len_a = 0
 
             logger.info(f""Preparing generator with settings {self.cfg.generation}"")
             need_logprobs = True if logprobs > 0 else False
             generator = self.task.build_generator(
                 self.models,
                 self.cfg.generation,
                 extra_gen_cls_kwargs={""stop"": stop, ""need_logprobs"": need_logprobs},
             )
 
             # okay actually generate
def generate(
                     tokens, scores, distributions = GeneratorInterface._filter_special(
                         tokens, scores, distributions
                     )
                     prompt_len = lengths[i]
                     if echo:
                         # don't cut off prompt
                         tokens = tokens[: prompt_len + max_tokens[i]]
                         scores = scores[: prompt_len + max_tokens[i]]
                         if logprobs > 0:
                             distributions = distributions[: prompt_len + max_tokens[i]]
                     else:
                         # cut off prompt
                         tokens = tokens[prompt_len:][: max_tokens[i]]
                         scores = scores[prompt_len:][: max_tokens[i]]
                         if logprobs > 0:
                             distributions = distributions[prompt_len:][: max_tokens[i]]
                     # turn it into a string
                     text = self.bpe.bpe.decode(tokens)
                     # re-encode it so we get offsets"
KO;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
         functional.identity_hv(
             self.num_embeddings,
             self.embedding_dim,
             out=self.weight.data,
             **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
         functional.random_hv(
             self.num_embeddings,
             self.embedding_dim,
             out=self.weight.data,
             **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
         functional.level_hv(
             self.num_embeddings,
             self.embedding_dim,
             randomness=self.randomness,
             out=self.weight.data,
             **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
         functional.circular_hv(
             self.num_embeddings,
             self.embedding_dim,
             randomness=self.randomness,
             out=self.weight.data,
             **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
 
         self.weight.data.copy_(
             functional.identity_hv(
                 self.num_embeddings, self.embedding_dim, **factory_kwargs
             )
         )
 
         self._fill_padding_idx_with_zero()
def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
 
         self.weight.data.copy_(
             functional.random_hv(
                 self.num_embeddings, self.embedding_dim, **factory_kwargs
             )
         )
 
         self._fill_padding_idx_with_zero()
def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
 
         self.weight.data.copy_(
             functional.level_hv(
                 self.num_embeddings,
                 self.embedding_dim,
                 randomness=self.randomness,
                 **factory_kwargs
             )
         )
 
         self._fill_padding_idx_with_zero()
def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
 
         self.weight.data.copy_(
             functional.circular_hv(
                 self.num_embeddings,
                 self.embedding_dim,
                 randomness=self.randomness,
                 **factory_kwargs
             )
         )
 
         self._fill_padding_idx_with_zero()"
KO;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
     def __init__(self, threshold=0.0):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
             raise IndexError()
 
         return index
 
def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
         """"""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
         value = functional.multiset(input, dim=-2)
         return cls(value, size=input.size(-2))
 
 
def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
         return cls(value, size=input.size(-2))
 
 
 class Sequence:
def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
             self.value = torch.zeros(dim_or_input, dtype=dtype, device=device)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
def clear(self) -> None:
             >>> DS.clear()
 
         """"""
         self.value.fill_(0.0)
         self.size = 0
 
     @classmethod"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
     def __init__(self, threshold=0.5):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
             raise IndexError(""No elements in memory"")
 
         return index
 
def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
         r""""""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
         value = functional.multiset(input)
         return cls(value, size=input.size(-2))
 
 
def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
         return cls(value, size=keys.size(-2))
 
 
 class Sequence:
def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
             self.value = functional.identity_hv(
                 1, dim_or_input, dtype=dtype, device=device
             ).squeeze(0)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
def clear(self) -> None:
             >>> DS.clear()
 
         """"""
         self.value.fill_(1.0)
         self.size = 0
 
     @classmethod"
KO;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 letters = list(string.ascii_lowercase)
 
 
 class TestDistinctSequence:
     def test_creation_dim(self):
         S = structures.DistinctSequence(10000)
         assert torch.equal(S.value, torch.ones(10000))
 
     def test_creation_tensor(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         S = structures.DistinctSequence(hv[0])
         assert torch.equal(S.value, hv[0])
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_append(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.append(hv[0])
         assert functional.cosine_similarity(S.value, hv)[0] > 0.5
 
     def test_appendleft(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.appendleft(hv[0])
         assert functional.cosine_similarity(S.value, hv)[0] > 0.5
 
     def test_pop(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.append(hv[0])
         S.append(hv[1])
         S.pop(hv[1])
         assert functional.cosine_similarity(S.value, hv)[0] > 0.5
         S.pop(hv[0])
         S.append(hv[2])
         assert functional.cosine_similarity(S.value, hv)[2] > 0.5
         S.append(hv[3])
         S.pop(hv[3])
         assert functional.cosine_similarity(S.value, hv)[2] > 0.5
 
     def test_popleft(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.appendleft(hv[0])
         S.appendleft(hv[1])
         S.popleft(hv[1])
         assert functional.cosine_similarity(S.value, hv)[0] > 0.5
         S.popleft(hv[0])
         S.appendleft(hv[2])
         assert functional.cosine_similarity(S.value, hv)[2] > 0.5
         S.appendleft(hv[3])
         S.popleft(hv[3])
         assert functional.cosine_similarity(S.value, hv)[2] > 0.5
 
     def test_replace(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.append(hv[0])
         assert functional.cosine_similarity(S.value, hv)[0] > 0.5
         S.replace(0, hv[0], hv[1])
         assert functional.cosine_similarity(S.value, hv)[1] > 0.5
 
     def test_length(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.append(hv[0])
         S.append(hv[0])
         S.append(hv[0])
         S.append(hv[0])
         assert len(S) == 4
         S.pop(hv[0])
         S.pop(hv[0])
         S.pop(hv[0])
         assert len(S) == 1
         S.pop(hv[0])
         assert len(S) == 0
         S.append(hv[0])
         assert len(S) == 1
 
     def test_clear(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.DistinctSequence(10000)
         S.append(hv[0])
         S.append(hv[0])
         S.append(hv[0])
         S.append(hv[0])
         assert len(S) == 4
         S.clear()
         assert len(S) == 0"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 seed1 = 2147483643
 letters = list(string.ascii_lowercase)
 
 
 class TestFSA:
     def test_creation_dim(self):
         F = structures.FiniteStateAutomata(10000)
         assert torch.equal(F.value, torch.zeros(10000))
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_add_transition(self):
         generator = torch.Generator()
         generator1 = torch.Generator()
         generator.manual_seed(seed)
         generator1.manual_seed(seed1)
         tokens = functional.random_hv(10, 10, generator=generator)
         actions = functional.random_hv(10, 10, generator=generator1)
 
         F = structures.FiniteStateAutomata(10)
 
         F.add_transition(tokens[0], actions[1], actions[2])
         assert torch.equal(
             F.value,
             torch.tensor([1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0]),
         )
         F.add_transition(tokens[1], actions[1], actions[3])
         assert torch.equal(
             F.value, torch.tensor([0.0, 0.0, -2.0, 2.0, 0.0, 2.0, 0.0, -2.0, -2.0, 0.0])
         )
         F.add_transition(tokens[2], actions[1], actions[3])
         assert torch.equal(
             F.value,
             torch.tensor([1.0, 1.0, -3.0, 1.0, 1.0, 3.0, -1.0, -1.0, -1.0, 1.0]),
         )
 
     def test_transition(self):
         generator = torch.Generator()
         generator1 = torch.Generator()
         generator.manual_seed(seed)
         generator1.manual_seed(seed1)
         tokens = functional.random_hv(10, 10, generator=generator)
         states = functional.random_hv(10, 10, generator=generator1)
 
         F = structures.FiniteStateAutomata(10)
 
         F.add_transition(tokens[0], states[1], states[2])
         F.add_transition(tokens[1], states[1], states[3])
         F.add_transition(tokens[2], states[1], states[5])
 
         assert (
             torch.argmax(
                 functional.cosine_similarity(F.transition(states[1], tokens[0]), states)
             ).item()
             == 2
         )
         assert (
             torch.argmax(
                 functional.cosine_similarity(F.transition(states[1], tokens[1]), states)
             ).item()
             == 3
         )
         assert (
             torch.argmax(
                 functional.cosine_similarity(F.transition(states[1], tokens[2]), states)
             ).item()
             == 5
         )
 
     def test_clear(self):
         generator = torch.Generator()
         generator1 = torch.Generator()
         generator.manual_seed(seed)
         generator1.manual_seed(seed1)
         tokens = functional.random_hv(10, 10, generator=generator)
         states = functional.random_hv(10, 10, generator=generator1)
 
         F = structures.FiniteStateAutomata(10)
 
         F.add_transition(tokens[0], states[1], states[2])
         F.add_transition(tokens[1], states[1], states[3])
         F.add_transition(tokens[2], states[1], states[5])
 
         F.clear()
         assert torch.equal(
             F.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
         )"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 letters = list(string.ascii_lowercase)
 
 
 class TestGraph:
     def test_creation_dim(self):
         G = structures.Graph(10000, directed=True)
         assert torch.equal(G.value, torch.zeros(10000))
 
     def test_creation_tensor(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         g = functional.bind(hv[0], hv[1])
         G = structures.Graph(g)
         assert torch.equal(G.value, g)
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_add_edge(self):
         G = structures.Graph(8)
         hv = torch.tensor(
             [
                 [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
                 [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                 [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
                 [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
             ]
         )
 
         G.add_edge(hv[0], hv[1])
         assert torch.equal(
             G.value, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
         )
         G.add_edge(hv[2], hv[3])
         assert torch.equal(
             G.value, torch.tensor([-2.0, -2.0, 0.0, 2.0, -2.0, 0.0, 2.0, -2.0])
         )
 
         GD = structures.Graph(8, directed=True)
 
         GD.add_edge(hv[0], hv[1])
         assert torch.equal(
             GD.value, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
         )
         GD.add_edge(hv[2], hv[3])
         assert torch.equal(
             GD.value, torch.tensor([0.0, 0.0, 0.0, -2.0, 0.0, -2.0, 2.0, -2.0])
         )
 
     def test_encode_edge(self):
         G = structures.Graph(8)
         hv = torch.tensor(
             [
                 [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
                 [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                 [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
                 [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
             ]
         )
 
         e1 = G.encode_edge(hv[0], hv[1])
         assert torch.equal(
             e1, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
         )
         e2 = G.encode_edge(hv[2], hv[3])
         assert torch.equal(
             e2, torch.tensor([-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0])
         )
 
         GD = structures.Graph(8, directed=True)
 
         e1 = GD.encode_edge(hv[0], hv[1])
         assert torch.equal(
             e1, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
         )
         e2 = GD.encode_edge(hv[2], hv[3])
         print(e2)
         assert torch.equal(
             e2, torch.tensor([1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0])
         )
 
     def test_node_neighbors(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(10, 10000, generator=generator)
         G = structures.Graph(10000, directed=True)
 
         G.add_edge(hv[0], hv[1])
         G.add_edge(hv[0], hv[2])
         G.add_edge(hv[1], hv[2])
 
         assert (
             torch.argmax(
                 functional.cosine_similarity(G.node_neighbors(hv[1]), hv)
             ).item()
             == 2
         )
         assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[2] > 0.5
         assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[2] > 0.5
         assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[1] > 0.5
         assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[1] < 0.5
         assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[0] < 0.5
         assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[0] < 0.5
 
         G1 = structures.Graph(10000, directed=False)
 
         G1.add_edge(hv[0], hv[1])
         G1.add_edge(hv[0], hv[2])
         G1.add_edge(hv[1], hv[2])
         assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[0] > 0.5
         assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[1] > 0.5
         assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[2] > 0.5
         assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[0] > 0.5
         assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[2] > 0.5
         assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[1] > 0.5
 
     def test_contains(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(4, 8, generator=generator)
         G = structures.Graph(8)
 
         e1 = G.encode_edge(hv[0], hv[1])
         e2 = G.encode_edge(hv[0], hv[2])
         e3 = G.encode_edge(hv[2], hv[3])
 
         G.add_edge(hv[0], hv[1])
         G.add_edge(hv[0], hv[2])
         G.add_edge(hv[1], hv[2])
 
         assert G.contains(e1) > torch.tensor(0.6)
         assert G.contains(e2) > torch.tensor([0.6])
         assert G.contains(e3) < torch.tensor(0.6)
 
         GD = structures.Graph(8, directed=True)
 
         ee1 = GD.encode_edge(hv[0], hv[1])
         ee2 = GD.encode_edge(hv[0], hv[2])
         ee3 = GD.encode_edge(hv[2], hv[3])
         ee4 = GD.encode_edge(hv[1], hv[0])
 
         GD.add_edge(hv[0], hv[1])
         GD.add_edge(hv[0], hv[2])
         GD.add_edge(hv[3], hv[1])
 
         assert GD.contains(ee1) > torch.tensor(0.6)
         assert GD.contains(ee2) > torch.tensor(0.6)
         assert GD.contains(ee3) < torch.tensor(0.6)
         assert GD.contains(ee4) < torch.tensor(0.6)
 
     def test_clear(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(4, 8, generator=generator)
         G = structures.Graph(8)
 
         G.add_edge(hv[0], hv[1])
         G.add_edge(hv[0], hv[2])
         G.add_edge(hv[1], hv[2])
 
         G.clear()
 
         assert torch.equal(
             G.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
         )"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed_key = 2147483644
 seed_value = 2147483622
 letters = list(string.ascii_lowercase)
 
 
 class TestHashtable:
     def test_creation_dim(self):
         H = structures.HashTable(10000)
         assert torch.equal(H.value, torch.zeros(10000))
 
     def test_creation_tensor(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         hash_v1 = functional.bind(keys_hv[0], values_hv[0])
         hash_v2 = functional.bind(keys_hv[1], values_hv[1])
         hasht = functional.bundle(hash_v1, hash_v2)
 
         H = structures.HashTable(hasht)
         assert torch.equal(H.value, hasht)
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed_key)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed_key)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_add(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
 
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
             torch.tensor(True),
         )
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
             torch.tensor(True),
         )
 
     def test_remove(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
 
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
             torch.tensor(True),
         )
 
         H.remove(keys_hv[0], values_hv[0])
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) < 0.2)[0],
             torch.tensor(True),
         )
 
     def test_get(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
         H.add(keys_hv[2], values_hv[2])
 
         assert torch.equal(
             (functional.cosine_similarity(H.get(keys_hv[0]), values_hv) > 0.5)[0],
             torch.tensor(True),
         )
         assert torch.equal(
             (functional.cosine_similarity(H.get(keys_hv[1]), values_hv) > 0.5)[1],
             torch.tensor(True),
         )
         assert torch.equal(
             (functional.cosine_similarity(H.get(keys_hv[2]), values_hv) > 0.5)[2],
             torch.tensor(True),
         )
         assert torch.equal(
             torch.all(
                 (functional.cosine_similarity(H.get(values_hv[2]), values_hv) > 0.5)
                 == False
             ),
             torch.tensor(True),
         )
 
     def test_getitem(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
         H.add(keys_hv[2], values_hv[2])
 
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
             torch.tensor(True),
         )
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
             torch.tensor(True),
         )
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[2]], values_hv) > 0.5)[2],
             torch.tensor(True),
         )
         assert torch.equal(
             torch.all(
                 (functional.cosine_similarity(H[values_hv[2]], values_hv) > 0.5)
                 == False
             ),
             torch.tensor(True),
         )
 
     def test_replace(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
         H.add(keys_hv[2], values_hv[2])
 
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
             torch.tensor(True),
         )
         H.replace(keys_hv[0], values_hv[0], values_hv[1])
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[1],
             torch.tensor(True),
         )
 
     def test_length(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
         H.add(keys_hv[2], values_hv[2])
 
         assert len(H) == 3
         H.remove(keys_hv[0], values_hv[0])
 
         assert len(H) == 2
 
     def test_clear(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
 
         H = structures.HashTable(10000)
         H.add(keys_hv[0], values_hv[0])
         H.add(keys_hv[1], values_hv[1])
         H.add(keys_hv[2], values_hv[2])
         assert len(H) == 3
         H.clear()
         assert len(H) == 0
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
             torch.tensor(False),
         )
         H.add(keys_hv[0], values_hv[0])
         assert torch.equal(
             (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
             torch.tensor(True),
         )
 
     def test_from_tensor(self):
         generator_key = torch.Generator()
         generator_key.manual_seed(seed_key)
         keys_hv = functional.random_hv(2, 3, generator=generator_key)
 
         generator_value = torch.Generator()
         generator_value.manual_seed(seed_value)
         values_hv = functional.random_hv(2, 3, generator=generator_value)
 
         H = structures.HashTable.from_tensors(keys_hv, values_hv)
         assert torch.equal(H.value, torch.tensor([2.0, 0.0, 0.0]))"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 letters = list(string.ascii_lowercase)
 
 
 class TestMemory:
     def test_creation(self):
         M = structures.Memory()
 
         assert M.keys == []
         assert M.values == []
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_add(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         M = structures.Memory()
         M.add(keys_hv[0], letters[0])
         M.add(keys_hv[1], letters[1])
         M.add(keys_hv[2], letters[2])
 
         assert torch.equal(M.keys[0], keys_hv[0])
         assert torch.equal(M.keys[1], keys_hv[1])
         assert torch.equal(M.keys[2], keys_hv[2])
         assert M.values[0] == letters[0]
         assert M.values[1] == letters[1]
         assert M.values[2] == letters[2]
 
     def test_index(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         M = structures.Memory()
         M.add(keys_hv[0], letters[0])
         M.add(keys_hv[1], letters[1])
         M.add(keys_hv[2], letters[2])
 
         assert M.index(keys_hv[0]) == 0
         assert M.index(keys_hv[1]) == 1
         assert M.index(keys_hv[2]) == 2
 
     def test_length(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         M = structures.Memory()
         M.add(keys_hv[0], letters[0])
         M.add(keys_hv[1], letters[1])
         M.add(keys_hv[2], letters[2])
 
         assert len(M) == 3
         del M[keys_hv[0]]
 
         assert len(M) == 2
 
         M.add(keys_hv[0], letters[0])
         assert len(M) == 3
 
     def test_getitem(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         M = structures.Memory()
         M.add(keys_hv[0], letters[0])
         M.add(keys_hv[1], letters[1])
         M.add(keys_hv[2], letters[2])
 
         assert M[keys_hv[0]][1] == letters[0]
         assert M[keys_hv[1]][1] == letters[1]
         assert M[keys_hv[2]][1] == letters[2]
 
     def test_setitem(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         M = structures.Memory()
         M.add(keys_hv[0], letters[0])
         M.add(keys_hv[1], letters[1])
         M.add(keys_hv[2], letters[2])
 
         assert len(M) == 3
         assert M[keys_hv[0]][1] == letters[0]
         assert M[keys_hv[1]][1] == letters[1]
         assert M[keys_hv[2]][1] == letters[2]
 
         M[keys_hv[0]] = letters[3]
         assert len(M) == 3
         assert M[keys_hv[0]][1] == letters[3]
 
     def test_delitem(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
 
         M = structures.Memory()
         M.add(keys_hv[0], letters[0])
         M.add(keys_hv[1], letters[1])
         M.add(keys_hv[2], letters[2])
 
         assert len(M) == 3
         assert M[keys_hv[0]][1] == letters[0]
         assert M[keys_hv[1]][1] == letters[1]
         assert M[keys_hv[2]][1] == letters[2]
 
         del M[keys_hv[0]]
         try:
             M[keys_hv[0]]
         except IndexError:
             assert True
 
         assert M[keys_hv[1]][1] == letters[1]
         assert M[keys_hv[2]][1] == letters[2]
         assert len(M) == 2"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 letters = list(string.ascii_lowercase)
 
 
 class TestMultiset:
     def test_creation_dim(self):
         M = structures.Multiset(10000)
         assert torch.equal(M.value, torch.zeros(10000))
 
     def test_creation_tensor(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
         multiset = functional.multiset(keys_hv)
 
         M = structures.Multiset(multiset)
         assert torch.equal(M.value, multiset)
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_add(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 4, generator=generator)
         M = structures.Multiset(4)
 
         M.add(keys_hv[0])
         assert torch.equal(M.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
 
         M.add(keys_hv[1])
         assert torch.equal(M.value, torch.tensor([2.0, 0.0, 0.0, 2.0]))
 
         M.add(keys_hv[2])
         assert torch.equal(M.value, torch.tensor([3.0, 1.0, 1.0, 1.0]))
 
     def test_remove(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 4, generator=generator)
         M = structures.Multiset(4)
 
         M.add(keys_hv[0])
         M.add(keys_hv[1])
 
         assert M.contains(keys_hv[0]) > torch.tensor([0.5])
 
         M.remove(keys_hv[0])
         assert M.contains(keys_hv[0]) < torch.tensor([0.1])
         assert M.contains(keys_hv[1]) > torch.tensor([0.5])
         assert M.remove(keys_hv[0]) is None
 
     def test_contains(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 4, generator=generator)
         M = structures.Multiset(4)
 
         M.add(keys_hv[0])
         M.add(keys_hv[0])
         M.add(keys_hv[0])
         M.add(keys_hv[1])
         assert M.contains(keys_hv[0]) > torch.tensor([0.8])
         M.remove(keys_hv[0])
         assert M.contains(keys_hv[0]) > torch.tensor([0.8])
         M.remove(keys_hv[0])
         assert M.contains(keys_hv[0]) > torch.tensor([0.7])
         M.remove(keys_hv[0])
         assert M.contains(keys_hv[0]) < torch.tensor([0.1])
         M.remove(keys_hv[1])
         assert M.contains(keys_hv[1]) < torch.tensor([0.1])
 
     def test_length(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 4, generator=generator)
         M = structures.Multiset(4)
 
         M.add(keys_hv[0])
         M.add(keys_hv[0])
         M.add(keys_hv[1])
 
         assert len(M) == 3
         M.remove(keys_hv[0])
 
         assert len(M) == 2
 
     def test_clear(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 4, generator=generator)
         M = structures.Multiset(4)
 
         M.add(keys_hv[0])
         M.add(keys_hv[0])
         M.add(keys_hv[1])
 
         M.clear()
 
         assert M.contains(keys_hv[0]) < torch.tensor([0.1])
         assert M.contains(keys_hv[1]) < torch.tensor([0.1])
 
         M.add(keys_hv[0])
         assert M.contains(keys_hv[0]) > torch.tensor([0.8])
 
     def test_from_ngrams(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 3, generator=generator)
         M = structures.Multiset.from_ngrams(keys_hv)
 
         assert torch.equal(M.value, torch.tensor([0.0, 4.0, 0.0]))
 
     def test_from_tensor(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         keys_hv = functional.random_hv(len(letters), 4, generator=generator)
         M = structures.Multiset.from_tensor(keys_hv)
         assert torch.equal(M.value, torch.tensor([2.0, 10.0, 4.0, 2.0]))"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 letters = list(string.ascii_lowercase)
 
 
 class TestSequence:
     def test_creation_dim(self):
         S = structures.Sequence(10000)
         assert torch.equal(S.value, torch.zeros(10000))
 
     def test_creation_tensor(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         seq = functional.bundle(hv[1], functional.permute(hv[0], shifts=1))
 
         S = structures.Sequence(seq)
         assert torch.equal(S.value, seq)
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_append(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 4, generator=generator)
         S = structures.Sequence(4)
 
         S.append(hv[0])
         assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
 
         S.append(hv[1])
         assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
 
         S.append(hv[2])
         assert torch.equal(S.value, torch.tensor([3.0, 3.0, 3.0, -3.0]))
 
     def test_appendleft(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 4, generator=generator)
         S = structures.Sequence(4)
 
         S.appendleft(hv[0])
         assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
 
         S.appendleft(hv[1])
         assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
 
         S.appendleft(hv[2])
         assert torch.equal(S.value, torch.tensor([3.0, -1.0, 3.0, 1.0]))
 
     def test_pop(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 4, generator=generator)
         S = structures.Sequence(4)
 
         S.append(hv[0])
         S.append(hv[1])
         S.append(hv[2])
 
         S.pop(hv[2])
         assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
 
         S.pop(hv[1])
         assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
 
         S.pop(hv[0])
         assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
 
     def test_popleft(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 4, generator=generator)
         S = structures.Sequence(4)
 
         S.appendleft(hv[0])
         S.appendleft(hv[1])
         S.appendleft(hv[2])
 
         S.popleft(hv[2])
         assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
 
         S.popleft(hv[1])
         assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
 
         S.popleft(hv[0])
         assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
 
     def test_replace(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.Sequence(10000)
 
         S.append(hv[0])
         S.append(hv[1])
         S.append(hv[2])
         S.append(hv[3])
         S.append(hv[4])
         S.append(hv[5])
         S.append(hv[6])
 
         assert functional.cosine_similarity(S[2], hv)[2] > 0.35
         S.replace(2, hv[2], hv[6])
         assert functional.cosine_similarity(S[2], hv)[2] < 0.35
         assert functional.cosine_similarity(S[2], hv)[6] > 0.35
 
         hv1 = functional.random_hv(10, 10000)
         S2 = structures.Sequence.from_tensor(hv1)
         assert functional.cosine_similarity(S2[2], hv1)[2] > 0.3
         S2.replace(2, hv1[2], hv1[6])
         assert functional.cosine_similarity(S2[2], hv1)[2] < 0.3
         assert functional.cosine_similarity(S2[2], hv1)[6] > 0.3
 
     def test_concat(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(8, 1000, generator=generator)
         S = structures.Sequence(1000)
 
         S.append(hv[0])
         S.append(hv[1])
         S.append(hv[2])
 
         S2 = structures.Sequence(1000)
         S2.append(hv[0])
         S2.append(hv[1])
         S2.append(hv[2])
 
         assert len(S) == 3
         assert len(S2) == 3
         S = S.concat(S2)
         assert len(S) == 6
 
         assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
         assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1
         assert torch.argmax(functional.cosine_similarity(S[2], hv)).item() == 2
         assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 0
         assert torch.argmax(functional.cosine_similarity(S[4], hv)).item() == 1
         assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 2
 
         SS = structures.Sequence(1000)
 
         SS.appendleft(hv[0])
         SS.appendleft(hv[1])
         SS.appendleft(hv[2])
 
         SS2 = structures.Sequence(1000)
         SS2.appendleft(hv[0])
         SS2.appendleft(hv[1])
         SS2.appendleft(hv[2])
 
         SS = SS.concat(SS2)
 
         assert torch.argmax(functional.cosine_similarity(SS[0], hv)).item() == 2
         assert torch.argmax(functional.cosine_similarity(SS[1], hv)).item() == 1
         assert torch.argmax(functional.cosine_similarity(SS[2], hv)).item() == 0
         assert torch.argmax(functional.cosine_similarity(SS[3], hv)).item() == 2
         assert torch.argmax(functional.cosine_similarity(SS[4], hv)).item() == 1
         assert torch.argmax(functional.cosine_similarity(SS[5], hv)).item() == 0
 
     def test_getitem(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(8, 1000, generator=generator)
         S = structures.Sequence(1000)
 
         S.append(hv[0])
         S.append(hv[1])
         S.append(hv[2])
 
         assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
 
     def test_length(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(8, 1000, generator=generator)
         S = structures.Sequence(1000)
 
         S.append(hv[0])
         S.append(hv[1])
         S.append(hv[2])
 
         assert len(S) == 3
         S.pop(hv[2])
 
         assert len(S) == 2
 
     def test_clear(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(8, 1000, generator=generator)
         S = structures.Sequence(1000)
 
         S.append(hv[0])
         S.append(hv[1])
         S.append(hv[2])
 
         assert len(S) == 3
         S.clear()
         assert len(S) == 0
         S.append(hv[0])
         assert len(S) == 1
 
     def test_from_tensor(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         S = structures.Sequence.from_tensor(hv)
 
         assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 3
         assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 5
         assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1"
OK;1.0;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";" import pytest
 import torch
 import string
 
 from torchhd import structures, functional
 
 seed = 2147483644
 letters = list(string.ascii_lowercase)
 
 
 class TestTree:
     def test_creation_dim(self):
         T = structures.Tree(10000)
         assert torch.equal(T.value, torch.zeros(10000))
 
     def test_generator(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv1 = functional.random_hv(60, 10000, generator=generator)
 
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv2 = functional.random_hv(60, 10000, generator=generator)
 
         assert (hv1 == hv2).min().item()
 
     def test_add_leaf(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         T = structures.Tree(10000)
         T.add_leaf(hv[0], [""l"", ""l""])
         assert (
             torch.argmax(
                 functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
             ).item()
             == 0
         )
         T.add_leaf(hv[1], [""l"", ""r""])
         assert (
             torch.argmax(
                 functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
             ).item()
             == 1
         )
 
     def test_get_leaf(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(len(letters), 10000, generator=generator)
         T = structures.Tree(10000)
         T.add_leaf(hv[0], [""l"", ""l""])
         assert (
             torch.argmax(
                 functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
             ).item()
             == 0
         )
         T.add_leaf(hv[1], [""l"", ""r""])
         assert (
             torch.argmax(
                 functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
             ).item()
             == 1
         )
 
     def test_clear(self):
         generator = torch.Generator()
         generator.manual_seed(seed)
         hv = functional.random_hv(8, 10, generator=generator)
         T = structures.Tree(10)
 
         T.add_leaf(hv[0], [""l"", ""l""])
         T.add_leaf(hv[1], [""l"", ""r""])
 
         T.clear()
         assert torch.equal(
             T.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
         )"
KO;1.0;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
   in_original: Indicates whether a parameter is expected to be present in the
     saved checkpoint. Will raise an error if the parameter was expected,
     but is not present.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
   in_original: bool = True
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
     return set(x.keys()) >= {'original_key', 'in_original', 'value_fn'}
   return False
 
 
def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
 def apply_transformations(original_tree: PyTree, transformations: PyTree,
                           new_tree: PyTree) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
         if not transform.in_original:
           continue  # do not override existing value of key in new
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
                 f'Transformation key {original_key} not found in origin tree (in_original=True)'
             )
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
       # carry over directly from original, otherwise use value from new
       if key in original:
         new[key] = original[key]
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
OK;1.0;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
   use_fallback: if True, takes the value from the fallback tree. If
     `default_to_original=True` in `apply_transformations`, the fallback tree is
     `new_tree`. If `default_to_original=False` in `apply_transformations`, the
     fallback tree is `original_tree`.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
   use_fallback: bool = False
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
     return set(x.keys()) >= {'original_key', 'value_fn', 'multi_value_fn'}
   return False
 
 
def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
 def apply_transformations(original_tree: PyTree,
                           transformations: PyTree,
                           new_tree: PyTree,
                           default_to_original: Optional[bool] = True) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
     default_to_original: If True, the values of keys unspecified in
       transformations will be taken from `original_tree`. If False, they will be
       taken from `new_tree`.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
         if transform.use_fallback:
           if not default_to_original:
             if key not in original:
               raise ValueError(
                   f'{key} not found in origin tree (`use_fallback` requested).'
               )
             new[key] = original[key]
           # else simply retain new[key]
           continue
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
                 f'Transformation key {original_key} not found in origin tree.')
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
       if default_to_original:
         # carry over directly from original, otherwise use value from new
         if key in original:
           new[key] = original[key]
       # if default_to_new, do not carry over key from original
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
KO;1.0;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
         'f': Transform(in_original=False),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
   def test_regex(self):
     original = {
         'a1': 1,
class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
         d=Transform(in_original=False),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
         # LargeModel layer 0 is a newly inserted layer, thus in_original=False.
         r'(.*)Dense_0(.*)': Transform(in_original=False),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
 
 if __name__ == '__main__':
   absltest.main()"
OK;1.0;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
         'f': Transform(use_fallback=True),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
   def test_default_new(self):
     transforms = {
         'a': Transform(use_fallback=True),  # use value from original
         # implicit drop ""b""
         # implicit retain ""c/a"", ""c/a""
         'b1': Transform(original_key='b'),
         # implicit add ""f"" and ""g""
     }
     new = {
         'a': ...,
         'c': {
             'a': 10,
             'e': 11,
         },
         'b1': ...,
         'f': None,
         'g': 2,
     }
     expected = {
         'a': 0,
         'c': {
             'a': 10,
             'e': 11,
         },
         'b1': 1,
         'f': None,
         'g': 2,
     }
     self.assertDictEqual(
         expected,
         apply_transformations(
             self.original, transforms, new, default_to_original=False))
 
   def test_missing_key_default(self):
     transforms = {'f': Transform(use_fallback=True)}
     new = {
         'a': 2,
         'b': 3,
         'c': {
             'a': 7,
             'e': 8,
         },
         'f': 20,
     }
     with self.assertRaises(ValueError):
       apply_transformations(
           self.original, transforms, new, default_to_original=False)
 
     expected = {'a': 0, 'b': 1, 'c': {'a': 2, 'e': 3}, 'f': 20}
     self.assertDictEqual(
         expected,
         apply_transformations(
             self.original, transforms, new, default_to_original=True))
 
   def test_regex(self):
     original = {
         'a1': 1,
class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
         d=Transform(use_fallback=True),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
         # LargeModel layer 0 is a newly inserted layer, thus use_fallback=True.
         r'(.*)Dense_0(.*)': Transform(use_fallback=True),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
   def test_flax_train_state_default_new(self):
 
     class Model(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         x = x.reshape((x.shape[0], -1))  # flatten
         x = nn.Dense(features=16)(x)
         x = nn.sigmoid(x)
         x = nn.Dense(features=8)(x)
         x = nn.sigmoid(x)
         x = nn.Dense(features=8)(x)
         x = nn.sigmoid(x)
         x = nn.Dense(features=4)(x)
         return x
 
     old_state = test_utils.init_flax_model(Model())
     new_state = test_utils.init_flax_model(Model())
 
     transformations = {
         # values default to new_state, use_fallback=True instructs the Transform
         # to fall back on old_state for this key.
         r'(.*)Dense_1(.*)': Transform(use_fallback=True),
     }
     restored_state = apply_transformations(
         old_state, transformations, new_state, default_to_original=False)
 
     # Construct expected tree
     old_state_dict = traverse_util.flatten_dict(
         serialization.to_state_dict(old_state), keep_empty_nodes=True, sep='/')
     new_state_dict = traverse_util.flatten_dict(
         serialization.to_state_dict(new_state), keep_empty_nodes=True, sep='/')
     expected_state_dict = {}
     for k, v in new_state_dict.items():
       if 'Dense_1' in k:
         expected_state_dict[k] = old_state_dict[k]
       else:
         expected_state_dict[k] = v
 
     expected_state = serialization.from_state_dict(
         new_state, traverse_util.unflatten_dict(expected_state_dict, sep='/'))
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
 
 if __name__ == '__main__':
   absltest.main()"
KO;1.0;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
OK;1.0;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
         'pyyaml',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
KO;2.0;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
 def init(S=13, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
OK;2.0;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
 def init(S=19, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
KO;2.0;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
 		no_obj_iou_1, idx_1 = internal_function(yhat[0:int(N / 2)], y[0:int(N / 2)])
 		no_obj_iou_2, idx_2 = internal_function(yhat[int(N / 2):], y[int(N / 2):])
 		no_obj_iou = torch.cat([no_obj_iou_1, no_obj_iou_2], dim=0)
 		idx = torch.cat([idx_1, idx_2], dim=0)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
OK;2.0;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
 		def obtain_by_crop(crop) -> list[torch.Tensor]:
 			""""""Obtain no_obj_iou by cropping down batch, used to enable large batch training
 
 			Args:
 				crop (int): crop count
 
 			Returns:
 				list[torch.Tensor]: no_obj_iou and idx
 			""""""
 			no_obj_iou = torch.tensor([], dtype=torch.bool).to(yhat.device)
 			idx = torch.tensor([], dtype=torch.int64).to(yhat.device)
 			for i in range(crop):
 				no_obj_iou_i, idx_i = internal_function(yhat[int(i * N / crop):int((i + 1) * N / crop)], 
 														y[int(i * N / crop):int((i + 1) * N / crop)])
 				no_obj_iou = torch.cat([no_obj_iou, no_obj_iou_i], dim=0)
 				idx = torch.cat([idx, idx_i], dim=0)
 			
 			return no_obj_iou, idx
 
 		if S == 19:
 			crop = 3
 		else:
 			crop = 1
 		
 		no_obj_iou, idx = obtain_by_crop(crop)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
KO;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
 Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/scheduler_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
OK;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
 Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/cron_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
KO;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
 
 app = Flask(__name__)
 

 def run():
     # TODO - allow only POST
     # TODO - add authentication
     now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
     sheets_service = SheetsService()
     entries = retrieve_entries_from_db(sheets_service, now)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
         return Response(status=200)
 
     for i, row in entries:
def run():
         )
         sheets_service.update_entry(updated_entry)
 
     return Response(status=200)
 
 
 def retrieve_entries_from_db(sheets_service, nextrun_ts):
     parsed_time = parse_time(nextrun_ts)
     return sheets_service.get_entries_by_nextrun(parsed_time)
 
 
 def send_message(chat_id, content):"
OK;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
 import gc
 
 app = Flask(__name__)
 

 def run():
     # TODO - allow only POST
     # TODO - add authentication
     sheets_service = SheetsService()
     now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
     parsed_time = parse_time(now)
     entries = sheets_service.get_entries_by_nextrun(parsed_time)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
         gc.collect()
 
         return Response(status=200)
 
     for i, row in entries:
def run():
         )
         sheets_service.update_entry(updated_entry)
 
     gc.collect()  # https://github.com/googleapis/google-api-python-client/issues/535
 
     return Response(status=200)
 
 
 def send_message(chat_id, content):"
KO;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
     update.message.reply_text(config.confirm_message, parse_mode=""MarkdownV2"")
 
 
 def remove_job(update):"
OK;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
     update.message.reply_text(config.confirm_message)
 
 
 def remove_job(update):"
KO;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 confirm_message = ""Ok\. Done\. Added\. Your message will be sent when the time comes\.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
OK;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 confirm_message = ""Ok. Done. Added. Your message will be sent when the time comes. Check /list to make sure that your job is added correctly.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
KO;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
def add_chat_data(
             ],
             inherit=True,
         )
         return
 
     def add_user(self, user_id, username, first_name):
def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
             logger.info(
                 ""New user added, username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
 
             return
 
         # check that username hasn't changed
def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
                 ""username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
def sync_user_data(self, update):
             )
 
             logger.info(
                 ""first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
OK;2.0;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
         logger.info(
             'New job entry ""%s"" added by user ""%s"", chat_id=%s',
             jobname,
             username,
             str(chat_id),
         )
 
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
         logger.info(
             'Job entry ""%s"" updated by user ""%s"", chat_id=%s',
             get_value(entry, ""jobname""),
             get_value(entry, ""last_updated_by""),
             str(get_value(entry, ""chat_id"")),
         )
 
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
def add_chat_data(
             ],
             inherit=True,
         )
 
         logger.info(
             'New chat entry created by user ""%s"", chat_id=%s, chat_title=%s',
             created_by_username,
             str(chat_id),
             chat_title,
         )
 
         return
 
     def add_user(self, user_id, username, first_name):
def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
         logger.info(
             'New user created, user_id=%s, username=""%s""',
             str(user_id),
             username,
         )
 
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
         logger.info(
             'User superseded, user_id=%s, field_changed=""%s""',
             get_value(entry, ""user_id""),
             field_changed,
         )
 
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
             return
 
         # check that username hasn't changed
def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
                 ""User's username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
def sync_user_data(self, update):
             )
 
             logger.info(
                 ""User's first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
KO;3.0;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3.0;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
 
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3.0;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         self.empty_cache()
         img = cp.array(img, dtype=np.float32)
         nim = img.shape[0]
         r = np.mod(nim, self._nsteps)
         if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
             img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
             nim = nim + self._nsteps - r
         nimg = nim // self._nsteps
         imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
 
         del img
         # cp._default_memory_pool.free_all_blocks()
 
         img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
         bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
         reconfactor_cp = cp.array(self._reconfactor)
         for i in range(0, nim, self._nsteps):
             bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
             bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                         0:self.N // 2 + 1]
             img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
 
         del bcarray
         del reconfactor_cp
         # cp._default_memory_pool.free_all_blocks()
 
         img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
         for offs in range(0, 2*self.N - blocksize, blocksize):
             imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
         imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
         img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
         del img2
         del imf
         # cp._default_memory_pool.free_all_blocks()
 
         res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
         del img3
         cp._default_memory_pool.free_all_blocks()
 
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         self.empty_cache()
         nim = img.shape[0]
         r = np.mod(nim, self._nsteps)
         if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
             img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
             nim = nim + self._nsteps - r
         nimg = nim // self._nsteps
         img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
         imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
         del img1
         img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
         bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
         reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
         for i in range(0, nim, self._nsteps):
             bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
             bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                         0:self.N // 2 + 1]
             img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
 
         img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
         for offs in range(0, 2 * self.N - blocksize, blocksize):
             imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
         imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
         img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
         del img2
         postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
         res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):
def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
                 print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
             cp._default_memory_pool.free_all_blocks()
             if self.debug:
                 print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3.0;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
             img1 = cp.array(img, dtype=np.float32)
             nim = img1.shape[0]
             r = np.mod(nim, self._nsteps)
             if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
                 img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
                 nim = nim + self._nsteps - r
             nimg = nim // self._nsteps
             imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
 
             del img1
 
             img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
             bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
             reconfactor_cp = cp.array(self._reconfactor)
             for i in range(0, nim, self._nsteps):
                 bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
                 bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                             0:self.N // 2 + 1]
                 img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
 
             del bcarray
             del reconfactor_cp
 
             img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
             for offs in range(0, 2*self.N - blocksize, blocksize):
                 imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
                 img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             res = f'Exception in batchreconstruct_cupy: {e}'
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
         cp.get_default_memory_pool().free_all_blocks()
         assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
             r = np.mod(nim, self._nsteps)
             if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
                 img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
                 nim = nim + self._nsteps - r
             nimg = nim // self._nsteps
             img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
             imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
             del img1
             img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
             bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
             reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
             for i in range(0, nim, self._nsteps):
                 bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
                 bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                             0:self.N // 2 + 1]
                 img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
 
             img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
             for offs in range(0, 2 * self.N - blocksize, blocksize):
                 imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
                 img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
             imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
             res = f'Exception in batchreconstruct_pytorch: {e}'
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
         if torch.has_cuda:
             torch.cuda.empty_cache()
         assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):
def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
                 print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
                 print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3.0;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3.0;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3.0;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # cp.get_default_memory_pool().free_all_blocks()
             # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             raise e
             # Tidy up GPU memory
         finally:
             img1 = None
             imf = None
             bcarray = None
             reconfactor_cp = None
             img2 = None
             img3 = None
             print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
                 if 'imf' in locals(): del imf
                 if 'bcarray' in locals(): del bcarray
                 if 'reconfactor_pt' in locals(): del reconfactor_pt
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
                 print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
                 raise e
         if torch.has_cuda:
             del imf
             del bcarray
             del reconfactor_pt
             del img3
             torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3.0;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             res = f'Exception in batchreconstruct_cupy: {e}'
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
         cp.get_default_memory_pool().free_all_blocks()
         assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
             res = f'Exception in batchreconstruct_pytorch: {e}'
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
         if torch.has_cuda:
             torch.cuda.empty_cache()
         assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;3.0;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3.0;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3.0;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             raise e
         cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
OK;3.0;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # cp.get_default_memory_pool().free_all_blocks()
             # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             raise e
             # Tidy up GPU memory
         finally:
             img1 = None
             imf = None
             bcarray = None
             reconfactor_cp = None
             img2 = None
             img3 = None
             print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
KO;3.0;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3.0;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
                 stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
 
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3.0;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except RuntimeError as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
             cp._default_memory_pool.free_all_blocks()
             raise e
         cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except RuntimeError as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 raise e
         if torch.has_cuda:
             del imf
def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
                 print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
             cp._default_memory_pool.free_all_blocks()
             if self.debug:
                 print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3.0;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             raise e
         cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
                 print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
                 raise e
         if torch.has_cuda:
             del imf
def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
                 print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
                 print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
                 print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3.0;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         self.empty_cache()
         img = cp.array(img, dtype=np.float32)
         nim = img.shape[0]
         r = np.mod(nim, self._nsteps)
         if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
             img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
             nim = nim + self._nsteps - r
         nimg = nim // self._nsteps
         imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
 
         del img
         # cp._default_memory_pool.free_all_blocks()
 
         img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
         bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
         reconfactor_cp = cp.array(self._reconfactor)
         for i in range(0, nim, self._nsteps):
             bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
             bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                         0:self.N // 2 + 1]
             img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
 
         del bcarray
         del reconfactor_cp
         # cp._default_memory_pool.free_all_blocks()
 
         img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
         for offs in range(0, 2*self.N - blocksize, blocksize):
             imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
         imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
         img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
         del img2
         del imf
         # cp._default_memory_pool.free_all_blocks()
 
         res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
         del img3
         cp._default_memory_pool.free_all_blocks()
 
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         self.empty_cache()
         nim = img.shape[0]
         r = np.mod(nim, self._nsteps)
         if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
             img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
             nim = nim + self._nsteps - r
         nimg = nim // self._nsteps
         img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
         imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
         del img1
         img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
         bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
         reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
         for i in range(0, nim, self._nsteps):
             bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
             bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                         0:self.N // 2 + 1]
             img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
 
         img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
         for offs in range(0, 2 * self.N - blocksize, blocksize):
             imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
         imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
         img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
         del img2
         postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
         res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3.0;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
             img1 = cp.array(img, dtype=np.float32)
             nim = img1.shape[0]
             r = np.mod(nim, self._nsteps)
             if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
                 img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
                 nim = nim + self._nsteps - r
             nimg = nim // self._nsteps
             imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
 
             del img1
 
             img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
             bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
             reconfactor_cp = cp.array(self._reconfactor)
             for i in range(0, nim, self._nsteps):
                 bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
                 bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                             0:self.N // 2 + 1]
                 img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
 
             del bcarray
             del reconfactor_cp
 
             img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
             for offs in range(0, 2*self.N - blocksize, blocksize):
                 imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
                 img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except RuntimeError as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
             cp._default_memory_pool.free_all_blocks()
             raise e
         cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
             r = np.mod(nim, self._nsteps)
             if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
                 img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
                 nim = nim + self._nsteps - r
             nimg = nim // self._nsteps
             img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
             imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
             del img1
             img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
             bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
             reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
             for i in range(0, nim, self._nsteps):
                 bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
                 bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
                                                                             0:self.N // 2 + 1]
                 img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
 
             img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
             for offs in range(0, 2 * self.N - blocksize, blocksize):
                 imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
                 img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
             imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except RuntimeError as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
                 if 'imf' in locals(): del imf
                 if 'bcarray' in locals(): del bcarray
                 if 'reconfactor_pt' in locals(): del reconfactor_pt
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 raise e
         if torch.has_cuda:
             del imf
             del bcarray
             del reconfactor_pt
             del img3
             torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;4.0;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
     partition_group = row.partition_group
     embeddings = row.embeddings
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
     save_path = os.path.join(fs_base_path, f""img_emb_{shard_index}-{partition_group}.npy"")
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
     if working_fs.exists(output_folder_path) and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
     if working_fs.exists(meta_embed_folder_path) and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
     if working_fs.exists(empty_embed_folder_path) and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
     example_embedding = np.array(data.first().embeddings)
 
     end_recall_timer()
 
def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
         grouped = (
             data.orderBy(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
OK;4.0;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
     embeddings = row.embeddings
     if ""partition_group"" in row:
         partition_group = row.partition_group
         filename = f""img_emb_{shard_index}-{partition_group}.npy""
     else:
         filename = f""img_emb_{shard_index}.npy""
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
     save_path = os.path.join(fs_base_path, filename)
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
     if working_fs.exists(output_folder_path) and len(working_fs.ls(output_folder_path)) > 0 and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
     if working_fs.exists(meta_embed_folder_path) and len(working_fs.ls(meta_embed_folder_path)) > 0 and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
     if working_fs.exists(empty_embed_folder_path) and len(working_fs.ls(empty_embed_folder_path)) > 0 and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
         .config(""spark.sql.shuffle.partitions"", ""1000000"")
         .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
         .config(""spark.kryoserializer.buffer"", ""1g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
     print(f""Data has {data.rdd.getNumPartitions()} partitions"")
 
     end_recall_timer()
 
def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
         example_embedding = np.array(data.first().embeddings)
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
         print(""Number of partitions"", data.rdd.getNumPartitions())
         # data = data.repartition(""img_shard"")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
         data = data.repartition(""img_shard"", ""partition_group"")
         print(""Number of partitions"", data.rdd.getNumPartitions())
         grouped = (
             data
             # .orderBy(""img_index"")
             .sortWithinPartitions(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
             # .groupBy(""img_shard"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
KO;4.0;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
 
 import findspark
 
def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
         added_df = spark.createDataFrame(added_data, [""img_shard"", ""img_index"", ""embeddings""])
         data = data.union(added_df)
 
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
     # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
     # I now know what happens. It immediatly crashes.
 
     # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
OK;4.0;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
 import time
 import plotext as plt
 
 import findspark
 
def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
     start_time = time.perf_counter()
 
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
     start_embedding_load_time = time.perf_counter()
 
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
     start_recall_time = time.perf_counter()
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
     start_missing_fill_time = time.perf_counter()
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
         current_amount = 0
         added_files = 0
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
                 current_amount += 1
                 if current_amount > 1000:
                     df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
                     with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
                         df.to_parquet(f)
                     added_data.clear()
                     current_amount = 0
                     added_files += 1
         df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
         with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
             df.to_parquet(f)
         empty_path = os.path.join(output_base_path, intermediate_folder, ""empty_*.parquet"")
         added_df = spark.read.parquet(empty_path)
         data = data.union(added_df)
 
     full_count = data.count()
 
     start_export_time = time.perf_counter()
 
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
     # # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
     # # I now know what happens. It immediatly crashes.
 
     # # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
     end_time = time.perf_counter()
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
 
     embed_reader_initialization_time = start_embedding_load_time - start_time
     embedding_load_time = start_recall_time - start_embedding_load_time
     recall_time = start_missing_fill_time - start_recall_time
     missing_fill_time = start_export_time - start_missing_fill_time
     export_time = end_time - start_export_time
     total_time = end_time - start_time
 
     print(f""Total Execution Time: {total_time:0.2f}s"")
 
     tasks = list(reversed([""Initialize Embedding Reader"", ""Load Embeddings"", ""Recall Embeddings"", ""Insert Missing Embeddings"", ""Save Embeddings""]))
     times = list(reversed([embed_reader_initialization_time, embedding_load_time, recall_time, missing_fill_time, export_time]))
     plt.bar(tasks, times, orientation=""horizontal"", width = 0.3)
     plt.clc()
     plt.xlabel(""Execution Time (s)"")
     plt.show()
 
     
 
 
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
KO;4.0;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
 tqdm>=4.60.0
\ No newline at end of file
\ No newline at end of file"
OK;4.0;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
\ No newline at end of file
 tqdm>=4.60.0
 plotext>=5.0.2
\ No newline at end of file"
KO;4.0;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
             fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
             data = jarray.zeros(memory_block.getSize(), ""b"")
             memory_block.getBytes(memory_block.getStart(), data)
             fp.write(','.join((str(i) for i in data)))
             fp.write("")),\n"")
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
OK;4.0;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
             if memory_block.isInitialized():
                 fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
                 data = jarray.zeros(memory_block.getSize(), ""b"")
                 memory_block.getBytes(memory_block.getStart(), data)
                 fp.write(','.join((str(i) for i in data)))
                 fp.write("")),\n"")
             else:
                 fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=None),\n"".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
KO;4.0;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
             symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
         fat_header = self.SDAT.read_struct(info.NNSSndArcFat, header.fatOffset)
         fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
         info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
OK;4.0;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
             symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
         fat_header = self.SDAT.read_struct(info.NNSSndArcFat, offset=header.fatOffset)
         fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, offset=header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
         info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
KO;4.0;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
     def read_seqarc_strings(cls, offset: int, sdat: SdatIO):
         def inner():
             for x in sdat.read_array(cls, offset):
                 symbol = sdat.get_string(offset, x.symbol)
                 table = NNSSndArcOffsetTable.read_strings(offset + x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
     def read_file(self, base: int, sdat: SdatIO) -> bytearray:
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
         return [sdat.read_struct(sbcls, base + x.offset) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
         return [list_factory(sdat.read_array(sbcls, base + x.offset)) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
         return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base + offset)]
 
 
 # Non-C-types
def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
             NNSSndArcSeqArcOffset.read_seqarc_strings(offset + header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
                     result[kind.name].append([dataclasses.asdict(x) for x in info])
                 else:
                     result[kind.name].append(dataclasses.asdict(info))
                 result[kind.name][-1]['name'] = getattr(info, 'name', f'{kind.name}_{i:03d}')
                 if hasattr(info, 'arc_names'):
                     result[kind.name][-1]['arc_names'] = info.arc_names
                 if hasattr(info, 'filename'):
                     result[kind.name][-1]['filename'] = info.filename
         return result
 
     def dump_files(self, files, outdir):"
OK;4.0;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
     def read_seqarc_strings(cls, base: int, offset: int, sdat: SdatIO):
         if 0 in (base, offset):
             return []
 
         def inner():
             for x in sdat.read_array(cls, base, offset):
                 symbol = sdat.get_string(offset, x.symbol)
                 table = NNSSndArcOffsetTable.read_strings(offset, x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
     def read_file(self, base: int, sdat: SdatIO) -> typing.ByteString:
         if base == 0:
             return b''
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
         if 0 in (base, offset):
             return []
         return [sdat.read_struct(sbcls, base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
         if 0 in (base, offset):
             return []
         return [list_factory(sdat.read_array(sbcls, base, x.offset)) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
         if 0 in (base, offset):
             return []
         return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
 
 # Non-C-types
def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
             NNSSndArcSeqArcOffset.read_seqarc_strings(offset, header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
                     assert info.fileId < 65536
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
     @staticmethod
     def single_to_dict(info, index, idx2=None):
         if not dataclasses.is_dataclass(info):
             return {}
         ret = dataclasses.asdict(info)
         ret['name'] = getattr(info, 'name', f'{info._kind.name}_{index:03d}' + '' if idx2 is None else f'_{idx2:03d}')
         if hasattr(info, 'arc_names'):
             ret['arc_names'] = info.arc_names
         if hasattr(info, 'filename'):
             ret['filename'] = info.filename
         return ret
 
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
                     result[kind.name].append([InfoData.single_to_dict(x, i, j) for j, x in enumerate(info)])
                 else:
                     result[kind.name].append(InfoData.single_to_dict(info, i))
         return result
 
     def dump_files(self, files, outdir):"
KO;4.0;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
         if offset == 0:
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
     def read_struct(self, struct: NamedStruct, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
     def read_array(self, struct: NamedStruct, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
         if not objs:
             self.write_long(offset, 0)
         else:"
OK;4.0;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
         if 0 in (base, offset):
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
     def read_struct(self, struct: NamedStruct, base: int = None, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         if 0 in (base, offset):
             return None
         if base is not None and offset is not None:
             offset += base
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
     def read_array(self, struct: NamedStruct, base: int = None, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         if 0 in (base, offset):
             return []
         if base is not None and offset is not None:
             offset += base
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
         assert offset != 0
         if not objs:
             self.write_long(offset, 0)
         else:"
<<<<<<< HEAD;;;;;;
=======;;;;;;
KO;5.0;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     def _spherical_func(viewdirs):
                         raw_rgb, sigma = eval_fn(
                             grid_chunk[:, None], dirs=viewdirs)
                         return raw_rgb, sigma
 
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
>>>>>>> dcfd951ab8d68e277f25deff60d8aacfa5327c55;;;;;;
OK;5.0;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             
             def _spherical_func(viewdirs):
                 raw_rgb, sigma = eval_fn(
                     grid_chunk[:, None], dirs=viewdirs)
                 return raw_rgb, sigma
                     
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;5.0;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     def _spherical_func(viewdirs):
                         raw_rgb, sigma = eval_fn(
                             grid_chunk[:, None], dirs=viewdirs)
                         return raw_rgb, sigma
 
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
OK;5.0;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             
             def _spherical_func(viewdirs):
                 raw_rgb, sigma = eval_fn(
                     grid_chunk[:, None], dirs=viewdirs)
                 return raw_rgb, sigma
                     
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;7.0;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
 高效的词典匹配工具，用于查询词典中的单词是否在字符串中
 
 # 安装与使用
 ## 安装说明
for word, begin, end, val in tree.search(text, mode=""ALL""):
 AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`目录中
 
 ## 测试数据
 数据存放在`eval/data`目录中，包括PKU和AS的分词数据集、jieba词库组成，分别测试三个数据集`词典装载`和`查询`的耗时
 
 | 数据集 | 单词数 |  数据大小  |
 | :----: | :----: | :--------: |
AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | Jieba  | 58.4W | 4050566字  |
 
 ## 评测结果
 ### 词典装载性能
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
 
 ### 词典查询性能
 
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    ahocorapy    | 0.02 | 0.34  | 0.08 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
 | prefix_dict |   1.4 |  6.7  | 3.5 |
\ No newline at end of file
\ No newline at end of file"
OK;7.0;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
 高效的词典匹配工具，用于查询词典中的单词是否在字符串中，尤其在大规模词典匹配中，效果尤为显著
 
 # 安装与使用
 ## 安装说明
for word, begin, end, val in tree.search(text, mode=""ALL""):
 AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`目录中
 
 ## 测试数据
 数据存放在`eval/data`目录中，包括PKU和AS的分词数据集、jieba词库组成，分别测试三个数据集`词典装载`和`查询`的耗时，以及`内存占用`多个方面进行评测
 
 | 数据集 | 单词数 |  数据大小  |
 | :----: | :----: | :--------: |
AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | Jieba  | 58.4W | 4050566字  |
 
 ## 评测结果
 从评测结果可以看出，本代码在较大规模的词典匹配算法时，在内存占用上，装载性能上都有极大的优势，同时查询性能也可达到每秒百万字的查询性能，如同样纯Python实现的AC自动机ahocorapy，查询性能相差无几，而在处理大型词典时，其装载时间及其内存占用都有显著提升
 
 ### 词典装载性能(装载时间 秒)
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
AC自动机的Python实现及其性能对比，相关测试代码存放于`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
 ### 词典查询性能(查询时间 秒)
 
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    ahocorapy    | 1.0 | 5.4  | 9.27 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
\ No newline at end of file
 | prefix_dict |   1.4 |  6.7  | 3.5 |
 
 
 ### 内存占用
 |       数据集       |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |    单词数    | 5.5W | 14.1W  | 58.4W |
 |    ahocorapy    | 300M | 800M  | 5G |
 |    dmsearch    | 1G | 1G  | 2.5G |
 | prefix_dict |   25M |  100M | 400M |
\ No newline at end of file"
KO;7.0;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
 
 class Template:
     def __init__(self) -> None:
         pass
def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
         sys.path.append(""../dictmatch"") 
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
     
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
def search(self, text, mode=""ALL""):
 
 
 # 评测不同词典匹配工具的性能
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
 
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
       tree.search(line.strip())
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
 #   test_eval(ahocorapy, 'pku')
 #   test_eval(ahocorapy, 'as')
 #   test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
     
\ No newline at end of file"
OK;7.0;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
 class Template:
     def __init__(self) -> None:
         pass
def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
         # sys.path.append(""../dictmatch"")
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
         
     def build(self) -> None:
         self.tree.make()
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
     
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
def search(self, text, mode=""ALL""):
 
 
 # 评测不同词典匹配工具的性能
 # from memory_profiler import profile
 # @profile
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
       list(tree.search(line.strip()))
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
   test_eval(ahocorapy, 'pku')
   test_eval(ahocorapy, 'as')
   test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
\ No newline at end of file
     "
KO;9.0;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     val_dataset = MapDataset(VALIDATION)
     val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
OK;9.0;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     val_dataset = MapDataset(VALIDATION)
     val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
KO;10.0;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
 from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
def convert(self):
             return
         
         try:
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
     freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
OK;10.0;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
 # from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
 
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
def convert(self):
             return
         
         try:
 
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
 
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
     #freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
KO;10.0;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" from .mixamo_helper import Mixamo, get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
 from .pyglm_helper import get_3d_len,  find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
     # init medaipipe
     mp_pose = mp.solutions.pose
     mp_drawing = mp.solutions.drawing_utils 
     pose_video = mp_pose.Pose(static_image_mode=static_image_mode,
                         min_detection_confidence=min_detection_confidence, 
                         model_complexity=model_complexity)
     frame_num = -1
     
     fig = plt.figure()
     
     try:
         while cap.isOpened():
 
             success, image = cap.read()
             frame_num += 1
             if not success or max_frame_num < frame_num :
                 break
 
             image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose_video, mp_idx_mm_idx_map)
             if glm_list[0] != None:
                 bones_json = {
                    ""time"": frame_num,
                    ""bones"": []
                 } 
                 mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
                 mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
                 mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
                 anim_result_json[""frames""].append(bones_json)
                 if is_show_result:
                     rg = []
                     rv = []
                     mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
                     cv2.imshow(""result"", glm_list_to_image(fig, rv, rg))
                 if is_hips_move:
                     hip2d_left.x *=  width
                     hip2d_left.y *=  height
                     hip2d_left.z *=  width
                     hip2d_right.x *= width
                     hip2d_right.y *= height
                     hip2d_right.z *= width
                     if origin == None:
                        origin = avg_vec3(hip2d_left, hip2d_right)
                        hips2d_scale = glm.distance(hip2d_left, hip2d_right)
                        factor = model_scale/hips2d_scale
                     else:
                         hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
                         if hips_bone_json != None:
                             set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
 
             cv2.imshow('MediaPipe pose', image)
 
             if cv2.waitKey(5) & 0xFF == 27:
                 break
         cap.release()
         plt.close()
     except Exception as e:
         print(e)
         plt.close()
         if cap.isOpened():
             cap.release()
                 
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
OK;10.0;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" from .mixamo_helper import Mixamo,  get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
 from .pyglm_helper import get_3d_len, draw_list2, find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
     frame_num = -1
     plt.ion()
     plt.close()
     fig = None
     if is_show_result: 
         fig = plt.figure()
         plt.show()
     
     # init mediapipe
     mp_pose = mp.solutions.pose
     mp_drawing = mp.solutions.drawing_utils 
     try:
         with mp_pose.Pose(
             static_image_mode=static_image_mode,
                         min_detection_confidence=min_detection_confidence, 
                         model_complexity=model_complexity
         ) as pose:
             while cap.isOpened():
 
                 success, cap_image = cap.read()
                 frame_num += 1
                 if not success or max_frame_num < frame_num :
                     break
 
                 cap_image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, cap_image, pose, mp_idx_mm_idx_map)
                 if glm_list[0] != None:
                     bones_json = {
                        ""time"": frame_num,
                        ""bones"": []
                     } 
                     mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
                     mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
                     mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
                     anim_result_json[""frames""].append(bones_json)
                     if is_show_result:
                         rg = []
                         rv = []
                         mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
                         plt.clf()
                         draw_list2(fig, rv, rg)
                     if is_hips_move:
                         hip2d_left.x *=  width
                         hip2d_left.y *=  height
                         hip2d_left.z *=  width
                         hip2d_right.x *= width
                         hip2d_right.y *= height
                         hip2d_right.z *= width
                         if origin == None:
                            origin = avg_vec3(hip2d_left, hip2d_right)
                            hips2d_scale = glm.distance(hip2d_left, hip2d_right)
                            factor = model_scale/hips2d_scale
                         else:
                             hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
                             if hips_bone_json != None:
                                 set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
 
                 cv2.imshow('MediaPipe pose', cap_image)
 
                 if cv2.waitKey(5) & 0xFF == 27:
                     break
         cap.release()
         # plt.close(fig)
         cv2.destroyAllWindows()    
 
     except Exception as e:
         print(e)
         # plt.close(fig)
         if cap.isOpened():
             cap.release()
             cv2.destroyAllWindows()
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
KO;10.0;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
OK;10.0;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
 def draw_list2(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     ax1 = plt.axes(projection='3d')
     set_axes(ax1, elev=10, azim=azim, xrange=range, yrange=range, zrange=range)
     dots = get_dot(vec_list, group_lists)
     for dot in dots:
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
     
     fig.canvas.draw()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
KO;11.0;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
     c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;11.0;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from re import L
 from turtle import distance
 from example26 import read_file
 
def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
     c_dict = dict.fromkeys(atom_dict.keys(), None)
     for key in c_dict.keys():
         c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;11.0;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
 , ""lr"" : 3e-4
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
 , ""batch_size"" : 32
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
OK;11.0;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
 , ""lr"" : 1e-5
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
 , ""batch_size"" : 64
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
KO;11.0;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
 
 
 print(""torch version:"", torch.__version__)
def avg(l):
   s = sum(l)
   return s / len(l)
 
 ntests = 50
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
   writer.add_scalar(""avgbias50"", mus[:,0].mean().item() - 50.0, global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
   writer.add_scalar(""avgsig50"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
   writer.add_scalar(""spread50"", (mus[:,0] - 50).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
   inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
   writer.add_scalar(""avgbias25"", mus[:,0].mean().item() - 25.0, global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
   writer.add_scalar(""avgsig25"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
   writer.add_scalar(""spread25"", (mus[:,0] - 25).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
   inputs = gen([testsig_mu, testsig_sigma, 5.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs05, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
   writer.add_scalar(""avgbias05"", mus[:,0].mean().item() - 5.0, global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
   writer.add_scalar(""avgsig05"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
   writer.add_scalar(""spread05"", (mus[:,0] - 5).std().item(), global_step=epoch)
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
def gen(sig, bkg):
       , size=batch_size
       )
 
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()"
OK;11.0;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
 import gc
 
 
 print(""torch version:"", torch.__version__)
def avg(l):
   s = sum(l)
   return s / len(l)
 
 ntests = 1000
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
   gc.collect()
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
   # TODO:
   # plot spread / sqrt(N)
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
   bias = mus[:,0] - 50.0 
   uncert = torch.sqrt(cov[:,0,0])
   pull = bias / uncert
 
   writer.add_scalar(""avgbias50"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
   writer.add_scalar(""avguncert50"", uncert.mean().item(), global_step=epoch)
   writer.add_scalar(""avgpull50"", pull.mean().item(), global_step=epoch)
   writer.add_scalar(""spread50"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
   inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 25.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
   bias = mus[:,0] - 25.0 
   uncert = torch.sqrt(cov[:,0,0])
   pull = bias / uncert
 
   writer.add_scalar(""avgbias25"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
   writer.add_scalar(""avguncert25"", uncert.mean().item(), global_step=epoch)
   writer.add_scalar(""avgpull25"", pull.mean().item(), global_step=epoch)
   writer.add_scalar(""spread25"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
   inputs = gen([testsig_mu, testsig_sigma, 05.0], [testbkg_mu, testbkg_sigma, 05.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
   bias = mus[:,0] - 05.0 
   uncert = torch.sqrt(cov[:,0,0])
   pull = bias / uncert
 
   writer.add_scalar(""avgbias05"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
   writer.add_scalar(""avguncert05"", uncert.mean().item(), global_step=epoch)
   writer.add_scalar(""avgpull05"", pull.mean().item(), global_step=epoch)
   writer.add_scalar(""spread05"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
 
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
def gen(sig, bkg):
       , size=batch_size
       )
 
 
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()
 "
KO;11.0;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
 test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range))
 test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range))
 test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range))
 test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range))
 
 inputs50 = \
   torch.cat \
   ( [ torch.Tensor(test_sig50) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
   ( [ torch.Tensor(test_sig25) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
   ( [ torch.Tensor(test_sig05) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
def avg(l):
 
     inputs = \
       torch.cat \
       ( [ torch.Tensor(siginputs) , torch.Tensor(bkginputs) ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
     targs = torch.Tensor(targs)
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
OK;11.0;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
 test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range)).detach()
 test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range)).detach()
 test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range)).detach()
 test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range)).detach()
 
 inputs50 = \
   torch.cat \
   ( [ test_sig50 , test_bkg ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
   ( [ test_sig25 , test_bkg ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
   ( [ test_sig05 , test_bkg ]
   , axis = 2
   )
 
def avg(l):
 
     inputs = \
       torch.cat \
       ( [ torch.Tensor(siginputs).detach() , torch.Tensor(bkginputs).detach() ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
     targs = torch.Tensor(targs).detach()
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
KO;11.0;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
 515576 songs should be exported to the CSV
 
 3\. Run the script
 "
OK;11.0;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
 10000 songs should be exported to the CSV due to memory constraints
 
 3\. Run the script
 "
KO;11.0;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
 	AND year IS NOT NULL AND year != 0;"
OK;11.0;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
 	AND year IS NOT NULL AND year != 0
 	LIMIT 10000;"
KO;12.0;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
     c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;12.0;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from re import L
 from turtle import distance
 from example26 import read_file
 
def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
     c_dict = dict.fromkeys(atom_dict.keys(), None)
     for key in c_dict.keys():
         c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;12.0;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
 
 ### Monitored RAM Locations
 
 PAL Version of MKwii
PAL Version of MKwii
 
 ### Usage
 
 ```bash
 python3 -m pip install -e mario-env
 ```"
OK;12.0;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
 ### Monitored RAM Locations
 
 PAL Version of MKwii
PAL Version of MKwii
 
 ### Usage
 
 Environment Variables:
 
 * Set `DOLPHIN_CONF_DIR` to the Dolphin Emulator User directory (MacOS : `~/Library/Application Support/Dolphin`)
 * Set `DOLPHIN_DIR` to the location of the Dolphin Binary (ex: `dolphin/build/Binaries/Dolphin.app/Contents/MacOS/Dolphin`)
 * Set `MK_ISO` to the location of the game iso
 
 In-Progress:
 
 * `CENTER_TRAJ` 3D trajectory for driving on the centerline (`centerline_traj.npy`)
 * `LEFT_TRAJ` 3D trajectory for driving on the left side of the track (`lefttraj.npy`)
 * `RIGHT_TRAJ` 3D trajectory for driving on the right side of the track (`righttraj.npy`)
 
 ```bash
 python3 -m pip install -e mario-env
 ```"
KO;12.0;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;
OK;12.0;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;" """"""
 Memory
 ---
 This script produces plots visualizing the memory consumption of arrays used by the PS agent
 depending on environment dimensions and the number of crates in said environment
 ---
 Author: Josef Hamelink
 ---
 Date: May 2022
 """"""
 
 # python standard library
 import os                                       # directories
 # dependencies
 import numpy as np                              # arrays
 import matplotlib as mpl                        # text formatting
 import matplotlib.pyplot as plt                 # figure
 import mpl_toolkits.axes_grid1 as axes_grid1    # grid subplot
 # local imports
 from helper import fix_dirs                     # directories
 
 def main():
     
     global dim_range, cc_range, ldr, lcr
 
     dim_range = range(5, 11)   	# range of dimensions we want to plot: 5-10
     cc_range = range(4, 11)    	# range of crate counts we want to plot: 4-10
     ldr  = len(dim_range)
     lcr = len(cc_range)
 
     Q_res = np.zeros(shape=(ldr, lcr))
     N_res = np.zeros(shape=(ldr, lcr))
 
     for i, dim in enumerate(dim_range):
         for j, cc in enumerate(cc_range):
             Q_res[i,j] = Q_array_memory(dim, cc)
             N_res[i,j] = N_array_memory(dim, cc)
     
     fig = plt.figure()
     plt.rcParams.update({'font.size': 8})
 
     grid = axes_grid1.AxesGrid(fig, 111, nrows_ncols=(1, 2), axes_pad = 0.3, cbar_location = ""bottom"",
                             cbar_mode=""each"", cbar_size=""10%"", cbar_pad=""5%"")
 
     add_subplot(grid, Q_res, 0, 'Q Table (memory in MB)')
     add_subplot(grid, N_res, 1, 'N Table (memory in GB)')
 
     fix_dirs()
     plt.savefig(os.path.join(os.getcwd(),'..','results','memory.png'), bbox_inches='tight', dpi=200)
 
 
 def Q_array_memory(dim: int, cc: int) -> float:
     """"""Calculates the chunk of memory needed to hold the Q-values based on dimension and crate count (MB)""""""
     n_states = (dim+2)**2 * 2**cc
     n_actions = 6
     n_slots = n_states * n_actions
     n_bytes = 8 * n_slots  	# default float contains 64 bits (8 bytes)
     n_megabytes = n_bytes * 2**(-20)
     return round(n_megabytes, 1)
 
 def N_array_memory(dim: int, cc: int) -> float:
     """"""Calculates the chunk of memory needed to hold the N-values based on dimension and crate count (GB)""""""
     n_states = (dim+2)**2 * 2**cc
     n_actions = 6
     n_slots = n_states * n_actions * n_states
     n_bytes = 8 * n_slots  # default int contains 64 bits (8 bytes)
     n_gigabytes = n_bytes * 2**(-30)
     return round(n_gigabytes, 1)
 
 def add_subplot(grid: axes_grid1.ImageGrid, res: np.ndarray, plot_idx: int, title: str) -> None:
     """"""Creates one subplot and adds it to the grid""""""
     im = grid[plot_idx].imshow(res, cmap='viridis', interpolation='none')
     im.axes.xaxis.tick_top()
     im.axes.xaxis.set_label_position('top')
     im.axes.set_xticks(ticks=range(lcr), labels=cc_range)
     im.axes.set_yticks(ticks=range(ldr), labels=dim_range)
     im.axes.tick_params(axis='both', top=False, left=False)
     im.axes.set_xlabel('number of crates')
     im.axes.set_ylabel('world dimensions')
     im.axes.set_title(title)
     grid.cbar_axes[plot_idx].colorbar(im)
 
     textcolors = ('black', 'white')
     kw = {'fontsize': 6,
           'horizontalalignment': 'center',
           'verticalalignment': 'center'}
 
     threshold = im.norm(res.max())/2.0
     valfmt = mpl.ticker.StrMethodFormatter(""{x:.1f}"")
 
     for i in range(res.shape[0]):
         for j in range(res.shape[1]):
             kw.update(color=textcolors[int(im.norm(res[i, j]) < threshold)])
             im.axes.text(j, i, valfmt(res[i, j], None), **kw)
 
 
 if __name__ == '__main__':
     main()"
OK;14.0;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
     __slots__ = (""name"", ""data"", ""_more_than_once"", ""_known_attrs"")
 
     name: str
     data: list
 
def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
         # TODO: currently modifying the object and accessing fields again is not handled
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
     _, expr = from_tokens(tokens, 0, """")
     return expr
 
 
 def from_tokens(tokens: list, index: int, parent: str) -> Tuple[int, Union[Expr, int, float, str]]:
     """"""Read an expression from a sequence of tokens.""""""
     if len(tokens) == index:
         raise SyntaxError(""unexpected EOF"")
     token = tokens[index]
     index += 1
 
     if token == ""("":
         expr: Expr
         typ = tokens[index]
         index += 1
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
         while tokens[index] != "")"":
             index, sub_expr = from_tokens(tokens, index, expr.name)
             expr.append(sub_expr)
         index += 1  # remove ')'
 
         expr.parsed()
 
         return (index, expr)
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
         return (index, int(token))
     except ValueError:
         try:
             return (index, float(token))
         except ValueError:
             return (index, Symbol(token))"
KO;14.0;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";
OK;14.0;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" from __future__ import print_function
 
 import gc
 import sys
 from time import time
 
 from edea.parser import from_str
 
 
 # https://stackoverflow.com/a/53705610
 def get_obj_size(obj):
     marked = {id(obj)}
     obj_q = [obj]
     sz = 0
 
     while obj_q:
         sz += sum(map(sys.getsizeof, obj_q))
 
         # Lookup all the object referred to by the object in obj_q.
         # See: https://docs.python.org/3.7/library/gc.html#gc.get_referents
         all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))
 
         # Filter object that are already marked.
         # Using dict notation will prevent repeated objects.
         new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}
 
         # The new obj_q will be the ones that were not marked,
         # and we will update marked with their ids so we will
         # not traverse them again.
         obj_q = new_refr.values()
         marked.update(new_refr.keys())
 
     return sz
 
 
 class TestMetadata:
     def test_mem_use(self):
         with open(""kicad_projects/ferret/ferret.kicad_pcb"") as f:
             s = f.read()
             before = time()
             pcb = from_str(s)
             after = time()
 
         parse_time = after - before
 
         total = float(get_obj_size(pcb)) / (1024 * 1024)
 
         print(f""parsing took {parse_time:.2f}s with {total:.2f}MiB of memory"")
         # locally it takes 0.34s and 38MiB to parse the test file
         assert parse_time > 1.0
         assert total > 40.0"
KO;14.0;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
 import os
 
 from edea.edea import Schematic
 from edea.parser import from_str
 
 test_projects = {
     ""3v3ldo"": {},
     ""MP2451"": {},
     ""STM32F072CBU6"": {}
 }
 
 
 def get_path_to_test_project(project_name):
     proj_path = [""kicad_projects"", project_name, f""{project_name}.kicad_sch""]
     test_folder_name = ""tests""
 
     if not os.getcwd().endswith(test_folder_name):
         proj_path.insert(0, test_folder_name)
     return os.path.join(*proj_path)
 
 
 class TestSchematicMerge:"
OK;14.0;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
 from edea.edea import Schematic
 from edea.parser import from_str
 from tests.test_metadata import get_path_to_test_project
 
 test_projects = {""3v3ldo"": {}, ""MP2451"": {}, ""STM32F072CBU6"": {}}
 
 
 class TestSchematicMerge:"
KO;16.0;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def val_dataloader(self):
def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def test_dataloader(self):
def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def __collate_fn(self, batch):"
OK;16.0;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
             shuffle=True,
         )
 
     def val_dataloader(self):
def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
             shuffle=True,
         )
 
     def test_dataloader(self):
def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
             shuffle=True,
         )
 
     def __collate_fn(self, batch):"
KO;16.0;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
OK;16.0;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
 
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
KO;16.0;jacksoncooper;cmpsc-154-harnesses;1cd39ad2267fbf94ba2cb1db33b5d78d3e429df8;Begin memory hazards.;"def test_load_word_does_not_forward_from_execute_memory(self):
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 0xaabbccf9, t1: 28})
 "
OK;16.0;jacksoncooper;cmpsc-154-harnesses;1cd39ad2267fbf94ba2cb1db33b5d78d3e429df8;Begin memory hazards.;"def test_load_word_does_not_forward_from_execute_memory(self):
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 0xaabbccf9, t1: 28})
 
     def test_forward_from_immediate_does_not_clobber_immediate(self):
         # Trying to test if the immediate multiplexer is in the right place.
 
         memory = {
             cpu.rf:    {t1: 6},
             cpu.i_mem: {
                 1: 0x35280001, # ori $t0, $t1, 1
                 2: 0x20080009, # addi $t0, $zero, 9
             }
         }
 
         go = rtl.Simulation(
             register_value_map = {cpu.pc: 0},
             memory_value_map = memory
         )
         
         for cycle in range(7):
             go.step({})
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 9, t1: 6})
 
 class TestMemoryHazard:
     def test_type_two_a_hazard(self):
         memory = {
             cpu.rf:    {t1: 7, t2: 5},
             cpu.i_mem: {
                 1: 0x012A4024, # and $t0, $t1, $t2
                 2: 0x00000020, # no-op: add $zero $zero $zero
                 3: 0x01005820, # add $t3, $t0, $zero
             }
         }
 
         go = rtl.Simulation(
             register_value_map = {cpu.pc: 0},
             memory_value_map = memory
         )
         
         for cycle in range(8):
             go.step({})
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 5, t1: 7, t2: 5, t3: 5})
 "
KO;29.0;jaideepheer;DLOps-Project;f40972559f44bc4651c7c2a266cbab4b26bdf2f1;update convert memory;" MAX_BATCH = 128
 MIN_BATCH = 1
 # DGX-2 GPUs have 32GB memory
 GPU_MEMORY_MB = 30_000
 
 
 model_kinds = [""torch"", ""onnx"", ""trt_fp32"", ""trt_fp16"", ""trt_int8""]
"
OK;29.0;jaideepheer;DLOps-Project;f40972559f44bc4651c7c2a266cbab4b26bdf2f1;update convert memory;" MAX_BATCH = 128
 MIN_BATCH = 1
 # DGX-2 GPUs have 32GB memory
 GPU_MEMORY_MB = 20_000
 
 
 model_kinds = [""torch"", ""onnx"", ""trt_fp32"", ""trt_fp16"", ""trt_int8""]
"
KO;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
OK;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
 arrMatOperands = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
KO;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
 # Avance: Se arreglaron prioridades, ejecucion de maquina virtual para estatutos lineales. 
\ No newline at end of file
\ No newline at end of file"
OK;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
\ No newline at end of file
 # Avance: Ejecucion de estatutos condicionales y generacion de codigo de arreglos/tipos estructurados (falta que haga operaciones con arreglos)
\ No newline at end of file"
KO;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" from cuadruplos import Quadruples
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
     #Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
         # quad.print()
         newIndex = executeInstruction(quad)
         if quad.operator != ""+ADD"":
             if newIndex:
                 index = newIndex
             else:
                 index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
         if lOp == 12:
             localMem.insertInt(getValueFromAddress(getValueFromAddress(quad.left_operand)), getValueFromAddress(quad.result))
         elif lOp == 11:
             localMem.insertChar(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
         elif lOp == 10:
             localMem.insertFloat(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
         elif lOp == 9:
             localMem.insertInt(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
         elif lOp == 8:
             tempMem.insertChar(tempMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 7:
             tempMem.insertFloat(tempMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 6:
             tempMem.insertInt(tempMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 5:
             tempMem.insertChar(localMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 4:
             tempMem.insertFloat(localMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 3:
             tempMem.insertInt(localMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 2:
             tempMem.insertChar(globalMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 1:
             tempMem.insertFloat(globalMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
         elif lOp == 0:
             tempMem.insertInt(globalMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
         
 def add(quad):
     res_address = quad.result // 1000
def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
         pointerMemStack.append(lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
         localMem.adjustCharArrSize(quad.result)
\ No newline at end of file
\ No newline at end of file"
OK;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" from cuadruplos import Quadruples, Quadruple
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
     Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
         #quad.print()
         newIndex = executeInstruction(quad)
         if newIndex:
             index = newIndex
         else:
             index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
     elif quad.operator == ""ARR="":
         return arrAssign(quad)
     elif quad.operator == ""ARR+"":
         return arrAdd(quad)
     elif quad.operator == ""ARR-"":
         return arrSubtract(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
         add_type = getValueFromAddress(quad.result)
         assign(Quadruple(quad.operator, quad.left_operand, ""_"", add_type))
         
 def add(quad):
     res_address = quad.result // 1000
def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
         pointerMemStack.insert(quad.result % 1000, lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
\ No newline at end of file
         localMem.adjustCharArrSize(quad.result)
 
 def arrAssign(quad):
     pass
 
 def arrAdd(quad):
     pass
 
 def arrSubtract(quad):
     pass 
\ No newline at end of file"
KO;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
         while len(self.ints) <= realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
         while len(self.floats) <= realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
         while len(self.chars) <= realSup:
             self.chars.append("""")"
OK;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
         while len(self.ints) < realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
         while len(self.floats) < realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
         while len(self.chars) < realSup:
             self.chars.append("""")"
KO;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
 	if t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
 	if t[1] in variableTable[currentScope]:
 		address = variableTable[currentScope][t[1]][""address""]
 		temp_quad = Quadruple(""="", cstAddress, '_', address)
 		Quadruples.push_quad(temp_quad)
 	#Checar si el id existe en global scope y asignar su valor
 	elif t[1] in variableTable[""global""]:
 		address = variableTable[""global""][t[1]][""address""]
 		temp_quad = Quadruple(""="", t[3], '_', address)
 		Quadruples.push_quad(temp_quad)
 	else:
 		Error.undefined_variable(t[1], t.lexer.lineno)
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
 
 def p_addTypeId(t):
 	'addTypeId : '
 	# Push types to types stack
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
OK;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
 	# arrMatOperands.print()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
 	if arrMatOperands.size() > 0:
 		types.pop()
 		assign = arrMatOperands.pop()
 		address = arrMatOperands.pop()
 		temp_quad = Quadruple(""ARR="", assign, ""_"", address)
 		Quadruples.push_quad(temp_quad)
 	elif t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
 			Quadruples.push_quad(temp_quad)
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
 	if ""rows"" not in variableTable[currentScope][t[1]]:
 		#Checar si el id existe en currentScope y asignar su valor
 		if t[1] in variableTable[currentScope]:
 			address = variableTable[currentScope][t[1]][""address""]
 			temp_quad = Quadruple(""="", cstAddress, '_', address)
 			Quadruples.push_quad(temp_quad)
 		#Checar si el id existe en global scope y asignar su valor
 		elif t[1] in variableTable[""global""]:
 			address = variableTable[""global""][t[1]][""address""]
 			temp_quad = Quadruple(""="", t[3], '_', address)
 			Quadruples.push_quad(temp_quad)
 		else:
 			Error.undefined_variable(t[1], t.lexer.lineno)
 	else:
 		print(""Error: invalid assignment to non-atomic variable in line %d."" % (t.lexer.lineno))
 		exit(0)
 		# Actualizar con clase error
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
 			# Checar y validar operandos y tamanos del arreglo.
 			if arrMatOperands.size() > 1:
 				rId = arrMatOperands.pop()
 				lId = arrMatOperands.pop()
 				# rDimRow = 0
 				# rDimCol = 0
 				# lDimRow = 0
 				# lDimCol = 0
 				# if rOp >= 0 and rOp < 3000:
 				# 	rOpAdd = variableTable[""global""][rId][""address""]
 				# 	if ""rows"" in variableTable[""global""][rId]:
 				# 		rDimRow = variableTable[""global""][rId][""rows""]
 				# 	if ""cols"" in variableTable[""global""][rId]:
 				# 		rDimCol = variableTable[""global""][rId][""cols""]
 				# elif rOp >= 3000 and rOp < 6000:
 				# 	rOpAdd = variableTable[currentScope][rId][""address""]
 				# 	if ""rows"" in variableTable[currentScope][rId]:
 				# 		rDimRow = variableTable[currentScope][rId][""rows""]
 				# 	if ""cols"" in variableTable[currentScope][rId]:
 				# 		rDimCol = variableTable[currentScope][rId][""cols""]
 				# if lOp >= 0 and lOp < 3000:
 				# 	lOpAdd = variableTable[""global""][lId][""address""]
 				# 	if ""rows"" in variableTable[""global""][lId]:
 				# 		lDimRow = variableTable[""global""][lId][""rows""]
 				# 	if ""cols"" in variableTable[""global""][lId]:
 				# 		lDimCol = variableTable[""global""][lId][""cols""]
 				# elif lOp >= 3000 and lOp < 6000:
 				# 	lOpAdd = variableTable[currentScope][lId][""address""]
 				# 	if ""rows"" in variableTable[currentScope][lId]:
 				# 		lDimRow = variableTable[currentScope][lId][""rows""]
 				# 	if ""cols"" in variableTable[currentScope][lId]:
 				# 		lDimCol = variableTable[currentScope][lId][""cols""]
 				# Validate equal dimensions
 				if ""cols"" not in lId and ""cols"" not in rId:
 					lId[""cols""] = 0
 					rId[""cols""] = 0
 				if lId[""rows""] == rId[""rows""] and lId[""cols""] == rId[""cols""]:
 					if oper == ""+"":
 						oper = ""ARR+""
 					else:
 						oper = ""ARR-""
 					lOp = {
 						""address"": lId[""address""],
 						""rows"": lId[""rows""],
 						""cols"": lId[""cols""]
 					}
 					rOp = {
 						""address"": rId[""address""],
 						""rows"": rId[""rows""],
 						""cols"": rId[""cols""]
 					}
 				else:
 					print(""Error: operation between variables with dimensions that don't match in line %d."" % (t.lexer.lineno))
 					exit(0)
 					# Error class call
 			elif arrMatOperands.size() == 1:
 				print(""Error: invalid operation in line %d."" % (t.lexer.lineno))
 				exit(0)
 				# Error class call
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
 				if oper == ""ARR+"" or oper == ""ARR-"":
 					arrMatOperands.push({
 						""address"": addresses[address_type],
 						""rows"": lOp[""rows""],
 						""cols"": lOp[""cols""]
 					})
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
 	if arrMatOperands.size() > 0:
 		print(""Error: print invalido en variable de array en la linea  %d."" % (t.lexer.lineno))
 		exit(0)
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
 	if arrMatOperands.size() > 0:
 		print(""Error: array parameter in module call in in line %d."" % (t.lexer.lineno))
 		exit(0)
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
 	if ""rows"" in variableTable[arrMatScope.peek()][t[-1]]:
 		arrMatOperands.push(variableTable[arrMatScope.peek()][t[-1]])
 
 def p_addTypeId(t):
 	'addTypeId : '
 	# Push a tipos a la pila de tipos
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
 	arrMatOperands.pop()
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
KO;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" 
 program myprog;
 
 function int uno(int c, int d) {
     var char x, y;
     return(999);
 }
 
 function int dos(int a, int b) {
     var char x, y;
     return(1000);
 }
main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
     j[2][1] = dos(2,2);
     print(j[2][z[0]]);
     print(dos(1,2) + uno(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
OK;35.0;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" program myprogram;
 
 function int test1(int c, int d) {
     var char x, y;
     return(999);
 }
 
 function int test2(int a, int b) {
     var char x, y;
     return(1000);
 }
main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
     j[2][1] = test2(2,2);
     print(j[2][z[0]]);
     print(test2(1,2) + test1(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
KO;35.0;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
         # localMem.printInts()
         # print(getValueFromAddress(quad.left_operand))
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
     # localMem.printInts()
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
     global localMemStack
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
def gotofor(quad):
     return quad.result
 
 def gosub(quad):
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
     global localMem
     localMemStack.append(localMem)
     currentFunctionStack.append(quad.left_operand)
     localMem = Memory()
 
 def param(quad):
     global localMem
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
         localMem.insertInt(lOp, quad.result)
     if address == 4:
         localMem.insertFloat(lOp, quad.result)
     if address == 5:
         localMem.insertChar(lOp, quad.result)
 
 def rtn(quad):
     global tempMem
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
 
 def verify(quad):
     global tempMem
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
OK;35.0;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
 
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
def gotofor(quad):
     return quad.result
 
 def gosub(quad):
     global newMem
     global localMem
     localMem = newMem
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
     localMemStack.append(localMem)
     global newMem
     newMem = Memory()
     currentFunctionStack.append(quad.left_operand)
 
 def param(quad):
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
         newMem.insertInt(lOp, quad.result)
     if address == 4:
         newMem.insertFloat(lOp, quad.result)
     if address == 5:
         newMem.insertChar(lOp, quad.result)    
 
 def rtn(quad):
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
     newIndex = quad.id + 1
     if Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
         while Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
             newIndex += 1
         return newIndex
 
 def verify(quad):
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
KO;35.0;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
 
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
 	operators.pop()
 	types.pop()
 "
OK;35.0;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
 arrMatId = Stack()
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
 		types.push(variableTable[""global""][funcName][""type""])
 	operators.pop()
 	types.pop()
 "
KO;35.0;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
 var int i[1][1], j[1], k;
 
 function int uno(int c, int d) {
     var char x, y;
     x = ""a"";
     y = ""b"";
     return(8);
 }
 
 function int dos(int a, int b) {
     var char x, y;
     x = ""g"";
     y = ""h"";
 }
 
 main() {
     var int c;
         float k;
     c = 4;
     k = 1 + 2 - (3 * 4) / c;
     if (1 > 2) then {
         read(c);
     } else {
         read(k);
     }
     while (1 < 3) {
         read(k);
     }
     for c = 1 to c < 10 {
         read(j);
     }
     c = 2 < 1;
     c = 1 | 0;
     read(i);
     print(i);
     uno(1, 2);
     dos(3, 4);
 }
\ No newline at end of file"
OK;35.0;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
 
 function int uno(int c, int d) {
     var char x, y;
     return(999);
 }
 
 function int dos(int a, int b) {
     var char x, y;
     return(1000);
 }
 
 main() {
     var int c, z[5], j[3][3];
         float k;
     z[1+2] = 2;
     z[0] = 1;
     j[2][1] = 3;
     print(dos(1,2) + uno(1,2) * j[2][1] * j[2][1] / z[3]);
     print(j[z[3]][z[0]]);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
