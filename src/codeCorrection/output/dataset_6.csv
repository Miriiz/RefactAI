Label;Page;Username;Repo;Commit;Bug;Code
KO;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
OK;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+memoryhammer.com
\ No newline at end of file"
KO;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;\ No newline at end of file
OK;1;abishekmuthian;memory-hammer;0b6b93b7ec14dc9934e00d70186ba21e3cdeb312;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+theme: jekyll-theme-minimal
\ No newline at end of file"
KO;1;abishekmuthian;memory-hammer;1d88f9e750a7570d2d52a849aec7daa0920315bb;Updated README.md with link to needgap on the problem statement and highlighted the memory_hammer.py file name.;"An always-on Anki review system.
 Click the above image for a video demo.
 
 ### Why
-To solve Anki review accumulation by making the cards available for review when its due using always-on display system.
 
 ### How
 Using e-paper display attached to a raspberry pi and memory-hammer software.
@@ -28,7 +28,7 @@ Using e-paper display attached to a raspberry pi and memory-hammer software.
 5. pip3.10 install -r requirements.txt
 
 ### Usage
-1. Edit the **Config** section of the memory_hammer.py with the IP address of your Anki Desktop and port for Anki Connect.
 2. python3.10 memory_hammer.py
 
 #### Download the Decks using Get Decks"
OK;1;abishekmuthian;memory-hammer;1d88f9e750a7570d2d52a849aec7daa0920315bb;Updated README.md with link to needgap on the problem statement and highlighted the memory_hammer.py file name.;"An always-on Anki review system.
 Click the above image for a video demo.
 
 ### Why
+To solve Anki review accumulation by making the cards available for review when its due using always-on display system. By extension addressing [Human Memory, lack of thereof](https://needgap.com/problems/41-human-memory-lack-of-thereof-psychology-neuroscience).
 
 ### How
 Using e-paper display attached to a raspberry pi and memory-hammer software.
@@ -28,7 +28,7 @@ Using e-paper display attached to a raspberry pi and memory-hammer software.
 5. pip3.10 install -r requirements.txt
 
 ### Usage
+1. Edit the **Config** section of the **memory_hammer.py** with the IP address of your Anki Desktop and port for Anki Connect.
 2. python3.10 memory_hammer.py
 
 #### Download the Decks using Get Decks"
KO;1;abishekmuthian;memory-hammer;9f099d913697c50d8fd2cb3c8311d60f0658a903;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;
OK;1;abishekmuthian;memory-hammer;9f099d913697c50d8fd2cb3c8311d60f0658a903;Merge branch 'main' of github.com:abishekmuthian/memory-hammer;"+                    GNU AFFERO GENERAL PUBLIC LICENSE
+                       Version 3, 19 November 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU Affero General Public License is a free, copyleft license for
+software and other kinds of works, specifically designed to ensure
+cooperation with the community in the case of network server software.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+our General Public Licenses are intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  Developers that use our General Public Licenses protect your rights
+with two steps: (1) assert copyright on the software, and (2) offer
+you this License which gives you legal permission to copy, distribute
+and/or modify the software.
+
+  A secondary benefit of defending all users' freedom is that
+improvements made in alternate versions of the program, if they
+receive widespread use, become available for other developers to
+incorporate.  Many developers of free software are heartened and
+encouraged by the resulting cooperation.  However, in the case of
+software used on network servers, this result may fail to come about.
+The GNU General Public License permits making a modified version and
+letting the public access it on a server without ever releasing its
+source code to the public.
+
+  The GNU Affero General Public License is designed specifically to
+ensure that, in such cases, the modified source code becomes available
+to the community.  It requires the operator of a network server to
+provide the source code of the modified version running there to the
+users of that server.  Therefore, public use of a modified version, on
+a publicly accessible server, gives the public access to the source
+code of the modified version.
+
+  An older license, called the Affero General Public License and
+published by Affero, was designed to accomplish similar goals.  This is
+a different license, not a version of the Affero GPL, but Affero has
+released a new version of the Affero GPL which permits relicensing under
+this license.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  ""This License"" refers to version 3 of the GNU Affero General Public License.
+
+  ""Copyright"" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  ""The Program"" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as ""you"".  ""Licensees"" and
+""recipients"" may be individuals or organizations.
+
+  To ""modify"" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a ""modified version"" of the
+earlier work or a work ""based on"" the earlier work.
+
+  A ""covered work"" means either the unmodified Program or a work based
+on the Program.
+
+  To ""propagate"" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To ""convey"" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays ""Appropriate Legal Notices""
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The ""source code"" for a work means the preferred form of the work
+for making modifications to it.  ""Object code"" means any non-source
+form of a work.
+
+  A ""Standard Interface"" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The ""System Libraries"" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+""Major Component"", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The ""Corresponding Source"" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    ""keep intact all notices"".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+""aggregate"" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A ""User Product"" is either (1) a ""consumer product"", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, ""normally used"" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  ""Installation Information"" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  ""Additional permissions"" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered ""further
+restrictions"" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An ""entity transaction"" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A ""contributor"" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's ""contributor version"".
+
+  A contributor's ""essential patent claims"" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, ""control"" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a ""patent license"" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To ""grant"" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  ""Knowingly relying"" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is ""discriminatory"" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Remote Network Interaction; Use with the GNU General Public License.
+
+  Notwithstanding any other provision of this License, if you modify the
+Program, your modified version must prominently offer all users
+interacting with it remotely through a computer network (if your version
+supports such interaction) an opportunity to receive the Corresponding
+Source of your version by providing access to the Corresponding Source
+from a network server at no charge, through some standard or customary
+means of facilitating copying of software.  This Corresponding Source
+shall include the Corresponding Source for any work covered by version 3
+of the GNU General Public License that is incorporated pursuant to the
+following paragraph.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the work with which it is combined will remain governed by version
+3 of the GNU General Public License.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU Affero General Public License from time to time.  Such new versions
+will be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU Affero General
+Public License ""or any later version"" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU Affero General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU Affero General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ""AS IS"" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the ""copyright"" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU Affero General Public License as published
+    by the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU Affero General Public License for more details.
+
+    You should have received a copy of the GNU Affero General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If your software can interact with users remotely through a computer
+network, you should also make sure that it provides a way for users to
+get its source.  For example, if your program is a web application, its
+interface could display a ""Source"" link that leads users to an archive
+of the code.  There are many ways you could offer source, and different
+solutions will be better for different programs; see section 13 for the
+specific requirements.
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a ""copyright disclaimer"" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU AGPL, see
+<https://www.gnu.org/licenses/>."
KO;1;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
-        M, G, T to specify MiB, GiB, or TiB, respectively.
-        Values without suffix are interpreted as bytes.
         Values are rounded to the closest MiB.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
-def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
@@ -60,29 +60,84 @@ def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
-def max_memory(max_memory):
-    """"""Interpret the config parameter --max-memory.""""""
     try:
-        matches = re.match(r""(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[%MGT])?"", max_memory)
         value = float(matches[""value""])
         unit = matches[""unit""]
-        if unit == ""%"":
-            max_memory = share_of_ram(share=(value / 100.0))
-        else:
-            # convert to MiB
-            if unit is None:
-                value *= 2**-10
-                if value < 1:
-                    value = 1
-            # elif unit == ""M"":
-            #    value *= 2 ** 1
-            elif unit == ""G"":
-                value *= 2**10
-            elif unit == ""T"":
-                value *= 2**20
-            max_memory = round(value)
     except TypeError:
-        raise ValueError(f""Could not interpret --max-memory: {max_memory}"")
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
@@ -95,4 +150,4 @@ def max_memory(max_memory):
     return max_memory
 
 
-MAX_JVM_MEMORY = max_memory(arguments.max_memory)"
OK;1;r5py;r5py;6817fcaf0c56366fa032146aada033b835536f64;"Fix incorrect conversion of bytes, refactor and add docstrings. (#131)

* Fix incorrect conversion of bytes, refactor and add docstrings.

* Minor fix for Flake8.

* Make functions ""private"" and allow allocating memory as bytes without suffix.

* Improve docstrings and add convertion for bytes.

* match only the entire string

* linted

Co-authored-by: Christoph Fink <christoph@christophfink.com>";"         Memory limit for the JVM running R5.
 
         Use % as a suffix to specify a share of total RAM;
+        K, M, G, T to specify KiB, MiB, GiB, or TiB, respectively.
         Values are rounded to the closest MiB.
+        Values without suffix are interpreted as bytes.
     """""",
     default=""80%"",
 )
 arguments = config.arguments()
 
 
+def _share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     """"""
     Calculate a share of total RAM.
 
@@ -60,29 +60,84 @@ def share_of_ram(share=0.8, leave_at_least=(2 * (2**10))):
     return share_of_ram
 
 
+def _parse_max_memory_string(max_memory):
+    """"""
+    Extract maximum memory value and unit from text input.
+
+    Arguments
+    ---------
+    max_memory : str
+        Input text from the config parameter --max-memory.
+
+    Returns
+    -------
+    tuple: a tuple containing
+        - value (float): Amount of memory to be allocated in a given unit.
+        - unit (str): The unit of memory.
+    """"""
     try:
+        matches = re.match(
+            r""^(?P<value>[0-9]+(\.[0-9]+)?)(?P<unit>[^0-9])?$"", max_memory
+        )
         value = float(matches[""value""])
         unit = matches[""unit""]
+
+        if unit is not None and unit not in ""%KMGT"":
+            raise ValueError(
+                ""Could not interpret the memory unit from --max-memory.""
+                ""The suffix for --max-memory should be '%', 'K', 'M', 'G' or 'T'.""
+                ""For example to allocate five gigabytes of memory, use: '5G'""
+            )
+        return value, unit
     except TypeError:
+        raise ValueError(
+            f""Could not interpret --max-memory: {max_memory}.""
+            f""To allocate memory, use e.g. '5G' for five gigabytes of memory.""
+        )
+
+
+def _get_max_memory(max_memory):
+    """"""
+    Interpret the config parameter --max-memory.
+
+    Arguments
+    ---------
+
+    max_memory : str
+        Memory limit for the JVM running R5.
+
+        Use % as a suffix to specify a share of total RAM;
+        K, M, G, T suffix specify KiB, MiB, GiB, or TiB, respectively.
+        Values are rounded to the closest MiB.
+        Values without suffix are interpreted as bytes.
+
+    Returns
+    -------
+    float
+        Maximum amount of memory allocated for R5 in MiB.
+    """"""
+
+    value, unit = _parse_max_memory_string(max_memory)
+
+    if unit == ""%"":
+        max_memory = _share_of_ram(share=(value / 100.0))
+    else:
+        # convert to MiB
+        if unit is None:
+            value *= 2**-20
+        elif unit == ""K"":
+            value *= 2**-10
+        elif unit == ""M"":
+            value *= 2**1
+        elif unit == ""G"":
+            value *= 2**10
+        elif unit == ""T"":
+            value *= 2**20
+
+        if value < 1:
+            value = 1
+
+        max_memory = round(value)
 
     if max_memory < ABSOLUTE_MINIMUM_MEMORY:
         max_memory = ABSOLUTE_MINIMUM_MEMORY
@@ -95,4 +150,4 @@ def max_memory(max_memory):
     return max_memory
 
 
+MAX_JVM_MEMORY = _get_max_memory(arguments.max_memory)"
KO;1;facebookresearch;metaseq;ca180bb6474d0e55051e2bb3db9af69ddb3725e3;"fix off by one in API (#145)

* making prompt_len independent of batchfy implmentation

* modify for echo=True case

* returning logprobs, to support logprob input

* setting need_logprobs depends on each request to save memory";"def generate(
             self.cfg.generation.max_len_a = 0
 
             logger.info(f""Preparing generator with settings {self.cfg.generation}"")
             generator = self.task.build_generator(
-                self.models, self.cfg.generation, extra_gen_cls_kwargs={""stop"": stop}
             )
 
             # okay actually generate
@@ -624,23 +627,19 @@ def generate(
                     tokens, scores, distributions = GeneratorInterface._filter_special(
                         tokens, scores, distributions
                     )
-                    prompt_len = src_lengths[i]
                     if echo:
                         # don't cut off prompt
-                        tokens = tokens[: prompt_len + max_tokens[i] - 1]
-                        scores = scores[: prompt_len + max_tokens[i] - 1]
                         if logprobs > 0:
-                            distributions = distributions[
-                                : prompt_len + max_tokens[i] - 1
-                            ]
                     else:
                         # cut off prompt
-                        tokens = tokens[prompt_len - 1 :][: max_tokens[i]]
-                        scores = scores[prompt_len - 1 :][: max_tokens[i]]
                         if logprobs > 0:
-                            distributions = distributions[prompt_len - 1 :][
-                                : max_tokens[i]
-                            ]
                     # turn it into a string
                     text = self.bpe.bpe.decode(tokens)
                     # re-encode it so we get offsets"
OK;1;facebookresearch;metaseq;ca180bb6474d0e55051e2bb3db9af69ddb3725e3;"fix off by one in API (#145)

* making prompt_len independent of batchfy implmentation

* modify for echo=True case

* returning logprobs, to support logprob input

* setting need_logprobs depends on each request to save memory";"def generate(
             self.cfg.generation.max_len_a = 0
 
             logger.info(f""Preparing generator with settings {self.cfg.generation}"")
+            need_logprobs = True if logprobs > 0 else False
             generator = self.task.build_generator(
+                self.models,
+                self.cfg.generation,
+                extra_gen_cls_kwargs={""stop"": stop, ""need_logprobs"": need_logprobs},
             )
 
             # okay actually generate
@@ -624,23 +627,19 @@ def generate(
                     tokens, scores, distributions = GeneratorInterface._filter_special(
                         tokens, scores, distributions
                     )
+                    prompt_len = lengths[i]
                     if echo:
                         # don't cut off prompt
+                        tokens = tokens[: prompt_len + max_tokens[i]]
+                        scores = scores[: prompt_len + max_tokens[i]]
                         if logprobs > 0:
+                            distributions = distributions[: prompt_len + max_tokens[i]]
                     else:
                         # cut off prompt
+                        tokens = tokens[prompt_len:][: max_tokens[i]]
+                        scores = scores[prompt_len:][: max_tokens[i]]
                         if logprobs > 0:
+                            distributions = distributions[prompt_len:][: max_tokens[i]]
                     # turn it into a string
                     text = self.bpe.bpe.decode(tokens)
                     # re-encode it so we get offsets"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.identity_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -84,11 +84,11 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.random_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -140,12 +140,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.level_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            randomness=self.randomness,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()
@@ -204,12 +206,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
-        functional.circular_hv(
-            self.num_embeddings,
-            self.embedding_dim,
-            randomness=self.randomness,
-            out=self.weight.data,
-            **factory_kwargs
         )
 
         self._fill_padding_idx_with_zero()"
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.identity_hv(
+                self.num_embeddings, self.embedding_dim, **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -84,11 +84,11 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.random_hv(
+                self.num_embeddings, self.embedding_dim, **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -140,12 +140,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.level_hv(
+                self.num_embeddings,
+                self.embedding_dim,
+                randomness=self.randomness,
+                **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()
@@ -204,12 +206,14 @@ def reset_parameters(self):
             ""device"": self.weight.data.device,
             ""dtype"": self.weight.data.dtype,
         }
+
+        self.weight.data.copy_(
+            functional.circular_hv(
+                self.num_embeddings,
+                self.embedding_dim,
+                randomness=self.randomness,
+                **factory_kwargs
+            )
         )
 
         self._fill_padding_idx_with_zero()"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
-    def __init__(self, threshold=0.0):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
@@ -82,7 +82,7 @@ def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
-            raise IndexError()
 
         return index
 
@@ -241,7 +241,7 @@ def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
-        """"""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
@@ -273,7 +273,7 @@ def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
-        value = functional.multiset(input, dim=-2)
         return cls(value, size=input.size(-2))
 
 
@@ -434,7 +434,7 @@ def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
-        return cls(value, size=input.size(-2))
 
 
 class Sequence:
@@ -663,7 +663,9 @@ def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
-            self.value = torch.zeros(dim_or_input, dtype=dtype, device=device)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
@@ -766,7 +768,7 @@ def clear(self) -> None:
             >>> DS.clear()
 
         """"""
-        self.value.fill_(0.0)
         self.size = 0
 
     @classmethod"
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"class Memory:
 
     """"""
 
+    def __init__(self, threshold=0.5):
         self.threshold = threshold
         self.keys: List[Tensor] = []
         self.values: List[Any] = []
@@ -82,7 +82,7 @@ def index(self, key: Tensor) -> int:
         value, index = torch.max(sim, 0)
 
         if value.item() < self.threshold:
+            raise IndexError(""No elements in memory"")
 
         return index
 
@@ -241,7 +241,7 @@ def clear(self) -> None:
 
     @classmethod
     def from_ngrams(cls, input: Tensor, n=3):
+        r""""""Creates a multiset from the ngrams of a set of hypervectors.
 
         See: :func:`~torchhd.functional.ngrams`.
 
@@ -273,7 +273,7 @@ def from_tensor(cls, input: Tensor):
             >>> M = structures.Multiset.from_tensor(x)
 
         """"""
+        value = functional.multiset(input)
         return cls(value, size=input.size(-2))
 
 
@@ -434,7 +434,7 @@ def from_tensors(cls, keys: Tensor, values: Tensor):
 
         """"""
         value = functional.hash_table(keys, values)
+        return cls(value, size=keys.size(-2))
 
 
 class Sequence:
@@ -663,7 +663,9 @@ def __init__(self, dim_or_input: int, **kwargs):
         else:
             dtype = kwargs.get(""dtype"", torch.get_default_dtype())
             device = kwargs.get(""device"", None)
+            self.value = functional.identity_hv(
+                1, dim_or_input, dtype=dtype, device=device
+            ).squeeze(0)
 
     def append(self, input: Tensor) -> None:
         """"""Appends the input tensor to the right of the sequence.
@@ -766,7 +768,7 @@ def clear(self) -> None:
             >>> DS.clear()
 
         """"""
+        self.value.fill_(1.0)
         self.size = 0
 
     @classmethod"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestDistinctSequence:
+    def test_creation_dim(self):
+        S = structures.DistinctSequence(10000)
+        assert torch.equal(S.value, torch.ones(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        S = structures.DistinctSequence(hv[0])
+        assert torch.equal(S.value, hv[0])
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_append(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+
+    def test_appendleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.appendleft(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+
+    def test_pop(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[1])
+        S.pop(hv[1])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.pop(hv[0])
+        S.append(hv[2])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+        S.append(hv[3])
+        S.pop(hv[3])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+
+    def test_popleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.appendleft(hv[0])
+        S.appendleft(hv[1])
+        S.popleft(hv[1])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.popleft(hv[0])
+        S.appendleft(hv[2])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+        S.appendleft(hv[3])
+        S.popleft(hv[3])
+        assert functional.cosine_similarity(S.value, hv)[2] > 0.5
+
+    def test_replace(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        assert functional.cosine_similarity(S.value, hv)[0] > 0.5
+        S.replace(0, hv[0], hv[1])
+        assert functional.cosine_similarity(S.value, hv)[1] > 0.5
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        assert len(S) == 4
+        S.pop(hv[0])
+        S.pop(hv[0])
+        S.pop(hv[0])
+        assert len(S) == 1
+        S.pop(hv[0])
+        assert len(S) == 0
+        S.append(hv[0])
+        assert len(S) == 1
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.DistinctSequence(10000)
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        S.append(hv[0])
+        assert len(S) == 4
+        S.clear()
+        assert len(S) == 0"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+seed1 = 2147483643
+letters = list(string.ascii_lowercase)
+
+
+class TestFSA:
+    def test_creation_dim(self):
+        F = structures.FiniteStateAutomata(10000)
+        assert torch.equal(F.value, torch.zeros(10000))
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_transition(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        actions = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], actions[1], actions[2])
+        assert torch.equal(
+            F.value,
+            torch.tensor([1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0]),
+        )
+        F.add_transition(tokens[1], actions[1], actions[3])
+        assert torch.equal(
+            F.value, torch.tensor([0.0, 0.0, -2.0, 2.0, 0.0, 2.0, 0.0, -2.0, -2.0, 0.0])
+        )
+        F.add_transition(tokens[2], actions[1], actions[3])
+        assert torch.equal(
+            F.value,
+            torch.tensor([1.0, 1.0, -3.0, 1.0, 1.0, 3.0, -1.0, -1.0, -1.0, 1.0]),
+        )
+
+    def test_transition(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        states = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], states[1], states[2])
+        F.add_transition(tokens[1], states[1], states[3])
+        F.add_transition(tokens[2], states[1], states[5])
+
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[0]), states)
+            ).item()
+            == 2
+        )
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[1]), states)
+            ).item()
+            == 3
+        )
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(F.transition(states[1], tokens[2]), states)
+            ).item()
+            == 5
+        )
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator1 = torch.Generator()
+        generator.manual_seed(seed)
+        generator1.manual_seed(seed1)
+        tokens = functional.random_hv(10, 10, generator=generator)
+        states = functional.random_hv(10, 10, generator=generator1)
+
+        F = structures.FiniteStateAutomata(10)
+
+        F.add_transition(tokens[0], states[1], states[2])
+        F.add_transition(tokens[1], states[1], states[3])
+        F.add_transition(tokens[2], states[1], states[5])
+
+        F.clear()
+        assert torch.equal(
+            F.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestGraph:
+    def test_creation_dim(self):
+        G = structures.Graph(10000, directed=True)
+        assert torch.equal(G.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        g = functional.bind(hv[0], hv[1])
+        G = structures.Graph(g)
+        assert torch.equal(G.value, g)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_edge(self):
+        G = structures.Graph(8)
+        hv = torch.tensor(
+            [
+                [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
+                [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
+                [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
+                [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
+            ]
+        )
+
+        G.add_edge(hv[0], hv[1])
+        assert torch.equal(
+            G.value, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        G.add_edge(hv[2], hv[3])
+        assert torch.equal(
+            G.value, torch.tensor([-2.0, -2.0, 0.0, 2.0, -2.0, 0.0, 2.0, -2.0])
+        )
+
+        GD = structures.Graph(8, directed=True)
+
+        GD.add_edge(hv[0], hv[1])
+        assert torch.equal(
+            GD.value, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        GD.add_edge(hv[2], hv[3])
+        assert torch.equal(
+            GD.value, torch.tensor([0.0, 0.0, 0.0, -2.0, 0.0, -2.0, 2.0, -2.0])
+        )
+
+    def test_encode_edge(self):
+        G = structures.Graph(8)
+        hv = torch.tensor(
+            [
+                [-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0],
+                [1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
+                [-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0],
+                [1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0],
+            ]
+        )
+
+        e1 = G.encode_edge(hv[0], hv[1])
+        assert torch.equal(
+            e1, torch.tensor([-1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        e2 = G.encode_edge(hv[2], hv[3])
+        assert torch.equal(
+            e2, torch.tensor([-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0])
+        )
+
+        GD = structures.Graph(8, directed=True)
+
+        e1 = GD.encode_edge(hv[0], hv[1])
+        assert torch.equal(
+            e1, torch.tensor([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0])
+        )
+        e2 = GD.encode_edge(hv[2], hv[3])
+        print(e2)
+        assert torch.equal(
+            e2, torch.tensor([1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0])
+        )
+
+    def test_node_neighbors(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(10, 10000, generator=generator)
+        G = structures.Graph(10000, directed=True)
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(G.node_neighbors(hv[1]), hv)
+            ).item()
+            == 2
+        )
+        assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[0]), hv)[1] > 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[1] < 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[2]), hv)[0] < 0.5
+        assert functional.cosine_similarity(G.node_neighbors(hv[1]), hv)[0] < 0.5
+
+        G1 = structures.Graph(10000, directed=False)
+
+        G1.add_edge(hv[0], hv[1])
+        G1.add_edge(hv[0], hv[2])
+        G1.add_edge(hv[1], hv[2])
+        assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[0] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[1] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[0]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[0] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[1]), hv)[2] > 0.5
+        assert functional.cosine_similarity(G1.node_neighbors(hv[2]), hv)[1] > 0.5
+
+    def test_contains(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(4, 8, generator=generator)
+        G = structures.Graph(8)
+
+        e1 = G.encode_edge(hv[0], hv[1])
+        e2 = G.encode_edge(hv[0], hv[2])
+        e3 = G.encode_edge(hv[2], hv[3])
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        assert G.contains(e1) > torch.tensor(0.6)
+        assert G.contains(e2) > torch.tensor([0.6])
+        assert G.contains(e3) < torch.tensor(0.6)
+
+        GD = structures.Graph(8, directed=True)
+
+        ee1 = GD.encode_edge(hv[0], hv[1])
+        ee2 = GD.encode_edge(hv[0], hv[2])
+        ee3 = GD.encode_edge(hv[2], hv[3])
+        ee4 = GD.encode_edge(hv[1], hv[0])
+
+        GD.add_edge(hv[0], hv[1])
+        GD.add_edge(hv[0], hv[2])
+        GD.add_edge(hv[3], hv[1])
+
+        assert GD.contains(ee1) > torch.tensor(0.6)
+        assert GD.contains(ee2) > torch.tensor(0.6)
+        assert GD.contains(ee3) < torch.tensor(0.6)
+        assert GD.contains(ee4) < torch.tensor(0.6)
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(4, 8, generator=generator)
+        G = structures.Graph(8)
+
+        G.add_edge(hv[0], hv[1])
+        G.add_edge(hv[0], hv[2])
+        G.add_edge(hv[1], hv[2])
+
+        G.clear()
+
+        assert torch.equal(
+            G.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed_key = 2147483644
+seed_value = 2147483622
+letters = list(string.ascii_lowercase)
+
+
+class TestHashtable:
+    def test_creation_dim(self):
+        H = structures.HashTable(10000)
+        assert torch.equal(H.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        hash_v1 = functional.bind(keys_hv[0], values_hv[0])
+        hash_v2 = functional.bind(keys_hv[1], values_hv[1])
+        hasht = functional.bundle(hash_v1, hash_v2)
+
+        H = structures.HashTable(hasht)
+        assert torch.equal(H.value, hasht)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed_key)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed_key)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+
+    def test_remove(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+
+        H.remove(keys_hv[0], values_hv[0])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) < 0.2)[0],
+            torch.tensor(True),
+        )
+
+    def test_get(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[0]), values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[1]), values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H.get(keys_hv[2]), values_hv) > 0.5)[2],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            torch.all(
+                (functional.cosine_similarity(H.get(values_hv[2]), values_hv) > 0.5)
+                == False
+            ),
+            torch.tensor(True),
+        )
+
+    def test_getitem(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[1]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[2]], values_hv) > 0.5)[2],
+            torch.tensor(True),
+        )
+        assert torch.equal(
+            torch.all(
+                (functional.cosine_similarity(H[values_hv[2]], values_hv) > 0.5)
+                == False
+            ),
+            torch.tensor(True),
+        )
+
+    def test_replace(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+        H.replace(keys_hv[0], values_hv[0], values_hv[1])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[1],
+            torch.tensor(True),
+        )
+
+    def test_length(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+
+        assert len(H) == 3
+        H.remove(keys_hv[0], values_hv[0])
+
+        assert len(H) == 2
+
+    def test_clear(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator_key)
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(len(letters), 10000, generator=generator_value)
+
+        H = structures.HashTable(10000)
+        H.add(keys_hv[0], values_hv[0])
+        H.add(keys_hv[1], values_hv[1])
+        H.add(keys_hv[2], values_hv[2])
+        assert len(H) == 3
+        H.clear()
+        assert len(H) == 0
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(False),
+        )
+        H.add(keys_hv[0], values_hv[0])
+        assert torch.equal(
+            (functional.cosine_similarity(H[keys_hv[0]], values_hv) > 0.5)[0],
+            torch.tensor(True),
+        )
+
+    def test_from_tensor(self):
+        generator_key = torch.Generator()
+        generator_key.manual_seed(seed_key)
+        keys_hv = functional.random_hv(2, 3, generator=generator_key)
+
+        generator_value = torch.Generator()
+        generator_value.manual_seed(seed_value)
+        values_hv = functional.random_hv(2, 3, generator=generator_value)
+
+        H = structures.HashTable.from_tensors(keys_hv, values_hv)
+        assert torch.equal(H.value, torch.tensor([2.0, 0.0, 0.0]))"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestMemory:
+    def test_creation(self):
+        M = structures.Memory()
+
+        assert M.keys == []
+        assert M.values == []
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert torch.equal(M.keys[0], keys_hv[0])
+        assert torch.equal(M.keys[1], keys_hv[1])
+        assert torch.equal(M.keys[2], keys_hv[2])
+        assert M.values[0] == letters[0]
+        assert M.values[1] == letters[1]
+        assert M.values[2] == letters[2]
+
+    def test_index(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert M.index(keys_hv[0]) == 0
+        assert M.index(keys_hv[1]) == 1
+        assert M.index(keys_hv[2]) == 2
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        del M[keys_hv[0]]
+
+        assert len(M) == 2
+
+        M.add(keys_hv[0], letters[0])
+        assert len(M) == 3
+
+    def test_getitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+    def test_setitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+        M[keys_hv[0]] = letters[3]
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[3]
+
+    def test_delitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+
+        M = structures.Memory()
+        M.add(keys_hv[0], letters[0])
+        M.add(keys_hv[1], letters[1])
+        M.add(keys_hv[2], letters[2])
+
+        assert len(M) == 3
+        assert M[keys_hv[0]][1] == letters[0]
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+
+        del M[keys_hv[0]]
+        try:
+            M[keys_hv[0]]
+        except IndexError:
+            assert True
+
+        assert M[keys_hv[1]][1] == letters[1]
+        assert M[keys_hv[2]][1] == letters[2]
+        assert len(M) == 2"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestMultiset:
+    def test_creation_dim(self):
+        M = structures.Multiset(10000)
+        assert torch.equal(M.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 10000, generator=generator)
+        multiset = functional.multiset(keys_hv)
+
+        M = structures.Multiset(multiset)
+        assert torch.equal(M.value, multiset)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        assert torch.equal(M.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        M.add(keys_hv[1])
+        assert torch.equal(M.value, torch.tensor([2.0, 0.0, 0.0, 2.0]))
+
+        M.add(keys_hv[2])
+        assert torch.equal(M.value, torch.tensor([3.0, 1.0, 1.0, 1.0]))
+
+    def test_remove(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        assert M.contains(keys_hv[0]) > torch.tensor([0.5])
+
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        assert M.contains(keys_hv[1]) > torch.tensor([0.5])
+        assert M.remove(keys_hv[0]) is None
+
+    def test_contains(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.7])
+        M.remove(keys_hv[0])
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        M.remove(keys_hv[1])
+        assert M.contains(keys_hv[1]) < torch.tensor([0.1])
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        assert len(M) == 3
+        M.remove(keys_hv[0])
+
+        assert len(M) == 2
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset(4)
+
+        M.add(keys_hv[0])
+        M.add(keys_hv[0])
+        M.add(keys_hv[1])
+
+        M.clear()
+
+        assert M.contains(keys_hv[0]) < torch.tensor([0.1])
+        assert M.contains(keys_hv[1]) < torch.tensor([0.1])
+
+        M.add(keys_hv[0])
+        assert M.contains(keys_hv[0]) > torch.tensor([0.8])
+
+    def test_from_ngrams(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 3, generator=generator)
+        M = structures.Multiset.from_ngrams(keys_hv)
+
+        assert torch.equal(M.value, torch.tensor([0.0, 4.0, 0.0]))
+
+    def test_from_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        keys_hv = functional.random_hv(len(letters), 4, generator=generator)
+        M = structures.Multiset.from_tensor(keys_hv)
+        assert torch.equal(M.value, torch.tensor([2.0, 10.0, 4.0, 2.0]))"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestSequence:
+    def test_creation_dim(self):
+        S = structures.Sequence(10000)
+        assert torch.equal(S.value, torch.zeros(10000))
+
+    def test_creation_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        seq = functional.bundle(hv[1], functional.permute(hv[0], shifts=1))
+
+        S = structures.Sequence(seq)
+        assert torch.equal(S.value, seq)
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_append(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.append(hv[0])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.append(hv[1])
+        assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
+
+        S.append(hv[2])
+        assert torch.equal(S.value, torch.tensor([3.0, 3.0, 3.0, -3.0]))
+
+    def test_appendleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.appendleft(hv[0])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.appendleft(hv[1])
+        assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
+
+        S.appendleft(hv[2])
+        assert torch.equal(S.value, torch.tensor([3.0, -1.0, 3.0, 1.0]))
+
+    def test_pop(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        S.pop(hv[2])
+        assert torch.equal(S.value, torch.tensor([2.0, 2.0, -2.0, 2.0]))
+
+        S.pop(hv[1])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.pop(hv[0])
+        assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
+
+    def test_popleft(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 4, generator=generator)
+        S = structures.Sequence(4)
+
+        S.appendleft(hv[0])
+        S.appendleft(hv[1])
+        S.appendleft(hv[2])
+
+        S.popleft(hv[2])
+        assert torch.equal(S.value, torch.tensor([2.0, 0.0, 2.0, 0.0]))
+
+        S.popleft(hv[1])
+        assert torch.equal(S.value, torch.tensor([1.0, -1.0, 1.0, 1.0]))
+
+        S.popleft(hv[0])
+        assert torch.equal(S.value, torch.tensor([0.0, 0.0, 0.0, 0.0]))
+
+    def test_replace(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.Sequence(10000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+        S.append(hv[3])
+        S.append(hv[4])
+        S.append(hv[5])
+        S.append(hv[6])
+
+        assert functional.cosine_similarity(S[2], hv)[2] > 0.35
+        S.replace(2, hv[2], hv[6])
+        assert functional.cosine_similarity(S[2], hv)[2] < 0.35
+        assert functional.cosine_similarity(S[2], hv)[6] > 0.35
+
+        hv1 = functional.random_hv(10, 10000)
+        S2 = structures.Sequence.from_tensor(hv1)
+        assert functional.cosine_similarity(S2[2], hv1)[2] > 0.3
+        S2.replace(2, hv1[2], hv1[6])
+        assert functional.cosine_similarity(S2[2], hv1)[2] < 0.3
+        assert functional.cosine_similarity(S2[2], hv1)[6] > 0.3
+
+    def test_concat(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        S2 = structures.Sequence(1000)
+        S2.append(hv[0])
+        S2.append(hv[1])
+        S2.append(hv[2])
+
+        assert len(S) == 3
+        assert len(S2) == 3
+        S = S.concat(S2)
+        assert len(S) == 6
+
+        assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(S[2], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(S[4], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 2
+
+        SS = structures.Sequence(1000)
+
+        SS.appendleft(hv[0])
+        SS.appendleft(hv[1])
+        SS.appendleft(hv[2])
+
+        SS2 = structures.Sequence(1000)
+        SS2.appendleft(hv[0])
+        SS2.appendleft(hv[1])
+        SS2.appendleft(hv[2])
+
+        SS = SS.concat(SS2)
+
+        assert torch.argmax(functional.cosine_similarity(SS[0], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(SS[1], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(SS[2], hv)).item() == 0
+        assert torch.argmax(functional.cosine_similarity(SS[3], hv)).item() == 2
+        assert torch.argmax(functional.cosine_similarity(SS[4], hv)).item() == 1
+        assert torch.argmax(functional.cosine_similarity(SS[5], hv)).item() == 0
+
+    def test_getitem(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert torch.argmax(functional.cosine_similarity(S[0], hv)).item() == 0
+
+    def test_length(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert len(S) == 3
+        S.pop(hv[2])
+
+        assert len(S) == 2
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 1000, generator=generator)
+        S = structures.Sequence(1000)
+
+        S.append(hv[0])
+        S.append(hv[1])
+        S.append(hv[2])
+
+        assert len(S) == 3
+        S.clear()
+        assert len(S) == 0
+        S.append(hv[0])
+        assert len(S) == 1
+
+    def test_from_tensor(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        S = structures.Sequence.from_tensor(hv)
+
+        assert torch.argmax(functional.cosine_similarity(S[3], hv)).item() == 3
+        assert torch.argmax(functional.cosine_similarity(S[5], hv)).item() == 5
+        assert torch.argmax(functional.cosine_similarity(S[1], hv)).item() == 1"
KO;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";
OK;1;hyperdimensional-computing;torchhd;c204b025683b3ad9b4da87e968444045e7b58294;"Add structures tests (#66)

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, starting with memory

* Structures testing, starting with multiset

* Structures testing, starting with multiset

* Structures testing, hashtable done

* Structures testing, sequence done, two functions do not work

* Structures testing, graph done, node neighbors not working and contains not working for directed graphs

* tests

* Structures testing sequence done

* Structures testing, sequence and graph fix. Tree and FSA done

* Structures testing, graph fixed

* Structures testing, distinct sequence. Fixed Distinct Sequence init fixed

* random hd incorrect out parameter

* Use identity hv for distinct sequence

* Update formatting

* Fix tests

* Fix embeddings out usage

Co-authored-by: verges <pverges8@gmail.com>";"+import pytest
+import torch
+import string
+
+from torchhd import structures, functional
+
+seed = 2147483644
+letters = list(string.ascii_lowercase)
+
+
+class TestTree:
+    def test_creation_dim(self):
+        T = structures.Tree(10000)
+        assert torch.equal(T.value, torch.zeros(10000))
+
+    def test_generator(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv1 = functional.random_hv(60, 10000, generator=generator)
+
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv2 = functional.random_hv(60, 10000, generator=generator)
+
+        assert (hv1 == hv2).min().item()
+
+    def test_add_leaf(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        T = structures.Tree(10000)
+        T.add_leaf(hv[0], [""l"", ""l""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
+            ).item()
+            == 0
+        )
+        T.add_leaf(hv[1], [""l"", ""r""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
+            ).item()
+            == 1
+        )
+
+    def test_get_leaf(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(len(letters), 10000, generator=generator)
+        T = structures.Tree(10000)
+        T.add_leaf(hv[0], [""l"", ""l""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""l""]), hv)
+            ).item()
+            == 0
+        )
+        T.add_leaf(hv[1], [""l"", ""r""])
+        assert (
+            torch.argmax(
+                functional.cosine_similarity(T.get_leaf([""l"", ""r""]), hv)
+            ).item()
+            == 1
+        )
+
+    def test_clear(self):
+        generator = torch.Generator()
+        generator.manual_seed(seed)
+        hv = functional.random_hv(8, 10, generator=generator)
+        T = structures.Tree(10)
+
+        T.add_leaf(hv[0], [""l"", ""l""])
+        T.add_leaf(hv[1], [""l"", ""r""])
+
+        T.clear()
+        assert torch.equal(
+            T.value, torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
+        )"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
-  in_original: Indicates whether a parameter is expected to be present in the
-    saved checkpoint. Will raise an error if the parameter was expected,
-    but is not present.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
@@ -66,14 +67,14 @@ class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
-  in_original: bool = True
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
-    return set(x.keys()) >= {'original_key', 'in_original', 'value_fn'}
   return False
 
 
@@ -86,8 +87,10 @@ def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
-def apply_transformations(original_tree: PyTree, transformations: PyTree,
-                          new_tree: PyTree) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
@@ -162,6 +165,9 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
@@ -187,8 +193,15 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
-        if not transform.in_original:
-          continue  # do not override existing value of key in new
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
@@ -199,8 +212,7 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
-                f'Transformation key {original_key} not found in origin tree (in_original=True)'
-            )
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
@@ -209,9 +221,11 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
-      # carry over directly from original, otherwise use value from new
-      if key in original:
-        new[key] = original[key]
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"class Transform:
     this Transform is a regex, backreferences (such as \1) will be replaced with
     the appropriate matched group in the regex. Note: not needed if
     multi_value_fn is provided.
+  use_fallback: if True, takes the value from the fallback tree. If
+    `default_to_original=True` in `apply_transformations`, the fallback tree is
+    `new_tree`. If `default_to_original=False` in `apply_transformations`, the
+    fallback tree is `original_tree`.
   value_fn: A function accepting a single value and returning a single value.
     The value provided as an argument is the value of the transformation key in
     the original PyTree.
@@ -66,14 +67,14 @@ class Transform:
     the value of the key in the new PyTree.
   """"""
   original_key: Optional[Union[str, Tuple[str]]] = None
+  use_fallback: bool = False
   value_fn: Optional[Callable[[Any], Any]] = None
   multi_value_fn: Optional[ValueTransformFunction] = None
 
 
 def _is_leaf(x):
   if isinstance(x, dict):
+    return set(x.keys()) >= {'original_key', 'value_fn', 'multi_value_fn'}
   return False
 
 
@@ -86,8 +87,10 @@ def _to_transform(x):
 
 # TODO(b/233406904) Add regex support.
 # TODO(b/233407026) Add additional error checking.
+def apply_transformations(original_tree: PyTree,
+                          transformations: PyTree,
+                          new_tree: PyTree,
+                          default_to_original: Optional[bool] = True) -> PyTree:
   r""""""Applies transformations to a pytree.
 
   Also uses `transformations` to provide structure to the output tree.
@@ -162,6 +165,9 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
     new_tree: a PyTree defining the structure of the output. A leaf value is
       only relevant if the key is not present in transformations or
       original_tree.
+    default_to_original: If True, the values of keys unspecified in
+      transformations will be taken from `original_tree`. If False, they will be
+      taken from `new_tree`.
 
   Returns:
     a transformed PyTree with the structure of `new_tree`
@@ -187,8 +193,15 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
       match = re.fullmatch(transform_key, key)
       if match:
         transform_found = True
+        if transform.use_fallback:
+          if not default_to_original:
+            if key not in original:
+              raise ValueError(
+                  f'{key} not found in origin tree (`use_fallback` requested).'
+              )
+            new[key] = original[key]
+          # else simply retain new[key]
+          continue
         if not (transform.multi_value_fn is None or transform.value_fn is None):
           raise ValueError(
               f'Cannot provide both multi_value_fn and value_fn in {transform}')
@@ -199,8 +212,7 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
             original_key = match.expand(transform.original_key)
           if original_key not in original:
             raise ValueError(
+                f'Transformation key {original_key} not found in origin tree.')
           if transform.value_fn is None:
             value_fn = lambda x: x
           else:
@@ -209,9 +221,11 @@ def apply_transformations(original_tree: PyTree, transformations: PyTree,
         else:
           new[key] = transform.multi_value_fn(original_tree)
     if not transform_found:
+      if default_to_original:
+        # carry over directly from original, otherwise use value from new
+        if key in original:
+          new[key] = original[key]
+      # if default_to_new, do not carry over key from original
 
   new = traverse_util.unflatten_dict(new, sep='/')
   return serialization.from_state_dict(new_tree, new)"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
-        'f': Transform(in_original=False),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
@@ -140,6 +140,60 @@ def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
   def test_regex(self):
     original = {
         'a1': 1,
@@ -268,7 +322,7 @@ class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
-        d=Transform(in_original=False),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
@@ -340,8 +394,8 @@ def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
-        # LargeModel layer 0 is a newly inserted layer, thus in_original=False.
-        r'(.*)Dense_0(.*)': Transform(in_original=False),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
@@ -371,6 +425,50 @@ def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
 
 if __name__ == '__main__':
   absltest.main()"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"def test_rename(self):
         },
         # moved from being inside ""c""
         'e1': Transform(original_key='c/e'),
+        'f': Transform(use_fallback=True),  # newly added
         # note: dropped ""b""
         # copied c/a and moved up
         'ca1': Transform(original_key='c/a'),
@@ -140,6 +140,60 @@ def test_partial_transformation(self):
     self.assertDictEqual(
         expected, apply_transformations(self.original, transforms, fallback))
 
+  def test_default_new(self):
+    transforms = {
+        'a': Transform(use_fallback=True),  # use value from original
+        # implicit drop ""b""
+        # implicit retain ""c/a"", ""c/a""
+        'b1': Transform(original_key='b'),
+        # implicit add ""f"" and ""g""
+    }
+    new = {
+        'a': ...,
+        'c': {
+            'a': 10,
+            'e': 11,
+        },
+        'b1': ...,
+        'f': None,
+        'g': 2,
+    }
+    expected = {
+        'a': 0,
+        'c': {
+            'a': 10,
+            'e': 11,
+        },
+        'b1': 1,
+        'f': None,
+        'g': 2,
+    }
+    self.assertDictEqual(
+        expected,
+        apply_transformations(
+            self.original, transforms, new, default_to_original=False))
+
+  def test_missing_key_default(self):
+    transforms = {'f': Transform(use_fallback=True)}
+    new = {
+        'a': 2,
+        'b': 3,
+        'c': {
+            'a': 7,
+            'e': 8,
+        },
+        'f': 20,
+    }
+    with self.assertRaises(ValueError):
+      apply_transformations(
+          self.original, transforms, new, default_to_original=False)
+
+    expected = {'a': 0, 'b': 1, 'c': {'a': 2, 'e': 3}, 'f': 20}
+    self.assertDictEqual(
+        expected,
+        apply_transformations(
+            self.original, transforms, new, default_to_original=True))
+
   def test_regex(self):
     original = {
         'a1': 1,
@@ -268,7 +322,7 @@ class NewTree:
         a1=Transform(original_key='a'),
         b=Transform(multi_value_fn=lambda t: t.b * 2),
         c=jax.tree_map(lambda _: Transform(), tree.c),
+        d=Transform(use_fallback=True),
         e=Transform(multi_value_fn=lambda t: t.c.y[0]),
         f=[
             Transform(multi_value_fn=lambda t: t.c.y[1]),
@@ -340,8 +394,8 @@ def __call__(self, x):
     new_state = test_utils.init_flax_model(LargeModel())
 
     transformations = {
+        # LargeModel layer 0 is a newly inserted layer, thus use_fallback=True.
+        r'(.*)Dense_0(.*)': Transform(use_fallback=True),
         # SmallModel layer 0 maps to LargeModel layer 1
         r'(.*)Dense_1(.*)': Transform(original_key=r'\1Dense_0\2'),
         # SmallModel layer 1 maps to LargeModel layer 2
@@ -371,6 +425,50 @@ def __call__(self, x):
 
     test_utils.assert_tree_equal(self, expected_state, restored_state)
 
+  def test_flax_train_state_default_new(self):
+
+    class Model(nn.Module):
+
+      @nn.compact
+      def __call__(self, x):
+        x = x.reshape((x.shape[0], -1))  # flatten
+        x = nn.Dense(features=16)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=8)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=8)(x)
+        x = nn.sigmoid(x)
+        x = nn.Dense(features=4)(x)
+        return x
+
+    old_state = test_utils.init_flax_model(Model())
+    new_state = test_utils.init_flax_model(Model())
+
+    transformations = {
+        # values default to new_state, use_fallback=True instructs the Transform
+        # to fall back on old_state for this key.
+        r'(.*)Dense_1(.*)': Transform(use_fallback=True),
+    }
+    restored_state = apply_transformations(
+        old_state, transformations, new_state, default_to_original=False)
+
+    # Construct expected tree
+    old_state_dict = traverse_util.flatten_dict(
+        serialization.to_state_dict(old_state), keep_empty_nodes=True, sep='/')
+    new_state_dict = traverse_util.flatten_dict(
+        serialization.to_state_dict(new_state), keep_empty_nodes=True, sep='/')
+    expected_state_dict = {}
+    for k, v in new_state_dict.items():
+      if 'Dense_1' in k:
+        expected_state_dict[k] = old_state_dict[k]
+      else:
+        expected_state_dict[k] = v
+
+    expected_state = serialization.from_state_dict(
+        new_state, traverse_util.unflatten_dict(expected_state_dict, sep='/'))
+
+    test_utils.assert_tree_equal(self, expected_state, restored_state)
+
 
 if __name__ == '__main__':
   absltest.main()"
KO;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
OK;1;google;orbax;59e06216ce8ccf8e2cd63c810b776820b3c890f7;"Add option to ""reverse"" the default fallback tree in apply_transformations.

This means that if `default_to_original=True` (the default option), keys unspecified in `transformations` will be taken from `original_tree`. Otherwise, they will be taken from `new_tree`.

This will facilitate users who wish to only restore a small number of keys from a checkpoint, while retaining a large majority of keys from an in-memory state.

PiperOrigin-RevId: 454615556";"         'jax',
         'jaxlib',
         'numpy',
+        'pyyaml',
         'tensorflow',
         'tensorstore >= 0.1.20',
     ],"
KO;2;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
-def init(S=13, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
OK;2;JeffersonQin;yolo-v2-pytorch;82b6ecde5f937ee72aa6ccb84906ae8d50ff6795;fix: change init value for maximum memory test;" __all__ = ['init', 'set', 'get']
 
 
+def init(S=19, B=5):
 	""""""Init the global variables""""""
 	global global_dict
 	global_dict = {}"
KO;2;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
-		no_obj_iou_1, idx_1 = internal_function(yhat[0:int(N / 2)], y[0:int(N / 2)])
-		no_obj_iou_2, idx_2 = internal_function(yhat[int(N / 2):], y[int(N / 2):])
-		no_obj_iou = torch.cat([no_obj_iou_1, no_obj_iou_2], dim=0)
-		idx = torch.cat([idx_1, idx_2], dim=0)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
OK;2;JeffersonQin;yolo-v2-pytorch;7d9756d866a442164cd389f740d3789e4f9bfdd1;feat: add new trick to save memory;"def internal_get_intersection():
 
 				return no_obj_iou, idx
 
+		def obtain_by_crop(crop) -> list[torch.Tensor]:
+			""""""Obtain no_obj_iou by cropping down batch, used to enable large batch training
+
+			Args:
+				crop (int): crop count
+
+			Returns:
+				list[torch.Tensor]: no_obj_iou and idx
+			""""""
+			no_obj_iou = torch.tensor([], dtype=torch.bool).to(yhat.device)
+			idx = torch.tensor([], dtype=torch.int64).to(yhat.device)
+			for i in range(crop):
+				no_obj_iou_i, idx_i = internal_function(yhat[int(i * N / crop):int((i + 1) * N / crop)], 
+														y[int(i * N / crop):int((i + 1) * N / crop)])
+				no_obj_iou = torch.cat([no_obj_iou, no_obj_iou_i], dim=0)
+				idx = torch.cat([idx, idx_i], dim=0)
+			
+			return no_obj_iou, idx
+
+		if S == 19:
+			crop = 3
+		else:
+			crop = 1
+		
+		no_obj_iou, idx = obtain_by_crop(crop)
 
 		# width and height (reversed tw and th)
 		anchors = G.get('anchors').to(yhat.device)"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
-Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/scheduler_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" # Recurring Messages Telebot
 
+Recurring Messages Telebot is a Telegram bot. It's available at https://t.me/cron_telebot. :sparkles:
 
 One project, two deployments/entrypoints. [bot.py](./bot.py) runs the Telegram bot, while [app.py](./app.py) runs the Flask application.
 "
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
 
 app = Flask(__name__)
 
@@ -19,12 +20,15 @@
 def run():
     # TODO - allow only POST
     # TODO - add authentication
-    now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
     sheets_service = SheetsService()
-    entries = retrieve_entries_from_db(sheets_service, now)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
         return Response(status=200)
 
     for i, row in entries:
@@ -43,12 +47,9 @@ def run():
         )
         sheets_service.update_entry(updated_entry)
 
-    return Response(status=200)
-
 
-def retrieve_entries_from_db(sheets_service, nextrun_ts):
-    parsed_time = parse_time(nextrun_ts)
-    return sheets_service.get_entries_by_nextrun(parsed_time)
 
 
 def send_message(chat_id, content):"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" from sheets import SheetsService, edit_entry_multiple_fields, parse_time
 import requests
 from helper import calc_next_run
+import gc
 
 app = Flask(__name__)
 
@@ -19,12 +20,15 @@
 def run():
     # TODO - allow only POST
     # TODO - add authentication
     sheets_service = SheetsService()
+    now = datetime.now(timezone(timedelta(hours=TZ_OFFSET)))
+    parsed_time = parse_time(now)
+    entries = sheets_service.get_entries_by_nextrun(parsed_time)
 
     if len(entries) < 1:
         logger.info(""No messages sent"")
+        gc.collect()
+
         return Response(status=200)
 
     for i, row in entries:
@@ -43,12 +47,9 @@ def run():
         )
         sheets_service.update_entry(updated_entry)
 
+    gc.collect()  # https://github.com/googleapis/google-api-python-client/issues/535
 
+    return Response(status=200)
 
 
 def send_message(chat_id, content):"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
-    update.message.reply_text(config.confirm_message, parse_mode=""MarkdownV2"")
 
 
 def remove_job(update):"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_message(update):
     )
 
     # reply
+    update.message.reply_text(config.confirm_message)
 
 
 def remove_job(update):"
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
-confirm_message = ""Ok\. Done\. Added\. Your message will be sent when the time comes\.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;" simple_prompt_message = ""\/add to create a new job""
 prompt_new_job_message = ""The job already got this field\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
 invalid_new_job_message = ""A job with this name already exists\. Please \/add and create a new job\. If you want to override, \/delete job and create again\.""
+confirm_message = ""Ok. Done. Added. Your message will be sent when the time comes. Check /list to make sure that your job is added correctly.""
 invalid_crontab_message = ""This expression is invalid. Please provide a valid expression. Click <a href='https://crontab.guru/'>here</a> if you need help.""  # html
 list_jobs_message = ""Hey, choose the job you are interested to know more about.\n\n(swipe left to reply to this message)""
 delete_success_message = ""Yeet! This job is now gone."""
KO;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -71,6 +78,13 @@ def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -147,6 +161,14 @@ def add_chat_data(
             ],
             inherit=True,
         )
         return
 
     def add_user(self, user_id, username, first_name):
@@ -155,6 +177,12 @@ def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -181,6 +209,12 @@ def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
@@ -203,12 +237,6 @@ def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
-            logger.info(
-                ""New user added, username=%s, user_id=%s"",
-                update.message.from_user.username,
-                update.message.from_user.id,
-            )
-
             return
 
         # check that username hasn't changed
@@ -222,7 +250,7 @@ def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
-                ""username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
@@ -238,7 +266,7 @@ def sync_user_data(self, update):
             )
 
             logger.info(
-                ""first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
OK;2;huishun98;recurring-messages-telebot;f5e150d11bd9cbcbe7ba4a38e634e98d87646863;#4 Fix memory issue;"def add_new_entry(self, chat_id, jobname, username):
             row=1, values=[now, now, username, str(chat_id), jobname], inherit=True
         )
 
+        logger.info(
+            'New job entry ""%s"" added by user ""%s"", chat_id=%s',
+            jobname,
+            username,
+            str(chat_id),
+        )
+
     def retrieve_latest_entry(self, chat_id):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -71,6 +78,13 @@ def update_entry(self, entry):
         entry[""chat_id""] = entry[""chat_id""].astype(str)
         self.main_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
+        logger.info(
+            'Job entry ""%s"" updated by user ""%s"", chat_id=%s',
+            get_value(entry, ""jobname""),
+            get_value(entry, ""last_updated_by""),
+            str(get_value(entry, ""chat_id"")),
+        )
+
     def retrieve_specific_entry(self, chat_id, jobname, include_removed=False):
         df = self.main_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -147,6 +161,14 @@ def add_chat_data(
             ],
             inherit=True,
         )
+
+        logger.info(
+            'New chat entry created by user ""%s"", chat_id=%s, chat_title=%s',
+            created_by_username,
+            str(chat_id),
+            chat_title,
+        )
+
         return
 
     def add_user(self, user_id, username, first_name):
@@ -155,6 +177,12 @@ def add_user(self, user_id, username, first_name):
             row=1, values=[str(user_id), username, first_name, now, now], inherit=True
         )
 
+        logger.info(
+            'New user created, user_id=%s, username=""%s""',
+            str(user_id),
+            username,
+        )
+
     def retrieve_user_data(self, user_id):
         df = self.user_data_worksheet.get_as_df()
         df[""gsheet_row_number""] = np.arange(df.shape[0]) + 2
@@ -181,6 +209,12 @@ def supersede_user(self, entry, field_changed):
 
         self.user_data_worksheet.update_row(row_number, entry.iloc[0].tolist())
 
+        logger.info(
+            'User superseded, user_id=%s, field_changed=""%s""',
+            get_value(entry, ""user_id""),
+            field_changed,
+        )
+
     def refresh_user(self, entry):
         now = parse_time(datetime.now(timezone(timedelta(hours=config.TZ_OFFSET))))
         entry = edit_entry_single_field(entry, ""last_used_at"", now)
@@ -203,12 +237,6 @@ def sync_user_data(self, update):
                 update.message.from_user.first_name,
             )
 
             return
 
         # check that username hasn't changed
@@ -222,7 +250,7 @@ def sync_user_data(self, update):
             self.sync_user_data(update)
 
             logger.info(
+                ""User's username updated, new username=%s, user_id=%s"",
                 update.message.from_user.username,
                 update.message.from_user.id,
             )
@@ -238,7 +266,7 @@ def sync_user_data(self, update):
             )
 
             logger.info(
+                ""User's first_name updated, new first_name=%s, username=%s, user_id=%s"",
                 update.message.from_user.first_name,
                 update.message.from_user.username,
                 update.message.from_user.id,"
KO;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
-        
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
+            start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
+            elapsed_time = time.time() - start_time
+            self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
+
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -847,80 +848,95 @@ def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
-        self.empty_cache()
-        img = cp.array(img, dtype=np.float32)
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
-
-        del img
-        # cp._default_memory_pool.free_all_blocks()
-
-        img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
-        bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
-        reconfactor_cp = cp.array(self._reconfactor)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
-
-        del bcarray
-        del reconfactor_cp
-        # cp._default_memory_pool.free_all_blocks()
-
-        img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
-        for offs in range(0, 2*self.N - blocksize, blocksize):
-            imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        del img2
-        del imf
-        # cp._default_memory_pool.free_all_blocks()
-
-        res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
-        del img3
-        cp._default_memory_pool.free_all_blocks()
 
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
-        self.empty_cache()
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
-        imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
-        del img1
-        img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
-        reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
-
-        img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        for offs in range(0, 2 * self.N - blocksize, blocksize):
-            imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        del img2
-        postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
-        res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):
@@ -963,12 +979,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
-                print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
-            cp._default_memory_pool.free_all_blocks()
             if self.debug:
-                print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3;andreabassi78;napari-sim-processor;cb6b3b0fa0efe90ae1fa2c25d2bfbee34aa2b99d;Merge branch 'gpumemoryclean';"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
+        self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -847,80 +848,95 @@ def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
+    def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
+        try:
+            # Sometimes we are called from a new thread and then the plan_cache needs to be
+            # reset to 0 to avoid running out of GPU memory
+            cp.fft.config.get_plan_cache().set_size(0)
+            img1 = cp.array(img, dtype=np.float32)
+            nim = img1.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
+
+            del img1
+
+            img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
+            bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
+            reconfactor_cp = cp.array(self._reconfactor)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
+
+            del bcarray
+            del reconfactor_cp
+
+            img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
+            for offs in range(0, 2*self.N - blocksize, blocksize):
+                imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            del img2
+            del imf
+            res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
+            del img3
+        except Exception as e:
+            res = f'Exception in batchreconstruct_cupy: {e}'
+        return res
+
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
+        res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
+        cp.get_default_memory_pool().free_all_blocks()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
+        return res
 
+    def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
+        assert pytorch, ""No pytorch present""
+        try:
+            nim = img.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
+            imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
+            del img1
+            img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
+            reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
+
+            img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            for offs in range(0, 2 * self.N - blocksize, blocksize):
+                imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            del img2
+            postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
+            res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except Exception as e:
+            res = f'Exception in batchreconstruct_pytorch: {e}'
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
+        res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
+        if torch.has_cuda:
+            torch.cuda.empty_cache()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):
@@ -963,12 +979,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
+                print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+            cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
-                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
+                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
-    def batchreconstructcompact_cupy(self, img, blocksize=128):
-        assert cupy, ""No CuPy present""
         try:
-            # cp.get_default_memory_pool().free_all_blocks()
-            # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-            # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -893,24 +887,17 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
-            raise e
-            # Tidy up GPU memory
-        finally:
-            img1 = None
-            imf = None
-            bcarray = None
-            reconfactor_cp = None
-            img2 = None
-            img3 = None
-            print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-            cp.get_default_memory_pool().free_all_blocks()
-            print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-            print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-            print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
-    def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
@@ -941,25 +928,15 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
-            if torch.has_cuda:
-                # Tidy up gpu memory
-                if 'img1' in locals(): del img1
-                if 'imf' in locals(): del imf
-                if 'bcarray' in locals(): del bcarray
-                if 'reconfactor_pt' in locals(): del reconfactor_pt
-                if 'img2' in locals(): del img2
-                if 'img3' in locals(): del img3
-                torch.cuda.empty_cache()
-                print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
-                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-                raise e
         if torch.has_cuda:
-            del imf
-            del bcarray
-            del reconfactor_pt
-            del img3
             torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3;andreabassi78;napari-sim-processor;2a1418211762fbcb22fbc66d523946dbd0d42c58;Trapped cuda out-of-memory exceptions for both pytorch and cupy, so that memory can be safely released,;"def batchreconstruct_cupy(self, img):
         # cp._default_memory_pool.free_all_blocks()
         return res
 
+    def _batchreconstructcompactworker_cupy(self, img, blocksize=128):
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -893,24 +887,17 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
+            res = f'Exception in batchreconstruct_cupy: {e}'
         return res
 
+    def batchreconstructcompact_cupy(self, img, blocksize=128):
+        assert cupy, ""No CuPy present""
+        res = self._batchreconstructcompactworker_cupy(img, blocksize=blocksize)
+        cp.get_default_memory_pool().free_all_blocks()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
+        return res
+
+    def _batchreconstructcompactworker_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
         try:
             nim = img.shape[0]
@@ -941,25 +928,15 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         except Exception as e:
+            res = f'Exception in batchreconstruct_pytorch: {e}'
+        return res
+
+    def batchreconstructcompact_pytorch(self, img, blocksize=128):
+        assert pytorch, ""No pytorch present""
+        res = self._batchreconstructcompactworker_pytorch(img, blocksize=blocksize)
         if torch.has_cuda:
             torch.cuda.empty_cache()
+        assert not isinstance(res, str), res    # if something went wrong in the worker function then a string is returned
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
-                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
OK;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=512)
             elif self.proc.current_data == Accel.USE_CUPY.value:
+                stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=512)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time"
KO;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -885,23 +890,24 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
-
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
             # Tidy up GPU memory
-            if 'img1' in locals(): del img1
-            if 'imf' in locals(): del imf
-            if 'bcarray' in locals(): del bcarray
-            if 'reconfactor_cp' in locals(): del reconfactor_cp
-            if 'img2' in locals(): del img2
-            if 'img3' in locals(): del img3
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
-            raise e
-        cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
OK;3;andreabassi78;napari-sim-processor;39fe12c5c221d1df0d9d7daf08ad59a017a15a61;work on clearing gpu memory on error;"def batchreconstruct_cupy(self, img):
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
         try:
+            # cp.get_default_memory_pool().free_all_blocks()
+            # print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+            # print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            # print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+
             # Sometimes we are called from a new thread and then the plan_cache needs to be
             # reset to 0 to avoid running out of GPU memory
             cp.fft.config.get_plan_cache().set_size(0)
@@ -885,23 +890,24 @@ def batchreconstructcompact_cupy(self, img, blocksize=128):
             img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
             del img2
             del imf
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
         except Exception as e:
+            raise e
             # Tidy up GPU memory
+        finally:
+            img1 = None
+            imf = None
+            bcarray = None
+            reconfactor_cp = None
+            img2 = None
+            img3 = None
+            print(f'\tcupy memory used before clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            print(f'\tcupy memory total before clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             cp.get_default_memory_pool().free_all_blocks()
             print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
             print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
             print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):"
KO;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
-                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize = 32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
-        
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
OK;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def update_sim_image(stack):
         @thread_worker(connect={'returned': update_sim_image})
         def _stack_reconstruction():
             warnings.filterwarnings('ignore')
+            start_time = time.time()
             stackSIM = np.zeros([sz,2*sy,2*sx], dtype=np.single)
             for zidx in range(sz):
                 phases_stack = np.squeeze(pa_stack[:,zidx,:,:])
@@ -833,22 +834,24 @@ def _stack_reconstruction():
                     stackSIM[zidx, :, :] = self.h.reconstruct_cupy(phases_stack.astype(np.float32))  # TODO:this is left after conversion from torch
                 else:
                     stackSIM[zidx,:,:] = self.h.reconstruct_rfftw(phases_stack)      
+            elapsed_time = time.time() - start_time
+            self.messageBox.setText(f'Stack reconstruction time {elapsed_time:.3f}s')
             return stackSIM
         
         @thread_worker(connect={'returned': update_sim_image})
         def _batch_reconstruction():
             warnings.filterwarnings('ignore')
             start_time = time.time()
             if self.proc.current_data == Accel.USE_TORCH.value:
+                stackSIM = self.h.batchreconstructcompact_pytorch(paz_stack, blocksize=32)
             elif self.proc.current_data == Accel.USE_CUPY.value:
                 stackSIM = self.h.batchreconstructcompact_cupy(paz_stack, blocksize=32)
             else:
                 stackSIM = self.h.batchreconstructcompact(paz_stack)
             elapsed_time = time.time() - start_time
             self.messageBox.setText(f'Batch reconstruction time {elapsed_time:.3f}s')
             return stackSIM
+
         # main function executed here
         assert self.isCalibrated, 'SIM processor not calibrated, unable to perform SIM reconstruction'
         fullstack = self.get_hyperstack()"
KO;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
-        except RuntimeError as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
-            cp._default_memory_pool.free_all_blocks()
             raise e
-        cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
@@ -931,7 +934,7 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
-        except RuntimeError as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
@@ -941,6 +944,9 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
                 raise e
         if torch.has_cuda:
             del imf
@@ -990,12 +996,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
-                print(f'\tcupy memory used: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total: {cp._default_memory_pool.total_bytes() / 1e9} GB')
-            cp._default_memory_pool.free_all_blocks()
             if self.debug:
-                print(f'\tcupy memory used after clearing: {cp._default_memory_pool.used_bytes() / 1e9} GB')
-                print(f'\tcupy memory total after clearing: {cp._default_memory_pool.total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
OK;3;andreabassi78;napari-sim-processor;5219be4d5e84d578bed1fb8bac9c859142d9d68e;work on clearing gpu memory on error;"def batchreconstructcompact_cupy(self, img, blocksize=128):
 
             res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
             del img3
+        except Exception as e:
             # Tidy up GPU memory
             if 'img1' in locals(): del img1
             if 'imf' in locals(): del imf
             if 'bcarray' in locals(): del bcarray
             if 'reconfactor_cp' in locals(): del reconfactor_cp
             if 'img2' in locals(): del img2
             if 'img3' in locals(): del img3
+            cp.get_default_memory_pool().free_all_blocks()
+            print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+            print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+            print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
             raise e
+        cp.get_default_memory_pool().free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
@@ -931,7 +934,7 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
             del img2
             postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
             res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except Exception as e:
             if torch.has_cuda:
                 # Tidy up gpu memory
                 if 'img1' in locals(): del img1
@@ -941,6 +944,9 @@ def batchreconstructcompact_pytorch(self, img, blocksize=128):
                 if 'img2' in locals(): del img2
                 if 'img3' in locals(): del img3
                 torch.cuda.empty_cache()
+                print(f'\ttorch cuda memory reserved after clearing: {torch.cuda.memory_reserved() / 1e9} GB')
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
                 raise e
         if torch.has_cuda:
             del imf
@@ -990,12 +996,12 @@ def empty_cache(self):
         if cupy:
             cp.fft.config.get_plan_cache().set_size(0)
             if self.debug:
+                print(f'\tcupy memory used: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
+            cp.get_default_memory_pool().free_all_blocks()
             if self.debug:
+                print(f'\tcupy memory used after clearing: {cp.get_default_memory_pool().used_bytes() / 1e9} GB')
+                print(f'\tcupy memory total after clearing: {cp.get_default_memory_pool().total_bytes() / 1e9} GB')
 
     def find_phase_pytorch(self, kx, ky, img):
         return self.find_phase(kx, ky, img, useTorch=True)"
KO;3;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -849,78 +850,104 @@ def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
-        self.empty_cache()
-        img = cp.array(img, dtype=np.float32)
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = cp.concatenate((img, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        imf = cp.fft.rfft2(img) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
-
-        del img
-        # cp._default_memory_pool.free_all_blocks()
-
-        img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
-        bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
-        reconfactor_cp = cp.array(self._reconfactor)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
-
-        del bcarray
-        del reconfactor_cp
-        # cp._default_memory_pool.free_all_blocks()
-
-        img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
-        for offs in range(0, 2*self.N - blocksize, blocksize):
-            imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
-        del img2
-        del imf
-        # cp._default_memory_pool.free_all_blocks()
-
-        res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
-        del img3
         cp._default_memory_pool.free_all_blocks()
-
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
-        self.empty_cache()
-        nim = img.shape[0]
-        r = np.mod(nim, self._nsteps)
-        if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
-            img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
-            nim = nim + self._nsteps - r
-        nimg = nim // self._nsteps
-        img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
-        imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
-        del img1
-        img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
-        reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
-        for i in range(0, nim, self._nsteps):
-            bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
-            bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
-                                                                        0:self.N // 2 + 1]
-            img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
-
-        img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
-        for offs in range(0, 2 * self.N - blocksize, blocksize):
-            imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-            img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
-        img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
-        del img2
-        postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
-        res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
         return res
 
     def batchreconstruct_pytorch(self, img):"
OK;3;andreabassi78;napari-sim-processor;d6cb6ca26697419f67085ace22fb0b7fb5f08f7f;Clean GPU memory on exception in batchreconstruct methods;"def calibrate_pytorch(self, img, findCarrier=True):
 
     def _calibrate(self, img, findCarrier=True, useTorch=False, useCupy=False):
         assert len(img) > self._nsteps - 1
+        self.empty_cache()
         self.N = len(img[0, :, :])
         if self.N != self._lastN:
             self._allocate_arrays()
@@ -849,78 +850,104 @@ def batchreconstruct_cupy(self, img):
 
     def batchreconstructcompact_cupy(self, img, blocksize=128):
         assert cupy, ""No CuPy present""
+        try:
+            # Sometimes we are called from a new thread and then the plan_cache needs to be
+            # reset to 0 to avoid running out of GPU memory
+            cp.fft.config.get_plan_cache().set_size(0)
+            img1 = cp.array(img, dtype=np.float32)
+            nim = img1.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img1 = cp.concatenate((img1, cp.zeros((self._nsteps - r, self.N, self.N), np.single)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            imf = cp.fft.rfft2(img1) * cp.array(self._prefilter[:, 0:self.N // 2 + 1])
+
+            del img1
+
+            img2 = cp.zeros((nim, 2 * self.N, 2 * self.N), dtype=np.single)
+            bcarray = cp.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=np.complex64)
+            reconfactor_cp = cp.array(self._reconfactor)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = cp.fft.irfft2(bcarray) * reconfactor_cp
+
+            del bcarray
+            del reconfactor_cp
+
+            img3 = cp.zeros((nimg, 2 * self.N, 2 * self.N), dtype=np.single)
+            for offs in range(0, 2*self.N - blocksize, blocksize):
+                imf = cp.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            imf = cp.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = cp.fft.irfft(imf, nimg, 0)
+            del img2
+            del imf
+
+            res = (cp.fft.irfft2(cp.fft.rfft2(img3) * self._postfilter_cp[:, :self.N + 1])).get()
+            del img3
+        except RuntimeError as e:
+            # Tidy up GPU memory
+            if 'img1' in locals(): del img1
+            if 'imf' in locals(): del imf
+            if 'bcarray' in locals(): del bcarray
+            if 'reconfactor_cp' in locals(): del reconfactor_cp
+            if 'img2' in locals(): del img2
+            if 'img3' in locals(): del img3
+            cp._default_memory_pool.free_all_blocks()
+            raise e
         cp._default_memory_pool.free_all_blocks()
         return res
 
     def batchreconstructcompact_pytorch(self, img, blocksize=128):
         assert pytorch, ""No pytorch present""
+        try:
+            nim = img.shape[0]
+            r = np.mod(nim, self._nsteps)
+            if r > 0:  # pad with empty frames so total number of frames is divisible by self._nsteps
+                img = np.concatenate((img, np.zeros((self._nsteps - r, self.N, self.N), img.dtype)))
+                nim = nim + self._nsteps - r
+            nimg = nim // self._nsteps
+            img1 = torch.as_tensor(np.single(img), dtype=torch.float32, device=self.tdev)
+            imf = torch.fft.rfft2(img1) * torch.as_tensor(self._prefilter[:, 0:self.N // 2 + 1], device=self.tdev)
+            del img1
+            img2 = torch.zeros((nim, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            bcarray = torch.zeros((self._nsteps, 2 * self.N, self.N + 1), dtype=torch.complex64, device=self.tdev)
+            reconfactor_pt = torch.as_tensor(self._reconfactor, device=self.tdev)
+            for i in range(0, nim, self._nsteps):
+                bcarray[:, 0:self.N // 2, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, 0:self.N // 2, 0:self.N // 2 + 1]
+                bcarray[:, 3 * self.N // 2:2 * self.N, 0:self.N // 2 + 1] = imf[i:i + self._nsteps, self.N // 2:self.N,
+                                                                            0:self.N // 2 + 1]
+                img2[i:i + self._nsteps, :, :] = torch.fft.irfft2(bcarray) * reconfactor_pt
+
+            img3 = torch.zeros((nimg, 2 * self.N, 2 * self.N), dtype=torch.float32, device=self.tdev)
+            for offs in range(0, 2 * self.N - blocksize, blocksize):
+                imf = torch.fft.rfft(img2[:, offs:offs + blocksize, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+                img3[:, offs:offs + blocksize, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            imf = torch.fft.rfft(img2[:, offs + blocksize:2 * self.N, 0:2 * self.N], nim, 0)[:nimg // 2 + 1, :, :]
+            img3[:, offs + blocksize:2 * self.N, 0:2 * self.N] = torch.fft.irfft(imf, nimg, 0)
+            del img2
+            postfilter_pt = torch.as_tensor(self._postfilter, device=self.tdev)
+            res = (torch.fft.irfft2(torch.fft.rfft2(img3) * postfilter_pt[:, :self.N + 1])).cpu().numpy()
+        except RuntimeError as e:
+            if torch.has_cuda:
+                # Tidy up gpu memory
+                if 'img1' in locals(): del img1
+                if 'imf' in locals(): del imf
+                if 'bcarray' in locals(): del bcarray
+                if 'reconfactor_pt' in locals(): del reconfactor_pt
+                if 'img2' in locals(): del img2
+                if 'img3' in locals(): del img3
+                torch.cuda.empty_cache()
+                raise e
+        if torch.has_cuda:
+            del imf
+            del bcarray
+            del reconfactor_pt
+            del img3
+            torch.cuda.empty_cache()
         return res
 
     def batchreconstruct_pytorch(self, img):"
KO;4;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
-    partition_group = row.partition_group
     embeddings = row.embeddings
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
-    save_path = os.path.join(fs_base_path, f""img_emb_{shard_index}-{partition_group}.npy"")
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
@@ -127,7 +131,7 @@ def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
-    if working_fs.exists(output_folder_path) and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
@@ -138,7 +142,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
-    if working_fs.exists(meta_embed_folder_path) and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
@@ -149,7 +153,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
-    if working_fs.exists(empty_embed_folder_path) and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
@@ -171,6 +175,9 @@ def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
@@ -194,7 +201,7 @@ def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
-    example_embedding = np.array(data.first().embeddings)
 
     end_recall_timer()
 
@@ -204,6 +211,7 @@ def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
@@ -249,13 +257,20 @@ def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
         grouped = (
-            data.orderBy(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
OK;4;Veldrovive;embedding-dataset-reordering;c04ed2494b3330d25221e47a201bdb8f0a265aa7;"Added memory tests
We are plauged by OOMs and memory allocation warnings and this introduces some measures to combat those
They don't work though";"def save_row(row, fs_path: str):
     """"""
     fs, fs_base_path = fsspec.core.url_to_fs(fs_path)
     shard_index = row.img_shard
     embeddings = row.embeddings
+    if ""partition_group"" in row:
+        partition_group = row.partition_group
+        filename = f""img_emb_{shard_index}-{partition_group}.npy""
+    else:
+        filename = f""img_emb_{shard_index}.npy""
     np_embeddings = np.array(embeddings)
     fs.makedirs(fs_base_path, exist_ok=True)
+    save_path = os.path.join(fs_base_path, filename)
     # print(f""Saving: {save_path}"")
     with fs.open(save_path, ""wb"") as f:
         np.save(f, np_embeddings)
@@ -127,7 +131,7 @@ def rm_folder(fs, folder_path):
             pass
     
     # Some unexpected behavior can occur if we do not remove existing folders so we do that at the top
+    if working_fs.exists(output_folder_path) and len(working_fs.ls(output_folder_path)) > 0 and not skip_sort:
         # Then we should delete it if we are overwriting or error if we aren't
         if overwrite:
             rm_folder(working_fs, output_folder_path)
@@ -138,7 +142,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(output_folder_path, exist_ok=True)
 
 
+    if working_fs.exists(meta_embed_folder_path) and len(working_fs.ls(meta_embed_folder_path)) > 0 and not skip_format_embed:
         # Then if we are not skipping embed, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, meta_embed_folder_path)
@@ -149,7 +153,7 @@ def rm_folder(fs, folder_path):
         working_fs.makedirs(meta_embed_folder_path, exist_ok=True)
 
     # And we need the same for the empty path
+    if working_fs.exists(empty_embed_folder_path) and len(working_fs.ls(empty_embed_folder_path)) > 0 and not skip_fill:
         # Then if we are not skipping empty, we need to check if we enabled overwrites
         if overwrite:
             rm_folder(working_fs, empty_embed_folder_path)
@@ -171,6 +175,9 @@ def rm_folder(fs, folder_path):
         .config(""spark.ui.showConsoleProgress"", ""true"")
         .config(""spark.executor.memory"", f""{memory}g"")
         .config(""spark.driver.memory"", f""{memory}g"")
+        .config(""spark.sql.shuffle.partitions"", ""1000000"")
+        .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
+        .config(""spark.kryoserializer.buffer"", ""1g"")
         .getOrCreate()
     )  # TODO: Add in the ability to have nodes
     sc = spark.sparkContext
@@ -194,7 +201,7 @@ def rm_folder(fs, folder_path):
     remote_path = os.path.join(output_base_path, intermediate_folder, ""meta_embed"", ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files. It doesn't. It needs to be able to connect to the s3 file system.
+    print(f""Data has {data.rdd.getNumPartitions()} partitions"")
 
     end_recall_timer()
 
@@ -204,6 +211,7 @@ def rm_folder(fs, folder_path):
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
         # This means if we do not account for these missing indices, we will be off by one for all subsequent embeddings in the shard
         # In order to fix these, we insert an empty embedding into every location where one is missing
+        example_embedding = np.array(data.first().embeddings)
         data.createOrReplaceTempView(""df"")
         missing_values = spark.sql(
             """"""
@@ -249,13 +257,20 @@ def rm_folder(fs, folder_path):
     end_export_timer = ra.start_timer(""Sort & Export"")
     if not skip_sort:
         print(""========= Grouping and Saving ========="")
+        print(""Number of partitions"", data.rdd.getNumPartitions())
+        # data = data.repartition(""img_shard"")
         data.createOrReplaceTempView(""df"")
         data = spark.sql(""""""
             SELECT *, FLOOR(img_index / 1000) as partition_group FROM df
         """""")
+        data = data.repartition(""img_shard"", ""partition_group"")
+        print(""Number of partitions"", data.rdd.getNumPartitions())
         grouped = (
+            data
+            # .orderBy(""img_index"")
+            .sortWithinPartitions(""img_index"")
             .groupBy(""img_shard"", ""partition_group"")
+            # .groupBy(""img_shard"")
             .agg(F.collect_list(""embeddings"").alias(""embeddings""))
         )
 "
KO;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
 
 import findspark
 
@@ -152,6 +154,8 @@ def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
@@ -164,6 +168,8 @@ def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
@@ -190,12 +196,14 @@ def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
@@ -215,6 +223,8 @@ def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
@@ -225,21 +235,58 @@ def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
-        added_df = spark.createDataFrame(added_data, [""img_shard"", ""img_index"", ""embeddings""])
         data = data.union(added_df)
 
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
-    # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
-    # I now know what happens. It immediatly crashes.
 
-    # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
OK;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";" import math
 import fsspec
 from tqdm import tqdm
+import time
+import plotext as plt
 
 import findspark
 
@@ -152,6 +154,8 @@ def reorder_embeddings(
 
     print(""Created spark instance.\nLoading embeddings."")
 
+    start_time = time.perf_counter()
+
     embedding_reader = EmbeddingReader(  # TODO: Figure out if some kind of authorization will be necessary
         embeddings_folder=embeddings_folder,
         metadata_folder=metadata_folder,
@@ -164,6 +168,8 @@ def reorder_embeddings(
 
     print(f""Embedding reader found {embedding_reader.count} embeddings"")
 
+    start_embedding_load_time = time.perf_counter()
+
     print(""========= Formatting Intermediate Embeddings ========="")
     if parallelize_reading:
         # Parallelize the downloading of the embeddings
@@ -190,12 +196,14 @@ def reorder_embeddings(
         )
 
     print(""========= Recalling and Reordering Embeddings ========="")
+    start_recall_time = time.perf_counter()
     # Recall the data that was saved by each worker into a single dataframe so that we can do a full sort
     remote_path = os.path.join(output_base_path, intermediate_folder, ""*.parquet"")
     print(""Recalling data from worker paths:"", remote_path)
     data = spark.read.parquet(remote_path)  # TODO: Verify this work with remote files
     example_embedding = np.array(data.first().embeddings)
 
+    start_missing_fill_time = time.perf_counter()
     if fill_missing:
         print(""========= Inserting Missing Data ========="")
         # If an image returned a error during webdataset creation, it will still take up an index, but will not be included in the embeddings
@@ -215,6 +223,8 @@ def reorder_embeddings(
         print(f""Found {missing_values.count()} missing ranges."")
         
         added_data = []
+        current_amount = 0
+        added_files = 0
         for row in tqdm(missing_values.collect()):
             shard = row.img_shard
             first_missing_index, next_full_index = row.first_missing_index, row.next_full_index
@@ -225,21 +235,58 @@ def reorder_embeddings(
                     continue
             for missing_index in range(first_missing_index, next_full_index):
                 added_data.append((shard, missing_index, np.zeros_like(example_embedding).tolist()))
+                current_amount += 1
+                if current_amount > 1000:
+                    df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
+                    with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
+                        df.to_parquet(f)
+                    added_data.clear()
+                    current_amount = 0
+                    added_files += 1
+        df = pd.DataFrame(data=added_data, columns=[""img_shard"", ""img_index"", ""embeddings""])
+        with working_fs.open(os.path.join(intermediate_folder_path, f'empty_{added_files}.parquet'), ""wb"") as f:
+            df.to_parquet(f)
+        empty_path = os.path.join(output_base_path, intermediate_folder, ""empty_*.parquet"")
+        added_df = spark.read.parquet(empty_path)
         data = data.union(added_df)
 
+    full_count = data.count()
+
+    start_export_time = time.perf_counter()
+
     print(""========= Grouping and Saving ========="")
     grouped = (
         data.orderBy(""img_index"")
         .groupBy(""img_shard"")
         .agg(F.collect_list(""embeddings"").alias(""embeddings""))
     )
+    # # TODO: Each group will be very large. In the hundereds of megabytes. Spark wants partitions to be under 1000KiB. Not sure what happens if you exceed that by a factor of 300.
+    # # I now know what happens. It immediatly crashes.
 
+    # # Parallelize saving the grouped embeddings as this also takes a while
 
     grouped.foreach(lambda row: save_row(row, output_folder_path))
+    end_time = time.perf_counter()
     shards = [row.img_shard for row in grouped.select(""img_shard"").collect()]
     working_fs.rm(intermediate_folder_path, recursive=True)
+
+    embed_reader_initialization_time = start_embedding_load_time - start_time
+    embedding_load_time = start_recall_time - start_embedding_load_time
+    recall_time = start_missing_fill_time - start_recall_time
+    missing_fill_time = start_export_time - start_missing_fill_time
+    export_time = end_time - start_export_time
+    total_time = end_time - start_time
+
+    print(f""Total Execution Time: {total_time:0.2f}s"")
+
+    tasks = list(reversed([""Initialize Embedding Reader"", ""Load Embeddings"", ""Recall Embeddings"", ""Insert Missing Embeddings"", ""Save Embeddings""]))
+    times = list(reversed([embed_reader_initialization_time, embedding_load_time, recall_time, missing_fill_time, export_time]))
+    plt.bar(tasks, times, orientation=""horizontal"", width = 0.3)
+    plt.clc()
+    plt.xlabel(""Execution Time (s)"")
+    plt.show()
+
+    
+
+
     return [os.path.join(output_folder, f""img_emb_{shard_index}.npy"") for shard_index in shards]"
KO;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
-tqdm>=4.60.0
\ No newline at end of file
\ No newline at end of file"
OK;4;Veldrovive;embedding-dataset-reordering;724cb69a22c59f162f82bf87cd083209d715f7aa;"Fixed a memory issue where a list was converted directly into a spark dataframe
Added a graph of execution time";"clip-retrieval>=2.29.0
 requests>=2.23.0
 aiohttp>=3.8.1
 fsspec>=2022.1.0
\ No newline at end of file
+tqdm>=4.60.0
+plotext>=5.0.2
\ No newline at end of file"
KO;4;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
-            fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
-            data = jarray.zeros(memory_block.getSize(), ""b"")
-            memory_block.getBytes(memory_block.getStart(), data)
-            fp.write(','.join((str(i) for i in data)))
-            fp.write("")),\n"")
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
OK;4;boricj;ghidra-unlinker;8de2100c9e59c0f02c9c791939b8f76cc8e37cc6;ExportTestCase: fix export of uninitialized memory blocks;"def toaddr(addr):
     fp.write(""# Memory blocks\nmemory_blocks = (\n"")
     for memory_block in memory_blocks:
         if memory_block.isLoaded():
+            if memory_block.isInitialized():
+                fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=("".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
+                data = jarray.zeros(memory_block.getSize(), ""b"")
+                memory_block.getBytes(memory_block.getStart(), data)
+                fp.write(','.join((str(i) for i in data)))
+                fp.write("")),\n"")
+            else:
+                fp.write(""  MockMemoryBlock(name='{}', address_range=MockAddressSet(MockAddress({}), MockAddress({})).getFirstRange(), data=None),\n"".format(memory_block.getName(), toaddr(memory_block.getStart()), toaddr(memory_block.getEnd())))
     fp.write("")\n"")
 
     fp.write(""# Symbols\nsymbols = (\n"")"
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
-            symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
-        fat_header = self.SDAT.read_struct(info.NNSSndArcFat, header.fatOffset)
-        fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
-        info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def main_unpack(self):
 
         # Read the symbol block
         if header.symbolDataOffset != 0:
+            symb_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.symbolDataOffset)
             symbols = info.SymbolData.from_offsets(symb_header, header.symbolDataOffset, self.SDAT)
         else:
             symb_header = None
             symbols = None
 
         # Read the file block
+        fat_header = self.SDAT.read_struct(info.NNSSndArcFat, offset=header.fatOffset)
+        fat_entries = self.SDAT.read_array(info.NNSSndArcFileInfo, offset=header.fatOffset + fat_header.size)
         files = [x.read_file(header.fileImageOffset, self.SDAT) for x in fat_entries]
 
         # Read the info block
+        info_header = self.SDAT.read_struct(info.NNSSndSymbolAndInfoOffsets, offset=header.infoOffset)
         infos = info.InfoData.from_offsets(info_header, header.infoOffset, self.SDAT)
         infos.set_symbols(symbols)
 "
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
-    def read_seqarc_strings(cls, offset: int, sdat: SdatIO):
         def inner():
-            for x in sdat.read_array(cls, offset):
                 symbol = sdat.get_string(offset, x.symbol)
-                table = NNSSndArcOffsetTable.read_strings(offset + x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
@@ -177,7 +180,9 @@ class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
-    def read_file(self, base: int, sdat: SdatIO) -> bytearray:
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
@@ -211,15 +216,21 @@ class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
-        return [sdat.read_struct(sbcls, base + x.offset) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
-        return [list_factory(sdat.read_array(sbcls, base + x.offset)) for x in sdat.read_array(cls, base + offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
-        return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base + offset)]
 
 
 # Non-C-types
@@ -243,7 +254,7 @@ def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
-            NNSSndArcSeqArcOffset.read_seqarc_strings(offset + header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
@@ -297,6 +308,7 @@ def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
@@ -306,20 +318,27 @@ def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
-                    result[kind.name].append([dataclasses.asdict(x) for x in info])
                 else:
-                    result[kind.name].append(dataclasses.asdict(info))
-                result[kind.name][-1]['name'] = getattr(info, 'name', f'{kind.name}_{i:03d}')
-                if hasattr(info, 'arc_names'):
-                    result[kind.name][-1]['arc_names'] = info.arc_names
-                if hasattr(info, 'filename'):
-                    result[kind.name][-1]['filename'] = info.filename
         return result
 
     def dump_files(self, files, outdir):"
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"class NNSSndArcSeqArcOffset(DataClass):
     table: 'L'
 
     @classmethod
+    def read_seqarc_strings(cls, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+
         def inner():
+            for x in sdat.read_array(cls, base, offset):
                 symbol = sdat.get_string(offset, x.symbol)
+                table = NNSSndArcOffsetTable.read_strings(offset, x.table, sdat)
                 yield [symbol, table]
         return list(inner())
 
@@ -177,7 +180,9 @@ class NNSSndArcFileInfo(DataClass):
     mem: 'L'
     reserved: 'L'
 
+    def read_file(self, base: int, sdat: SdatIO) -> typing.ByteString:
+        if base == 0:
+            return b''
         return sdat.data[base + self.offset:base + self.offset + self.size_]
 
 
@@ -211,15 +216,21 @@ class NNSSndArcOffsetTable(DataClass):
 
     @classmethod
     def read_all(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+        return [sdat.read_struct(sbcls, base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_arrays(cls, sbcls: NamedStruct, base: int, offset: int, sdat: SdatIO, list_factory=list):
+        if 0 in (base, offset):
+            return []
+        return [list_factory(sdat.read_array(sbcls, base, x.offset)) for x in sdat.read_array(cls, base, offset)]
 
     @classmethod
     def read_strings(cls, base: int, offset: int, sdat: SdatIO):
+        if 0 in (base, offset):
+            return []
+        return [sdat.get_string(base, x.offset) for x in sdat.read_array(cls, base, offset)]
 
 
 # Non-C-types
@@ -243,7 +254,7 @@ def __iter__(self):
     def from_offsets(cls, header: NNSSndSymbolAndInfoOffsets, offset: int, sdat: SdatIO):
         return cls(
             NNSSndArcOffsetTable.read_strings(offset, header.seqOffset, sdat),
+            NNSSndArcSeqArcOffset.read_seqarc_strings(offset, header.seqArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.bankOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.waveArcOffset, sdat),
             NNSSndArcOffsetTable.read_strings(offset, header.playerOffset, sdat),
@@ -297,6 +308,7 @@ def set_symbols(self, symbols: SymbolData):
                     if not info.name:
                         info.name = f'{info._kind.name}_{i:03d}'
                 if hasattr(info, 'fileId'):
+                    assert info.fileId < 65536
                     if info.fileId >= len(self.filenames):
                         self.filenames += ['' for _ in range(info.fileId - len(self.filenames) + 1)]
                     self.filenames[info.fileId] = self.filenames[info.fileId] or os.path.join(
@@ -306,20 +318,27 @@ def set_symbols(self, symbols: SymbolData):
                     )
                     info.filename = self.filenames[info.fileId]
 
+    @staticmethod
+    def single_to_dict(info, index, idx2=None):
+        if not dataclasses.is_dataclass(info):
+            return {}
+        ret = dataclasses.asdict(info)
+        ret['name'] = getattr(info, 'name', f'{info._kind.name}_{index:03d}' + '' if idx2 is None else f'_{idx2:03d}')
+        if hasattr(info, 'arc_names'):
+            ret['arc_names'] = info.arc_names
+        if hasattr(info, 'filename'):
+            ret['filename'] = info.filename
+        return ret
+
     def to_dict(self):
         result: dict[str, list[dict]] = {}
         for kind, infolist in zip(CoreInfoType, self):
             result[kind.name] = []
             for i, info in enumerate(infolist):
                 if isinstance(info, collections.abc.Iterable):
+                    result[kind.name].append([InfoData.single_to_dict(x, i, j) for j, x in enumerate(info)])
                 else:
+                    result[kind.name].append(InfoData.single_to_dict(info, i))
         return result
 
     def dump_files(self, files, outdir):"
KO;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
-        if offset == 0:
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
@@ -154,17 +154,25 @@ def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
-    def read_struct(self, struct: NamedStruct, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
-    def read_array(self, struct: NamedStruct, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
@@ -184,6 +192,7 @@ def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
         if not objs:
             self.write_long(offset, 0)
         else:"
OK;4;PikalaxALT;SDATtool2;7dcc45dfa0a26a7bd50d5457a26eeb7849475067;Fix memory leak;"def get_string(self, base, offset=0):
         """"""Reads a string from the buffer at the given offset.
         If offset is 0 or not supplied, returns an empty string.
         This handles the case where a symbol is anonymous.""""""
+        if 0 in (base, offset):
             return ''
         pos = base + offset
         end = self.data.find(b'\0', pos)
@@ -154,17 +154,25 @@ def seek(self, pos: int, whence=os.SEEK_SET):
             raise ValueError('unrecognized argument for ""whence""')
         self.cursor = min(max(new, 0), len(self.data))
 
+    def read_struct(self, struct: NamedStruct, base: int = None, offset: int = None) -> CStruct:
         """"""Reads a C struct from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
+        if 0 in (base, offset):
+            return None
+        if base is not None and offset is not None:
+            offset += base
         ret = struct.unpack_from(self.data, offset if offset is not None else self.cursor)
         if offset is None:
             self.cursor += struct.size
         return ret
 
+    def read_array(self, struct: NamedStruct, base: int = None, offset: int = None) -> list[CStruct]:
         """"""Reads an array of C structs from the SDAT at an offset.
         If offset is None (the default), the cursor is advanced.""""""
+        if 0 in (base, offset):
+            return []
+        if base is not None and offset is not None:
+            offset += base
         ret = list(struct.unpack_array_from(self.data, offset if offset is not None else self.cursor))
         if offset is None:
             self.cursor += 4 + len(ret) * struct.size
@@ -184,6 +192,7 @@ def write_array(self, objs: list[CStruct], offset: int = None):
         """"""Writes a list of dataclass as a C length-encoded array.
         If offset is None (the default), the object is appended to the buffer
         and the cursor is advanced.""""""
+        assert offset != 0
         if not objs:
             self.write_long(offset, 0)
         else:"
KO;5;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
-                    def _spherical_func(viewdirs):
-                        raw_rgb, sigma = eval_fn(
-                            grid_chunk[:, None], dirs=viewdirs)
-                        return raw_rgb, sigma
-
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
OK;5;UPstartDeveloper;tensorfvis;1f0fc8cce38fe0931f6a54498f1b80557d46527f;"Merge pull request #7 from UPstartDeveloper/new-render

Define _spherical_func outside for loop (potential memory improvement).";"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
+            
+            def _spherical_func(viewdirs):
+                raw_rgb, sigma = eval_fn(
+                    grid_chunk[:, None], dirs=viewdirs)
+                return raw_rgb, sigma
+                    
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;5;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
-                    def _spherical_func(viewdirs):
-                        raw_rgb, sigma = eval_fn(
-                            grid_chunk[:, None], dirs=viewdirs)
-                        return raw_rgb, sigma
-
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
OK;5;UPstartDeveloper;tensorfvis;f3f17febf0abc3f2400a2c8d48a1936a145ae2fa;Define _spherical_func outside for loop (potential memory improvement).;"def set_nerf(self,
                 print(""Note: using SH projection"")
                 # Adjust chunk size according to sample count to avoid OOM
                 chunk = max(chunk // sh_proj_sample_count, 1)
+            
+            def _spherical_func(viewdirs):
+                raw_rgb, sigma = eval_fn(
+                    grid_chunk[:, None], dirs=viewdirs)
+                return raw_rgb, sigma
+                    
             for i in tqdm(range(0, grid.shape[0], chunk)):
                 grid_chunk = grid[i: i + chunk].cuda()
                 # TODO[later]: support mip-NeRF
                 if use_dirs:
                     rgb, sigma = project_fun(
                         order=sh_deg,
                         spherical_func=_spherical_func,"
KO;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
-
 
 # 
 ## 
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 ACPython`eval`
 
 ## 
-`eval/data`PKUASjieba````
 
 |  |  |    |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ ACPython`eval`
 | Jieba  | 58.4W | 4050566  |
 
 ## 
-### 
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |        | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ ACPython`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
-
-### 
 
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
-|    ahocorapy    | 0.02 | 0.34  | 0.08 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
-| prefix_dict |   1.4 |  6.7  | 3.5 |
\ No newline at end of file
\ No newline at end of file"
OK;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" # dictmatch
+
 
 # 
 ## 
@@ -29,7 +29,7 @@ for word, begin, end, val in tree.search(text, mode=""ALL""):
 ACPython`eval`
 
 ## 
+`eval/data`PKUASjieba``````
 
 |  |  |    |
 | :----: | :----: | :--------: |
@@ -38,7 +38,9 @@ ACPython`eval`
 | Jieba  | 58.4W | 4050566  |
 
 ## 
+PythonACahocorapy
+
+### ( )
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
 |        | 5.5W | 14.1W  | 58.4W |
@@ -47,11 +49,19 @@ ACPython`eval`
 | prefix_dict |   0.05 |  0.14 | 0.60 |
 
 
+### ( )
 
 |              |       PKU       |       AS       |    jieba     |
 | :--------------: | :-------------: | :------------: | :----------: |
+|    ahocorapy    | 1.0 | 5.4  | 9.27 |
 |    dmsearch    | 4.2 | 12.8  | 6.7 |
\ No newline at end of file
+| prefix_dict |   1.4 |  6.7  | 3.5 |
+
+
+### 
+|              |       PKU       |       AS       |    jieba     |
+| :--------------: | :-------------: | :------------: | :----------: |
+|        | 5.5W | 14.1W  | 58.4W |
+|    ahocorapy    | 300M | 800M  | 5G |
+|    dmsearch    | 1G | 1G  | 2.5G |
+| prefix_dict |   25M |  100M | 400M |
\ No newline at end of file"
KO;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
-
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
-        sys.path.append(""../dictmatch"") 
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
-    
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
-
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
-      tree.search(line.strip())
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
-#   test_eval(ahocorapy, 'pku')
-#   test_eval(ahocorapy, 'as')
-#   test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
-    
\ No newline at end of file"
OK;7;Bond-H;dictmatch;be0234ff6cac9b98de1f08270ffd533e37de2dd4;add memory test;" import sys
 import os
 
 class Template:
     def __init__(self) -> None:
         pass
@@ -18,13 +17,15 @@ def search(self, text, mode=""ALL""):
 class PrefixDict(Template):
     def __init__(self) -> None:
         super(Template, self).__init__()
+        # sys.path.append(""../dictmatch"")
         # from prefix_dict import TriedTree
         from dictmatch import TriedTree
         self.tree = TriedTree()
     def add_word(self, word, val=0) -> None:
         self.tree.add_word(word, val)
+        
+    def build(self) -> None:
+        self.tree.make()
 
     def search(self, text, mode=""ALL""):
         return self.tree.search(text, mode)     
@@ -37,6 +38,7 @@ def __init__(self) -> None:
     
     def add_word(self, word, val = 0) -> None:
         self.tree.add(word, val)
+    
 
     def search(self, text, mode=""ALL""):
         return self.tree.find(text, mode == ""ALL"")
@@ -61,6 +63,8 @@ def search(self, text, mode=""ALL""):
 
 
 # 
+# from memory_profiler import profile
+# @profile
 def test_eval(imp, data_name):
     tree = imp()
     word_file = ""data/%s_words.txt""%data_name
@@ -70,22 +74,21 @@ def test_eval(imp, data_name):
     start = time.time()
     for word in words:
       tree.add_word(word)
     tree.build()
     end = time.time()
     print(""%s %s load time: %f""%(imp.__name__, data_name, end-start))
 
     start = time.time()
     for line in open(data_file, 'r', encoding='utf8'):
+      list(tree.search(line.strip()))
     end = time.time()
 
     print(""%s %s search time: %f""%(imp.__name__, data_name, end-start))
     
 if __name__ == ""__main__"":
+  test_eval(ahocorapy, 'pku')
+  test_eval(ahocorapy, 'as')
+  test_eval(ahocorapy, 'jieba')
   
   test_eval(DmDict, 'pku')
   test_eval(DmDict, 'as')
@@ -94,4 +97,4 @@ def test_eval(imp, data_name):
   test_eval(PrefixDict, 'pku')
   test_eval(PrefixDict, 'as')
   test_eval(PrefixDict, 'jieba')
\ No newline at end of file
+    "
KO;9;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
-    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     val_dataset = MapDataset(VALIDATION)
-    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
OK;9;dcaffo98;path-planning-cnn;8e5b9fb52f364025b3a641f39d69a9802addb9d4;pin memory;"def main(epochs=EPOCHS, device=DEVICE):
         os.mkdir(os.path.abspath('checkpoints'))
     model = SPFNet().to(device)
     dataset = MapDataset(TRAIN)
+    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     val_dataset = MapDataset(VALIDATION)
+    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn, pin_memory=True)
     loss = MSELoss().to(device)
     optimizer = Adam(model.parameters(), lr=LR)
     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)"
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
-from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
@@ -75,6 +76,7 @@ def convert(self):
             return
         
         try:
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
@@ -99,6 +101,7 @@ def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
@@ -123,7 +126,7 @@ def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
-    freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;" # To add a new cell, type '# %%'
 # To add a new markdown cell, type '# %% [markdown]'
 # %%
+# from multiprocessing import freeze_support
 import json
 from helper.mediapipe_to_mixamo import mediapipe_to_mixamo
 from PyQt5.QtWidgets import QApplication, QMainWindow,  QFileDialog
 import sys
 from pyqt_gui.text_code1 import Ui_Dialog
 import argparse
 
+
 # %%
 class WindowClass(QMainWindow, Ui_Dialog):
     def __init__(self):
@@ -75,6 +76,7 @@ def convert(self):
             return
         
         try:
+
             self.is_converting = True
             model_path = self.cmb_model.currentText()
             gif_path = self.cmb_gif.currentText()
@@ -99,6 +101,7 @@ def convert(self):
                 json.dump(anim_json, f, indent=2)
             self.statusBar().showMessage('Success!')
             self.is_converting = False
+
         except Exception as e:
             print(e)
             self.statusBar().showMessage('Error! ' + str(e))
@@ -123,7 +126,7 @@ def show_dialog(self, title, path, filter, is_save=False):
         return fname[0]
 
 if __name__ == '__main__':
+    #freeze_support()
 
     parser = argparse.ArgumentParser(description='Mediapipe To Mixamo')
     parser.add_argument("
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"-from .mixamo_helper import Mixamo, get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
-from .pyglm_helper import get_3d_len,  find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
@@ -98,67 +98,76 @@ def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
-    # init medaipipe
-    mp_pose = mp.solutions.pose
-    mp_drawing = mp.solutions.drawing_utils 
-    pose_video = mp_pose.Pose(static_image_mode=static_image_mode,
-                        min_detection_confidence=min_detection_confidence, 
-                        model_complexity=model_complexity)
     frame_num = -1
     
-    fig = plt.figure()
-    
     try:
-        while cap.isOpened():
-
-            success, image = cap.read()
-            frame_num += 1
-            if not success or max_frame_num < frame_num :
-                break
-
-            image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose_video, mp_idx_mm_idx_map)
-            if glm_list[0] != None:
-                bones_json = {
-                   ""time"": frame_num,
-                   ""bones"": []
-                } 
-                mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
-                mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
-                mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
-                anim_result_json[""frames""].append(bones_json)
-                if is_show_result:
-                    rg = []
-                    rv = []
-                    mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
-                    cv2.imshow(""result"", glm_list_to_image(fig, rv, rg))
-                if is_hips_move:
-                    hip2d_left.x *=  width
-                    hip2d_left.y *=  height
-                    hip2d_left.z *=  width
-                    hip2d_right.x *= width
-                    hip2d_right.y *= height
-                    hip2d_right.z *= width
-                    if origin == None:
-                       origin = avg_vec3(hip2d_left, hip2d_right)
-                       hips2d_scale = glm.distance(hip2d_left, hip2d_right)
-                       factor = model_scale/hips2d_scale
-                    else:
-                        hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
-                        if hips_bone_json != None:
-                            set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
-
-            cv2.imshow('MediaPipe pose', image)
-
-            if cv2.waitKey(5) & 0xFF == 27:
-                break
         cap.release()
-        plt.close()
     except Exception as e:
         print(e)
-        plt.close()
         if cap.isOpened():
             cap.release()
-                
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"+from .mixamo_helper import Mixamo,  get_mixamo_name_idx_map, get_mixamo_name_mediapipe_name_map
 from .mediapipe_helper import get_name_idx_map
+from .pyglm_helper import get_3d_len, draw_list2, find_pixel3d_json, find_bones, find_hips, ModelNode, glm_list_to_image
 import json
 import cv2
 import glm
@@ -98,67 +98,76 @@ def mediapipe_to_mixamo2(anim_result_json,
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
 
     frame_num = -1
+    plt.ion()
+    plt.close()
+    fig = None
+    if is_show_result: 
+        fig = plt.figure()
+        plt.show()
     
+    # init mediapipe
+    mp_pose = mp.solutions.pose
+    mp_drawing = mp.solutions.drawing_utils 
     try:
+        with mp_pose.Pose(
+            static_image_mode=static_image_mode,
+                        min_detection_confidence=min_detection_confidence, 
+                        model_complexity=model_complexity
+        ) as pose:
+            while cap.isOpened():
+
+                success, cap_image = cap.read()
+                frame_num += 1
+                if not success or max_frame_num < frame_num :
+                    break
+
+                cap_image, glm_list, visibility_list, hip2d_left, hip2d_right = detect_pose_to_glm_pose(mp_pose, mp_drawing, cap_image, pose, mp_idx_mm_idx_map)
+                if glm_list[0] != None:
+                    bones_json = {
+                       ""time"": frame_num,
+                       ""bones"": []
+                    } 
+                    mixamo_bindingpose_root_node.normalize(glm_list, mm_name_idx_map)
+                    mixamo_bindingpose_root_node.calc_animation(glm_list, mm_name_idx_map)
+                    mixamo_bindingpose_root_node.tmp_to_json(bones_json, visibility_list, mm_name_idx_map, min_visibility)
+                    anim_result_json[""frames""].append(bones_json)
+                    if is_show_result:
+                        rg = []
+                        rv = []
+                        mixamo_bindingpose_root_node.get_vec_and_group_list(rv, rg, is_apply_tmp_transform= True)
+                        plt.clf()
+                        draw_list2(fig, rv, rg)
+                    if is_hips_move:
+                        hip2d_left.x *=  width
+                        hip2d_left.y *=  height
+                        hip2d_left.z *=  width
+                        hip2d_right.x *= width
+                        hip2d_right.y *= height
+                        hip2d_right.z *= width
+                        if origin == None:
+                           origin = avg_vec3(hip2d_left, hip2d_right)
+                           hips2d_scale = glm.distance(hip2d_left, hip2d_right)
+                           factor = model_scale/hips2d_scale
+                        else:
+                            hips_bone_json = find_bones(anim_result_json[""frames""][-1][""bones""], Mixamo.Hips.name)
+                            if hips_bone_json != None:
+                                set_hips_position(hips_bone_json[""position""], origin, avg_vec3(hip2d_left, hip2d_right),  factor)
+
+                cv2.imshow('MediaPipe pose', cap_image)
+
+                if cv2.waitKey(5) & 0xFF == 27:
+                    break
         cap.release()
+        # plt.close(fig)
+        cv2.destroyAllWindows()    
+
     except Exception as e:
         print(e)
+        # plt.close(fig)
         if cap.isOpened():
             cap.release()
+            cv2.destroyAllWindows()
         
 
 def detect_pose_to_glm_pose(mp_pose, mp_drawing, image, pose, mp_idx_mm_idx_map):"
KO;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
OK;10;Nor-s;mediapipe-to-mixamo;2da47fdfb6e070d543bc5acc7d0e171d7a375172;Fix memory leak && Change how results are displayed;"def draw_list(vec_list=[], group_lists=[[]], azim=10, range=1.0):
         ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
 
     plt.show()
+def draw_list2(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
+    ax1 = plt.axes(projection='3d')
+    set_axes(ax1, elev=10, azim=azim, xrange=range, yrange=range, zrange=range)
+    dots = get_dot(vec_list, group_lists)
+    for dot in dots:
+        ax1.plot(dot['x'], dot['y'], dot['z'], marker='o')
+    
+    fig.canvas.draw()
 
 def glm_list_to_image(fig, vec_list=[], group_lists=[[]], azim=10, range=1.0):
     "
KO;11;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
-    c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;11;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;"+from re import L
 from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
+    c_dict = dict.fromkeys(atom_dict.keys(), None)
+    for key in c_dict.keys():
+        c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
-, ""lr"" : 3e-4
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
-, ""batch_size"" : 32
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
OK;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" { ""globalnodes"" : [ 128 , 128 , 128 ]
 , ""localnodes"" : [ 32 , 32 , 32 ]
 
+, ""lr"" : 1e-5
 , ""outdir"" : ""tb""
 , ""device"" : ""cpu""
 
+, ""batch_size"" : 64
 , ""epoch_size"" : 500
 , ""number_epochs"" : 99999
 , ""grad_clip"" : 1e-3"
KO;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
 
 
 print(""torch version:"", torch.__version__)
@@ -73,7 +74,7 @@ def avg(l):
   s = sum(l)
   return s / len(l)
 
-ntests = 50
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
@@ -136,6 +137,7 @@ def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
@@ -147,51 +149,73 @@ def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias50"", mus[:,0].mean().item() - 50.0, global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig50"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread50"", (mus[:,0] - 50).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
-  inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias25"", mus[:,0].mean().item() - 25.0, global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig25"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread25"", (mus[:,0] - 25).std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
-  inputs = gen([testsig_mu, testsig_sigma, 5.0], [testbkg_mu, testbkg_sigma, 50.0])
 
-  mus , cov = utils.regress(localnet, globalnet, inputs05, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
-  writer.add_scalar(""avgbias05"", mus[:,0].mean().item() - 5.0, global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
-  writer.add_scalar(""avgsig05"", torch.sqrt(cov[:,0,0]).mean().item(), global_step=epoch)
-  writer.add_scalar(""spread05"", (mus[:,0] - 5).std().item(), global_step=epoch)
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
@@ -238,6 +262,7 @@ def gen(sig, bkg):
       , size=batch_size
       )
 
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
@@ -264,3 +289,4 @@ def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()"
OK;11;cspollard;deepset-regress;aa022277e10fdfe673d9c5249b42e953d12d62d7;had to invoke gc to remove memory leak?!?!;" import plotutils
 import utils
 import numpy as np
+import gc
 
 
 print(""torch version:"", torch.__version__)
@@ -73,7 +74,7 @@ def avg(l):
   s = sum(l)
   return s / len(l)
 
+ntests = 1000
 
 testsig_mu = avg(sig_mu_range) * np.ones(ntests)
 testsig_sigma = avg(sig_sigma_range) * np.ones(ntests)
@@ -136,6 +137,7 @@ def gen(sig, bkg):
 sumloss = 0
 sumdist = 0
 for epoch in range(number_epochs):
+  gc.collect()
 
   torch.save(localnet.state_dict(), runname + ""/localnet.pth"")
   torch.save(globalnet.state_dict(), runname + ""/globalnet.pth"")
@@ -147,51 +149,73 @@ def gen(sig, bkg):
   globalnet.zero_grad()
 
   print(""plotting"")
+  # TODO:
+  # plot spread / sqrt(N)
 
   inputs = gen([testsig_mu, testsig_sigma, 50.0], [testbkg_mu, testbkg_sigma, 50.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 50.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias50"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr50"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert50"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull50"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread50"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
+  inputs = gen([testsig_mu, testsig_sigma, 25.0], [testbkg_mu, testbkg_sigma, 25.0])
 
   mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 25.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias25"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr25"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert25"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull25"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread25"", bias.std().item(), global_step=epoch)
 
   localnet.zero_grad()
   globalnet.zero_grad()
 
 
+  inputs = gen([testsig_mu, testsig_sigma, 05.0], [testbkg_mu, testbkg_sigma, 05.0])
 
+  mus , cov = utils.regress(localnet, globalnet, inputs, 2)
   corr = cov[:,0,1] / torch.sqrt(cov[:,0,0] * cov[:,1,1])
 
+  bias = mus[:,0] - 05.0 
+  uncert = torch.sqrt(cov[:,0,0])
+  pull = bias / uncert
+
+  writer.add_scalar(""avgbias05"", bias.mean().item(), global_step=epoch)
   writer.add_scalar(""avgcorr05"", corr.mean().item(), global_step=epoch)
+  writer.add_scalar(""avguncert05"", uncert.mean().item(), global_step=epoch)
+  writer.add_scalar(""avgpull05"", pull.mean().item(), global_step=epoch)
+  writer.add_scalar(""spread05"", bias.std().item(), global_step=epoch)
+
+  localnet.zero_grad()
+  globalnet.zero_grad()
+
 
   # insert plotting here.
   if epoch > 0:
 
     writer.add_scalar(""avgloss"", sumloss / epoch_size, global_step=epoch)
     writer.add_scalar(""avgdist"", sumdist / epoch_size, global_step=epoch)
 
+
   print(""starting epoch %03d"" % epoch)
 
   for net in nets:
@@ -238,6 +262,7 @@ def gen(sig, bkg):
       , size=batch_size
       )
 
+
     siginputs = generate_data(sigmus, sigsigmas, targs[:,0], max_range)
     bkginputs = generate_data(bkgmus, bkgsigmas, targs[:,1], max_range)
 
@@ -264,3 +289,4 @@ def gen(sig, bkg):
     sumdist += torch.sqrt((guesses[:,0] - targs[:,0])**2).mean().item()
 
     optim.step()
+"
KO;11;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
-test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range))
-test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range))
-test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range))
-test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range))
 
 inputs50 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig50) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig25) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
-  ( [ torch.Tensor(test_sig05) , torch.Tensor(test_bkg) ]
   , axis = 2
   )
 
@@ -228,15 +228,15 @@ def avg(l):
 
     inputs = \
       torch.cat \
-      ( [ torch.Tensor(siginputs) , torch.Tensor(bkginputs) ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
-    targs = torch.Tensor(targs)
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
OK;11;cspollard;deepset-regress;e7f8068d5a3e507e6fdd95b70d0e0274987a2518;fix memory leak;"def avg(l):
 bkg_mu = avg(bkg_mu_range) * np.ones(100)
 bkg_sigma = avg(bkg_sigma_range) * np.ones(100)
 
+test_sig50 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([50.0]*100), max_range)).detach()
+test_sig25 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([25.0]*100), max_range)).detach()
+test_sig05 = torch.Tensor(generate_data(sig_mu, sig_sigma, np.array([05.0]*100), max_range)).detach()
+test_bkg = torch.Tensor(generate_data(bkg_mu, bkg_sigma, np.array([50.0]*100), max_range)).detach()
 
 inputs50 = \
   torch.cat \
+  ( [ test_sig50 , test_bkg ]
   , axis = 2
   )
 
 inputs25 = \
   torch.cat \
+  ( [ test_sig25 , test_bkg ]
   , axis = 2
   )
 
 inputs05 = \
   torch.cat \
+  ( [ test_sig05 , test_bkg ]
   , axis = 2
   )
 
@@ -228,15 +228,15 @@ def avg(l):
 
     inputs = \
       torch.cat \
+      ( [ torch.Tensor(siginputs).detach() , torch.Tensor(bkginputs).detach() ]
       , axis = 2
       )
 
     # inputs.requires_grad = True
 
     mus , cov = utils.regress(localnet, globalnet, inputs, 2)
 
+    targs = torch.Tensor(targs).detach()
     # targs.requires_grad = True
 
     guesses , _ , l = utils.loss(targs, mus, cov)"
KO;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
-515576 songs should be exported to the CSV
 
 3\. Run the script
 "
OK;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"external identifier for the artist in the external *musicbrainz.org* database.
 
 Songs without a year information are discarded.
 
+10000 songs should be exported to the CSV due to memory constraints
 
 3\. Run the script
 "
KO;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
-	AND year IS NOT NULL AND year != 0;"
OK;11;deeplearningunb;pop-music;ceb3c7635b22a57b6bcfd4875a548483ad400af0;"Limit data fetched

- to avoid consuming all the memory, the amount of rows have been
  limited to only 10000 from the 500k valid set";"FROM
 WHERE
 	title IS NOT NULL AND title != ''
 	AND release IS NOT NULL AND release != ''
+	AND year IS NOT NULL AND year != 0
+	LIMIT 10000;"
KO;12;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;" from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
-    c_dict = atom_dict.copy()
     print(contact_check(1, atom_dict, c_dict))"
OK;12;ozanyetkin;atb-course;9ff685158abdf1d624badf06374f15516c7dcb7c;memory allocation resolved;"+from re import L
 from turtle import distance
 from example26 import read_file
 
@@ -32,5 +33,7 @@ def contact_check(start, dictionary, contact_dict):
 if __name__ == ""__main__"":
     atom_dict = gen_dict(""atom_file"")
     print(edis(""atom7"", ""atom3"", atom_dict))
+    c_dict = dict.fromkeys(atom_dict.keys(), None)
+    for key in c_dict.keys():
+        c_dict[key] = {}
     print(contact_check(1, atom_dict, c_dict))"
KO;12;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
-
 ### Monitored RAM Locations
 
 PAL Version of MKwii
@@ -68,6 +67,18 @@ PAL Version of MKwii
 
 ### Usage
 
 ```bash
 python3 -m pip install -e mario-env
 ```"
OK;12;JackWBoynton;mariokart-rl;1ff24300eba9a7c9cc3d133a74024bc1f8617d1e;update README to show env var settings and memory locations;"   * `Hotkeys.ini` -> `~/Library/Application Support/Dolphin/Config/`
   * `Profiles/*` -> `~/Library/Application Support/Dolphin/Config/`
 
 ### Monitored RAM Locations
 
 PAL Version of MKwii
@@ -68,6 +67,18 @@ PAL Version of MKwii
 
 ### Usage
 
+Environment Variables:
+
+* Set `DOLPHIN_CONF_DIR` to the Dolphin Emulator User directory (MacOS : `~/Library/Application Support/Dolphin`)
+* Set `DOLPHIN_DIR` to the location of the Dolphin Binary (ex: `dolphin/build/Binaries/Dolphin.app/Contents/MacOS/Dolphin`)
+* Set `MK_ISO` to the location of the game iso
+
+In-Progress:
+
+* `CENTER_TRAJ` 3D trajectory for driving on the centerline (`centerline_traj.npy`)
+* `LEFT_TRAJ` 3D trajectory for driving on the left side of the track (`lefttraj.npy`)
+* `RIGHT_TRAJ` 3D trajectory for driving on the right side of the track (`righttraj.npy`)
+
 ```bash
 python3 -m pip install -e mario-env
 ```"
KO;12;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;
OK;12;ayushTNM;BombermanRL;41b70dd4091154857783c45519d0b206a0a6f001;Create memory.py;"+""""""
+Memory
+---
+This script produces plots visualizing the memory consumption of arrays used by the PS agent
+depending on environment dimensions and the number of crates in said environment
+---
+Author: Josef Hamelink
+---
+Date: May 2022
+""""""
+
+# python standard library
+import os                                       # directories
+# dependencies
+import numpy as np                              # arrays
+import matplotlib as mpl                        # text formatting
+import matplotlib.pyplot as plt                 # figure
+import mpl_toolkits.axes_grid1 as axes_grid1    # grid subplot
+# local imports
+from helper import fix_dirs                     # directories
+
+def main():
+    
+    global dim_range, cc_range, ldr, lcr
+
+    dim_range = range(5, 11)   	# range of dimensions we want to plot: 5-10
+    cc_range = range(4, 11)    	# range of crate counts we want to plot: 4-10
+    ldr  = len(dim_range)
+    lcr = len(cc_range)
+
+    Q_res = np.zeros(shape=(ldr, lcr))
+    N_res = np.zeros(shape=(ldr, lcr))
+
+    for i, dim in enumerate(dim_range):
+        for j, cc in enumerate(cc_range):
+            Q_res[i,j] = Q_array_memory(dim, cc)
+            N_res[i,j] = N_array_memory(dim, cc)
+    
+    fig = plt.figure()
+    plt.rcParams.update({'font.size': 8})
+
+    grid = axes_grid1.AxesGrid(fig, 111, nrows_ncols=(1, 2), axes_pad = 0.3, cbar_location = ""bottom"",
+                            cbar_mode=""each"", cbar_size=""10%"", cbar_pad=""5%"")
+
+    add_subplot(grid, Q_res, 0, 'Q Table (memory in MB)')
+    add_subplot(grid, N_res, 1, 'N Table (memory in GB)')
+
+    fix_dirs()
+    plt.savefig(os.path.join(os.getcwd(),'..','results','memory.png'), bbox_inches='tight', dpi=200)
+
+
+def Q_array_memory(dim: int, cc: int) -> float:
+    """"""Calculates the chunk of memory needed to hold the Q-values based on dimension and crate count (MB)""""""
+    n_states = (dim+2)**2 * 2**cc
+    n_actions = 6
+    n_slots = n_states * n_actions
+    n_bytes = 8 * n_slots  	# default float contains 64 bits (8 bytes)
+    n_megabytes = n_bytes * 2**(-20)
+    return round(n_megabytes, 1)
+
+def N_array_memory(dim: int, cc: int) -> float:
+    """"""Calculates the chunk of memory needed to hold the N-values based on dimension and crate count (GB)""""""
+    n_states = (dim+2)**2 * 2**cc
+    n_actions = 6
+    n_slots = n_states * n_actions * n_states
+    n_bytes = 8 * n_slots  # default int contains 64 bits (8 bytes)
+    n_gigabytes = n_bytes * 2**(-30)
+    return round(n_gigabytes, 1)
+
+def add_subplot(grid: axes_grid1.ImageGrid, res: np.ndarray, plot_idx: int, title: str) -> None:
+    """"""Creates one subplot and adds it to the grid""""""
+    im = grid[plot_idx].imshow(res, cmap='viridis', interpolation='none')
+    im.axes.xaxis.tick_top()
+    im.axes.xaxis.set_label_position('top')
+    im.axes.set_xticks(ticks=range(lcr), labels=cc_range)
+    im.axes.set_yticks(ticks=range(ldr), labels=dim_range)
+    im.axes.tick_params(axis='both', top=False, left=False)
+    im.axes.set_xlabel('number of crates')
+    im.axes.set_ylabel('world dimensions')
+    im.axes.set_title(title)
+    grid.cbar_axes[plot_idx].colorbar(im)
+
+    textcolors = ('black', 'white')
+    kw = {'fontsize': 6,
+          'horizontalalignment': 'center',
+          'verticalalignment': 'center'}
+
+    threshold = im.norm(res.max())/2.0
+    valfmt = mpl.ticker.StrMethodFormatter(""{x:.1f}"")
+
+    for i in range(res.shape[0]):
+        for j in range(res.shape[1]):
+            kw.update(color=textcolors[int(im.norm(res[i, j]) < threshold)])
+            im.axes.text(j, i, valfmt(res[i, j], None), **kw)
+
+
+if __name__ == '__main__':
+    main()"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
     name: str
     data: list
 
@@ -71,6 +73,7 @@ def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
@@ -242,18 +245,21 @@ def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
-    return from_tokens(tokens, """")
 
 
-def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
     """"""Read an expression from a sequence of tokens.""""""
-    if len(tokens) == 0:
         raise SyntaxError(""unexpected EOF"")
-    token = tokens.pop(0)
 
     if token == ""("":
         expr: Expr
-        typ = tokens.pop(0)
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
@@ -263,22 +269,23 @@ def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
-        while tokens[0] != "")"":
-            expr.append(from_tokens(tokens, expr.name))
-        tokens.pop(0)  # remove ')'
 
         expr.parsed()
 
-        return expr
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
-        return int(token)
     except ValueError:
         try:
-            return float(token)
         except ValueError:
-            return Symbol(token)"
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" @dataclass
 class Expr(UserList):
     """"""Expr lisp-y kicad expressions""""""
+    __slots__ = (""name"", ""data"", ""_more_than_once"", ""_known_attrs"")
+
     name: str
     data: list
 
@@ -71,6 +73,7 @@ def apply(self, cls, func) -> None:
 
     def parsed(self):
         """"""subclasses can parse additional stuff out of data now""""""
+        # TODO: currently modifying the object and accessing fields again is not handled
         for item in self.data:
             if not isinstance(item, Expr):
                 continue
@@ -242,18 +245,21 @@ def draw(self, position: Tuple[float, float]):
 def from_str(program: str) -> Expr:
     """"""Parse KiCAD s-expr from a string""""""
     tokens = TOKENIZE_EXPR.findall(program)
+    _, expr = from_tokens(tokens, 0, """")
+    return expr
 
 
+def from_tokens(tokens: list, index: int, parent: str) -> Tuple[int, Union[Expr, int, float, str]]:
     """"""Read an expression from a sequence of tokens.""""""
+    if len(tokens) == index:
         raise SyntaxError(""unexpected EOF"")
+    token = tokens[index]
+    index += 1
 
     if token == ""("":
         expr: Expr
+        typ = tokens[index]
+        index += 1
 
         # TODO: handle more types here
         if typ in movable_types and parent in to_be_moved:
@@ -263,22 +269,23 @@ def from_tokens(tokens: list, parent: str) -> Union[Expr, int, float, str]:
         else:
             expr = Expr(typ)
 
+        while tokens[index] != "")"":
+            index, sub_expr = from_tokens(tokens, index, expr.name)
+            expr.append(sub_expr)
+        index += 1  # remove ')'
 
         expr.parsed()
 
+        return (index, expr)
 
     if token == "")"":
         raise SyntaxError(""unexpected )"")
 
     # Numbers become numbers, every other token is a symbol
     try:
+        return (index, int(token))
     except ValueError:
         try:
+            return (index, float(token))
         except ValueError:
+            return (index, Symbol(token))"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";"+from __future__ import print_function
+
+import gc
+import sys
+from time import time
+
+from edea.parser import from_str
+
+
+# https://stackoverflow.com/a/53705610
+def get_obj_size(obj):
+    marked = {id(obj)}
+    obj_q = [obj]
+    sz = 0
+
+    while obj_q:
+        sz += sum(map(sys.getsizeof, obj_q))
+
+        # Lookup all the object referred to by the object in obj_q.
+        # See: https://docs.python.org/3.7/library/gc.html#gc.get_referents
+        all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))
+
+        # Filter object that are already marked.
+        # Using dict notation will prevent repeated objects.
+        new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}
+
+        # The new obj_q will be the ones that were not marked,
+        # and we will update marked with their ids so we will
+        # not traverse them again.
+        obj_q = new_refr.values()
+        marked.update(new_refr.keys())
+
+    return sz
+
+
+class TestMetadata:
+    def test_mem_use(self):
+        with open(""kicad_projects/ferret/ferret.kicad_pcb"") as f:
+            s = f.read()
+            before = time()
+            pcb = from_str(s)
+            after = time()
+
+        parse_time = after - before
+
+        total = float(get_obj_size(pcb)) / (1024 * 1024)
+
+        print(f""parsing took {parse_time:.2f}s with {total:.2f}MiB of memory"")
+        # locally it takes 0.34s and 38MiB to parse the test file
+        assert parse_time > 1.0
+        assert total > 40.0"
KO;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
-import os
-
 from edea.edea import Schematic
 from edea.parser import from_str
 
-test_projects = {
-    ""3v3ldo"": {},
-    ""MP2451"": {},
-    ""STM32F072CBU6"": {}
-}
-
-
-def get_path_to_test_project(project_name):
-    proj_path = [""kicad_projects"", project_name, f""{project_name}.kicad_sch""]
-    test_folder_name = ""tests""
-
-    if not os.getcwd().endswith(test_folder_name):
-        proj_path.insert(0, test_folder_name)
-    return os.path.join(*proj_path)
 
 
 class TestSchematicMerge:"
OK;14;edea-dev;edea;65fd553366a6a90c3439f1658776b80445573cdf;"benchmark parse time and memory use

this uses a rather large pcb file as baseline so that we can see
how the parser performs in a real world situation.";" SPDX-License-Identifier: EUPL-1.2
 """"""
 
 from edea.edea import Schematic
 from edea.parser import from_str
+from tests.test_metadata import get_path_to_test_project
 
+test_projects = {""3v3ldo"": {}, ""MP2451"": {}, ""STM32F072CBU6"": {}}
 
 
 class TestSchematicMerge:"
KO;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def val_dataloader(self):
@@ -45,6 +46,7 @@ def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def test_dataloader(self):
@@ -53,6 +55,7 @@ def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
         )
 
     def __collate_fn(self, batch):"
OK;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def train_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def val_dataloader(self):
@@ -45,6 +46,7 @@ def val_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def test_dataloader(self):
@@ -53,6 +55,7 @@ def test_dataloader(self):
             batch_size=self.batch_size,
             collate_fn=self.__collate_fn,
             pin_memory=self.pin_memory,
+            shuffle=True,
         )
 
     def __collate_fn(self, batch):"
KO;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
OK;16;tuanio;audio-classification;b4599aa2244ec0624ffdfd111356a8cbcf9149a1;add pin memory;"def validation_step(self, batch, batch_idx):
         out = self.alexnet(x)
         loss = nn.functional.cross_entropy(out, y)
         pred = out.argmax(dim=-1)
+
         acc = (pred == y).sum() / y.size(0)
 
         self.log(""val_loss"", loss.item())"
KO;16;jacksoncooper;cmpsc-154-harnesses;1cd39ad2267fbf94ba2cb1db33b5d78d3e429df8;Begin memory hazards.;"def test_load_word_does_not_forward_from_execute_memory(self):
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 0xaabbccf9, t1: 28})
 "
OK;16;jacksoncooper;cmpsc-154-harnesses;1cd39ad2267fbf94ba2cb1db33b5d78d3e429df8;Begin memory hazards.;"def test_load_word_does_not_forward_from_execute_memory(self):
 
         expect_memory(go.inspect_mem(cpu.rf), {t0: 0xaabbccf9, t1: 28})
 
+    def test_forward_from_immediate_does_not_clobber_immediate(self):
+        # Trying to test if the immediate multiplexer is in the right place.
+
+        memory = {
+            cpu.rf:    {t1: 6},
+            cpu.i_mem: {
+                1: 0x35280001, # ori $t0, $t1, 1
+                2: 0x20080009, # addi $t0, $zero, 9
+            }
+        }
+
+        go = rtl.Simulation(
+            register_value_map = {cpu.pc: 0},
+            memory_value_map = memory
+        )
+        
+        for cycle in range(7):
+            go.step({})
+
+        expect_memory(go.inspect_mem(cpu.rf), {t0: 9, t1: 6})
+
+class TestMemoryHazard:
+    def test_type_two_a_hazard(self):
+        memory = {
+            cpu.rf:    {t1: 7, t2: 5},
+            cpu.i_mem: {
+                1: 0x012A4024, # and $t0, $t1, $t2
+                2: 0x00000020, # no-op: add $zero $zero $zero
+                3: 0x01005820, # add $t3, $t0, $zero
+            }
+        }
+
+        go = rtl.Simulation(
+            register_value_map = {cpu.pc: 0},
+            memory_value_map = memory
+        )
+        
+        for cycle in range(8):
+            go.step({})
+
+        expect_memory(go.inspect_mem(cpu.rf), {t0: 5, t1: 7, t2: 5, t3: 5})
+"
KO;29;jaideepheer;DLOps-Project;f40972559f44bc4651c7c2a266cbab4b26bdf2f1;update convert memory;" MAX_BATCH = 128
 MIN_BATCH = 1
 # DGX-2 GPUs have 32GB memory
-GPU_MEMORY_MB = 30_000
 
 
 model_kinds = [""torch"", ""onnx"", ""trt_fp32"", ""trt_fp16"", ""trt_int8""]"
OK;29;jaideepheer;DLOps-Project;f40972559f44bc4651c7c2a266cbab4b26bdf2f1;update convert memory;" MAX_BATCH = 128
 MIN_BATCH = 1
 # DGX-2 GPUs have 32GB memory
+GPU_MEMORY_MB = 20_000
 
 
 model_kinds = [""torch"", ""onnx"", ""trt_fp32"", ""trt_fp16"", ""trt_int8""]"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def values(self):
 operators = Stack()
 operands = Stack()
 types = Stack()
+arrMatOperands = Stack()
 addresses = {
     ""gInt"": 0,
     ""gFloat"": 1000,"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
-# Avance: Se arreglaron prioridades, ejecucion de maquina virtual para estatutos lineales. 
\ No newline at end of file
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" # Proyecto Compiladores
 Ivan Anguiano A00817460
  Proyecto Compiladores FJ22
\ No newline at end of file
+# Avance: Ejecucion de estatutos condicionales y generacion de codigo de arreglos/tipos estructurados (falta que haga operaciones con arreglos)
\ No newline at end of file"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"-from cuadruplos import Quadruples
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
@@ -50,16 +50,15 @@ def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
-    #Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
-        # quad.print()
         newIndex = executeInstruction(quad)
-        if quad.operator != ""+ADD"":
-            if newIndex:
-                index = newIndex
-            else:
-                index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
@@ -106,6 +105,12 @@ def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
@@ -210,32 +215,8 @@ def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
-        if lOp == 12:
-            localMem.insertInt(getValueFromAddress(getValueFromAddress(quad.left_operand)), getValueFromAddress(quad.result))
-        elif lOp == 11:
-            localMem.insertChar(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 10:
-            localMem.insertFloat(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 9:
-            localMem.insertInt(cstMemMap[quad.left_operand], getValueFromAddress(quad.result))
-        elif lOp == 8:
-            tempMem.insertChar(tempMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 7:
-            tempMem.insertFloat(tempMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 6:
-            tempMem.insertInt(tempMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 5:
-            tempMem.insertChar(localMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 4:
-            tempMem.insertFloat(localMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 3:
-            tempMem.insertInt(localMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 2:
-            tempMem.insertChar(globalMem.getChar(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 1:
-            tempMem.insertFloat(globalMem.getFloat(quad.left_operand), getValueFromAddress(quad.result))
-        elif lOp == 0:
-            tempMem.insertInt(globalMem.getInt(quad.left_operand), getValueFromAddress(quad.result))
         
 def add(quad):
     res_address = quad.result // 1000
@@ -255,7 +236,7 @@ def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
-        pointerMemStack.append(lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
@@ -513,4 +494,13 @@ def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
-        localMem.adjustCharArrSize(quad.result)
\ No newline at end of file
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"+from cuadruplos import Quadruples, Quadruple
 from memoria import Memory
 from EstructuraDatos import variableTable
 from errores import *
@@ -50,16 +50,15 @@ def executeQuads():
             cstMemMap[variableTable[""constants""][cst][""address""]] = cst
     index = 0
     print(cstMemMap)
+    Quadruples.print_all()
     while len(Quadruples.quadruples) > index:    
         quad = Quadruples.quadruples[index]
+        #quad.print()
         newIndex = executeInstruction(quad)
+        if newIndex:
+            index = newIndex
+        else:
+            index += 1                    
 
 def executeInstruction(quad):
     if quad.operator == ""="":
@@ -106,6 +105,12 @@ def executeInstruction(quad):
         return rtn(quad)
     elif quad.operator == ""VERIFY"":
         return verify(quad)
+    elif quad.operator == ""ARR="":
+        return arrAssign(quad)
+    elif quad.operator == ""ARR+"":
+        return arrAdd(quad)
+    elif quad.operator == ""ARR-"":
+        return arrSubtract(quad)
 
 def assign(quad):
     add_type = quad.result // 1000
@@ -210,32 +215,8 @@ def assign(quad):
         elif lOp == 2:
             tempMem.insertChar(globalMem.getInt(quad.left_operand), quad.result)
     if add_type == 12:
+        add_type = getValueFromAddress(quad.result)
+        assign(Quadruple(quad.operator, quad.left_operand, ""_"", add_type))
         
 def add(quad):
     res_address = quad.result // 1000
@@ -255,7 +236,7 @@ def add(quad):
         tempMem.insertFloat(result, quad.result)
     # Address addition for array and matrix (base address + access index)
     elif res_address == 12:
+        pointerMemStack.insert(quad.result % 1000, lOp + rOp)
 
 def subtract(quad):
     res_address = quad.result // 1000
@@ -513,4 +494,13 @@ def verify(quad):
     elif arrType == 4:
         localMem.adjustFloatArrSize(quad.result)
     elif arrType == 5:
\ No newline at end of file
+        localMem.adjustCharArrSize(quad.result)
+
+def arrAssign(quad):
+    pass
+
+def arrAdd(quad):
+    pass
+
+def arrSubtract(quad):
+    pass 
\ No newline at end of file"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.ints) <= realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.floats) <= realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
-        while len(self.chars) <= realSup:
             self.chars.append("""")"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def printChars(self):
 
     def adjustIntArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.ints) < realSup:
             self.ints.append(0)
 
     def adjustFloatArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.floats) < realSup:
             self.floats.append(0.0)
 
     def adjustCharArrSize(self, supLim):
         realSup = supLim % 1000
+        while len(self.chars) < realSup:
             self.chars.append("""")"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
@@ -76,7 +77,13 @@ def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
-	if t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
@@ -102,6 +109,7 @@ def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
@@ -218,17 +226,23 @@ def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
-	if t[1] in variableTable[currentScope]:
-		address = variableTable[currentScope][t[1]][""address""]
-		temp_quad = Quadruple(""="", cstAddress, '_', address)
-		Quadruples.push_quad(temp_quad)
-	#Checar si el id existe en global scope y asignar su valor
-	elif t[1] in variableTable[""global""]:
-		address = variableTable[""global""][t[1]][""address""]
-		temp_quad = Quadruple(""="", t[3], '_', address)
-		Quadruples.push_quad(temp_quad)
 	else:
-		Error.undefined_variable(t[1], t.lexer.lineno)
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
@@ -613,6 +627,65 @@ def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
@@ -625,6 +698,12 @@ def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
@@ -766,6 +845,9 @@ def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
@@ -831,6 +913,9 @@ def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
@@ -874,10 +959,12 @@ def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
 
 def p_addTypeId(t):
 	'addTypeId : '
-	# Push types to types stack
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
@@ -890,6 +977,7 @@ def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"def p_program(t):
 	#operators.print()
 	#Quadruples.print_all()
 	#variableTable.clear()
+	# arrMatOperands.print()
 
 #GlobalTable: Inicializar programa y crear variableTable
 def p_globalTable(t):
@@ -76,7 +77,13 @@ def p_programFunc(t):
 def p_assignment(t):
 	'assignment : ID dimArray EQUAL Expression2 SEMICOLON'
 	#Si id esta en currentScope, generar cuadruplo y asignar su valor en varTable
+	if arrMatOperands.size() > 0:
+		types.pop()
+		assign = arrMatOperands.pop()
+		address = arrMatOperands.pop()
+		temp_quad = Quadruple(""ARR="", assign, ""_"", address)
+		Quadruples.push_quad(temp_quad)
+	elif t[1] in variableTable[currentScope]:
 		if types.pop() == variableTable[currentScope][t[1]][""type""]:
 			if ""rows"" in variableTable[currentScope][t[1]]:
 				types.pop()
@@ -102,6 +109,7 @@ def p_assignment(t):
 				address = variableTable[""global""][t[1]][""address""]
 				temp_quad = Quadruple(""="", operands.pop(), '_', address)
 				operands.pop()
+			Quadruples.push_quad(temp_quad)
 		else:
 			Error.type_mismatch(t[1],t.lexer.lineno - 1)
 	else:
@@ -218,17 +226,23 @@ def p_forAssignment(t):
 	else:
 		cstAddress = variableTable[""constants""][t[3]][""address""]
 	#Checar si el id existe en currentScope y asignar su valor
+	if ""rows"" not in variableTable[currentScope][t[1]]:
+		#Checar si el id existe en currentScope y asignar su valor
+		if t[1] in variableTable[currentScope]:
+			address = variableTable[currentScope][t[1]][""address""]
+			temp_quad = Quadruple(""="", cstAddress, '_', address)
+			Quadruples.push_quad(temp_quad)
+		#Checar si el id existe en global scope y asignar su valor
+		elif t[1] in variableTable[""global""]:
+			address = variableTable[""global""][t[1]][""address""]
+			temp_quad = Quadruple(""="", t[3], '_', address)
+			Quadruples.push_quad(temp_quad)
+		else:
+			Error.undefined_variable(t[1], t.lexer.lineno)
 	else:
+		print(""Error: invalid assignment to non-atomic variable in line %d."" % (t.lexer.lineno))
+		exit(0)
+		# Actualizar con clase error
 
 
 #pushLoop: Push al id del cuadruplo al stack de ""saltos""
@@ -613,6 +627,65 @@ def p_evaluateTerm(t):
 			lType = types.pop()
 			#Checar cubo semantico con tipos y operador
 			resType = semanticCube[(lType, rType, oper)]
+			# Checar y validar operandos y tamanos del arreglo.
+			if arrMatOperands.size() > 1:
+				rId = arrMatOperands.pop()
+				lId = arrMatOperands.pop()
+				# rDimRow = 0
+				# rDimCol = 0
+				# lDimRow = 0
+				# lDimCol = 0
+				# if rOp >= 0 and rOp < 3000:
+				# 	rOpAdd = variableTable[""global""][rId][""address""]
+				# 	if ""rows"" in variableTable[""global""][rId]:
+				# 		rDimRow = variableTable[""global""][rId][""rows""]
+				# 	if ""cols"" in variableTable[""global""][rId]:
+				# 		rDimCol = variableTable[""global""][rId][""cols""]
+				# elif rOp >= 3000 and rOp < 6000:
+				# 	rOpAdd = variableTable[currentScope][rId][""address""]
+				# 	if ""rows"" in variableTable[currentScope][rId]:
+				# 		rDimRow = variableTable[currentScope][rId][""rows""]
+				# 	if ""cols"" in variableTable[currentScope][rId]:
+				# 		rDimCol = variableTable[currentScope][rId][""cols""]
+				# if lOp >= 0 and lOp < 3000:
+				# 	lOpAdd = variableTable[""global""][lId][""address""]
+				# 	if ""rows"" in variableTable[""global""][lId]:
+				# 		lDimRow = variableTable[""global""][lId][""rows""]
+				# 	if ""cols"" in variableTable[""global""][lId]:
+				# 		lDimCol = variableTable[""global""][lId][""cols""]
+				# elif lOp >= 3000 and lOp < 6000:
+				# 	lOpAdd = variableTable[currentScope][lId][""address""]
+				# 	if ""rows"" in variableTable[currentScope][lId]:
+				# 		lDimRow = variableTable[currentScope][lId][""rows""]
+				# 	if ""cols"" in variableTable[currentScope][lId]:
+				# 		lDimCol = variableTable[currentScope][lId][""cols""]
+				# Validate equal dimensions
+				if ""cols"" not in lId and ""cols"" not in rId:
+					lId[""cols""] = 0
+					rId[""cols""] = 0
+				if lId[""rows""] == rId[""rows""] and lId[""cols""] == rId[""cols""]:
+					if oper == ""+"":
+						oper = ""ARR+""
+					else:
+						oper = ""ARR-""
+					lOp = {
+						""address"": lId[""address""],
+						""rows"": lId[""rows""],
+						""cols"": lId[""cols""]
+					}
+					rOp = {
+						""address"": rId[""address""],
+						""rows"": rId[""rows""],
+						""cols"": rId[""cols""]
+					}
+				else:
+					print(""Error: operation between variables with dimensions that don't match in line %d."" % (t.lexer.lineno))
+					exit(0)
+					# Error class call
+			elif arrMatOperands.size() == 1:
+				print(""Error: invalid operation in line %d."" % (t.lexer.lineno))
+				exit(0)
+				# Error class call
 			# Checar tipo de resultado y evaluar expresion
 			if resType != ""error"":
 				address_type = ""t""
@@ -625,6 +698,12 @@ def p_evaluateTerm(t):
 				temp_quad = Quadruple(oper, lOp, rOp, addresses[address_type])
 				Quadruples.push_quad(temp_quad)
 				operands.push(addresses[address_type])
+				if oper == ""ARR+"" or oper == ""ARR-"":
+					arrMatOperands.push({
+						""address"": addresses[address_type],
+						""rows"": lOp[""rows""],
+						""cols"": lOp[""cols""]
+					})
 				addresses[address_type] += 1
 				types.push(resType)
 			else:
@@ -766,6 +845,9 @@ def p_addPrintString(t):
 def p_addPrint(t):
 	'addPrint : '
 	# Generar cuadruplo print
+	if arrMatOperands.size() > 0:
+		print(""Error: print invalido en variable de array en la linea  %d."" % (t.lexer.lineno))
+		exit(0)
 	temp_quad = Quadruple(""print"", '_', '_', operands.pop())
 	Quadruples.push_quad(temp_quad)
 	types.pop()
@@ -831,6 +913,9 @@ def p_generateParam(t):
 	'generateParam : '
 	global funcName
 	global paramNum
+	if arrMatOperands.size() > 0:
+		print(""Error: array parameter in module call in in line %d."" % (t.lexer.lineno))
+		exit(0)
 	arg = operands.pop()
 	argType = types.pop()
 	paramList = functionDir[funcName][""params""].values()
@@ -874,10 +959,12 @@ def p_addOperandId(t):
 		arrMatScope.push(""global"")
 	else:
 		Error.undefined_variable(arrMatId.peek(), t.lexer.lineno)
+	if ""rows"" in variableTable[arrMatScope.peek()][t[-1]]:
+		arrMatOperands.push(variableTable[arrMatScope.peek()][t[-1]])
 
 def p_addTypeId(t):
 	'addTypeId : '
+	# Push a tipos a la pila de tipos
 	if arrMatId.peek() in variableTable[currentScope]:
 		types.push(variableTable[currentScope][arrMatId.peek()][""type""])
 	elif arrMatId.peek() in variableTable[""global""]:
@@ -890,6 +977,7 @@ def p_readIDType(t):
 	global arrMatId
 	operands.pop()
 	operators.push(""Mat"")
+	arrMatOperands.pop()
 	#TODO GLOBAL
 	if types.pop() != variableTable[currentScope][arrMatId.peek()][""type""]:
 		Error.type_mismatch(arrMatId.peek(), t.lexer.lineno)"
KO;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;" 
-program myprog;
-
-function int uno(int c, int d) {
     var char x, y;
     return(999);
 }
 
-function int dos(int a, int b) {
     var char x, y;
     return(1000);
 }
@@ -17,9 +16,9 @@ main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
-    j[2][1] = dos(2,2);
     print(j[2][z[0]]);
-    print(dos(1,2) + uno(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;90b4a46bbbfe16c1b78a276c05324e413e6de738;Started special operators, memory fix and adjustment;"+program myprogram;
 
+function int test1(int c, int d) {
     var char x, y;
     return(999);
 }
 
+function int test2(int a, int b) {
     var char x, y;
     return(1000);
 }
@@ -17,9 +16,9 @@ main() {
     z[1+2] = 2;
     z[0] = 1;
     z[1] = 3;
+    j[2][1] = test2(2,2);
     print(j[2][z[0]]);
+    print(test2(1,2) + test1(1,2) * z[1] * j[2][z[0]]);
     print(j[2][z[0]] + j[z[3]][z[0]] * z[1] / 2);
     print(""END OF MAIN"");
 }
\ No newline at end of file"
KO;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
-        # localMem.printInts()
-        # print(getValueFromAddress(quad.left_operand))
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
@@ -389,14 +388,12 @@ def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
-    # localMem.printInts()
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
-    global localMemStack
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
@@ -415,28 +412,29 @@ def gotofor(quad):
     return quad.result
 
 def gosub(quad):
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
-    global localMem
     localMemStack.append(localMem)
     currentFunctionStack.append(quad.left_operand)
-    localMem = Memory()
 
 def param(quad):
-    global localMem
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
-        localMem.insertInt(lOp, quad.result)
     if address == 4:
-        localMem.insertFloat(lOp, quad.result)
     if address == 5:
-        localMem.insertChar(lOp, quad.result)
 
 def rtn(quad):
-    global tempMem
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
@@ -449,9 +447,13 @@ def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
 
 def verify(quad):
-    global tempMem
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
OK;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;"def assign(quad):
         elif lOp == 2:
             localMem.insertChar(globalMem.getChar(quad.left_operand), quad.result)
     if add_type == 6:
+
         if lOp != 12:
             tempMem.insertInt(getValueFromAddress(quad.left_operand), quad.result)
         if lOp == 12:
@@ -389,14 +388,12 @@ def read(quad):
             localMem.insertChar(input_val, quad.result)
     
 def printScreen(quad):
     if quad.result >= 12000:
         print(getValueFromAddress(getValueFromAddress(quad.result)))
     else:
         print(getValueFromAddress(quad.result))
 
 def endFunc(quad):
     global localMem
     currentFunctionStack.pop()
     localMem = localMemStack.pop()
@@ -415,28 +412,29 @@ def gotofor(quad):
     return quad.result
 
 def gosub(quad):
+    global newMem
+    global localMem
+    localMem = newMem
     functionReturnStack.append(quad.id + 1)
     return quad.result
 
 def era(quad):
     localMemStack.append(localMem)
+    global newMem
+    newMem = Memory()
     currentFunctionStack.append(quad.left_operand)
 
 def param(quad):
     address = quad.result // 1000
     lOp = getValueFromAddress(quad.left_operand)
     if address == 3:
+        newMem.insertInt(lOp, quad.result)
     if address == 4:
+        newMem.insertFloat(lOp, quad.result)
     if address == 5:
+        newMem.insertChar(lOp, quad.result)    
 
 def rtn(quad):
     address = quad.result // 1000
     rtn_address = Quadruples.quadruples[functionReturnStack[len(functionReturnStack) - 1]].result
     rtnVal = getValueFromAddress(quad.result)
@@ -449,9 +447,13 @@ def rtn(quad):
     else:
         tempMem.insertChar(rtnVal, rtn_address)
         globalMem.insertChar(rtnVal, currentFunctionStack[len(currentFunctionStack) - 1])
+    newIndex = quad.id + 1
+    if Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
+        while Quadruples.quadruples[newIndex].operator != ""ENDFUNC"":
+            newIndex += 1
+        return newIndex
 
 def verify(quad):
     arrType = quad.result // 1000
     check = getValueFromAddress(quad.left_operand)
     # print(check)"
KO;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
-
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
@@ -822,6 +822,7 @@ def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
 	operators.pop()
 	types.pop()
 "
OK;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" from maquinavirtual import executeQuads
 
 tokens = lexer.tokens
+arrMatId = Stack()
 
 def p_program(t):
 	'program : PROGRAM ID globalTable SEMICOLON declaration programFunc main'
@@ -822,6 +822,7 @@ def p_generateGosub(t):
 		tmp_quad = Quadruple(""="", variableTable[""global""][funcName][""address""], ""_"", tmpAddress)
 		Quadruples.push_quad(tmp_quad)
 		operands.push(tmpAddress)
+		types.push(variableTable[""global""][funcName][""type""])
 	operators.pop()
 	types.pop()
 "
KO;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
-var int i[1][1], j[1], k;
 
 function int uno(int c, int d) {
     var char x, y;
-    x = ""a"";
-    y = ""b"";
-    return(8);
 }
 
 function int dos(int a, int b) {
     var char x, y;
-    x = ""g"";
-    y = ""h"";
 }
 
 main() {
-    var int c;
         float k;
-    c = 4;
-    k = 1 + 2 - (3 * 4) / c;
-    if (1 > 2) then {
-        read(c);
-    } else {
-        read(k);
-    }
-    while (1 < 3) {
-        read(k);
-    }
-    for c = 1 to c < 10 {
-        read(j);
-    }
-    c = 2 < 1;
-    c = 1 | 0;
-    read(i);
-    print(i);
-    uno(1, 2);
-    dos(3, 4);
 }
\ No newline at end of file"
OK;35;iaal96;PCompis-2022;8175139ac47903cff6eff230f11bca20f9c5c26e;module and memory fix, updated test file;" program myprog;
 
 function int uno(int c, int d) {
     var char x, y;
+    return(999);
 }
 
 function int dos(int a, int b) {
     var char x, y;
+    return(1000);
 }
 
 main() {
+    var int c, z[5], j[3][3];
         float k;
+    z[1+2] = 2;
+    z[0] = 1;
+    j[2][1] = 3;
+    print(dos(1,2) + uno(1,2) * j[2][1] * j[2][1] / z[3]);
+    print(j[z[3]][z[0]]);
+    print(""END OF MAIN"");
 }
\ No newline at end of file"
